
fault.o:     file format elf64-x86-64


Disassembly of section .text:

0000000000000000 <ftrace_raw_event_x86_exceptions>:
#include <linux/tracepoint.h>

extern void trace_irq_vector_regfunc(void);
extern void trace_irq_vector_unregfunc(void);

DECLARE_EVENT_CLASS(x86_exceptions,
   0:	55                   	push   %rbp
   1:	48 89 e5             	mov    %rsp,%rbp
   4:	41 57                	push   %r15
   6:	41 56                	push   %r14
   8:	41 55                	push   %r13
   a:	41 54                	push   %r12
   c:	53                   	push   %rbx
   d:	48 89 fb             	mov    %rdi,%rbx
  10:	48 83 ec 18          	sub    $0x18,%rsp
}

static __always_inline int constant_test_bit(long nr, const volatile unsigned long *addr)
{
	return ((1UL << (nr & (BITS_PER_LONG-1))) &
		(addr[nr >> _BITOPS_LONG_SHIFT])) != 0;
  14:	48 8b 43 38          	mov    0x38(%rbx),%rax
  18:	48 8b 7f 10          	mov    0x10(%rdi),%rdi
  1c:	a8 20                	test   $0x20,%al
  1e:	74 10                	je     30 <ftrace_raw_event_x86_exceptions+0x30>
  20:	48 83 c4 18          	add    $0x18,%rsp
  24:	5b                   	pop    %rbx
  25:	41 5c                	pop    %r12
  27:	41 5d                	pop    %r13
  29:	41 5e                	pop    %r14
  2b:	41 5f                	pop    %r15
  2d:	5d                   	pop    %rbp
  2e:	c3                   	retq   
  2f:	90                   	nop
  30:	49 89 f7             	mov    %rsi,%r15
  33:	49 89 d5             	mov    %rdx,%r13
  36:	49 89 cc             	mov    %rcx,%r12
#define __PV_IS_CALLEE_SAVE(func)			\
	((struct paravirt_callee_save) { func })

static inline notrace unsigned long arch_local_save_flags(void)
{
	return PVOP_CALLEE0(unsigned long, pv_irq_ops.save_fl);
  39:	ff 14 25 00 00 00 00 	callq  *0x0
  40:	8b 57 40             	mov    0x40(%rdi),%edx
  43:	48 8d 7d d0          	lea    -0x30(%rbp),%rdi
  47:	49 89 c0             	mov    %rax,%r8
 * We mask the PREEMPT_NEED_RESCHED bit so as not to confuse all current users
 * that think a non-zero value indicates we cannot preempt.
 */
static __always_inline int preempt_count(void)
{
	return __this_cpu_read_4(__preempt_count) & ~PREEMPT_NEED_RESCHED;
  4a:	65 44 8b 34 25 00 00 	mov    %gs:0x0,%r14d
  51:	00 00 
  53:	41 81 e6 ff ff ff 7f 	and    $0x7fffffff,%r14d
  5a:	b9 20 00 00 00       	mov    $0x20,%ecx
  5f:	48 89 de             	mov    %rbx,%rsi
  62:	45 89 f1             	mov    %r14d,%r9d
  65:	48 89 45 c0          	mov    %rax,-0x40(%rbp)
  69:	e8 00 00 00 00       	callq  6e <ftrace_raw_event_x86_exceptions+0x6e>
  6e:	48 85 c0             	test   %rax,%rax
  71:	74 ad                	je     20 <ftrace_raw_event_x86_exceptions+0x20>
  73:	48 89 c7             	mov    %rax,%rdi
  76:	48 89 45 c8          	mov    %rax,-0x38(%rbp)
  7a:	e8 00 00 00 00       	callq  7f <ftrace_raw_event_x86_exceptions+0x7f>
  7f:	4c 89 78 08          	mov    %r15,0x8(%rax)
  83:	49 8b 95 80 00 00 00 	mov    0x80(%r13),%rdx
  8a:	48 89 c6             	mov    %rax,%rsi
  8d:	4c 8b 45 c8          	mov    -0x38(%rbp),%r8
  91:	4c 89 60 18          	mov    %r12,0x18(%rax)
  95:	48 89 df             	mov    %rbx,%rdi
  98:	48 89 50 10          	mov    %rdx,0x10(%rax)
  9c:	48 8b 55 d0          	mov    -0x30(%rbp),%rdx
  a0:	4c 89 c1             	mov    %r8,%rcx
  a3:	e8 00 00 00 00       	callq  a8 <ftrace_raw_event_x86_exceptions+0xa8>
  a8:	85 c0                	test   %eax,%eax
  aa:	4c 8b 45 c8          	mov    -0x38(%rbp),%r8
  ae:	0f 85 6c ff ff ff    	jne    20 <ftrace_raw_event_x86_exceptions+0x20>
  b4:	48 8b 55 c0          	mov    -0x40(%rbp),%rdx
  b8:	48 8b 7d d0          	mov    -0x30(%rbp),%rdi
  bc:	44 89 f1             	mov    %r14d,%ecx
  bf:	4c 89 c6             	mov    %r8,%rsi
  c2:	e8 00 00 00 00       	callq  c7 <ftrace_raw_event_x86_exceptions+0xc7>
  c7:	e9 54 ff ff ff       	jmpq   20 <ftrace_raw_event_x86_exceptions+0x20>
  cc:	0f 1f 40 00          	nopl   0x0(%rax)

00000000000000d0 <ftrace_raw_output_x86_exceptions>:
  d0:	55                   	push   %rbp
  d1:	48 89 d6             	mov    %rdx,%rsi
  d4:	48 89 e5             	mov    %rsp,%rbp
  d7:	41 54                	push   %r12
  d9:	53                   	push   %rbx
  da:	48 89 fb             	mov    %rdi,%rbx
  dd:	4c 8b a7 b8 20 00 00 	mov    0x20b8(%rdi),%r12
  e4:	e8 00 00 00 00       	callq  e9 <ftrace_raw_output_x86_exceptions+0x19>
  e9:	85 c0                	test   %eax,%eax
  eb:	75 2c                	jne    119 <ftrace_raw_output_x86_exceptions+0x49>
  ed:	49 8b 4c 24 10       	mov    0x10(%r12),%rcx
  f2:	49 8b 54 24 08       	mov    0x8(%r12),%rdx
  f7:	48 8d bb ac 10 00 00 	lea    0x10ac(%rbx),%rdi
  fe:	4d 8b 44 24 18       	mov    0x18(%r12),%r8
 103:	31 c0                	xor    %eax,%eax
 105:	48 c7 c6 00 00 00 00 	mov    $0x0,%rsi
 10c:	e8 00 00 00 00       	callq  111 <ftrace_raw_output_x86_exceptions+0x41>
 111:	85 c0                	test   %eax,%eax
 113:	0f 95 c0             	setne  %al
 116:	0f b6 c0             	movzbl %al,%eax
 119:	5b                   	pop    %rbx
 11a:	41 5c                	pop    %r12
 11c:	5d                   	pop    %rbp
 11d:	c3                   	retq   
 11e:	66 90                	xchg   %ax,%ax

0000000000000120 <perf_trace_x86_exceptions>:
 120:	55                   	push   %rbp
 121:	49 89 f9             	mov    %rdi,%r9
 124:	48 89 e5             	mov    %rsp,%rbp
 127:	41 57                	push   %r15
 129:	41 56                	push   %r14
 12b:	41 55                	push   %r13
 12d:	41 54                	push   %r12
 12f:	53                   	push   %rbx
 130:	48 81 ec c0 00 00 00 	sub    $0xc0,%rsp
 137:	48 8b 9f 80 00 00 00 	mov    0x80(%rdi),%rbx
 13e:	65 48 03 1c 25 00 00 	add    %gs:0x0,%rbx
 145:	00 00 
 147:	48 83 3b 00          	cmpq   $0x0,(%rbx)
 14b:	0f 84 a3 00 00 00    	je     1f4 <perf_trace_x86_exceptions+0xd4>
 * - bp for callchains
 * - eflags, for future purposes, just in case
 */
static inline void perf_fetch_caller_regs(struct pt_regs *regs)
{
	memset(regs, 0, sizeof(*regs));
 151:	4c 8d a5 30 ff ff ff 	lea    -0xd0(%rbp),%r12
 158:	31 c0                	xor    %eax,%eax
 15a:	49 89 cd             	mov    %rcx,%r13
 15d:	b9 15 00 00 00       	mov    $0x15,%ecx
 162:	49 89 f7             	mov    %rsi,%r15
 165:	49 89 d6             	mov    %rdx,%r14
 168:	4c 89 e7             	mov    %r12,%rdi
 16b:	f3 48 ab             	rep stos %rax,%es:(%rdi)

	perf_arch_fetch_caller_regs(regs, CALLER_ADDR0);
 16e:	48 8b 45 08          	mov    0x8(%rbp),%rax
 172:	48 89 45 b0          	mov    %rax,-0x50(%rbp)

static inline unsigned long caller_frame_pointer(void)
{
	struct stack_frame *frame;

	get_bp(frame);
 176:	48 89 e8             	mov    %rbp,%rax
 179:	48 8b 00             	mov    (%rax),%rax
 17c:	48 c7 45 b8 10 00 00 	movq   $0x10,-0x48(%rbp)
 183:	00 
 184:	48 c7 45 c0 00 00 00 	movq   $0x0,-0x40(%rbp)
 18b:	00 
 18c:	48 89 85 50 ff ff ff 	mov    %rax,-0xb0(%rbp)
 193:	48 89 65 c8          	mov    %rsp,-0x38(%rbp)
 197:	41 0f b7 71 40       	movzwl 0x40(%r9),%esi
 19c:	48 8d 8d 2c ff ff ff 	lea    -0xd4(%rbp),%rcx
 1a3:	4c 89 e2             	mov    %r12,%rdx
 1a6:	bf 24 00 00 00       	mov    $0x24,%edi
 1ab:	e8 00 00 00 00       	callq  1b0 <perf_trace_x86_exceptions+0x90>
 1b0:	48 85 c0             	test   %rax,%rax
 1b3:	74 3f                	je     1f4 <perf_trace_x86_exceptions+0xd4>
 1b5:	4c 89 78 08          	mov    %r15,0x8(%rax)
 1b9:	49 8b 96 80 00 00 00 	mov    0x80(%r14),%rdx
static inline void
perf_trace_buf_submit(void *raw_data, int size, int rctx, u64 addr,
		       u64 count, struct pt_regs *regs, void *head,
		       struct task_struct *task)
{
	perf_tp_event(addr, count, raw_data, size, regs, head, rctx, task);
 1c0:	49 89 d9             	mov    %rbx,%r9
 1c3:	4c 89 68 18          	mov    %r13,0x18(%rax)
 1c7:	4d 89 e0             	mov    %r12,%r8
 1ca:	b9 24 00 00 00       	mov    $0x24,%ecx
 1cf:	be 01 00 00 00       	mov    $0x1,%esi
 1d4:	31 ff                	xor    %edi,%edi
 1d6:	48 89 50 10          	mov    %rdx,0x10(%rax)
 1da:	8b 95 2c ff ff ff    	mov    -0xd4(%rbp),%edx
 1e0:	48 c7 44 24 08 00 00 	movq   $0x0,0x8(%rsp)
 1e7:	00 00 
 1e9:	89 14 24             	mov    %edx,(%rsp)
 1ec:	48 89 c2             	mov    %rax,%rdx
 1ef:	e8 00 00 00 00       	callq  1f4 <perf_trace_x86_exceptions+0xd4>
 1f4:	48 81 c4 c0 00 00 00 	add    $0xc0,%rsp
 1fb:	5b                   	pop    %rbx
 1fc:	41 5c                	pop    %r12
 1fe:	41 5d                	pop    %r13
 200:	41 5e                	pop    %r14
 202:	41 5f                	pop    %r15
 204:	5d                   	pop    %rbp
 205:	c3                   	retq   
 206:	66 2e 0f 1f 84 00 00 	nopw   %cs:0x0(%rax,%rax,1)
 20d:	00 00 00 

0000000000000210 <vmalloc_sync_all>:
}

#else /* CONFIG_X86_64: */

void vmalloc_sync_all(void)
{
 210:	e8 00 00 00 00       	callq  215 <vmalloc_sync_all+0x5>
 215:	55                   	push   %rbp
	sync_global_pgds(VMALLOC_START & PGDIR_MASK, VMALLOC_END);
 216:	48 be ff ff ff ff ff 	movabs $0xffffe8ffffffffff,%rsi
 21d:	e8 ff ff 
 220:	48 bf 00 00 00 00 00 	movabs $0xffffc90000000000,%rdi
 227:	c9 ff ff 
}

#else /* CONFIG_X86_64: */

void vmalloc_sync_all(void)
{
 22a:	48 89 e5             	mov    %rsp,%rbp
	sync_global_pgds(VMALLOC_START & PGDIR_MASK, VMALLOC_END);
 22d:	e8 00 00 00 00       	callq  232 <vmalloc_sync_all+0x22>
}
 232:	5d                   	pop    %rbp
 233:	c3                   	retq   

Disassembly of section .text.unlikely:

0000000000000000 <pte_offset_kernel>:
{
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
}

static inline pte_t *pte_offset_kernel(pmd_t *pmd, unsigned long address)
{
   0:	55                   	push   %rbp

	if (sizeof(pmdval_t) > sizeof(long))
		ret =  PVOP_CALLEE2(pmdval_t, pv_mmu_ops.pmd_val,
				    pmd.pmd, (u64)pmd.pmd >> 32);
	else
		ret =  PVOP_CALLEE1(pmdval_t, pv_mmu_ops.pmd_val,
   1:	48 8b 3f             	mov    (%rdi),%rdi
   4:	48 89 e5             	mov    %rsp,%rbp
   7:	ff 14 25 00 00 00 00 	callq  *0x0
 * this function returns the index of the entry in the pte page which would
 * control the given virtual address
 */
static inline unsigned long pte_index(unsigned long address)
{
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
   e:	48 c1 ee 09          	shr    $0x9,%rsi
	return (unsigned long)native_pmd_val(pmd) == 0;
}

static inline unsigned long pmd_page_vaddr(pmd_t pmd)
{
	return (unsigned long)__va(pmd_val(pmd) & PTE_PFN_MASK);
  12:	48 ba 00 00 00 00 00 	movabs $0xffff880000000000,%rdx
  19:	88 ff ff 
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
}

static inline pte_t *pte_offset_kernel(pmd_t *pmd, unsigned long address)
{
	return (pte_t *)pmd_page_vaddr(*pmd) + pte_index(address);
  1c:	81 e6 f8 0f 00 00    	and    $0xff8,%esi
	return (unsigned long)native_pmd_val(pmd) == 0;
}

static inline unsigned long pmd_page_vaddr(pmd_t pmd)
{
	return (unsigned long)__va(pmd_val(pmd) & PTE_PFN_MASK);
  22:	48 01 d6             	add    %rdx,%rsi
  25:	48 ba 00 f0 ff ff ff 	movabs $0x3ffffffff000,%rdx
  2c:	3f 00 00 
  2f:	48 21 d0             	and    %rdx,%rax
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
}

static inline pte_t *pte_offset_kernel(pmd_t *pmd, unsigned long address)
{
	return (pte_t *)pmd_page_vaddr(*pmd) + pte_index(address);
  32:	48 01 f0             	add    %rsi,%rax
}
  35:	5d                   	pop    %rbp
  36:	c3                   	retq   

0000000000000037 <pmd_offset>:
 */
#define pud_page(pud)		pfn_to_page(pud_val(pud) >> PAGE_SHIFT)

/* Find an entry in the second-level page table.. */
static inline pmd_t *pmd_offset(pud_t *pud, unsigned long address)
{
  37:	55                   	push   %rbp

	if (sizeof(pudval_t) > sizeof(long))
		ret =  PVOP_CALLEE2(pudval_t, pv_mmu_ops.pud_val,
				    pud.pud, (u64)pud.pud >> 32);
	else
		ret =  PVOP_CALLEE1(pudval_t, pv_mmu_ops.pud_val,
  38:	48 8b 3f             	mov    (%rdi),%rdi
  3b:	48 89 e5             	mov    %rsp,%rbp
  3e:	ff 14 25 00 00 00 00 	callq  *0x0
 * this macro returns the index of the entry in the pmd page which would
 * control the given virtual address
 */
static inline unsigned long pmd_index(unsigned long address)
{
	return (address >> PMD_SHIFT) & (PTRS_PER_PMD - 1);
  45:	48 c1 ee 12          	shr    $0x12,%rsi
	return pud_flags(pud) & _PAGE_PRESENT;
}

static inline unsigned long pud_page_vaddr(pud_t pud)
{
	return (unsigned long)__va((unsigned long)pud_val(pud) & PTE_PFN_MASK);
  49:	48 ba 00 00 00 00 00 	movabs $0xffff880000000000,%rdx
  50:	88 ff ff 
#define pud_page(pud)		pfn_to_page(pud_val(pud) >> PAGE_SHIFT)

/* Find an entry in the second-level page table.. */
static inline pmd_t *pmd_offset(pud_t *pud, unsigned long address)
{
	return (pmd_t *)pud_page_vaddr(*pud) + pmd_index(address);
  53:	81 e6 f8 0f 00 00    	and    $0xff8,%esi
	return pud_flags(pud) & _PAGE_PRESENT;
}

static inline unsigned long pud_page_vaddr(pud_t pud)
{
	return (unsigned long)__va((unsigned long)pud_val(pud) & PTE_PFN_MASK);
  59:	48 01 d6             	add    %rdx,%rsi
  5c:	48 ba 00 f0 ff ff ff 	movabs $0x3ffffffff000,%rdx
  63:	3f 00 00 
  66:	48 21 d0             	and    %rdx,%rax
#define pud_page(pud)		pfn_to_page(pud_val(pud) >> PAGE_SHIFT)

/* Find an entry in the second-level page table.. */
static inline pmd_t *pmd_offset(pud_t *pud, unsigned long address)
{
	return (pmd_t *)pud_page_vaddr(*pud) + pmd_index(address);
  69:	48 01 f0             	add    %rsi,%rax
}
  6c:	5d                   	pop    %rbp
  6d:	c3                   	retq   

000000000000006e <pud_offset>:
{
	return (address >> PUD_SHIFT) & (PTRS_PER_PUD - 1);
}

static inline pud_t *pud_offset(pgd_t *pgd, unsigned long address)
{
  6e:	55                   	push   %rbp

	if (sizeof(pgdval_t) > sizeof(long))
		ret =  PVOP_CALLEE2(pgdval_t, pv_mmu_ops.pgd_val,
				    pgd.pgd, (u64)pgd.pgd >> 32);
	else
		ret =  PVOP_CALLEE1(pgdval_t, pv_mmu_ops.pgd_val,
  6f:	48 8b 3f             	mov    (%rdi),%rdi
  72:	48 89 e5             	mov    %rsp,%rbp
  75:	ff 14 25 00 00 00 00 	callq  *0x0
#define pgd_page(pgd)		pfn_to_page(pgd_val(pgd) >> PAGE_SHIFT)

/* to find an entry in a page-table-directory. */
static inline unsigned long pud_index(unsigned long address)
{
	return (address >> PUD_SHIFT) & (PTRS_PER_PUD - 1);
  7c:	48 c1 ee 1b          	shr    $0x1b,%rsi
	return pgd_flags(pgd) & _PAGE_PRESENT;
}

static inline unsigned long pgd_page_vaddr(pgd_t pgd)
{
	return (unsigned long)__va((unsigned long)pgd_val(pgd) & PTE_PFN_MASK);
  80:	48 ba 00 00 00 00 00 	movabs $0xffff880000000000,%rdx
  87:	88 ff ff 
	return (address >> PUD_SHIFT) & (PTRS_PER_PUD - 1);
}

static inline pud_t *pud_offset(pgd_t *pgd, unsigned long address)
{
	return (pud_t *)pgd_page_vaddr(*pgd) + pud_index(address);
  8a:	81 e6 f8 0f 00 00    	and    $0xff8,%esi
	return pgd_flags(pgd) & _PAGE_PRESENT;
}

static inline unsigned long pgd_page_vaddr(pgd_t pgd)
{
	return (unsigned long)__va((unsigned long)pgd_val(pgd) & PTE_PFN_MASK);
  90:	48 01 d6             	add    %rdx,%rsi
  93:	48 ba 00 f0 ff ff ff 	movabs $0x3ffffffff000,%rdx
  9a:	3f 00 00 
  9d:	48 21 d0             	and    %rdx,%rax
	return (address >> PUD_SHIFT) & (PTRS_PER_PUD - 1);
}

static inline pud_t *pud_offset(pgd_t *pgd, unsigned long address)
{
	return (pud_t *)pgd_page_vaddr(*pgd) + pud_index(address);
  a0:	48 01 f0             	add    %rsi,%rax
}
  a3:	5d                   	pop    %rbp
  a4:	c3                   	retq   

00000000000000a5 <bad_address>:
		 struct task_struct *tsk)
{
}

static int bad_address(void *p)
{
  a5:	e8 00 00 00 00       	callq  aa <bad_address+0x5>
  aa:	55                   	push   %rbp
DECLARE_PER_CPU(unsigned long, kernel_stack);

static inline struct thread_info *current_thread_info(void)
{
	struct thread_info *ti;
	ti = (void *)(this_cpu_read_stable(kernel_stack) +
  ab:	65 48 8b 14 25 00 00 	mov    %gs:0x0,%rdx
  b2:	00 00 
  b4:	48 89 e5             	mov    %rsp,%rbp
	unsigned long dummy;

	return probe_kernel_address((unsigned long *)p, dummy);
  b7:	48 8b 8a 48 e0 ff ff 	mov    -0x1fb8(%rdx),%rcx
  be:	48 c7 82 48 e0 ff ff 	movq   $0xffffffffffffffff,-0x1fb8(%rdx)
  c5:	ff ff ff ff 
 * The various preempt_count add/sub methods
 */

static __always_inline void __preempt_count_add(int val)
{
	__this_cpu_add_4(__preempt_count, val);
  c9:	65 ff 04 25 00 00 00 	incl   %gs:0x0
  d0:	00 
			      ret, "w", "w", "=r", 2);
		return ret;
	case 4:__get_user_asm(*(u32 *)dst, (u32 __user *)src,
			      ret, "l", "k", "=r", 4);
		return ret;
	case 8:__get_user_asm(*(u64 *)dst, (u64 __user *)src,
  d1:	31 c0                	xor    %eax,%eax
  d3:	66 66 90             	data32 xchg %ax,%ax
  d6:	48 8b 37             	mov    (%rdi),%rsi
  d9:	66 66 90             	data32 xchg %ax,%ax
}

static __always_inline void __preempt_count_sub(int val)
{
	__this_cpu_add_4(__preempt_count, -val);
  dc:	65 ff 0c 25 00 00 00 	decl   %gs:0x0
  e3:	00 
  e4:	48 89 8a 48 e0 ff ff 	mov    %rcx,-0x1fb8(%rdx)
}
  eb:	5d                   	pop    %rbp
  ec:	c3                   	retq   

00000000000000ed <dump_pagetable>:

static void dump_pagetable(unsigned long address)
{
  ed:	e8 00 00 00 00       	callq  f2 <dump_pagetable+0x5>
  f2:	55                   	push   %rbp
  f3:	48 89 e5             	mov    %rsp,%rbp
  f6:	41 54                	push   %r12
  f8:	49 89 fc             	mov    %rdi,%r12
  fb:	53                   	push   %rbx
	PVOP_VCALL1(pv_mmu_ops.write_cr2, x);
}

static inline unsigned long read_cr3(void)
{
	return PVOP_CALL0(unsigned long, pv_mmu_ops.read_cr3);
  fc:	ff 14 25 00 00 00 00 	callq  *0x0
	pgd_t *base = __va(read_cr3() & PHYSICAL_PAGE_MASK);
 103:	48 bb 00 f0 ff ff ff 	movabs $0x3ffffffff000,%rbx
 10a:	3f 00 00 
 10d:	48 ba 00 00 00 00 00 	movabs $0xffff880000000000,%rdx
 114:	88 ff ff 
 117:	48 21 c3             	and    %rax,%rbx
 11a:	48 01 d3             	add    %rdx,%rbx
	pgd_t *pgd = base + pgd_index(address);
 11d:	4c 89 e2             	mov    %r12,%rdx
 120:	48 c1 ea 24          	shr    $0x24,%rdx
 124:	81 e2 f8 0f 00 00    	and    $0xff8,%edx
 12a:	48 01 d3             	add    %rdx,%rbx
	pud_t *pud;
	pmd_t *pmd;
	pte_t *pte;

	if (bad_address(pgd))
 12d:	48 89 df             	mov    %rbx,%rdi
 130:	e8 70 ff ff ff       	callq  a5 <bad_address>
 135:	85 c0                	test   %eax,%eax
 137:	0f 85 14 01 00 00    	jne    251 <dump_pagetable+0x164>

	if (sizeof(pgdval_t) > sizeof(long))
		ret =  PVOP_CALLEE2(pgdval_t, pv_mmu_ops.pgd_val,
				    pgd.pgd, (u64)pgd.pgd >> 32);
	else
		ret =  PVOP_CALLEE1(pgdval_t, pv_mmu_ops.pgd_val,
 13d:	48 8b 3b             	mov    (%rbx),%rdi
 140:	ff 14 25 00 00 00 00 	callq  *0x0
		goto bad;

	printk("PGD %lx ", pgd_val(*pgd));
 147:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
 14e:	48 89 c6             	mov    %rax,%rsi
 151:	31 c0                	xor    %eax,%eax
 153:	e8 00 00 00 00       	callq  158 <dump_pagetable+0x6b>

	if (!pgd_present(*pgd))
 158:	f6 03 01             	testb  $0x1,(%rbx)
 15b:	0f 84 e0 00 00 00    	je     241 <dump_pagetable+0x154>
		goto out;

	pud = pud_offset(pgd, address);
 161:	48 89 df             	mov    %rbx,%rdi
 164:	4c 89 e6             	mov    %r12,%rsi
 167:	e8 02 ff ff ff       	callq  6e <pud_offset>
	if (bad_address(pud))
 16c:	48 89 c7             	mov    %rax,%rdi
	printk("PGD %lx ", pgd_val(*pgd));

	if (!pgd_present(*pgd))
		goto out;

	pud = pud_offset(pgd, address);
 16f:	48 89 c3             	mov    %rax,%rbx
	if (bad_address(pud))
 172:	e8 2e ff ff ff       	callq  a5 <bad_address>
 177:	85 c0                	test   %eax,%eax
 179:	0f 85 d2 00 00 00    	jne    251 <dump_pagetable+0x164>

	if (sizeof(pudval_t) > sizeof(long))
		ret =  PVOP_CALLEE2(pudval_t, pv_mmu_ops.pud_val,
				    pud.pud, (u64)pud.pud >> 32);
	else
		ret =  PVOP_CALLEE1(pudval_t, pv_mmu_ops.pud_val,
 17f:	48 8b 3b             	mov    (%rbx),%rdi
 182:	ff 14 25 00 00 00 00 	callq  *0x0
		goto bad;

	printk("PUD %lx ", pud_val(*pud));
 189:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
 190:	48 89 c6             	mov    %rax,%rsi
 193:	31 c0                	xor    %eax,%eax
 195:	e8 00 00 00 00       	callq  19a <dump_pagetable+0xad>
 19a:	48 8b 3b             	mov    (%rbx),%rdi
	if (!pud_present(*pud) || pud_large(*pud))
 19d:	40 f6 c7 01          	test   $0x1,%dil
 1a1:	0f 84 9a 00 00 00    	je     241 <dump_pagetable+0x154>
 1a7:	ff 14 25 00 00 00 00 	callq  *0x0
	return (pmd_t *)pud_page_vaddr(*pud) + pmd_index(address);
}

static inline int pud_large(pud_t pud)
{
	return (pud_val(pud) & (_PAGE_PSE | _PAGE_PRESENT)) ==
 1ae:	25 81 00 00 00       	and    $0x81,%eax
 1b3:	48 3d 81 00 00 00    	cmp    $0x81,%rax
 1b9:	0f 84 82 00 00 00    	je     241 <dump_pagetable+0x154>
		goto out;

	pmd = pmd_offset(pud, address);
 1bf:	48 89 df             	mov    %rbx,%rdi
 1c2:	4c 89 e6             	mov    %r12,%rsi
 1c5:	e8 6d fe ff ff       	callq  37 <pmd_offset>
	if (bad_address(pmd))
 1ca:	48 89 c7             	mov    %rax,%rdi

	printk("PUD %lx ", pud_val(*pud));
	if (!pud_present(*pud) || pud_large(*pud))
		goto out;

	pmd = pmd_offset(pud, address);
 1cd:	48 89 c3             	mov    %rax,%rbx
	if (bad_address(pmd))
 1d0:	e8 d0 fe ff ff       	callq  a5 <bad_address>
 1d5:	85 c0                	test   %eax,%eax
 1d7:	75 78                	jne    251 <dump_pagetable+0x164>

	if (sizeof(pmdval_t) > sizeof(long))
		ret =  PVOP_CALLEE2(pmdval_t, pv_mmu_ops.pmd_val,
				    pmd.pmd, (u64)pmd.pmd >> 32);
	else
		ret =  PVOP_CALLEE1(pmdval_t, pv_mmu_ops.pmd_val,
 1d9:	48 8b 3b             	mov    (%rbx),%rdi
 1dc:	ff 14 25 00 00 00 00 	callq  *0x0
		goto bad;

	printk("PMD %lx ", pmd_val(*pmd));
 1e3:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
 1ea:	48 89 c6             	mov    %rax,%rsi
 1ed:	31 c0                	xor    %eax,%eax
 1ef:	e8 00 00 00 00       	callq  1f4 <dump_pagetable+0x107>
	return native_pud_val(pud) & PTE_FLAGS_MASK;
}

static inline pmdval_t pmd_flags(pmd_t pmd)
{
	return native_pmd_val(pmd) & PTE_FLAGS_MASK;
 1f4:	48 b8 ff 0f 00 00 00 	movabs $0xffffc00000000fff,%rax
 1fb:	c0 ff ff 
 1fe:	48 23 03             	and    (%rbx),%rax
	if (!pmd_present(*pmd) || pmd_large(*pmd))
 201:	a9 81 01 00 00       	test   $0x181,%eax
 206:	74 39                	je     241 <dump_pagetable+0x154>
 208:	a8 80                	test   $0x80,%al
 20a:	75 35                	jne    241 <dump_pagetable+0x154>
		goto out;

	pte = pte_offset_kernel(pmd, address);
 20c:	48 89 df             	mov    %rbx,%rdi
 20f:	4c 89 e6             	mov    %r12,%rsi
 212:	e8 e9 fd ff ff       	callq  0 <pte_offset_kernel>
	if (bad_address(pte))
 217:	48 89 c7             	mov    %rax,%rdi

	printk("PMD %lx ", pmd_val(*pmd));
	if (!pmd_present(*pmd) || pmd_large(*pmd))
		goto out;

	pte = pte_offset_kernel(pmd, address);
 21a:	48 89 c3             	mov    %rax,%rbx
	if (bad_address(pte))
 21d:	e8 83 fe ff ff       	callq  a5 <bad_address>
 222:	85 c0                	test   %eax,%eax
 224:	75 2b                	jne    251 <dump_pagetable+0x164>

	if (sizeof(pteval_t) > sizeof(long))
		ret = PVOP_CALLEE2(pteval_t, pv_mmu_ops.pte_val,
				   pte.pte, (u64)pte.pte >> 32);
	else
		ret = PVOP_CALLEE1(pteval_t, pv_mmu_ops.pte_val,
 226:	48 8b 3b             	mov    (%rbx),%rdi
 229:	ff 14 25 00 00 00 00 	callq  *0x0
		goto bad;

	printk("PTE %lx", pte_val(*pte));
 230:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
 237:	48 89 c6             	mov    %rax,%rsi
 23a:	31 c0                	xor    %eax,%eax
 23c:	e8 00 00 00 00       	callq  241 <dump_pagetable+0x154>
out:
	printk("\n");
 241:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
 248:	31 c0                	xor    %eax,%eax
 24a:	e8 00 00 00 00       	callq  24f <dump_pagetable+0x162>
	return;
 24f:	eb 0e                	jmp    25f <dump_pagetable+0x172>
bad:
	printk("BAD\n");
 251:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
 258:	31 c0                	xor    %eax,%eax
 25a:	e8 00 00 00 00       	callq  25f <dump_pagetable+0x172>
}
 25f:	5b                   	pop    %rbx
 260:	41 5c                	pop    %r12
 262:	5d                   	pop    %rbp
 263:	c3                   	retq   

0000000000000264 <force_sig_info_fault>:
}

static void
force_sig_info_fault(int si_signo, int si_code, unsigned long address,
		     struct task_struct *tsk, int fault)
{
 264:	e8 00 00 00 00       	callq  269 <force_sig_info_fault+0x5>
 269:	55                   	push   %rbp
	info.si_errno	= 0;
	info.si_code	= si_code;
	info.si_addr	= (void __user *)address;
	if (fault & VM_FAULT_HWPOISON_LARGE)
		lsb = hstate_index_to_shift(VM_FAULT_GET_HINDEX(fault)); 
	if (fault & VM_FAULT_HWPOISON)
 26a:	41 c1 e0 1b          	shl    $0x1b,%r8d
}

static void
force_sig_info_fault(int si_signo, int si_code, unsigned long address,
		     struct task_struct *tsk, int fault)
{
 26e:	48 89 d0             	mov    %rdx,%rax
	info.si_errno	= 0;
	info.si_code	= si_code;
	info.si_addr	= (void __user *)address;
	if (fault & VM_FAULT_HWPOISON_LARGE)
		lsb = hstate_index_to_shift(VM_FAULT_GET_HINDEX(fault)); 
	if (fault & VM_FAULT_HWPOISON)
 271:	41 c1 f8 1f          	sar    $0x1f,%r8d
}

static void
force_sig_info_fault(int si_signo, int si_code, unsigned long address,
		     struct task_struct *tsk, int fault)
{
 275:	48 89 ca             	mov    %rcx,%rdx
 278:	48 89 e5             	mov    %rsp,%rbp
 27b:	48 83 c4 80          	add    $0xffffffffffffff80,%rsp
	info.si_errno	= 0;
	info.si_code	= si_code;
	info.si_addr	= (void __user *)address;
	if (fault & VM_FAULT_HWPOISON_LARGE)
		lsb = hstate_index_to_shift(VM_FAULT_GET_HINDEX(fault)); 
	if (fault & VM_FAULT_HWPOISON)
 27f:	41 83 e0 0c          	and    $0xc,%r8d
	unsigned lsb = 0;
	siginfo_t info;

	info.si_signo	= si_signo;
	info.si_errno	= 0;
	info.si_code	= si_code;
 283:	89 75 88             	mov    %esi,-0x78(%rbp)
		lsb = hstate_index_to_shift(VM_FAULT_GET_HINDEX(fault)); 
	if (fault & VM_FAULT_HWPOISON)
		lsb = PAGE_SHIFT;
	info.si_addr_lsb = lsb;

	force_sig_info(si_signo, &info, tsk);
 286:	48 8d 75 80          	lea    -0x80(%rbp),%rsi
		     struct task_struct *tsk, int fault)
{
	unsigned lsb = 0;
	siginfo_t info;

	info.si_signo	= si_signo;
 28a:	89 7d 80             	mov    %edi,-0x80(%rbp)
	info.si_errno	= 0;
 28d:	c7 45 84 00 00 00 00 	movl   $0x0,-0x7c(%rbp)
	info.si_code	= si_code;
	info.si_addr	= (void __user *)address;
 294:	48 89 45 90          	mov    %rax,-0x70(%rbp)
	if (fault & VM_FAULT_HWPOISON_LARGE)
		lsb = hstate_index_to_shift(VM_FAULT_GET_HINDEX(fault)); 
	if (fault & VM_FAULT_HWPOISON)
		lsb = PAGE_SHIFT;
	info.si_addr_lsb = lsb;
 298:	66 44 89 45 98       	mov    %r8w,-0x68(%rbp)

	force_sig_info(si_signo, &info, tsk);
 29d:	e8 00 00 00 00       	callq  2a2 <force_sig_info_fault+0x3e>
}
 2a2:	c9                   	leaveq 
 2a3:	c3                   	retq   

00000000000002a4 <pgtable_bad>:
}

static noinline void
pgtable_bad(struct pt_regs *regs, unsigned long error_code,
	    unsigned long address)
{
 2a4:	e8 00 00 00 00       	callq  2a9 <pgtable_bad+0x5>
 2a9:	55                   	push   %rbp
 2aa:	48 89 e5             	mov    %rsp,%rbp
 2ad:	41 57                	push   %r15
 2af:	49 89 f7             	mov    %rsi,%r15
 2b2:	41 56                	push   %r14
 2b4:	41 55                	push   %r13
 2b6:	49 89 fd             	mov    %rdi,%r13
 2b9:	41 54                	push   %r12
 2bb:	49 89 d4             	mov    %rdx,%r12
 2be:	53                   	push   %rbx

DECLARE_PER_CPU(struct task_struct *, current_task);

static __always_inline struct task_struct *get_current(void)
{
	return this_cpu_read_stable(current_task);
 2bf:	65 48 8b 1c 25 00 00 	mov    %gs:0x0,%rbx
 2c6:	00 00 
	struct task_struct *tsk;
	unsigned long flags;
	int sig;
	flags = oops_begin();
 2c8:	e8 00 00 00 00       	callq  2cd <pgtable_bad+0x29>
	tsk = current;
	sig = SIGKILL;
	printk(KERN_ALERT "%s: Corrupted page table at address %lx\n",
	       tsk->comm, address);
 2cd:	48 8d b3 d0 04 00 00 	lea    0x4d0(%rbx),%rsi
	unsigned long flags;
	int sig;
	flags = oops_begin();
	tsk = current;
	sig = SIGKILL;
	printk(KERN_ALERT "%s: Corrupted page table at address %lx\n",
 2d4:	4c 89 e2             	mov    %r12,%rdx
	    unsigned long address)
{
	struct task_struct *tsk;
	unsigned long flags;
	int sig;
	flags = oops_begin();
 2d7:	49 89 c6             	mov    %rax,%r14
	tsk = current;
	sig = SIGKILL;
	printk(KERN_ALERT "%s: Corrupted page table at address %lx\n",
 2da:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
 2e1:	31 c0                	xor    %eax,%eax
 2e3:	e8 00 00 00 00       	callq  2e8 <pgtable_bad+0x44>
	       tsk->comm, address);
	dump_pagetable(address);
 2e8:	4c 89 e7             	mov    %r12,%rdi
 2eb:	e8 fd fd ff ff       	callq  ed <dump_pagetable>

	tsk->thread.cr2		= address;
 2f0:	4c 89 a3 70 05 00 00 	mov    %r12,0x570(%rbx)
	tsk->thread.trap_nr	= X86_TRAP_PF;
	tsk->thread.error_code	= error_code;
 2f7:	4c 89 bb 80 05 00 00 	mov    %r15,0x580(%rbx)

	if (__die("Bad pagetable", regs, error_code))
 2fe:	4c 89 fa             	mov    %r15,%rdx
 301:	4c 89 ee             	mov    %r13,%rsi
	printk(KERN_ALERT "%s: Corrupted page table at address %lx\n",
	       tsk->comm, address);
	dump_pagetable(address);

	tsk->thread.cr2		= address;
	tsk->thread.trap_nr	= X86_TRAP_PF;
 304:	48 c7 83 78 05 00 00 	movq   $0xe,0x578(%rbx)
 30b:	0e 00 00 00 
	tsk->thread.error_code	= error_code;

	if (__die("Bad pagetable", regs, error_code))
 30f:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
 316:	e8 00 00 00 00       	callq  31b <pgtable_bad+0x77>
	struct task_struct *tsk;
	unsigned long flags;
	int sig;
	flags = oops_begin();
	tsk = current;
	sig = SIGKILL;
 31b:	83 f8 01             	cmp    $0x1,%eax
	tsk->thread.error_code	= error_code;

	if (__die("Bad pagetable", regs, error_code))
		sig = 0;

	oops_end(flags, regs, sig);
 31e:	4c 89 ee             	mov    %r13,%rsi
 321:	4c 89 f7             	mov    %r14,%rdi
	struct task_struct *tsk;
	unsigned long flags;
	int sig;
	flags = oops_begin();
	tsk = current;
	sig = SIGKILL;
 324:	19 d2                	sbb    %edx,%edx
 326:	83 e2 09             	and    $0x9,%edx
	tsk->thread.error_code	= error_code;

	if (__die("Bad pagetable", regs, error_code))
		sig = 0;

	oops_end(flags, regs, sig);
 329:	e8 00 00 00 00       	callq  32e <pgtable_bad+0x8a>
}
 32e:	5b                   	pop    %rbx
 32f:	41 5c                	pop    %r12
 331:	41 5d                	pop    %r13
 333:	41 5e                	pop    %r14
 335:	41 5f                	pop    %r15
 337:	5d                   	pop    %rbp
 338:	c3                   	retq   

0000000000000339 <spurious_fault_check>:
			BUG();
	}
}

static int spurious_fault_check(unsigned long error_code, pte_t *pte)
{
 339:	e8 00 00 00 00       	callq  33e <spurious_fault_check+0x5>
 33e:	55                   	push   %rbp
	if ((error_code & PF_WRITE) && !pte_write(*pte))
 33f:	40 f6 c7 02          	test   $0x2,%dil
			BUG();
	}
}

static int spurious_fault_check(unsigned long error_code, pte_t *pte)
{
 343:	48 89 e5             	mov    %rsp,%rbp
	if ((error_code & PF_WRITE) && !pte_write(*pte))
 346:	74 12                	je     35a <spurious_fault_check+0x21>
	return pte.pte;
}

static inline pteval_t pte_flags(pte_t pte)
{
	return native_pte_val(pte) & PTE_FLAGS_MASK;
 348:	48 b8 ff 0f 00 00 00 	movabs $0xffffc00000000fff,%rax
 34f:	c0 ff ff 
 352:	48 23 06             	and    (%rsi),%rax
 355:	83 e0 02             	and    $0x2,%eax
 358:	74 1f                	je     379 <spurious_fault_check+0x40>
		return 0;

	if ((error_code & PF_INSTR) && !pte_exec(*pte))
 35a:	40 80 e7 10          	and    $0x10,%dil
		return 0;

	return 1;
 35e:	b8 01 00 00 00       	mov    $0x1,%eax
static int spurious_fault_check(unsigned long error_code, pte_t *pte)
{
	if ((error_code & PF_WRITE) && !pte_write(*pte))
		return 0;

	if ((error_code & PF_INSTR) && !pte_exec(*pte))
 363:	74 14                	je     379 <spurious_fault_check+0x40>
 365:	48 b8 ff 0f 00 00 00 	movabs $0xffffc00000000fff,%rax
 36c:	c0 ff ff 
 36f:	48 23 06             	and    (%rsi),%rax
 372:	48 f7 d0             	not    %rax
		return 0;
 375:	48 c1 e8 3f          	shr    $0x3f,%rax

	return 1;
}
 379:	5d                   	pop    %rbp
 37a:	c3                   	retq   

000000000000037b <is_prefetch.isra.14>:
		return 0;
	}
}

static int
is_prefetch(struct pt_regs *regs, unsigned long error_code, unsigned long addr)
 37b:	e8 00 00 00 00       	callq  380 <is_prefetch.isra.14+0x5>

	/*
	 * If it was a exec (instruction fetch) fault on NX page, then
	 * do not ignore the fault:
	 */
	if (error_code & PF_INSTR)
 380:	40 80 e6 10          	and    $0x10,%sil
 384:	0f 85 b5 01 00 00    	jne    53f <is_prefetch.isra.14+0x1c4>
		return 0;
	}
}

static int
is_prefetch(struct pt_regs *regs, unsigned long error_code, unsigned long addr)
 38a:	55                   	push   %rbp
	 * do not ignore the fault:
	 */
	if (error_code & PF_INSTR)
		return 0;

	instr = (void *)convert_ip_to_linear(current, regs);
 38b:	48 89 fe             	mov    %rdi,%rsi
		return 0;
	}
}

static int
is_prefetch(struct pt_regs *regs, unsigned long error_code, unsigned long addr)
 38e:	48 89 e5             	mov    %rsp,%rbp
 391:	41 55                	push   %r13
 393:	49 89 fd             	mov    %rdi,%r13
 396:	41 54                	push   %r12
 398:	65 4c 8b 24 25 00 00 	mov    %gs:0x0,%r12
 39f:	00 00 
 3a1:	53                   	push   %rbx
	 * do not ignore the fault:
	 */
	if (error_code & PF_INSTR)
		return 0;

	instr = (void *)convert_ip_to_linear(current, regs);
 3a2:	4c 89 e7             	mov    %r12,%rdi
 3a5:	e8 00 00 00 00       	callq  3aa <is_prefetch.isra.14+0x2f>
	max_instr = instr + 15;

	if (user_mode(regs) && instr >= (unsigned char *)TASK_SIZE)
 3aa:	41 f6 85 88 00 00 00 	testb  $0x3,0x88(%r13)
 3b1:	03 
	 */
	if (error_code & PF_INSTR)
		return 0;

	instr = (void *)convert_ip_to_linear(current, regs);
	max_instr = instr + 15;
 3b2:	48 8d 58 0f          	lea    0xf(%rax),%rbx

	if (user_mode(regs) && instr >= (unsigned char *)TASK_SIZE)
 3b6:	75 11                	jne    3c9 <is_prefetch.isra.14+0x4e>
 3b8:	65 4c 8b 04 25 00 00 	mov    %gs:0x0,%r8
 3bf:	00 00 
	int ret = 0;

	if (!__builtin_constant_p(size))
		return copy_user_generic(dst, (__force void *)src, size);
	switch (size) {
	case 1:__get_user_asm(*(u8 *)dst, (u8 __user *)src,
 3c1:	45 31 e4             	xor    %r12d,%r12d
 3c4:	e9 b6 00 00 00       	jmpq   47f <is_prefetch.isra.14+0x104>
 3c9:	65 48 8b 14 25 00 00 	mov    %gs:0x0,%rdx
 3d0:	00 00 
 3d2:	48 8b 8a 38 e0 ff ff 	mov    -0x1fc8(%rdx),%rcx
 3d9:	48 ba 00 f0 ff ff ff 	movabs $0x7ffffffff000,%rdx
 3e0:	7f 00 00 
 3e3:	f7 c1 00 00 00 20    	test   $0x20000000,%ecx
 3e9:	74 17                	je     402 <is_prefetch.isra.14+0x87>
 3eb:	41 f6 84 24 df 02 00 	testb  $0x8,0x2df(%r12)
 3f2:	00 08 
 3f4:	ba 00 00 00 c0       	mov    $0xc0000000,%edx
 3f9:	b9 00 e0 ff ff       	mov    $0xffffe000,%ecx
 3fe:	48 0f 44 d1          	cmove  %rcx,%rdx
 402:	48 39 d0             	cmp    %rdx,%rax
 405:	72 b1                	jb     3b8 <is_prefetch.isra.14+0x3d>
 407:	e9 2a 01 00 00       	jmpq   536 <is_prefetch.isra.14+0x1bb>
		return 0;

	while (instr < max_instr) {
		unsigned char opcode;

		if (probe_kernel_address(instr, opcode))
 40c:	4d 8b 90 48 e0 ff ff 	mov    -0x1fb8(%r8),%r10
 413:	49 c7 80 48 e0 ff ff 	movq   $0xffffffffffffffff,-0x1fb8(%r8)
 41a:	ff ff ff ff 
 * The various preempt_count add/sub methods
 */

static __always_inline void __preempt_count_add(int val)
{
	__this_cpu_add_4(__preempt_count, val);
 41e:	65 ff 04 25 00 00 00 	incl   %gs:0x0
 425:	00 
 426:	44 89 e2             	mov    %r12d,%edx
 429:	66 66 90             	data32 xchg %ax,%ax
 42c:	44 8a 08             	mov    (%rax),%r9b
 42f:	66 66 90             	data32 xchg %ax,%ax
}

static __always_inline void __preempt_count_sub(int val)
{
	__this_cpu_add_4(__preempt_count, -val);
 432:	65 ff 0c 25 00 00 00 	decl   %gs:0x0
 439:	00 
 43a:	85 d2                	test   %edx,%edx
 43c:	4d 89 90 48 e0 ff ff 	mov    %r10,-0x1fb8(%r8)
 443:	0f 85 ed 00 00 00    	jne    536 <is_prefetch.isra.14+0x1bb>
 */
static inline int
check_prefetch_opcode(struct pt_regs *regs, unsigned char *instr,
		      unsigned char opcode, int *prefetch)
{
	unsigned char instr_hi = opcode & 0xf0;
 449:	44 88 c9             	mov    %r9b,%cl
	unsigned char instr_lo = opcode & 0x0f;
 44c:	45 88 cb             	mov    %r9b,%r11b
	while (instr < max_instr) {
		unsigned char opcode;

		if (probe_kernel_address(instr, opcode))
			break;
		instr++;
 44f:	48 ff c0             	inc    %rax
 */
static inline int
check_prefetch_opcode(struct pt_regs *regs, unsigned char *instr,
		      unsigned char opcode, int *prefetch)
{
	unsigned char instr_hi = opcode & 0xf0;
 452:	83 e1 f0             	and    $0xfffffff0,%ecx
	unsigned char instr_lo = opcode & 0x0f;
 455:	41 83 e3 0f          	and    $0xf,%r11d

	switch (instr_hi) {
 459:	80 f9 30             	cmp    $0x30,%cl
 45c:	74 38                	je     496 <is_prefetch.isra.14+0x11b>
 45e:	76 29                	jbe    489 <is_prefetch.isra.14+0x10e>
 460:	80 f9 60             	cmp    $0x60,%cl
 463:	74 62                	je     4c7 <is_prefetch.isra.14+0x14c>
 465:	80 f9 f0             	cmp    $0xf0,%cl
 468:	74 6c                	je     4d6 <is_prefetch.isra.14+0x15b>
 46a:	80 f9 40             	cmp    $0x40,%cl
 46d:	0f 85 c3 00 00 00    	jne    536 <is_prefetch.isra.14+0x1bb>
 *   Check that here and ignore it.
 *
 * Opcode checker based on code by Richard Brunner.
 */
static inline int
check_prefetch_opcode(struct pt_regs *regs, unsigned char *instr,
 473:	49 8b 95 88 00 00 00 	mov    0x88(%r13),%rdx
		 * Need to figure out under what instruction mode the
		 * instruction was issued. Could check the LDT for lm,
		 * but for now it's good enough to assume that long
		 * mode only uses well known segments or kernel.
		 */
		return (!user_mode(regs) || user_64bit_mode(regs));
 47a:	f6 c2 03             	test   $0x3,%dl
 47d:	75 29                	jne    4a8 <is_prefetch.isra.14+0x12d>
	max_instr = instr + 15;

	if (user_mode(regs) && instr >= (unsigned char *)TASK_SIZE)
		return 0;

	while (instr < max_instr) {
 47f:	48 39 c3             	cmp    %rax,%rbx
 482:	77 88                	ja     40c <is_prefetch.isra.14+0x91>
 484:	e9 ad 00 00 00       	jmpq   536 <is_prefetch.isra.14+0x1bb>
		      unsigned char opcode, int *prefetch)
{
	unsigned char instr_hi = opcode & 0xf0;
	unsigned char instr_lo = opcode & 0x0f;

	switch (instr_hi) {
 489:	84 c9                	test   %cl,%cl
 48b:	74 58                	je     4e5 <is_prefetch.isra.14+0x16a>
 48d:	80 f9 20             	cmp    $0x20,%cl
 490:	0f 85 a0 00 00 00    	jne    536 <is_prefetch.isra.14+0x1bb>
		 * Values 0x26,0x2E,0x36,0x3E are valid x86 prefixes.
		 * In X86_64 long mode, the CPU will signal invalid
		 * opcode if some of these prefixes are present so
		 * X86_64 will never get here anyway
		 */
		return ((instr_lo & 7) == 0x6);
 496:	41 83 e1 07          	and    $0x7,%r9d
 49a:	31 d2                	xor    %edx,%edx
 49c:	41 80 f9 06          	cmp    $0x6,%r9b
 4a0:	0f 94 c2             	sete   %dl
 4a3:	e9 86 00 00 00       	jmpq   52e <is_prefetch.isra.14+0x1b3>
	 * selector.  We do not allow long mode selectors in the LDT.
	 */
	return regs->cs == __USER_CS;
#else
	/* Headers are too twisted for this to go in paravirt.h. */
	return regs->cs == __USER_CS || regs->cs == pv_info.extra_user_64bit_cs;
 4a8:	48 83 fa 33          	cmp    $0x33,%rdx
 4ac:	74 12                	je     4c0 <is_prefetch.isra.14+0x145>
 4ae:	0f b7 0d 00 00 00 00 	movzwl 0x0(%rip),%ecx        # 4b5 <is_prefetch.isra.14+0x13a>
 4b5:	48 39 ca             	cmp    %rcx,%rdx
 4b8:	0f 94 c2             	sete   %dl
 4bb:	0f b6 d2             	movzbl %dl,%edx
 4be:	eb 6e                	jmp    52e <is_prefetch.isra.14+0x1b3>
 4c0:	ba 01 00 00 00       	mov    $0x1,%edx
 4c5:	eb 67                	jmp    52e <is_prefetch.isra.14+0x1b3>
		 */
		return (!user_mode(regs) || user_64bit_mode(regs));
#endif
	case 0x60:
		/* 0x64 thru 0x67 are valid prefixes in all modes. */
		return (instr_lo & 0xC) == 0x4;
 4c7:	41 83 e1 0c          	and    $0xc,%r9d
 4cb:	31 d2                	xor    %edx,%edx
 4cd:	41 80 f9 04          	cmp    $0x4,%r9b
 4d1:	0f 94 c2             	sete   %dl
 4d4:	eb 58                	jmp    52e <is_prefetch.isra.14+0x1b3>
	case 0xF0:
		/* 0xF0, 0xF2, 0xF3 are valid prefixes in all modes. */
		return !instr_lo || (instr_lo>>1) == 1;
 4d6:	45 84 db             	test   %r11b,%r11b
 4d9:	74 a4                	je     47f <is_prefetch.isra.14+0x104>
 4db:	41 d0 eb             	shr    %r11b
 4de:	41 fe cb             	dec    %r11b
 4e1:	75 53                	jne    536 <is_prefetch.isra.14+0x1bb>
 4e3:	eb 9a                	jmp    47f <is_prefetch.isra.14+0x104>
	case 0x00:
		/* Prefetch instruction is 0x0F0D or 0x0F18 */
		if (probe_kernel_address(instr, opcode))
 4e5:	49 c7 80 48 e0 ff ff 	movq   $0xffffffffffffffff,-0x1fb8(%r8)
 4ec:	ff ff ff ff 
 * The various preempt_count add/sub methods
 */

static __always_inline void __preempt_count_add(int val)
{
	__this_cpu_add_4(__preempt_count, val);
 4f0:	65 ff 04 25 00 00 00 	incl   %gs:0x0
 4f7:	00 
	preempt_count_inc();
	/*
	 * make sure to have issued the store before a pagefault
	 * can hit.
	 */
	barrier();
 4f8:	66 66 90             	data32 xchg %ax,%ax
 4fb:	8a 08                	mov    (%rax),%cl
 4fd:	66 66 90             	data32 xchg %ax,%ax
}

static __always_inline void __preempt_count_sub(int val)
{
	__this_cpu_add_4(__preempt_count, -val);
 500:	65 ff 0c 25 00 00 00 	decl   %gs:0x0
 507:	00 
 508:	85 d2                	test   %edx,%edx
 50a:	4d 89 90 48 e0 ff ff 	mov    %r10,-0x1fb8(%r8)
 511:	75 23                	jne    536 <is_prefetch.isra.14+0x1bb>
			return 0;

		*prefetch = (instr_lo == 0xF) &&
 513:	31 c0                	xor    %eax,%eax
 515:	41 80 fb 0f          	cmp    $0xf,%r11b
 519:	75 1d                	jne    538 <is_prefetch.isra.14+0x1bd>
			(opcode == 0x0D || opcode == 0x18);
 51b:	80 f9 18             	cmp    $0x18,%cl
 51e:	0f 94 c0             	sete   %al
 521:	80 f9 0d             	cmp    $0xd,%cl
 524:	0f 94 c2             	sete   %dl
 527:	09 d0                	or     %edx,%eax
	case 0x00:
		/* Prefetch instruction is 0x0F0D or 0x0F18 */
		if (probe_kernel_address(instr, opcode))
			return 0;

		*prefetch = (instr_lo == 0xF) &&
 529:	0f b6 c0             	movzbl %al,%eax
 52c:	eb 0a                	jmp    538 <is_prefetch.isra.14+0x1bd>
		unsigned char opcode;

		if (probe_kernel_address(instr, opcode))
			break;
		instr++;
		if (!check_prefetch_opcode(regs, instr, opcode, &prefetch))
 52e:	85 d2                	test   %edx,%edx
 530:	0f 85 49 ff ff ff    	jne    47f <is_prefetch.isra.14+0x104>
		      unsigned char opcode, int *prefetch)
{
	unsigned char instr_hi = opcode & 0xf0;
	unsigned char instr_lo = opcode & 0x0f;

	switch (instr_hi) {
 536:	31 c0                	xor    %eax,%eax
		instr++;
		if (!check_prefetch_opcode(regs, instr, opcode, &prefetch))
			break;
	}
	return prefetch;
}
 538:	5b                   	pop    %rbx
 539:	41 5c                	pop    %r12
 53b:	41 5d                	pop    %r13
 53d:	5d                   	pop    %rbp
 53e:	c3                   	retq   
	/*
	 * If it was a exec (instruction fetch) fault on NX page, then
	 * do not ignore the fault:
	 */
	if (error_code & PF_INSTR)
		return 0;
 53f:	31 c0                	xor    %eax,%eax
		instr++;
		if (!check_prefetch_opcode(regs, instr, opcode, &prefetch))
			break;
	}
	return prefetch;
}
 541:	c3                   	retq   

0000000000000542 <no_context>:
}

static noinline void
no_context(struct pt_regs *regs, unsigned long error_code,
	   unsigned long address, int signal, int si_code)
{
 542:	e8 00 00 00 00       	callq  547 <no_context+0x5>
 547:	55                   	push   %rbp
 548:	48 89 e5             	mov    %rsp,%rbp
 54b:	41 57                	push   %r15
 54d:	49 89 d7             	mov    %rdx,%r15
 550:	41 56                	push   %r14
 552:	65 4c 8b 34 25 00 00 	mov    %gs:0x0,%r14
 559:	00 00 
 55b:	41 55                	push   %r13
 55d:	41 89 cd             	mov    %ecx,%r13d
 560:	41 54                	push   %r12
 562:	49 89 f4             	mov    %rsi,%r12
 565:	53                   	push   %rbx
 566:	48 89 fb             	mov    %rdi,%rbx
 569:	48 83 ec 10          	sub    $0x10,%rsp
 56d:	44 89 45 cc          	mov    %r8d,-0x34(%rbp)
	unsigned long *stackend;
	unsigned long flags;
	int sig;

	/* Are we prepared to handle this kernel fault? */
	if (fixup_exception(regs)) {
 571:	e8 00 00 00 00       	callq  576 <no_context+0x34>
 576:	85 c0                	test   %eax,%eax
 578:	74 68                	je     5e2 <no_context+0xa0>
 * We mask the PREEMPT_NEED_RESCHED bit so as not to confuse all current users
 * that think a non-zero value indicates we cannot preempt.
 */
static __always_inline int preempt_count(void)
{
	return __this_cpu_read_4(__preempt_count) & ~PREEMPT_NEED_RESCHED;
 57a:	65 8b 04 25 00 00 00 	mov    %gs:0x0,%eax
 581:	00 
		/*
		 * Any interrupt that takes a fault gets the fixup. This makes
		 * the below recursive fault logic only apply to a faults from
		 * task context.
		 */
		if (in_interrupt())
 582:	a9 00 ff 1f 00       	test   $0x1fff00,%eax
 587:	0f 85 33 02 00 00    	jne    7c0 <no_context+0x27e>
 58d:	65 48 8b 04 25 00 00 	mov    %gs:0x0,%rax
 594:	00 00 
		 * Per the above we're !in_interrupt(), aka. task context.
		 *
		 * In this case we need to make sure we're not recursively
		 * faulting through the emulate_vsyscall() logic.
		 */
		if (current_thread_info()->sig_on_uaccess_error && signal) {
 596:	f6 80 88 e0 ff ff 01 	testb  $0x1,-0x1f78(%rax)
 59d:	0f 84 1d 02 00 00    	je     7c0 <no_context+0x27e>
 5a3:	45 85 ed             	test   %r13d,%r13d
 5a6:	0f 84 14 02 00 00    	je     7c0 <no_context+0x27e>
			tsk->thread.trap_nr = X86_TRAP_PF;
			tsk->thread.error_code = error_code | PF_USER;
			tsk->thread.cr2 = address;

			/* XXX: hwpoison faults will set the wrong code. */
			force_sig_info_fault(signal, si_code, address, tsk, 0);
 5ac:	8b 75 cc             	mov    -0x34(%rbp),%esi
		 * In this case we need to make sure we're not recursively
		 * faulting through the emulate_vsyscall() logic.
		 */
		if (current_thread_info()->sig_on_uaccess_error && signal) {
			tsk->thread.trap_nr = X86_TRAP_PF;
			tsk->thread.error_code = error_code | PF_USER;
 5af:	49 83 cc 04          	or     $0x4,%r12
		 *
		 * In this case we need to make sure we're not recursively
		 * faulting through the emulate_vsyscall() logic.
		 */
		if (current_thread_info()->sig_on_uaccess_error && signal) {
			tsk->thread.trap_nr = X86_TRAP_PF;
 5b3:	49 c7 86 78 05 00 00 	movq   $0xe,0x578(%r14)
 5ba:	0e 00 00 00 
			tsk->thread.error_code = error_code | PF_USER;
 5be:	4d 89 a6 80 05 00 00 	mov    %r12,0x580(%r14)
			tsk->thread.cr2 = address;
 5c5:	4d 89 be 70 05 00 00 	mov    %r15,0x570(%r14)

			/* XXX: hwpoison faults will set the wrong code. */
			force_sig_info_fault(signal, si_code, address, tsk, 0);
 5cc:	45 31 c0             	xor    %r8d,%r8d
 5cf:	4c 89 f1             	mov    %r14,%rcx
 5d2:	4c 89 fa             	mov    %r15,%rdx
 5d5:	44 89 ef             	mov    %r13d,%edi
 5d8:	e8 87 fc ff ff       	callq  264 <force_sig_info_fault>
 5dd:	e9 de 01 00 00       	jmpq   7c0 <no_context+0x27e>
	 *
	 * 64-bit:
	 *
	 *   Hall of shame of CPU/BIOS bugs.
	 */
	if (is_prefetch(regs, error_code, address))
 5e2:	4c 89 e6             	mov    %r12,%rsi
 5e5:	48 89 df             	mov    %rbx,%rdi
 5e8:	e8 8e fd ff ff       	callq  37b <is_prefetch.isra.14>
 5ed:	85 c0                	test   %eax,%eax
 5ef:	0f 85 cb 01 00 00    	jne    7c0 <no_context+0x27e>
 * Does nothing on 32-bit.
 */
static int is_errata93(struct pt_regs *regs, unsigned long address)
{
#if defined(CONFIG_X86_64) && defined(CONFIG_CPU_SUP_AMD)
	if (boot_cpu_data.x86_vendor != X86_VENDOR_AMD
 5f5:	66 81 3d 00 00 00 00 	cmpw   $0x20f,0x0(%rip)        # 5fe <no_context+0xbc>
 5fc:	0f 02 
 5fe:	75 6a                	jne    66a <no_context+0x128>
	    || boot_cpu_data.x86 != 0xf)
		return 0;

	if (address != regs->ip)
 600:	4c 39 bb 80 00 00 00 	cmp    %r15,0x80(%rbx)
 607:	75 61                	jne    66a <no_context+0x128>
		return 0;

	if ((address >> 32) != 0)
 609:	4c 89 f8             	mov    %r15,%rax
 60c:	48 c1 e8 20          	shr    $0x20,%rax
 610:	75 58                	jne    66a <no_context+0x128>
		return 0;

	address |= 0xffffffffUL << 32;
 612:	49 bd 00 00 00 00 ff 	movabs $0xffffffff00000000,%r13
 619:	ff ff ff 
 61c:	4d 09 fd             	or     %r15,%r13
	if ((address >= (u64)_stext && address <= (u64)_etext) ||
 61f:	49 81 fd 00 00 00 00 	cmp    $0x0,%r13
 626:	72 09                	jb     631 <no_context+0xef>
 628:	49 81 fd 00 00 00 00 	cmp    $0x0,%r13
 62f:	76 0f                	jbe    640 <no_context+0xfe>
	    (address >= MODULES_VADDR && address <= MODULES_END)) {
 631:	49 8d 85 00 00 00 60 	lea    0x60000000(%r13),%rax

	if ((address >> 32) != 0)
		return 0;

	address |= 0xffffffffUL << 32;
	if ((address >= (u64)_stext && address <= (u64)_etext) ||
 638:	48 3d 00 00 00 5f    	cmp    $0x5f000000,%rax
 63e:	77 2a                	ja     66a <no_context+0x128>
	    (address >= MODULES_VADDR && address <= MODULES_END)) {
		printk_once(errata93_warning);
 640:	80 3d 00 00 00 00 00 	cmpb   $0x0,0x0(%rip)        # 647 <no_context+0x105>
 647:	75 15                	jne    65e <no_context+0x11c>
 649:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
 650:	31 c0                	xor    %eax,%eax
 652:	c6 05 00 00 00 00 01 	movb   $0x1,0x0(%rip)        # 659 <no_context+0x117>
 659:	e8 00 00 00 00       	callq  65e <no_context+0x11c>
		regs->ip = address;
 65e:	4c 89 ab 80 00 00 00 	mov    %r13,0x80(%rbx)
 665:	e9 56 01 00 00       	jmpq   7c0 <no_context+0x27e>

	/*
	 * Oops. The kernel tried to access some bad page. We'll have to
	 * terminate things with extreme prejudice:
	 */
	flags = oops_begin();
 66a:	e8 00 00 00 00       	callq  66f <no_context+0x12d>
 66f:	49 89 c5             	mov    %rax,%r13

static void
show_fault_oops(struct pt_regs *regs, unsigned long error_code,
		unsigned long address)
{
	if (!oops_may_print())
 672:	e8 00 00 00 00       	callq  677 <no_context+0x135>
 677:	85 c0                	test   %eax,%eax
 679:	0f 84 c8 00 00 00    	je     747 <no_context+0x205>
		return;

	if (error_code & PF_INSTR) {
 67f:	41 f6 c4 10          	test   $0x10,%r12b
 683:	74 5a                	je     6df <no_context+0x19d>
		unsigned int level;

		pte_t *pte = lookup_address(address, &level);
 685:	48 8d 75 d4          	lea    -0x2c(%rbp),%rsi
 689:	4c 89 ff             	mov    %r15,%rdi
 68c:	e8 00 00 00 00       	callq  691 <no_context+0x14f>

		if (pte && pte_present(*pte) && !pte_exec(*pte))
 691:	48 85 c0             	test   %rax,%rax
 694:	74 49                	je     6df <no_context+0x19d>
 696:	48 ba ff 0f 00 00 00 	movabs $0xffffc00000000fff,%rdx
 69d:	c0 ff ff 
 6a0:	48 23 10             	and    (%rax),%rdx
 6a3:	f7 c2 01 01 00 00    	test   $0x101,%edx
 6a9:	74 34                	je     6df <no_context+0x19d>
 6ab:	48 85 d2             	test   %rdx,%rdx
 6ae:	79 2f                	jns    6df <no_context+0x19d>
 6b0:	65 48 8b 04 25 00 00 	mov    %gs:0x0,%rax
 6b7:	00 00 
			printk(nx_warning, from_kuid(&init_user_ns, current_uid()));
 6b9:	48 8b 80 c8 04 00 00 	mov    0x4c8(%rax),%rax
 6c0:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
 6c7:	8b 70 04             	mov    0x4(%rax),%esi
 6ca:	e8 00 00 00 00       	callq  6cf <no_context+0x18d>
 6cf:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
 6d6:	89 c6                	mov    %eax,%esi
 6d8:	31 c0                	xor    %eax,%eax
 6da:	e8 00 00 00 00       	callq  6df <no_context+0x19d>
	}

	printk(KERN_ALERT "BUG: unable to handle kernel ");
 6df:	31 c0                	xor    %eax,%eax
 6e1:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
 6e8:	e8 00 00 00 00       	callq  6ed <no_context+0x1ab>
	if (address < PAGE_SIZE)
 6ed:	49 81 ff ff 0f 00 00 	cmp    $0xfff,%r15
 6f4:	77 10                	ja     706 <no_context+0x1c4>
		printk(KERN_CONT "NULL pointer dereference");
 6f6:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
 6fd:	31 c0                	xor    %eax,%eax
 6ff:	e8 00 00 00 00       	callq  704 <no_context+0x1c2>
 704:	eb 0e                	jmp    714 <no_context+0x1d2>
	else
		printk(KERN_CONT "paging request");
 706:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
 70d:	31 c0                	xor    %eax,%eax
 70f:	e8 00 00 00 00       	callq  714 <no_context+0x1d2>

	printk(KERN_CONT " at %p\n", (void *) address);
 714:	4c 89 fe             	mov    %r15,%rsi
 717:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
 71e:	31 c0                	xor    %eax,%eax
 720:	e8 00 00 00 00       	callq  725 <no_context+0x1e3>
	printk(KERN_ALERT "IP:");
 725:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
 72c:	31 c0                	xor    %eax,%eax
 72e:	e8 00 00 00 00       	callq  733 <no_context+0x1f1>
	printk_address(regs->ip);
 733:	48 8b bb 80 00 00 00 	mov    0x80(%rbx),%rdi
 73a:	e8 00 00 00 00       	callq  73f <no_context+0x1fd>

	dump_pagetable(address);
 73f:	4c 89 ff             	mov    %r15,%rdi
 742:	e8 a6 f9 ff ff       	callq  ed <dump_pagetable>
	flags = oops_begin();

	show_fault_oops(regs, error_code, address);

	stackend = end_of_stack(tsk);
	if (tsk != &init_task && *stackend != STACK_END_MAGIC)
 747:	49 81 fe 00 00 00 00 	cmp    $0x0,%r14
	task_thread_info(p)->task = p;
}

static inline unsigned long *end_of_stack(struct task_struct *p)
{
	return (unsigned long *)(task_thread_info(p) + 1);
 74e:	49 8b 46 08          	mov    0x8(%r14),%rax
 752:	74 18                	je     76c <no_context+0x22a>
 754:	48 81 78 68 9d 6e ac 	cmpq   $0x57ac6e9d,0x68(%rax)
 75b:	57 
 75c:	74 0e                	je     76c <no_context+0x22a>
		printk(KERN_EMERG "Thread overran stack, or stack corrupted\n");
 75e:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
 765:	31 c0                	xor    %eax,%eax
 767:	e8 00 00 00 00       	callq  76c <no_context+0x22a>
	tsk->thread.cr2		= address;
	tsk->thread.trap_nr	= X86_TRAP_PF;
	tsk->thread.error_code	= error_code;

	sig = SIGKILL;
	if (__die("Oops", regs, error_code))
 76c:	4c 89 e2             	mov    %r12,%rdx
	if (tsk != &init_task && *stackend != STACK_END_MAGIC)
		printk(KERN_EMERG "Thread overran stack, or stack corrupted\n");

	tsk->thread.cr2		= address;
	tsk->thread.trap_nr	= X86_TRAP_PF;
	tsk->thread.error_code	= error_code;
 76f:	4d 89 a6 80 05 00 00 	mov    %r12,0x580(%r14)

	sig = SIGKILL;
	if (__die("Oops", regs, error_code))
 776:	48 89 de             	mov    %rbx,%rsi
 779:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi

	stackend = end_of_stack(tsk);
	if (tsk != &init_task && *stackend != STACK_END_MAGIC)
		printk(KERN_EMERG "Thread overran stack, or stack corrupted\n");

	tsk->thread.cr2		= address;
 780:	4d 89 be 70 05 00 00 	mov    %r15,0x570(%r14)
	tsk->thread.trap_nr	= X86_TRAP_PF;
 787:	49 c7 86 78 05 00 00 	movq   $0xe,0x578(%r14)
 78e:	0e 00 00 00 
	tsk->thread.error_code	= error_code;

	sig = SIGKILL;
	if (__die("Oops", regs, error_code))
 792:	e8 00 00 00 00       	callq  797 <no_context+0x255>

	tsk->thread.cr2		= address;
	tsk->thread.trap_nr	= X86_TRAP_PF;
	tsk->thread.error_code	= error_code;

	sig = SIGKILL;
 797:	83 f8 01             	cmp    $0x1,%eax
	if (__die("Oops", regs, error_code))
		sig = 0;

	/* Executive summary in case the body of the oops scrolled away */
	printk(KERN_DEFAULT "CR2: %016lx\n", address);
 79a:	4c 89 fe             	mov    %r15,%rsi
 79d:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi

	tsk->thread.cr2		= address;
	tsk->thread.trap_nr	= X86_TRAP_PF;
	tsk->thread.error_code	= error_code;

	sig = SIGKILL;
 7a4:	45 19 e4             	sbb    %r12d,%r12d
	if (__die("Oops", regs, error_code))
		sig = 0;

	/* Executive summary in case the body of the oops scrolled away */
	printk(KERN_DEFAULT "CR2: %016lx\n", address);
 7a7:	31 c0                	xor    %eax,%eax

	tsk->thread.cr2		= address;
	tsk->thread.trap_nr	= X86_TRAP_PF;
	tsk->thread.error_code	= error_code;

	sig = SIGKILL;
 7a9:	41 83 e4 09          	and    $0x9,%r12d
	if (__die("Oops", regs, error_code))
		sig = 0;

	/* Executive summary in case the body of the oops scrolled away */
	printk(KERN_DEFAULT "CR2: %016lx\n", address);
 7ad:	e8 00 00 00 00       	callq  7b2 <no_context+0x270>

	oops_end(flags, regs, sig);
 7b2:	44 89 e2             	mov    %r12d,%edx
 7b5:	48 89 de             	mov    %rbx,%rsi
 7b8:	4c 89 ef             	mov    %r13,%rdi
 7bb:	e8 00 00 00 00       	callq  7c0 <no_context+0x27e>
}
 7c0:	58                   	pop    %rax
 7c1:	5a                   	pop    %rdx
 7c2:	5b                   	pop    %rbx
 7c3:	41 5c                	pop    %r12
 7c5:	41 5d                	pop    %r13
 7c7:	41 5e                	pop    %r14
 7c9:	41 5f                	pop    %r15
 7cb:	5d                   	pop    %rbp
 7cc:	c3                   	retq   

00000000000007cd <__bad_area_nosemaphore>:
}

static void
__bad_area_nosemaphore(struct pt_regs *regs, unsigned long error_code,
		       unsigned long address, int si_code)
{
 7cd:	e8 00 00 00 00       	callq  7d2 <__bad_area_nosemaphore+0x5>
 7d2:	55                   	push   %rbp
 7d3:	48 89 e5             	mov    %rsp,%rbp
 7d6:	41 57                	push   %r15
 7d8:	41 89 cf             	mov    %ecx,%r15d
 7db:	41 56                	push   %r14
 7dd:	65 4c 8b 34 25 00 00 	mov    %gs:0x0,%r14
 7e4:	00 00 
 7e6:	41 55                	push   %r13
 7e8:	49 89 d5             	mov    %rdx,%r13
 7eb:	41 54                	push   %r12
 7ed:	49 89 fc             	mov    %rdi,%r12
 7f0:	53                   	push   %rbx
 7f1:	48 89 f3             	mov    %rsi,%rbx
 7f4:	48 83 ec 10          	sub    $0x10,%rsp
	struct task_struct *tsk = current;

	/* User mode accesses just cause a SIGSEGV */
	if (error_code & PF_USER) {
 7f8:	40 f6 c6 04          	test   $0x4,%sil
 7fc:	74 35                	je     833 <__bad_area_nosemaphore+0x66>
	PVOP_VCALLEE0(pv_irq_ops.irq_disable);
}

static inline notrace void arch_local_irq_enable(void)
{
	PVOP_VCALLEE0(pv_irq_ops.irq_enable);
 7fe:	ff 14 25 00 00 00 00 	callq  *0x0

		/*
		 * Valid to do another page fault here because this one came
		 * from user space:
		 */
		if (is_prefetch(regs, error_code, address))
 805:	e8 71 fb ff ff       	callq  37b <is_prefetch.isra.14>
 80a:	85 c0                	test   %eax,%eax
 80c:	0f 85 78 01 00 00    	jne    98a <__bad_area_nosemaphore+0x1bd>

	printk(KERN_CONT "\n");
}

static void
__bad_area_nosemaphore(struct pt_regs *regs, unsigned long error_code,
 812:	49 8b 84 24 88 00 00 	mov    0x88(%r12),%rax
 819:	00 
 * segment in LDT is compatibility mode.
 */
static int is_errata100(struct pt_regs *regs, unsigned long address)
{
#ifdef CONFIG_X86_64
	if ((regs->cs == __USER32_CS || (regs->cs & (1<<2))) && (address >> 32))
 81a:	48 83 f8 23          	cmp    $0x23,%rax
 81e:	74 04                	je     824 <__bad_area_nosemaphore+0x57>
 820:	a8 04                	test   $0x4,%al
 822:	74 21                	je     845 <__bad_area_nosemaphore+0x78>
 824:	4c 89 e8             	mov    %r13,%rax
 827:	48 c1 e8 20          	shr    $0x20,%rax
 82b:	0f 85 59 01 00 00    	jne    98a <__bad_area_nosemaphore+0x1bd>
 831:	eb 12                	jmp    845 <__bad_area_nosemaphore+0x78>
	}

	if (is_f00f_bug(regs, address))
		return;

	no_context(regs, error_code, address, SIGSEGV, si_code);
 833:	41 89 c8             	mov    %ecx,%r8d
 836:	b9 0b 00 00 00       	mov    $0xb,%ecx
 83b:	e8 02 fd ff ff       	callq  542 <no_context>
 840:	e9 45 01 00 00       	jmpq   98a <__bad_area_nosemaphore+0x1bd>
#ifdef CONFIG_X86_64
		/*
		 * Instruction fetch faults in the vsyscall page might need
		 * emulation.
		 */
		if (unlikely((error_code & PF_INSTR) &&
 845:	f6 c3 10             	test   $0x10,%bl
 848:	74 11                	je     85b <__bad_area_nosemaphore+0x8e>
 84a:	4c 89 e8             	mov    %r13,%rax
 84d:	48 25 00 f0 ff ff    	and    $0xfffffffffffff000,%rax
 853:	48 3d 00 00 60 ff    	cmp    $0xffffffffff600000,%rax
 859:	74 23                	je     87e <__bad_area_nosemaphore+0xb1>
 85b:	65 48 8b 04 25 00 00 	mov    %gs:0x0,%rax
 862:	00 00 
 864:	48 8b 80 38 e0 ff ff 	mov    -0x1fc8(%rax),%rax
			if (emulate_vsyscall(regs, address))
				return;
		}
#endif
		/* Kernel addresses are always protection faults: */
		if (address >= TASK_SIZE)
 86b:	a9 00 00 00 20       	test   $0x20000000,%eax
 870:	75 20                	jne    892 <__bad_area_nosemaphore+0xc5>
 872:	48 b8 00 f0 ff ff ff 	movabs $0x7ffffffff000,%rax
 879:	7f 00 00 
 87c:	eb 32                	jmp    8b0 <__bad_area_nosemaphore+0xe3>
		 * Instruction fetch faults in the vsyscall page might need
		 * emulation.
		 */
		if (unlikely((error_code & PF_INSTR) &&
			     ((address & ~0xfff) == VSYSCALL_START))) {
			if (emulate_vsyscall(regs, address))
 87e:	4c 89 ee             	mov    %r13,%rsi
 881:	4c 89 e7             	mov    %r12,%rdi
 884:	e8 00 00 00 00       	callq  889 <__bad_area_nosemaphore+0xbc>
 889:	84 c0                	test   %al,%al
 88b:	74 ce                	je     85b <__bad_area_nosemaphore+0x8e>
 88d:	e9 f8 00 00 00       	jmpq   98a <__bad_area_nosemaphore+0x1bd>
 892:	65 48 8b 14 25 00 00 	mov    %gs:0x0,%rdx
 899:	00 00 
				return;
		}
#endif
		/* Kernel addresses are always protection faults: */
		if (address >= TASK_SIZE)
 89b:	f6 82 df 02 00 00 08 	testb  $0x8,0x2df(%rdx)
 8a2:	b8 00 00 00 c0       	mov    $0xc0000000,%eax
 8a7:	ba 00 e0 ff ff       	mov    $0xffffe000,%edx
 8ac:	48 0f 44 c2          	cmove  %rdx,%rax
			error_code |= PF_PROT;
 8b0:	48 89 da             	mov    %rbx,%rdx
 8b3:	48 83 ca 01          	or     $0x1,%rdx
 8b7:	4c 39 e8             	cmp    %r13,%rax
 8ba:	48 0f 46 da          	cmovbe %rdx,%rbx

		if (likely(show_unhandled_signals))
 8be:	83 3d 00 00 00 00 00 	cmpl   $0x0,0x0(%rip)        # 8c5 <__bad_area_nosemaphore+0xf8>
 8c5:	0f 84 90 00 00 00    	je     95b <__bad_area_nosemaphore+0x18e>
 */
static inline void
show_signal_msg(struct pt_regs *regs, unsigned long error_code,
		unsigned long address, struct task_struct *tsk)
{
	if (!unhandled_signal(tsk, SIGSEGV))
 8cb:	be 0b 00 00 00       	mov    $0xb,%esi
 8d0:	4c 89 f7             	mov    %r14,%rdi
 8d3:	e8 00 00 00 00       	callq  8d8 <__bad_area_nosemaphore+0x10b>
 8d8:	85 c0                	test   %eax,%eax
 8da:	74 7f                	je     95b <__bad_area_nosemaphore+0x18e>
		return;

	if (!printk_ratelimit())
 8dc:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
 8e3:	e8 00 00 00 00       	callq  8e8 <__bad_area_nosemaphore+0x11b>
 8e8:	85 c0                	test   %eax,%eax
 8ea:	74 6f                	je     95b <__bad_area_nosemaphore+0x18e>
pid_t __task_pid_nr_ns(struct task_struct *task, enum pid_type type,
			struct pid_namespace *ns);

static inline pid_t task_pid_nr(struct task_struct *tsk)
{
	return tsk->pid;
 8ec:	41 8b 8e e4 02 00 00 	mov    0x2e4(%r14),%ecx
		return;

	printk("%s%s[%d]: segfault at %lx ip %p sp %p error %lx",
 8f3:	49 8b 84 24 98 00 00 	mov    0x98(%r12),%rax
 8fa:	00 
		task_pid_nr(tsk) > 1 ? KERN_INFO : KERN_EMERG,
		tsk->comm, task_pid_nr(tsk), address,
 8fb:	49 8d 96 d0 04 00 00 	lea    0x4d0(%r14),%rdx
		return;

	if (!printk_ratelimit())
		return;

	printk("%s%s[%d]: segfault at %lx ip %p sp %p error %lx",
 902:	4d 8b 8c 24 80 00 00 	mov    0x80(%r12),%r9
 909:	00 
 90a:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
 911:	48 c7 c6 00 00 00 00 	mov    $0x0,%rsi
 918:	4d 89 e8             	mov    %r13,%r8
 91b:	48 89 5c 24 08       	mov    %rbx,0x8(%rsp)
 920:	83 f9 02             	cmp    $0x2,%ecx
 923:	48 89 04 24          	mov    %rax,(%rsp)
 927:	48 0f 4c f7          	cmovl  %rdi,%rsi
 92b:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
 932:	31 c0                	xor    %eax,%eax
 934:	e8 00 00 00 00       	callq  939 <__bad_area_nosemaphore+0x16c>
		task_pid_nr(tsk) > 1 ? KERN_INFO : KERN_EMERG,
		tsk->comm, task_pid_nr(tsk), address,
		(void *)regs->ip, (void *)regs->sp, error_code);

	print_vma_addr(KERN_CONT " in ", regs->ip);
 939:	49 8b b4 24 80 00 00 	mov    0x80(%r12),%rsi
 940:	00 
 941:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
 948:	e8 00 00 00 00       	callq  94d <__bad_area_nosemaphore+0x180>

	printk(KERN_CONT "\n");
 94d:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
 954:	31 c0                	xor    %eax,%eax
 956:	e8 00 00 00 00       	callq  95b <__bad_area_nosemaphore+0x18e>
			error_code |= PF_PROT;

		if (likely(show_unhandled_signals))
			show_signal_msg(regs, error_code, address, tsk);

		tsk->thread.cr2		= address;
 95b:	4d 89 ae 70 05 00 00 	mov    %r13,0x570(%r14)
		tsk->thread.error_code	= error_code;
 962:	49 89 9e 80 05 00 00 	mov    %rbx,0x580(%r14)
		tsk->thread.trap_nr	= X86_TRAP_PF;

		force_sig_info_fault(SIGSEGV, si_code, address, tsk, 0);
 969:	45 31 c0             	xor    %r8d,%r8d
		if (likely(show_unhandled_signals))
			show_signal_msg(regs, error_code, address, tsk);

		tsk->thread.cr2		= address;
		tsk->thread.error_code	= error_code;
		tsk->thread.trap_nr	= X86_TRAP_PF;
 96c:	49 c7 86 78 05 00 00 	movq   $0xe,0x578(%r14)
 973:	0e 00 00 00 

		force_sig_info_fault(SIGSEGV, si_code, address, tsk, 0);
 977:	4c 89 f1             	mov    %r14,%rcx
 97a:	4c 89 ea             	mov    %r13,%rdx
 97d:	44 89 fe             	mov    %r15d,%esi
 980:	bf 0b 00 00 00       	mov    $0xb,%edi
 985:	e8 da f8 ff ff       	callq  264 <force_sig_info_fault>

	if (is_f00f_bug(regs, address))
		return;

	no_context(regs, error_code, address, SIGSEGV, si_code);
}
 98a:	58                   	pop    %rax
 98b:	5a                   	pop    %rdx
 98c:	5b                   	pop    %rbx
 98d:	41 5c                	pop    %r12
 98f:	41 5d                	pop    %r13
 991:	41 5e                	pop    %r14
 993:	41 5f                	pop    %r15
 995:	5d                   	pop    %rbp
 996:	c3                   	retq   

0000000000000997 <bad_area_nosemaphore>:

static noinline void
bad_area_nosemaphore(struct pt_regs *regs, unsigned long error_code,
		     unsigned long address)
{
 997:	e8 00 00 00 00       	callq  99c <bad_area_nosemaphore+0x5>
 99c:	55                   	push   %rbp
	__bad_area_nosemaphore(regs, error_code, address, SEGV_MAPERR);
 99d:	b9 01 00 03 00       	mov    $0x30001,%ecx
}

static noinline void
bad_area_nosemaphore(struct pt_regs *regs, unsigned long error_code,
		     unsigned long address)
{
 9a2:	48 89 e5             	mov    %rsp,%rbp
	__bad_area_nosemaphore(regs, error_code, address, SEGV_MAPERR);
 9a5:	e8 23 fe ff ff       	callq  7cd <__bad_area_nosemaphore>
}
 9aa:	5d                   	pop    %rbp
 9ab:	c3                   	retq   

00000000000009ac <bad_area_access_error>:
}

static noinline void
bad_area_access_error(struct pt_regs *regs, unsigned long error_code,
		      unsigned long address)
{
 9ac:	e8 00 00 00 00       	callq  9b1 <bad_area_access_error+0x5>
 9b1:	55                   	push   %rbp
 9b2:	65 48 8b 04 25 00 00 	mov    %gs:0x0,%rax
 9b9:	00 00 
 9bb:	48 89 e5             	mov    %rsp,%rbp
 9be:	41 55                	push   %r13
 9c0:	49 89 d5             	mov    %rdx,%r13
 9c3:	41 54                	push   %r12
 9c5:	49 89 f4             	mov    %rsi,%r12
 9c8:	53                   	push   %rbx
 9c9:	48 89 fb             	mov    %rdi,%rbx

	/*
	 * Something tried to access memory that isn't in our memory map..
	 * Fix it, but check if it's kernel or user first..
	 */
	up_read(&mm->mmap_sem);
 9cc:	48 8b b8 a8 02 00 00 	mov    0x2a8(%rax),%rdi
 9d3:	48 83 c7 78          	add    $0x78,%rdi
 9d7:	e8 00 00 00 00       	callq  9dc <bad_area_access_error+0x30>

	__bad_area_nosemaphore(regs, error_code, address, si_code);
 9dc:	4c 89 ea             	mov    %r13,%rdx
 9df:	4c 89 e6             	mov    %r12,%rsi
 9e2:	48 89 df             	mov    %rbx,%rdi
 9e5:	b9 02 00 03 00       	mov    $0x30002,%ecx
 9ea:	e8 de fd ff ff       	callq  7cd <__bad_area_nosemaphore>
static noinline void
bad_area_access_error(struct pt_regs *regs, unsigned long error_code,
		      unsigned long address)
{
	__bad_area(regs, error_code, address, SEGV_ACCERR);
}
 9ef:	5b                   	pop    %rbx
 9f0:	41 5c                	pop    %r12
 9f2:	41 5d                	pop    %r13
 9f4:	5d                   	pop    %rbp
 9f5:	c3                   	retq   

00000000000009f6 <mm_fault_error>:
}

static noinline void
mm_fault_error(struct pt_regs *regs, unsigned long error_code,
	       unsigned long address, unsigned int fault)
{
 9f6:	e8 00 00 00 00       	callq  9fb <mm_fault_error+0x5>
 9fb:	55                   	push   %rbp
 9fc:	65 48 8b 04 25 00 00 	mov    %gs:0x0,%rax
 a03:	00 00 
 a05:	48 89 e5             	mov    %rsp,%rbp
 a08:	41 57                	push   %r15
 a0a:	41 56                	push   %r14
 a0c:	49 89 fe             	mov    %rdi,%r14
 a0f:	41 55                	push   %r13
 a11:	49 89 d5             	mov    %rdx,%r13
 a14:	41 54                	push   %r12
 a16:	41 89 cc             	mov    %ecx,%r12d
 a19:	53                   	push   %rbx
	return test_and_clear_ti_thread_flag(task_thread_info(tsk), flag);
}

static inline int test_tsk_thread_flag(struct task_struct *tsk, int flag)
{
	return test_ti_thread_flag(task_thread_info(tsk), flag);
 a1a:	48 8b 50 08          	mov    0x8(%rax),%rdx
 a1e:	48 89 f3             	mov    %rsi,%rbx
 a21:	48 8b 72 10          	mov    0x10(%rdx),%rsi
	return unlikely(sigismember(&p->pending.signal, SIGKILL));
}

static inline int fatal_signal_pending(struct task_struct *p)
{
	return signal_pending(p) && __fatal_signal_pending(p);
 a25:	40 80 e6 04          	and    $0x4,%sil
 a29:	74 0d                	je     a38 <mm_fault_error+0x42>
 a2b:	f6 80 01 06 00 00 01 	testb  $0x1,0x601(%rax)
 a32:	0f 85 07 01 00 00    	jne    b3f <mm_fault_error+0x149>
		up_read(&current->mm->mmap_sem);
		no_context(regs, error_code, address, 0, 0);
		return;
	}

	if (fault & VM_FAULT_OOM) {
 a38:	41 f6 c4 01          	test   $0x1,%r12b
 a3c:	74 4b                	je     a89 <mm_fault_error+0x93>
 a3e:	65 48 8b 04 25 00 00 	mov    %gs:0x0,%rax
 a45:	00 00 
		/* Kernel mode? Handle exceptions or die: */
		if (!(error_code & PF_USER)) {
			up_read(&current->mm->mmap_sem);
 a47:	48 8b b8 a8 02 00 00 	mov    0x2a8(%rax),%rdi
 a4e:	48 83 c7 78          	add    $0x78,%rdi
		return;
	}

	if (fault & VM_FAULT_OOM) {
		/* Kernel mode? Handle exceptions or die: */
		if (!(error_code & PF_USER)) {
 a52:	f6 c3 04             	test   $0x4,%bl
 a55:	75 23                	jne    a7a <mm_fault_error+0x84>
			up_read(&current->mm->mmap_sem);
 a57:	e8 00 00 00 00       	callq  a5c <mm_fault_error+0x66>
			no_context(regs, error_code, address,
 a5c:	41 b8 01 00 03 00    	mov    $0x30001,%r8d
 a62:	b9 0b 00 00 00       	mov    $0xb,%ecx
 a67:	4c 89 ea             	mov    %r13,%rdx
 a6a:	48 89 de             	mov    %rbx,%rsi
 a6d:	4c 89 f7             	mov    %r14,%rdi
 a70:	e8 cd fa ff ff       	callq  542 <no_context>
				   SIGSEGV, SEGV_MAPERR);
			return;
 a75:	e9 f1 00 00 00       	jmpq   b6b <mm_fault_error+0x175>
		}

		up_read(&current->mm->mmap_sem);
 a7a:	e8 00 00 00 00       	callq  a7f <mm_fault_error+0x89>
		/*
		 * We ran out of memory, call the OOM killer, and return the
		 * userspace (which will retry the fault, or kill us if we got
		 * oom-killed):
		 */
		pagefault_out_of_memory();
 a7f:	e8 00 00 00 00       	callq  a84 <mm_fault_error+0x8e>
 a84:	e9 e2 00 00 00       	jmpq   b6b <mm_fault_error+0x175>
	} else {
		if (fault & (VM_FAULT_SIGBUS|VM_FAULT_HWPOISON|
 a89:	41 f6 c4 32          	test   $0x32,%r12b
 a8d:	0f 84 aa 00 00 00    	je     b3d <mm_fault_error+0x147>
 a93:	65 4c 8b 3c 25 00 00 	mov    %gs:0x0,%r15
 a9a:	00 00 
{
	struct task_struct *tsk = current;
	struct mm_struct *mm = tsk->mm;
	int code = BUS_ADRERR;

	up_read(&mm->mmap_sem);
 a9c:	49 8b 87 a8 02 00 00 	mov    0x2a8(%r15),%rax
 aa3:	48 8d 78 78          	lea    0x78(%rax),%rdi
 aa7:	e8 00 00 00 00       	callq  aac <mm_fault_error+0xb6>

	/* Kernel mode? Handle exceptions or die: */
	if (!(error_code & PF_USER)) {
 aac:	f6 c3 04             	test   $0x4,%bl
 aaf:	75 1e                	jne    acf <mm_fault_error+0xd9>
		no_context(regs, error_code, address, SIGBUS, BUS_ADRERR);
 ab1:	41 b8 02 00 03 00    	mov    $0x30002,%r8d
 ab7:	b9 07 00 00 00       	mov    $0x7,%ecx
 abc:	4c 89 ea             	mov    %r13,%rdx
 abf:	48 89 de             	mov    %rbx,%rsi
 ac2:	4c 89 f7             	mov    %r14,%rdi
 ac5:	e8 78 fa ff ff       	callq  542 <no_context>
 aca:	e9 9c 00 00 00       	jmpq   b6b <mm_fault_error+0x175>
		return;
	}

	/* User-space => ok to do another page fault: */
	if (is_prefetch(regs, error_code, address))
 acf:	48 89 de             	mov    %rbx,%rsi
 ad2:	4c 89 f7             	mov    %r14,%rdi
 ad5:	e8 a1 f8 ff ff       	callq  37b <is_prefetch.isra.14>
 ada:	85 c0                	test   %eax,%eax
 adc:	0f 85 89 00 00 00    	jne    b6b <mm_fault_error+0x175>
	tsk->thread.cr2		= address;
	tsk->thread.error_code	= error_code;
	tsk->thread.trap_nr	= X86_TRAP_PF;

#ifdef CONFIG_MEMORY_FAILURE
	if (fault & (VM_FAULT_HWPOISON|VM_FAULT_HWPOISON_LARGE)) {
 ae2:	41 f6 c4 30          	test   $0x30,%r12b

	/* User-space => ok to do another page fault: */
	if (is_prefetch(regs, error_code, address))
		return;

	tsk->thread.cr2		= address;
 ae6:	4d 89 af 70 05 00 00 	mov    %r13,0x570(%r15)
	tsk->thread.error_code	= error_code;
 aed:	49 89 9f 80 05 00 00 	mov    %rbx,0x580(%r15)
	tsk->thread.trap_nr	= X86_TRAP_PF;
 af4:	49 c7 87 78 05 00 00 	movq   $0xe,0x578(%r15)
 afb:	0e 00 00 00 
do_sigbus(struct pt_regs *regs, unsigned long error_code, unsigned long address,
	  unsigned int fault)
{
	struct task_struct *tsk = current;
	struct mm_struct *mm = tsk->mm;
	int code = BUS_ADRERR;
 aff:	be 02 00 03 00       	mov    $0x30002,%esi
	tsk->thread.cr2		= address;
	tsk->thread.error_code	= error_code;
	tsk->thread.trap_nr	= X86_TRAP_PF;

#ifdef CONFIG_MEMORY_FAILURE
	if (fault & (VM_FAULT_HWPOISON|VM_FAULT_HWPOISON_LARGE)) {
 b04:	74 22                	je     b28 <mm_fault_error+0x132>
		printk(KERN_ERR
 b06:	41 8b 97 e4 02 00 00 	mov    0x2e4(%r15),%edx
	"MCE: Killing %s:%d due to hardware memory corruption fault at %lx\n",
			tsk->comm, tsk->pid, address);
 b0d:	49 8d b7 d0 04 00 00 	lea    0x4d0(%r15),%rsi
	tsk->thread.error_code	= error_code;
	tsk->thread.trap_nr	= X86_TRAP_PF;

#ifdef CONFIG_MEMORY_FAILURE
	if (fault & (VM_FAULT_HWPOISON|VM_FAULT_HWPOISON_LARGE)) {
		printk(KERN_ERR
 b14:	4c 89 e9             	mov    %r13,%rcx
 b17:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
 b1e:	e8 00 00 00 00       	callq  b23 <mm_fault_error+0x12d>
	"MCE: Killing %s:%d due to hardware memory corruption fault at %lx\n",
			tsk->comm, tsk->pid, address);
		code = BUS_MCEERR_AR;
 b23:	be 04 00 03 00       	mov    $0x30004,%esi
	}
#endif
	force_sig_info_fault(SIGBUS, code, address, tsk, fault);
 b28:	45 89 e0             	mov    %r12d,%r8d
 b2b:	4c 89 f9             	mov    %r15,%rcx
 b2e:	4c 89 ea             	mov    %r13,%rdx
 b31:	bf 07 00 00 00       	mov    $0x7,%edi
 b36:	e8 29 f7 ff ff       	callq  264 <force_sig_info_fault>
 b3b:	eb 2e                	jmp    b6b <mm_fault_error+0x175>
	} else {
		if (fault & (VM_FAULT_SIGBUS|VM_FAULT_HWPOISON|
			     VM_FAULT_HWPOISON_LARGE))
			do_sigbus(regs, error_code, address, fault);
		else
			BUG();
 b3d:	0f 0b                	ud2    

static noinline void
mm_fault_error(struct pt_regs *regs, unsigned long error_code,
	       unsigned long address, unsigned int fault)
{
	if (fatal_signal_pending(current) && !(error_code & PF_USER)) {
 b3f:	f6 c3 04             	test   $0x4,%bl
 b42:	0f 85 f0 fe ff ff    	jne    a38 <mm_fault_error+0x42>
		up_read(&current->mm->mmap_sem);
 b48:	48 8b b8 a8 02 00 00 	mov    0x2a8(%rax),%rdi
 b4f:	48 83 c7 78          	add    $0x78,%rdi
 b53:	e8 00 00 00 00       	callq  b58 <mm_fault_error+0x162>
		no_context(regs, error_code, address, 0, 0);
 b58:	45 31 c0             	xor    %r8d,%r8d
 b5b:	31 c9                	xor    %ecx,%ecx
 b5d:	4c 89 ea             	mov    %r13,%rdx
 b60:	48 89 de             	mov    %rbx,%rsi
 b63:	4c 89 f7             	mov    %r14,%rdi
 b66:	e8 d7 f9 ff ff       	callq  542 <no_context>
			     VM_FAULT_HWPOISON_LARGE))
			do_sigbus(regs, error_code, address, fault);
		else
			BUG();
	}
}
 b6b:	5b                   	pop    %rbx
 b6c:	41 5c                	pop    %r12
 b6e:	41 5d                	pop    %r13
 b70:	41 5e                	pop    %r14
 b72:	41 5f                	pop    %r15
 b74:	5d                   	pop    %rbp
 b75:	c3                   	retq   

0000000000000b76 <bad_area>:
	__bad_area_nosemaphore(regs, error_code, address, si_code);
}

static noinline void
bad_area(struct pt_regs *regs, unsigned long error_code, unsigned long address)
{
 b76:	e8 00 00 00 00       	callq  b7b <bad_area+0x5>
 b7b:	55                   	push   %rbp
 b7c:	65 48 8b 04 25 00 00 	mov    %gs:0x0,%rax
 b83:	00 00 
 b85:	48 89 e5             	mov    %rsp,%rbp
 b88:	41 55                	push   %r13
 b8a:	49 89 d5             	mov    %rdx,%r13
 b8d:	41 54                	push   %r12
 b8f:	49 89 f4             	mov    %rsi,%r12
 b92:	53                   	push   %rbx
 b93:	48 89 fb             	mov    %rdi,%rbx

	/*
	 * Something tried to access memory that isn't in our memory map..
	 * Fix it, but check if it's kernel or user first..
	 */
	up_read(&mm->mmap_sem);
 b96:	48 8b b8 a8 02 00 00 	mov    0x2a8(%rax),%rdi
 b9d:	48 83 c7 78          	add    $0x78,%rdi
 ba1:	e8 00 00 00 00       	callq  ba6 <bad_area+0x30>

	__bad_area_nosemaphore(regs, error_code, address, si_code);
 ba6:	4c 89 ea             	mov    %r13,%rdx
 ba9:	4c 89 e6             	mov    %r12,%rsi
 bac:	48 89 df             	mov    %rbx,%rdi
 baf:	b9 01 00 03 00       	mov    $0x30001,%ecx
 bb4:	e8 14 fc ff ff       	callq  7cd <__bad_area_nosemaphore>

static noinline void
bad_area(struct pt_regs *regs, unsigned long error_code, unsigned long address)
{
	__bad_area(regs, error_code, address, SEGV_MAPERR);
}
 bb9:	5b                   	pop    %rbx
 bba:	41 5c                	pop    %r12
 bbc:	41 5d                	pop    %r13
 bbe:	5d                   	pop    %rbp
 bbf:	c3                   	retq   

Disassembly of section .altinstr_replacement:

0000000000000000 <.altinstr_replacement>:
   0:	0f 01 cb             	stac   
   3:	0f 01 ca             	clac   
   6:	0f 01 cb             	stac   
   9:	0f 01 ca             	clac   
   c:	0f 01 cb             	stac   
   f:	0f 01 ca             	clac   
  12:	0f 0d 08             	prefetchw (%rax)

Disassembly of section .fixup:

0000000000000000 <.fixup>:
   0:	b8 08 00 00 00       	mov    $0x8,%eax
   5:	48 31 f6             	xor    %rsi,%rsi
   8:	e9 00 00 00 00       	jmpq   d <.fixup+0xd>
   d:	ba 01 00 00 00       	mov    $0x1,%edx
  12:	45 30 c9             	xor    %r9b,%r9b
  15:	e9 00 00 00 00       	jmpq   1a <.fixup+0x1a>
  1a:	ba 01 00 00 00       	mov    $0x1,%edx
  1f:	30 c9                	xor    %cl,%cl
  21:	e9 00 00 00 00       	jmpq   26 <pgd_lock+0x6>

Disassembly of section .kprobes.text:

0000000000000000 <vmalloc_fault>:
 *   Handle a fault on the vmalloc area
 *
 * This assumes no large pages in there.
 */
static noinline __kprobes int vmalloc_fault(unsigned long address)
{
   0:	e8 00 00 00 00       	callq  5 <vmalloc_fault+0x5>
   5:	55                   	push   %rbp
	pud_t *pud, *pud_ref;
	pmd_t *pmd, *pmd_ref;
	pte_t *pte, *pte_ref;

	/* Make sure we are in vmalloc area: */
	if (!(address >= VMALLOC_START && address < VMALLOC_END))
   6:	48 ba 00 00 00 00 00 	movabs $0x370000000000,%rdx
   d:	37 00 00 
  10:	48 b8 fe ff ff ff ff 	movabs $0x1ffffffffffe,%rax
  17:	1f 00 00 
  1a:	48 01 fa             	add    %rdi,%rdx
 *   Handle a fault on the vmalloc area
 *
 * This assumes no large pages in there.
 */
static noinline __kprobes int vmalloc_fault(unsigned long address)
{
  1d:	48 89 e5             	mov    %rsp,%rbp
  20:	41 56                	push   %r14
	pud_t *pud, *pud_ref;
	pmd_t *pmd, *pmd_ref;
	pte_t *pte, *pte_ref;

	/* Make sure we are in vmalloc area: */
	if (!(address >= VMALLOC_START && address < VMALLOC_END))
  22:	48 39 c2             	cmp    %rax,%rdx
 *   Handle a fault on the vmalloc area
 *
 * This assumes no large pages in there.
 */
static noinline __kprobes int vmalloc_fault(unsigned long address)
{
  25:	41 55                	push   %r13
  27:	41 54                	push   %r12
  29:	53                   	push   %rbx
  2a:	48 89 fb             	mov    %rdi,%rbx
	pud_t *pud, *pud_ref;
	pmd_t *pmd, *pmd_ref;
	pte_t *pte, *pte_ref;

	/* Make sure we are in vmalloc area: */
	if (!(address >= VMALLOC_START && address < VMALLOC_END))
  2d:	0f 87 cd 01 00 00    	ja     200 <vmalloc_fault+0x200>
  33:	65 8b 04 25 00 00 00 	mov    %gs:0x0,%eax
  3a:	00 
		return -1;

	WARN_ON_ONCE(in_nmi());
  3b:	a9 00 00 10 00       	test   $0x100000,%eax
  40:	74 21                	je     63 <vmalloc_fault+0x63>
  42:	80 3d 00 00 00 00 00 	cmpb   $0x0,0x0(%rip)        # 49 <vmalloc_fault+0x49>
  49:	75 18                	jne    63 <vmalloc_fault+0x63>
  4b:	be 70 01 00 00       	mov    $0x170,%esi
  50:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
  57:	e8 00 00 00 00       	callq  5c <vmalloc_fault+0x5c>
  5c:	c6 05 00 00 00 00 01 	movb   $0x1,0x0(%rip)        # 63 <vmalloc_fault+0x63>
	/*
	 * Copy kernel mappings over when needed. This can also
	 * happen within a race in page table update. In the later
	 * case just flush:
	 */
	pgd = pgd_offset(current->active_mm, address);
  63:	48 89 d8             	mov    %rbx,%rax
  66:	48 c1 e8 24          	shr    $0x24,%rax
  6a:	25 f8 0f 00 00       	and    $0xff8,%eax
  6f:	49 89 c4             	mov    %rax,%r12
	pgd_ref = pgd_offset_k(address);
  72:	48 03 05 00 00 00 00 	add    0x0(%rip),%rax        # 79 <vmalloc_fault+0x79>
  79:	65 48 8b 14 25 00 00 	mov    %gs:0x0,%rdx
  80:	00 00 
	/*
	 * Copy kernel mappings over when needed. This can also
	 * happen within a race in page table update. In the later
	 * case just flush:
	 */
	pgd = pgd_offset(current->active_mm, address);
  82:	48 8b 92 b0 02 00 00 	mov    0x2b0(%rdx),%rdx
  89:	48 8b 30             	mov    (%rax),%rsi
	pgd_ref = pgd_offset_k(address);
  8c:	49 89 c5             	mov    %rax,%r13
	/*
	 * Copy kernel mappings over when needed. This can also
	 * happen within a race in page table update. In the later
	 * case just flush:
	 */
	pgd = pgd_offset(current->active_mm, address);
  8f:	4c 03 62 40          	add    0x40(%rdx),%r12
	pgd_ref = pgd_offset_k(address);
	if (pgd_none(*pgd_ref))
  93:	48 85 f6             	test   %rsi,%rsi
  96:	0f 84 64 01 00 00    	je     200 <vmalloc_fault+0x200>
  9c:	49 8b 3c 24          	mov    (%r12),%rdi
		return -1;

	if (pgd_none(*pgd)) {
  a0:	48 85 ff             	test   %rdi,%rdi
  a3:	75 13                	jne    b8 <vmalloc_fault+0xb8>

	if (sizeof(pgdval_t) > sizeof(long))
		PVOP_VCALL3(pv_mmu_ops.set_pgd, pgdp,
			    val, (u64)val >> 32);
	else
		PVOP_VCALL2(pv_mmu_ops.set_pgd, pgdp,
  a5:	4c 89 e7             	mov    %r12,%rdi
  a8:	ff 14 25 00 00 00 00 	callq  *0x0
	PVOP_VCALL0(pv_mmu_ops.lazy_mode.leave);
}

static inline void arch_flush_lazy_mmu_mode(void)
{
	PVOP_VCALL0(pv_mmu_ops.lazy_mode.flush);
  af:	ff 14 25 00 00 00 00 	callq  *0x0
  b6:	eb 3c                	jmp    f4 <vmalloc_fault+0xf4>

	if (sizeof(pgdval_t) > sizeof(long))
		ret =  PVOP_CALLEE2(pgdval_t, pv_mmu_ops.pgd_val,
				    pgd.pgd, (u64)pgd.pgd >> 32);
	else
		ret =  PVOP_CALLEE1(pgdval_t, pv_mmu_ops.pgd_val,
  b8:	ff 14 25 00 00 00 00 	callq  *0x0
  bf:	48 89 c2             	mov    %rax,%rdx
  c2:	49 8b 7d 00          	mov    0x0(%r13),%rdi
  c6:	ff 14 25 00 00 00 00 	callq  *0x0
	return pgd_flags(pgd) & _PAGE_PRESENT;
}

static inline unsigned long pgd_page_vaddr(pgd_t pgd)
{
	return (unsigned long)__va((unsigned long)pgd_val(pgd) & PTE_PFN_MASK);
  cd:	48 be 00 f0 ff ff ff 	movabs $0x3ffffffff000,%rsi
  d4:	3f 00 00 
  d7:	48 b9 00 00 00 00 00 	movabs $0xffff880000000000,%rcx
  de:	88 ff ff 
  e1:	48 21 f2             	and    %rsi,%rdx
  e4:	48 21 f0             	and    %rsi,%rax
  e7:	48 01 ca             	add    %rcx,%rdx
  ea:	48 01 c8             	add    %rcx,%rax
		set_pgd(pgd, *pgd_ref);
		arch_flush_lazy_mmu_mode();
	} else {
		BUG_ON(pgd_page_vaddr(*pgd) != pgd_page_vaddr(*pgd_ref));
  ed:	48 39 c2             	cmp    %rax,%rdx
  f0:	74 02                	je     f4 <vmalloc_fault+0xf4>
  f2:	0f 0b                	ud2    
	/*
	 * Below here mismatches are bugs because these lower tables
	 * are shared:
	 */

	pud = pud_offset(pgd, address);
  f4:	48 89 de             	mov    %rbx,%rsi
  f7:	4c 89 e7             	mov    %r12,%rdi
  fa:	e8 00 00 00 00       	callq  ff <vmalloc_fault+0xff>
	pud_ref = pud_offset(pgd_ref, address);
  ff:	4c 89 ef             	mov    %r13,%rdi
 102:	48 89 de             	mov    %rbx,%rsi
	/*
	 * Below here mismatches are bugs because these lower tables
	 * are shared:
	 */

	pud = pud_offset(pgd, address);
 105:	49 89 c6             	mov    %rax,%r14
	pud_ref = pud_offset(pgd_ref, address);
 108:	e8 00 00 00 00       	callq  10d <vmalloc_fault+0x10d>
	if (pud_none(*pud_ref))
 10d:	48 83 38 00          	cmpq   $0x0,(%rax)
	 * Below here mismatches are bugs because these lower tables
	 * are shared:
	 */

	pud = pud_offset(pgd, address);
	pud_ref = pud_offset(pgd_ref, address);
 111:	49 89 c5             	mov    %rax,%r13
	if (pud_none(*pud_ref))
 114:	0f 84 e6 00 00 00    	je     200 <vmalloc_fault+0x200>
 11a:	49 8b 3e             	mov    (%r14),%rdi
		return -1;

	if (pud_none(*pud) || pud_page_vaddr(*pud) != pud_page_vaddr(*pud_ref))
 11d:	48 85 ff             	test   %rdi,%rdi
 120:	74 3d                	je     15f <vmalloc_fault+0x15f>

	if (sizeof(pudval_t) > sizeof(long))
		ret =  PVOP_CALLEE2(pudval_t, pv_mmu_ops.pud_val,
				    pud.pud, (u64)pud.pud >> 32);
	else
		ret =  PVOP_CALLEE1(pudval_t, pv_mmu_ops.pud_val,
 122:	ff 14 25 00 00 00 00 	callq  *0x0
 129:	48 89 c2             	mov    %rax,%rdx
 12c:	49 8b 7d 00          	mov    0x0(%r13),%rdi
 130:	ff 14 25 00 00 00 00 	callq  *0x0
	return pud_flags(pud) & _PAGE_PRESENT;
}

static inline unsigned long pud_page_vaddr(pud_t pud)
{
	return (unsigned long)__va((unsigned long)pud_val(pud) & PTE_PFN_MASK);
 137:	49 bc 00 f0 ff ff ff 	movabs $0x3ffffffff000,%r12
 13e:	3f 00 00 
 141:	48 89 c1             	mov    %rax,%rcx
 144:	48 b8 00 00 00 00 00 	movabs $0xffff880000000000,%rax
 14b:	88 ff ff 
 14e:	4c 21 e2             	and    %r12,%rdx
 151:	4c 21 e1             	and    %r12,%rcx
 154:	48 01 c2             	add    %rax,%rdx
 157:	48 01 c1             	add    %rax,%rcx
 15a:	48 39 ca             	cmp    %rcx,%rdx
 15d:	74 02                	je     161 <vmalloc_fault+0x161>
		BUG();
 15f:	0f 0b                	ud2    

	pmd = pmd_offset(pud, address);
 161:	4c 89 f7             	mov    %r14,%rdi
 164:	48 89 de             	mov    %rbx,%rsi
 167:	e8 00 00 00 00       	callq  16c <vmalloc_fault+0x16c>
	pmd_ref = pmd_offset(pud_ref, address);
 16c:	48 89 de             	mov    %rbx,%rsi
 16f:	4c 89 ef             	mov    %r13,%rdi
		return -1;

	if (pud_none(*pud) || pud_page_vaddr(*pud) != pud_page_vaddr(*pud_ref))
		BUG();

	pmd = pmd_offset(pud, address);
 172:	49 89 c6             	mov    %rax,%r14
	pmd_ref = pmd_offset(pud_ref, address);
 175:	e8 00 00 00 00       	callq  17a <vmalloc_fault+0x17a>
	if (pmd_none(*pmd_ref))
 17a:	48 83 38 00          	cmpq   $0x0,(%rax)

	if (pud_none(*pud) || pud_page_vaddr(*pud) != pud_page_vaddr(*pud_ref))
		BUG();

	pmd = pmd_offset(pud, address);
	pmd_ref = pmd_offset(pud_ref, address);
 17e:	48 89 c2             	mov    %rax,%rdx
	if (pmd_none(*pmd_ref))
 181:	74 7d                	je     200 <vmalloc_fault+0x200>
 183:	49 8b 3e             	mov    (%r14),%rdi
		return -1;

	if (pmd_none(*pmd) || pmd_page(*pmd) != pmd_page(*pmd_ref))
 186:	48 85 ff             	test   %rdi,%rdi
 189:	74 1f                	je     1aa <vmalloc_fault+0x1aa>

	if (sizeof(pmdval_t) > sizeof(long))
		ret =  PVOP_CALLEE2(pmdval_t, pv_mmu_ops.pmd_val,
				    pmd.pmd, (u64)pmd.pmd >> 32);
	else
		ret =  PVOP_CALLEE1(pmdval_t, pv_mmu_ops.pmd_val,
 18b:	ff 14 25 00 00 00 00 	callq  *0x0
 192:	48 89 c1             	mov    %rax,%rcx
 195:	48 8b 3a             	mov    (%rdx),%rdi
 198:	ff 14 25 00 00 00 00 	callq  *0x0
 19f:	4c 21 e1             	and    %r12,%rcx
 1a2:	4c 21 e0             	and    %r12,%rax
 1a5:	48 39 c8             	cmp    %rcx,%rax
 1a8:	74 02                	je     1ac <vmalloc_fault+0x1ac>
		BUG();
 1aa:	0f 0b                	ud2    

	pte_ref = pte_offset_kernel(pmd_ref, address);
 1ac:	48 89 de             	mov    %rbx,%rsi
 1af:	48 89 d7             	mov    %rdx,%rdi
 1b2:	e8 00 00 00 00       	callq  1b7 <vmalloc_fault+0x1b7>
	if (!pte_present(*pte_ref))
 1b7:	48 f7 00 01 01 00 00 	testq  $0x101,(%rax)
		return -1;

	if (pmd_none(*pmd) || pmd_page(*pmd) != pmd_page(*pmd_ref))
		BUG();

	pte_ref = pte_offset_kernel(pmd_ref, address);
 1be:	49 89 c5             	mov    %rax,%r13
	if (!pte_present(*pte_ref))
 1c1:	74 3d                	je     200 <vmalloc_fault+0x200>
		return -1;

	pte = pte_offset_kernel(pmd, address);
 1c3:	4c 89 f7             	mov    %r14,%rdi
 1c6:	48 89 de             	mov    %rbx,%rsi
 1c9:	e8 00 00 00 00       	callq  1ce <vmalloc_fault+0x1ce>
 1ce:	48 8b 38             	mov    (%rax),%rdi
	/*
	 * Don't use pte_page here, because the mappings can point
	 * outside mem_map, and the NUMA hash lookup cannot handle
	 * that:
	 */
	if (!pte_present(*pte) || pte_pfn(*pte) != pte_pfn(*pte_ref))
 1d1:	f7 c7 01 01 00 00    	test   $0x101,%edi
 1d7:	74 25                	je     1fe <vmalloc_fault+0x1fe>

	if (sizeof(pteval_t) > sizeof(long))
		ret = PVOP_CALLEE2(pteval_t, pv_mmu_ops.pte_val,
				   pte.pte, (u64)pte.pte >> 32);
	else
		ret = PVOP_CALLEE1(pteval_t, pv_mmu_ops.pte_val,
 1d9:	ff 14 25 00 00 00 00 	callq  *0x0
 1e0:	48 89 c2             	mov    %rax,%rdx
 1e3:	49 8b 7d 00          	mov    0x0(%r13),%rdi
 1e7:	ff 14 25 00 00 00 00 	callq  *0x0
 1ee:	48 89 c7             	mov    %rax,%rdi
	return pte_flags(pte) & _PAGE_SPECIAL;
}

static inline unsigned long pte_pfn(pte_t pte)
{
	return (pte_val(pte) & PTE_PFN_MASK) >> PAGE_SHIFT;
 1f1:	4c 21 e2             	and    %r12,%rdx
		BUG();

	return 0;
 1f4:	31 c0                	xor    %eax,%eax
 1f6:	4c 21 e7             	and    %r12,%rdi
	/*
	 * Don't use pte_page here, because the mappings can point
	 * outside mem_map, and the NUMA hash lookup cannot handle
	 * that:
	 */
	if (!pte_present(*pte) || pte_pfn(*pte) != pte_pfn(*pte_ref))
 1f9:	48 39 d7             	cmp    %rdx,%rdi
 1fc:	74 05                	je     203 <vmalloc_fault+0x203>
		BUG();
 1fe:	0f 0b                	ud2    
	pmd_t *pmd, *pmd_ref;
	pte_t *pte, *pte_ref;

	/* Make sure we are in vmalloc area: */
	if (!(address >= VMALLOC_START && address < VMALLOC_END))
		return -1;
 200:	83 c8 ff             	or     $0xffffffff,%eax
	 */
	if (!pte_present(*pte) || pte_pfn(*pte) != pte_pfn(*pte_ref))
		BUG();

	return 0;
}
 203:	5b                   	pop    %rbx
 204:	41 5c                	pop    %r12
 206:	41 5d                	pop    %r13
 208:	41 5e                	pop    %r14
 20a:	5d                   	pop    %rbp
 20b:	c3                   	retq   

000000000000020c <spurious_fault>:
 * There are no security implications to leaving a stale TLB when
 * increasing the permissions on a page.
 */
static noinline __kprobes int
spurious_fault(unsigned long error_code, unsigned long address)
{
 20c:	e8 00 00 00 00       	callq  211 <spurious_fault+0x5>
 211:	55                   	push   %rbp
	pmd_t *pmd;
	pte_t *pte;
	int ret;

	/* Reserved-bit violation or user access to kernel space? */
	if (error_code & (PF_USER | PF_RSVD))
 212:	40 f6 c7 0c          	test   $0xc,%dil
 * There are no security implications to leaving a stale TLB when
 * increasing the permissions on a page.
 */
static noinline __kprobes int
spurious_fault(unsigned long error_code, unsigned long address)
{
 216:	48 89 e5             	mov    %rsp,%rbp
 219:	41 56                	push   %r14
 21b:	41 55                	push   %r13
 21d:	41 54                	push   %r12
 21f:	53                   	push   %rbx
 220:	48 89 fb             	mov    %rdi,%rbx
	pmd_t *pmd;
	pte_t *pte;
	int ret;

	/* Reserved-bit violation or user access to kernel space? */
	if (error_code & (PF_USER | PF_RSVD))
 223:	74 08                	je     22d <spurious_fault+0x21>
		return 0;
 225:	45 31 e4             	xor    %r12d,%r12d
 228:	e9 fc 00 00 00       	jmpq   329 <spurious_fault+0x11d>

	pgd = init_mm.pgd + pgd_index(address);
 22d:	48 89 f7             	mov    %rsi,%rdi
 230:	49 89 f5             	mov    %rsi,%r13
 233:	48 c1 ef 24          	shr    $0x24,%rdi
 237:	81 e7 f8 0f 00 00    	and    $0xff8,%edi
 23d:	48 03 3d 00 00 00 00 	add    0x0(%rip),%rdi        # 244 <spurious_fault+0x38>
	if (!pgd_present(*pgd))
 244:	f6 07 01             	testb  $0x1,(%rdi)
 247:	74 dc                	je     225 <spurious_fault+0x19>
		return 0;

	pud = pud_offset(pgd, address);
 249:	e8 00 00 00 00       	callq  24e <spurious_fault+0x42>
 24e:	48 8b 38             	mov    (%rax),%rdi
 251:	48 89 c2             	mov    %rax,%rdx
	if (!pud_present(*pud))
 254:	40 f6 c7 01          	test   $0x1,%dil
 258:	74 cb                	je     225 <spurious_fault+0x19>

	if (sizeof(pudval_t) > sizeof(long))
		ret =  PVOP_CALLEE2(pudval_t, pv_mmu_ops.pud_val,
				    pud.pud, (u64)pud.pud >> 32);
	else
		ret =  PVOP_CALLEE1(pudval_t, pv_mmu_ops.pud_val,
 25a:	ff 14 25 00 00 00 00 	callq  *0x0
	return (pmd_t *)pud_page_vaddr(*pud) + pmd_index(address);
}

static inline int pud_large(pud_t pud)
{
	return (pud_val(pud) & (_PAGE_PSE | _PAGE_PRESENT)) ==
 261:	25 81 00 00 00       	and    $0x81,%eax
		return 0;

	if (pud_large(*pud))
 266:	48 3d 81 00 00 00    	cmp    $0x81,%rax
 26c:	75 13                	jne    281 <spurious_fault+0x75>
		return spurious_fault_check(error_code, (pte_t *) pud);
 26e:	48 89 d6             	mov    %rdx,%rsi
 271:	48 89 df             	mov    %rbx,%rdi
 274:	e8 00 00 00 00       	callq  279 <spurious_fault+0x6d>
 279:	41 89 c4             	mov    %eax,%r12d
 27c:	e9 a8 00 00 00       	jmpq   329 <spurious_fault+0x11d>

	pmd = pmd_offset(pud, address);
 281:	4c 89 ee             	mov    %r13,%rsi
 284:	48 89 d7             	mov    %rdx,%rdi
	return native_pud_val(pud) & PTE_FLAGS_MASK;
}

static inline pmdval_t pmd_flags(pmd_t pmd)
{
	return native_pmd_val(pmd) & PTE_FLAGS_MASK;
 287:	49 bc ff 0f 00 00 00 	movabs $0xffffc00000000fff,%r12
 28e:	c0 ff ff 
 291:	e8 00 00 00 00       	callq  296 <spurious_fault+0x8a>
 296:	4c 23 20             	and    (%rax),%r12
 299:	49 89 c6             	mov    %rax,%r14
	if (!pmd_present(*pmd))
 29c:	41 f7 c4 81 01 00 00 	test   $0x181,%r12d
 2a3:	74 80                	je     225 <spurious_fault+0x19>
		return 0;

	if (pmd_large(*pmd))
 2a5:	41 81 e4 80 00 00 00 	and    $0x80,%r12d
 2ac:	74 10                	je     2be <spurious_fault+0xb2>
		return spurious_fault_check(error_code, (pte_t *) pmd);
 2ae:	48 89 c6             	mov    %rax,%rsi
 2b1:	48 89 df             	mov    %rbx,%rdi
 2b4:	e8 00 00 00 00       	callq  2b9 <spurious_fault+0xad>
 2b9:	41 89 c4             	mov    %eax,%r12d
 2bc:	eb 6b                	jmp    329 <spurious_fault+0x11d>

	pte = pte_offset_kernel(pmd, address);
 2be:	4c 89 ee             	mov    %r13,%rsi
 2c1:	48 89 c7             	mov    %rax,%rdi
 2c4:	e8 00 00 00 00       	callq  2c9 <spurious_fault+0xbd>
	if (!pte_present(*pte))
 2c9:	48 f7 00 01 01 00 00 	testq  $0x101,(%rax)
 2d0:	0f 84 4f ff ff ff    	je     225 <spurious_fault+0x19>
		return 0;

	ret = spurious_fault_check(error_code, pte);
 2d6:	48 89 c6             	mov    %rax,%rsi
 2d9:	48 89 df             	mov    %rbx,%rdi
 2dc:	e8 00 00 00 00       	callq  2e1 <spurious_fault+0xd5>
	if (!ret)
 2e1:	85 c0                	test   %eax,%eax
 2e3:	0f 84 3c ff ff ff    	je     225 <spurious_fault+0x19>

	/*
	 * Make sure we have permissions in PMD.
	 * If not, then there's a bug in the page tables:
	 */
	ret = spurious_fault_check(error_code, (pte_t *) pmd);
 2e9:	4c 89 f6             	mov    %r14,%rsi
 2ec:	48 89 df             	mov    %rbx,%rdi
 2ef:	e8 00 00 00 00       	callq  2f4 <spurious_fault+0xe8>
	WARN_ONCE(!ret, "PMD has incorrect permission bits\n");
 2f4:	85 c0                	test   %eax,%eax
 2f6:	75 2e                	jne    326 <spurious_fault+0x11a>
 2f8:	80 3d 00 00 00 00 00 	cmpb   $0x0,0x0(%rip)        # 2ff <spurious_fault+0xf3>
 2ff:	0f 85 20 ff ff ff    	jne    225 <spurious_fault+0x19>
 305:	48 c7 c2 00 00 00 00 	mov    $0x0,%rdx
 30c:	be c3 03 00 00       	mov    $0x3c3,%esi
 311:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
 318:	e8 00 00 00 00       	callq  31d <spurious_fault+0x111>
 31d:	c6 05 00 00 00 00 01 	movb   $0x1,0x0(%rip)        # 324 <spurious_fault+0x118>
 324:	eb 03                	jmp    329 <spurious_fault+0x11d>
 326:	41 89 c4             	mov    %eax,%r12d

	return ret;
}
 329:	5b                   	pop    %rbx
 32a:	44 89 e0             	mov    %r12d,%eax
 32d:	41 5c                	pop    %r12
 32f:	41 5d                	pop    %r13
 331:	41 5e                	pop    %r14
 333:	5d                   	pop    %rbp
 334:	c3                   	retq   
 335:	66 66 2e 0f 1f 84 00 	data32 nopw %cs:0x0(%rax,%rax,1)
 33c:	00 00 00 00 

0000000000000340 <__do_page_fault>:
 * and the problem, and then passes it off to one of the appropriate
 * routines.
 */
static void __kprobes
__do_page_fault(struct pt_regs *regs, unsigned long error_code)
{
 340:	e8 00 00 00 00       	callq  345 <__do_page_fault+0x5>
 345:	55                   	push   %rbp
 346:	65 48 8b 04 25 00 00 	mov    %gs:0x0,%rax
 34d:	00 00 
 34f:	48 89 e5             	mov    %rsp,%rbp
 352:	41 57                	push   %r15
 354:	41 56                	push   %r14
 356:	41 55                	push   %r13
 358:	41 54                	push   %r12
 35a:	49 89 fc             	mov    %rdi,%r12
 35d:	53                   	push   %rbx
 35e:	48 89 f3             	mov    %rsi,%rbx
 361:	48 81 ec d8 00 00 00 	sub    $0xd8,%rsp
 368:	48 89 85 18 ff ff ff 	mov    %rax,-0xe8(%rbp)
	struct mm_struct *mm;
	int fault;
	unsigned int flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;

	tsk = current;
	mm = tsk->mm;
 36f:	4c 8b a8 a8 02 00 00 	mov    0x2a8(%rax),%r13
	PVOP_VCALL1(pv_cpu_ops.write_cr0, x);
}

static inline unsigned long read_cr2(void)
{
	return PVOP_CALL0(unsigned long, pv_mmu_ops.read_cr2);
 376:	ff 14 25 00 00 00 00 	callq  *0x0
 37d:	49 89 c6             	mov    %rax,%r14
	 * Detect and handle instructions that would cause a page fault for
	 * both a tracked kernel page and a userspace page.
	 */
	if (kmemcheck_active(regs))
		kmemcheck_hide(regs);
	prefetchw(&mm->mmap_sem);
 380:	49 8d 45 78          	lea    0x78(%r13),%rax
 384:	48 89 85 28 ff ff ff 	mov    %rax,-0xd8(%rbp)
 * Useful for spinlocks to avoid one state transition in the
 * cache coherency protocol:
 */
static inline void prefetchw(const void *x)
{
	alternative_input(BASE_PREFETCH,
 38b:	0f 18 08             	prefetcht0 (%rax)
 * handled by mmiotrace:
 */
static inline int __kprobes
kmmio_fault(struct pt_regs *regs, unsigned long addr)
{
	if (unlikely(is_kmmio_active()))
 38e:	8b 05 00 00 00 00    	mov    0x0(%rip),%eax        # 394 <__do_page_fault+0x54>
 394:	85 c0                	test   %eax,%eax
 396:	0f 85 27 05 00 00    	jne    8c3 <__do_page_fault+0x583>
	 *
	 * This verifies that the fault happens in kernel space
	 * (error_code & 4) == 0, and that the fault was not a
	 * protection error (error_code & 9) == 0.
	 */
	if (unlikely(fault_in_kernel_space(address))) {
 39c:	48 b8 ff ef ff ff ff 	movabs $0x7fffffffefff,%rax
 3a3:	7f 00 00 
 3a6:	49 39 c6             	cmp    %rax,%r14
 3a9:	0f 87 a1 04 00 00    	ja     850 <__do_page_fault+0x510>
static inline int __kprobes kprobes_fault(struct pt_regs *regs)
{
	int ret = 0;

	/* kprobe_running() needs smp_processor_id() */
	if (kprobes_built_in() && !user_mode_vm(regs)) {
 3af:	41 f6 84 24 88 00 00 	testb  $0x3,0x88(%r12)
 3b6:	00 03 
 3b8:	0f 84 ea 01 00 00    	je     5a8 <__do_page_fault+0x268>
	PVOP_VCALLEE0(pv_irq_ops.irq_disable);
}

static inline notrace void arch_local_irq_enable(void)
{
	PVOP_VCALLEE0(pv_irq_ops.irq_enable);
 3be:	ff 14 25 00 00 00 00 	callq  *0x0
	 * potential system fault or CPU buglet:
	 */
	if (user_mode_vm(regs)) {
		local_irq_enable();
		error_code |= PF_USER;
		flags |= FAULT_FLAG_USER;
 3c5:	41 bf a8 00 00 00    	mov    $0xa8,%r15d
	 * User-mode registers count as a user access even for any
	 * potential system fault or CPU buglet:
	 */
	if (user_mode_vm(regs)) {
		local_irq_enable();
		error_code |= PF_USER;
 3cb:	48 83 cb 04          	or     $0x4,%rbx
		if (regs->flags & X86_EFLAGS_IF)
			local_irq_enable();
	}


	if (unlikely(error_code & PF_RSVD)){
 3cf:	48 89 d8             	mov    %rbx,%rax
 3d2:	83 e0 08             	and    $0x8,%eax
 3d5:	48 89 85 10 ff ff ff 	mov    %rax,-0xf0(%rbp)
 3dc:	0f 85 d5 04 00 00    	jne    8b7 <__do_page_fault+0x577>
			 /* skipping size check since replacement size = 0 */
			 : : "i" (X86_FEATURE_ALWAYS) : : t_warn);

#endif

		asm_volatile_goto("1: jmp %l[t_no]\n"
 3e2:	eb 44                	jmp    428 <__do_page_fault+0xe8>
		return false;

	if (!static_cpu_has(X86_FEATURE_SMAP))
		return false;

	if (error_code & PF_USER)
 3e4:	f6 c3 04             	test   $0x4,%bl
 3e7:	75 3f                	jne    428 <__do_page_fault+0xe8>
		return false;

	if (!user_mode_vm(regs) && (regs->flags & X86_EFLAGS_AC))
 3e9:	41 f6 84 24 88 00 00 	testb  $0x3,0x88(%r12)
 3f0:	00 03 
 3f2:	74 24                	je     418 <__do_page_fault+0xd8>
		}
		pgtable_bad(regs, error_code, address);*/
		flags|=FAULT_FLAG_RSVD;
	}
	if (unlikely(smap_violation(error_code, regs))) {
		bad_area_nosemaphore(regs, error_code, address);
 3f4:	4c 89 f2             	mov    %r14,%rdx
 3f7:	48 89 de             	mov    %rbx,%rsi
 3fa:	4c 89 e7             	mov    %r12,%rdi
 3fd:	e8 00 00 00 00       	callq  402 <__do_page_fault+0xc2>
	}

	check_v8086_mode(regs, address, tsk);

	up_read(&mm->mmap_sem);
}
 402:	48 81 c4 d8 00 00 00 	add    $0xd8,%rsp
 409:	5b                   	pop    %rbx
 40a:	41 5c                	pop    %r12
 40c:	41 5d                	pop    %r13
 40e:	41 5e                	pop    %r14
 410:	41 5f                	pop    %r15
 412:	5d                   	pop    %rbp
 413:	c3                   	retq   
 414:	0f 1f 40 00          	nopl   0x0(%rax)
		return false;

	if (error_code & PF_USER)
		return false;

	if (!user_mode_vm(regs) && (regs->flags & X86_EFLAGS_AC))
 418:	41 f6 84 24 92 00 00 	testb  $0x4,0x92(%r12)
 41f:	00 04 
 421:	74 d1                	je     3f4 <__do_page_fault+0xb4>
 423:	0f 1f 44 00 00       	nopl   0x0(%rax,%rax,1)
 428:	65 8b 04 25 00 00 00 	mov    %gs:0x0,%eax
 42f:	00 

	/*
	 * If we're in an interrupt, have no user context or are running
	 * in an atomic region then we must not take the fault:
	 */
	if (unlikely(in_atomic() || !mm)) {
 430:	a9 ff ff df 7f       	test   $0x7fdfffff,%eax
 435:	75 bd                	jne    3f4 <__do_page_fault+0xb4>
 437:	4d 85 ed             	test   %r13,%r13
 43a:	74 b8                	je     3f4 <__do_page_fault+0xb4>
 43c:	0f 1f 44 00 00       	nopl   0x0(%rax,%rax,1)
	 * the source reference check when there is a possibility of a
	 * deadlock. Attempt to lock the address space, if we cannot we then
	 * validate the source. If this is invalid we can skip the address
	 * space check, thus avoiding the deadlock:
	 */
	if (unlikely(!down_read_trylock(&mm->mmap_sem))) {
 441:	48 8b bd 28 ff ff ff 	mov    -0xd8(%rbp),%rdi
	}

	perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, regs, address);

	if (error_code & PF_WRITE)
		flags |= FAULT_FLAG_WRITE;
 448:	44 89 f8             	mov    %r15d,%eax
 44b:	48 89 d9             	mov    %rbx,%rcx
 44e:	83 c8 01             	or     $0x1,%eax
 451:	83 e1 02             	and    $0x2,%ecx
 454:	44 0f 45 f8          	cmovne %eax,%r15d
 458:	48 89 8d 20 ff ff ff 	mov    %rcx,-0xe0(%rbp)
	 * the source reference check when there is a possibility of a
	 * deadlock. Attempt to lock the address space, if we cannot we then
	 * validate the source. If this is invalid we can skip the address
	 * space check, thus avoiding the deadlock:
	 */
	if (unlikely(!down_read_trylock(&mm->mmap_sem))) {
 45f:	e8 00 00 00 00       	callq  464 <__do_page_fault+0x124>
 464:	85 c0                	test   %eax,%eax
 466:	0f 84 93 03 00 00    	je     7ff <__do_page_fault+0x4bf>
		 * down_read():
		 */
		might_sleep();
	}

	vma = find_vma(mm, address);
 46c:	4c 89 f6             	mov    %r14,%rsi
 46f:	4c 89 ef             	mov    %r13,%rdi
 472:	e8 00 00 00 00       	callq  477 <__do_page_fault+0x137>
	if (unlikely(!vma)) {
 477:	48 85 c0             	test   %rax,%rax
		 * down_read():
		 */
		might_sleep();
	}

	vma = find_vma(mm, address);
 47a:	49 89 c1             	mov    %rax,%r9
	if (unlikely(!vma)) {
 47d:	0f 84 69 03 00 00    	je     7ec <__do_page_fault+0x4ac>
		bad_area(regs, error_code, address);
		return;
	}
	if (likely(vma->vm_start <= address))
 483:	48 8b 08             	mov    (%rax),%rcx
 486:	4c 39 f1             	cmp    %r14,%rcx
 489:	0f 87 fd 02 00 00    	ja     78c <__do_page_fault+0x44c>
int show_unhandled_signals = 1;

static inline int
access_error(unsigned long error_code, struct vm_area_struct *vma)
{
	if (error_code & PF_WRITE) {
 48f:	48 83 bd 20 ff ff ff 	cmpq   $0x0,-0xe0(%rbp)
 496:	00 
 497:	0f 84 c3 00 00 00    	je     560 <__do_page_fault+0x220>
		/* write, present and write, not present: */
		if (unlikely(!(vma->vm_flags & VM_WRITE)))
 49d:	41 f6 41 50 02       	testb  $0x2,0x50(%r9)
 4a2:	0f 84 ce 00 00 00    	je     576 <__do_page_fault+0x236>
	/*
	 * If for any reason at all we couldn't handle the fault,
	 * make sure we exit gracefully rather than endlessly redo
	 * the fault:
	 */
	fault = handle_mm_fault(mm, vma, address, flags);
 4a8:	44 89 f9             	mov    %r15d,%ecx
 4ab:	4c 89 f2             	mov    %r14,%rdx
 4ae:	4c 89 ce             	mov    %r9,%rsi
 4b1:	4c 89 ef             	mov    %r13,%rdi
 4b4:	e8 00 00 00 00       	callq  4b9 <__do_page_fault+0x179>
	/*
	 * If we need to retry but a fatal signal is pending, handle the
	 * signal first. We do not need to release the mmap_sem because it
	 * would already be released in __lock_page_or_retry in mm/filemap.c.
	 */
	if (unlikely((fault & VM_FAULT_RETRY) && fatal_signal_pending(current)))
 4b9:	41 89 c0             	mov    %eax,%r8d
 4bc:	41 81 e0 00 04 00 00 	and    $0x400,%r8d
 4c3:	0f 85 82 02 00 00    	jne    74b <__do_page_fault+0x40b>
		return;

	if (unlikely(fault & VM_FAULT_ERROR)) {
 4c9:	a9 33 08 00 00       	test   $0x833,%eax
 4ce:	0f 85 a3 02 00 00    	jne    777 <__do_page_fault+0x437>
		mm_fault_error(regs, error_code, address, fault);
		return;
	}
	if(unlikely(fault& VM_FAULT_RSVD)){
 4d4:	a8 40                	test   $0x40,%al
 4d6:	0f 85 47 03 00 00    	jne    823 <__do_page_fault+0x4e3>
	/*
	 * Major/minor page fault accounting is only done on the
	 * initial attempt. If we go through a retry, it is extremely
	 * likely that the page will be found in page cache at that point.
	 */
	if (flags & FAULT_FLAG_ALLOW_RETRY) {
 4dc:	41 f6 c7 08          	test   $0x8,%r15b
 4e0:	74 66                	je     548 <__do_page_fault+0x208>
		if (fault & VM_FAULT_MAJOR) {
 4e2:	a8 04                	test   $0x4,%al
			tsk->maj_flt++;
 4e4:	48 8b 85 18 ff ff ff 	mov    -0xe8(%rbp),%rax
	 * Major/minor page fault accounting is only done on the
	 * initial attempt. If we go through a retry, it is extremely
	 * likely that the page will be found in page cache at that point.
	 */
	if (flags & FAULT_FLAG_ALLOW_RETRY) {
		if (fault & VM_FAULT_MAJOR) {
 4eb:	0f 84 9f 00 00 00    	je     590 <__do_page_fault+0x250>
			tsk->maj_flt++;
 4f1:	48 83 80 70 04 00 00 	addq   $0x1,0x470(%rax)
 4f8:	01 
 4f9:	0f 1f 44 00 00       	nopl   0x0(%rax,%rax,1)
		} else {
			tsk->min_flt++;
			perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN, 1,
				      regs, address);
		}
		if (fault & VM_FAULT_RETRY) {
 4fe:	45 85 c0             	test   %r8d,%r8d
 501:	74 45                	je     548 <__do_page_fault+0x208>
			/* Clear FAULT_FLAG_ALLOW_RETRY to avoid any risk
			 * of starvation. */
			flags &= ~FAULT_FLAG_ALLOW_RETRY;
 503:	41 83 e7 f7          	and    $0xfffffff7,%r15d
			flags |= FAULT_FLAG_TRIED;
 507:	41 83 cf 40          	or     $0x40,%r15d
		    !search_exception_tables(regs->ip)) {
			bad_area_nosemaphore(regs, error_code, address);
			return;
		}
retry:
		down_read(&mm->mmap_sem);
 50b:	48 8b bd 28 ff ff ff 	mov    -0xd8(%rbp),%rdi
 512:	e8 00 00 00 00       	callq  517 <__do_page_fault+0x1d7>
 517:	e9 50 ff ff ff       	jmpq   46c <__do_page_fault+0x12c>
 51c:	0f 1f 40 00          	nopl   0x0(%rax)
perf_sw_event(u32 event_id, u64 nr, struct pt_regs *regs, u64 addr)
{
	struct pt_regs hot_regs;

	if (static_key_false(&perf_swevent_enabled[event_id])) {
		if (!regs) {
 520:	4d 85 e4             	test   %r12,%r12
 523:	0f 84 4f 01 00 00    	je     678 <__do_page_fault+0x338>
 529:	4c 89 e2             	mov    %r12,%rdx
			perf_fetch_caller_regs(&hot_regs);
			regs = &hot_regs;
		}
		__perf_sw_event(event_id, nr, regs, addr);
 52c:	4c 89 f1             	mov    %r14,%rcx
 52f:	be 01 00 00 00       	mov    $0x1,%esi
 534:	bf 02 00 00 00       	mov    $0x2,%edi
 539:	e8 00 00 00 00       	callq  53e <__do_page_fault+0x1fe>
 53e:	e9 fe fe ff ff       	jmpq   441 <__do_page_fault+0x101>
 543:	0f 1f 44 00 00       	nopl   0x0(%rax,%rax,1)
		}
	}

	check_v8086_mode(regs, address, tsk);

	up_read(&mm->mmap_sem);
 548:	48 8b bd 28 ff ff ff 	mov    -0xd8(%rbp),%rdi
 54f:	e8 00 00 00 00       	callq  554 <__do_page_fault+0x214>
 554:	e9 a9 fe ff ff       	jmpq   402 <__do_page_fault+0xc2>
 559:	0f 1f 80 00 00 00 00 	nopl   0x0(%rax)
			return 1;
		return 0;
	}

	/* read, present: */
	if (unlikely(error_code & PF_PROT)&&(!(error_code&PF_RSVD)))
 560:	f6 c3 01             	test   $0x1,%bl
 563:	0f 85 73 03 00 00    	jne    8dc <__do_page_fault+0x59c>
		return 1;

	/* read, not present: */
	if (unlikely(!(vma->vm_flags & (VM_READ | VM_EXEC | VM_WRITE))))
 569:	41 f6 41 50 07       	testb  $0x7,0x50(%r9)
 56e:	66 90                	xchg   %ax,%ax
 570:	0f 85 32 ff ff ff    	jne    4a8 <__do_page_fault+0x168>
	 * Ok, we have a good vm_area for this memory access, so
	 * we can handle it..
	 */
good_area:
	if (unlikely(access_error(error_code, vma))) {
		bad_area_access_error(regs, error_code, address);
 576:	4c 89 f2             	mov    %r14,%rdx
 579:	48 89 de             	mov    %rbx,%rsi
 57c:	4c 89 e7             	mov    %r12,%rdi
 57f:	e8 00 00 00 00       	callq  584 <__do_page_fault+0x244>
		return;
 584:	e9 79 fe ff ff       	jmpq   402 <__do_page_fault+0xc2>
 589:	0f 1f 80 00 00 00 00 	nopl   0x0(%rax)
		if (fault & VM_FAULT_MAJOR) {
			tsk->maj_flt++;
			perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ, 1,
				      regs, address);
		} else {
			tsk->min_flt++;
 590:	48 83 80 68 04 00 00 	addq   $0x1,0x468(%rax)
 597:	01 
 598:	0f 1f 44 00 00       	nopl   0x0(%rax,%rax,1)
# define STATIC_KEY_INIT_NOP GENERIC_NOP5_ATOMIC
#endif

static __always_inline bool arch_static_branch(struct static_key *key)
{
	asm_volatile_goto("1:"
 59d:	e9 5c ff ff ff       	jmpq   4fe <__do_page_fault+0x1be>
 5a2:	66 0f 1f 44 00 00    	nopw   0x0(%rax,%rax,1)
struct hlist_head * kretprobe_inst_table_head(struct task_struct *tsk);

/* kprobe_running() will just return the current_kprobe on this CPU */
static inline struct kprobe *kprobe_running(void)
{
	return (__this_cpu_read(current_kprobe));
 5a8:	65 48 8b 14 25 00 00 	mov    %gs:0x0,%rdx
 5af:	00 00 
	return 0;
}

static inline int __kprobes kprobes_fault(struct pt_regs *regs)
{
	int ret = 0;
 5b1:	31 c0                	xor    %eax,%eax

	/* kprobe_running() needs smp_processor_id() */
	if (kprobes_built_in() && !user_mode_vm(regs)) {
		preempt_disable();
		if (kprobe_running() && kprobe_fault_handler(regs, 14))
 5b3:	48 85 d2             	test   %rdx,%rdx
 5b6:	74 15                	je     5cd <__do_page_fault+0x28d>
 5b8:	be 0e 00 00 00       	mov    $0xe,%esi
 5bd:	4c 89 e7             	mov    %r12,%rdi
 5c0:	e8 00 00 00 00       	callq  5c5 <__do_page_fault+0x285>
 5c5:	85 c0                	test   %eax,%eax
 5c7:	0f 95 c0             	setne  %al
 5ca:	0f b6 c0             	movzbl %al,%eax

		return;
	}

	/* kprobes don't want to hook the spurious faults: */
	if (unlikely(kprobes_fault(regs)))
 5cd:	85 c0                	test   %eax,%eax
 5cf:	0f 85 2d fe ff ff    	jne    402 <__do_page_fault+0xc2>
	 * vmalloc fault has been handled.
	 *
	 * User-mode registers count as a user access even for any
	 * potential system fault or CPU buglet:
	 */
	if (user_mode_vm(regs)) {
 5d5:	41 f6 84 24 88 00 00 	testb  $0x3,0x88(%r12)
 5dc:	00 03 
 5de:	0f 85 da fd ff ff    	jne    3be <__do_page_fault+0x7e>
		local_irq_enable();
		error_code |= PF_USER;
		flags |= FAULT_FLAG_USER;
	} else {
		if (regs->flags & X86_EFLAGS_IF)
 5e4:	41 f6 84 24 91 00 00 	testb  $0x2,0x91(%r12)
 5eb:	00 02 
	struct vm_area_struct *vma;
	struct task_struct *tsk;
	unsigned long address;
	struct mm_struct *mm;
	int fault;
	unsigned int flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;
 5ed:	41 bf 28 00 00 00    	mov    $0x28,%r15d
	if (user_mode_vm(regs)) {
		local_irq_enable();
		error_code |= PF_USER;
		flags |= FAULT_FLAG_USER;
	} else {
		if (regs->flags & X86_EFLAGS_IF)
 5f3:	0f 84 d6 fd ff ff    	je     3cf <__do_page_fault+0x8f>
 5f9:	ff 14 25 00 00 00 00 	callq  *0x0
 600:	e9 ca fd ff ff       	jmpq   3cf <__do_page_fault+0x8f>
 605:	0f 1f 00             	nopl   (%rax)
perf_sw_event(u32 event_id, u64 nr, struct pt_regs *regs, u64 addr)
{
	struct pt_regs hot_regs;

	if (static_key_false(&perf_swevent_enabled[event_id])) {
		if (!regs) {
 608:	4d 85 e4             	test   %r12,%r12
 60b:	0f 84 af 00 00 00    	je     6c0 <__do_page_fault+0x380>
 611:	4c 89 e2             	mov    %r12,%rdx
			perf_fetch_caller_regs(&hot_regs);
			regs = &hot_regs;
		}
		__perf_sw_event(event_id, nr, regs, addr);
 614:	4c 89 f1             	mov    %r14,%rcx
 617:	be 01 00 00 00       	mov    $0x1,%esi
 61c:	bf 05 00 00 00       	mov    $0x5,%edi
 621:	44 89 85 08 ff ff ff 	mov    %r8d,-0xf8(%rbp)
 628:	e8 00 00 00 00       	callq  62d <__do_page_fault+0x2ed>
 62d:	44 8b 85 08 ff ff ff 	mov    -0xf8(%rbp),%r8d
 634:	e9 c5 fe ff ff       	jmpq   4fe <__do_page_fault+0x1be>
 639:	0f 1f 80 00 00 00 00 	nopl   0x0(%rax)
perf_sw_event(u32 event_id, u64 nr, struct pt_regs *regs, u64 addr)
{
	struct pt_regs hot_regs;

	if (static_key_false(&perf_swevent_enabled[event_id])) {
		if (!regs) {
 640:	4d 85 e4             	test   %r12,%r12
 643:	0f 84 bf 00 00 00    	je     708 <__do_page_fault+0x3c8>
 649:	4c 89 e2             	mov    %r12,%rdx
			perf_fetch_caller_regs(&hot_regs);
			regs = &hot_regs;
		}
		__perf_sw_event(event_id, nr, regs, addr);
 64c:	4c 89 f1             	mov    %r14,%rcx
 64f:	be 01 00 00 00       	mov    $0x1,%esi
 654:	bf 06 00 00 00       	mov    $0x6,%edi
 659:	44 89 85 08 ff ff ff 	mov    %r8d,-0xf8(%rbp)
 660:	e8 00 00 00 00       	callq  665 <__do_page_fault+0x325>
 665:	44 8b 85 08 ff ff ff 	mov    -0xf8(%rbp),%r8d
 66c:	e9 8d fe ff ff       	jmpq   4fe <__do_page_fault+0x1be>
 671:	0f 1f 80 00 00 00 00 	nopl   0x0(%rax)
 * - bp for callchains
 * - eflags, for future purposes, just in case
 */
static inline void perf_fetch_caller_regs(struct pt_regs *regs)
{
	memset(regs, 0, sizeof(*regs));
 678:	48 8d 95 30 ff ff ff 	lea    -0xd0(%rbp),%rdx
 67f:	4c 89 e0             	mov    %r12,%rax
 682:	b9 15 00 00 00       	mov    $0x15,%ecx
 687:	48 89 d7             	mov    %rdx,%rdi
 68a:	f3 48 ab             	rep stos %rax,%es:(%rdi)

	perf_arch_fetch_caller_regs(regs, CALLER_ADDR0);
 68d:	48 8b 45 08          	mov    0x8(%rbp),%rax
 691:	48 89 45 b0          	mov    %rax,-0x50(%rbp)
 695:	48 89 e8             	mov    %rbp,%rax
 698:	48 8b 00             	mov    (%rax),%rax
 69b:	48 c7 45 b8 10 00 00 	movq   $0x10,-0x48(%rbp)
 6a2:	00 
 6a3:	48 c7 45 c0 00 00 00 	movq   $0x0,-0x40(%rbp)
 6aa:	00 
 6ab:	48 89 85 50 ff ff ff 	mov    %rax,-0xb0(%rbp)
 6b2:	48 89 65 c8          	mov    %rsp,-0x38(%rbp)
 6b6:	e9 71 fe ff ff       	jmpq   52c <__do_page_fault+0x1ec>
 6bb:	0f 1f 44 00 00       	nopl   0x0(%rax,%rax,1)
 * - bp for callchains
 * - eflags, for future purposes, just in case
 */
static inline void perf_fetch_caller_regs(struct pt_regs *regs)
{
	memset(regs, 0, sizeof(*regs));
 6c0:	48 8d 95 30 ff ff ff 	lea    -0xd0(%rbp),%rdx
 6c7:	4c 89 e0             	mov    %r12,%rax
 6ca:	b9 15 00 00 00       	mov    $0x15,%ecx
 6cf:	48 89 d7             	mov    %rdx,%rdi
 6d2:	f3 48 ab             	rep stos %rax,%es:(%rdi)

	perf_arch_fetch_caller_regs(regs, CALLER_ADDR0);
 6d5:	48 8b 45 08          	mov    0x8(%rbp),%rax
 6d9:	48 89 45 b0          	mov    %rax,-0x50(%rbp)
 6dd:	48 89 e8             	mov    %rbp,%rax
 6e0:	48 8b 00             	mov    (%rax),%rax
 6e3:	48 c7 45 b8 10 00 00 	movq   $0x10,-0x48(%rbp)
 6ea:	00 
 6eb:	48 c7 45 c0 00 00 00 	movq   $0x0,-0x40(%rbp)
 6f2:	00 
 6f3:	48 89 85 50 ff ff ff 	mov    %rax,-0xb0(%rbp)
 6fa:	48 89 65 c8          	mov    %rsp,-0x38(%rbp)
 6fe:	e9 11 ff ff ff       	jmpq   614 <__do_page_fault+0x2d4>
 703:	0f 1f 44 00 00       	nopl   0x0(%rax,%rax,1)
 * - bp for callchains
 * - eflags, for future purposes, just in case
 */
static inline void perf_fetch_caller_regs(struct pt_regs *regs)
{
	memset(regs, 0, sizeof(*regs));
 708:	48 8d 95 30 ff ff ff 	lea    -0xd0(%rbp),%rdx
 70f:	4c 89 e0             	mov    %r12,%rax
 712:	b9 15 00 00 00       	mov    $0x15,%ecx
 717:	48 89 d7             	mov    %rdx,%rdi
 71a:	f3 48 ab             	rep stos %rax,%es:(%rdi)

	perf_arch_fetch_caller_regs(regs, CALLER_ADDR0);
 71d:	48 8b 45 08          	mov    0x8(%rbp),%rax
 721:	48 89 45 b0          	mov    %rax,-0x50(%rbp)
 725:	48 89 e8             	mov    %rbp,%rax
 728:	48 8b 00             	mov    (%rax),%rax
 72b:	48 c7 45 b8 10 00 00 	movq   $0x10,-0x48(%rbp)
 732:	00 
 733:	48 c7 45 c0 00 00 00 	movq   $0x0,-0x40(%rbp)
 73a:	00 
 73b:	48 89 85 50 ff ff ff 	mov    %rax,-0xb0(%rbp)
 742:	48 89 65 c8          	mov    %rsp,-0x38(%rbp)
 746:	e9 01 ff ff ff       	jmpq   64c <__do_page_fault+0x30c>
 74b:	65 48 8b 14 25 00 00 	mov    %gs:0x0,%rdx
 752:	00 00 
	return test_and_clear_ti_thread_flag(task_thread_info(tsk), flag);
}

static inline int test_tsk_thread_flag(struct task_struct *tsk, int flag)
{
	return test_ti_thread_flag(task_thread_info(tsk), flag);
 754:	48 8b 4a 08          	mov    0x8(%rdx),%rcx
 758:	48 8b 49 10          	mov    0x10(%rcx),%rcx
	return unlikely(sigismember(&p->pending.signal, SIGKILL));
}

static inline int fatal_signal_pending(struct task_struct *p)
{
	return signal_pending(p) && __fatal_signal_pending(p);
 75c:	80 e1 04             	and    $0x4,%cl
 75f:	0f 84 64 fd ff ff    	je     4c9 <__do_page_fault+0x189>
 765:	f6 82 01 06 00 00 01 	testb  $0x1,0x601(%rdx)
 76c:	0f 84 57 fd ff ff    	je     4c9 <__do_page_fault+0x189>
 772:	e9 8b fc ff ff       	jmpq   402 <__do_page_fault+0xc2>
	 */
	if (unlikely((fault & VM_FAULT_RETRY) && fatal_signal_pending(current)))
		return;

	if (unlikely(fault & VM_FAULT_ERROR)) {
		mm_fault_error(regs, error_code, address, fault);
 777:	89 c1                	mov    %eax,%ecx
 779:	4c 89 f2             	mov    %r14,%rdx
 77c:	48 89 de             	mov    %rbx,%rsi
 77f:	4c 89 e7             	mov    %r12,%rdi
 782:	e8 00 00 00 00       	callq  787 <__do_page_fault+0x447>
		return;
 787:	e9 76 fc ff ff       	jmpq   402 <__do_page_fault+0xc2>
		bad_area(regs, error_code, address);
		return;
	}
	if (likely(vma->vm_start <= address))
		goto good_area;
	if (unlikely(!(vma->vm_flags & VM_GROWSDOWN))) {
 78c:	f6 40 51 01          	testb  $0x1,0x51(%rax)
 790:	0f 84 5d 01 00 00    	je     8f3 <__do_page_fault+0x5b3>
		printk("bad not growndoan at %lx, vma=%lx start=%lx,end=%lx\n",address,vma,vma->vm_start,vma->vm_end);
		bad_area(regs, error_code, address);
		return;
	}
	if (error_code & PF_USER) {
 796:	f6 c3 04             	test   $0x4,%bl
 799:	74 15                	je     7b0 <__do_page_fault+0x470>
		 * Accessing the stack below %sp is always a bug.
		 * The large cushion allows instructions like enter
		 * and pusha to work. ("enter $65535, $31" pushes
		 * 32 pointers and then decrements %sp by 65535.)
		 */
		if (unlikely(address + 65536 + 32 * sizeof(unsigned long) < regs->sp)) {
 79b:	49 8d 86 00 01 01 00 	lea    0x10100(%r14),%rax
 7a2:	49 3b 84 24 98 00 00 	cmp    0x98(%r12),%rax
 7a9:	00 
 7aa:	0f 82 59 01 00 00    	jb     909 <__do_page_fault+0x5c9>
				printk("bad not +655 at %lx, vma=%lx start=%lx,end=%lx\n",address,vma,vma->vm_start,vma->vm_end);
bad_area(regs, error_code, address);
			return;
		}
	}
	if (unlikely(expand_stack(vma, address))) {
 7b0:	4c 89 cf             	mov    %r9,%rdi
 7b3:	4c 89 f6             	mov    %r14,%rsi
 7b6:	4c 89 8d 08 ff ff ff 	mov    %r9,-0xf8(%rbp)
 7bd:	e8 00 00 00 00       	callq  7c2 <__do_page_fault+0x482>
 7c2:	85 c0                	test   %eax,%eax
 7c4:	4c 8b 8d 08 ff ff ff 	mov    -0xf8(%rbp),%r9
 7cb:	0f 84 be fc ff ff    	je     48f <__do_page_fault+0x14f>
			printk("bad not expend at %lx, vma=%lx start=%lx,end=%lx\n",address,vma,vma->vm_start,vma->vm_end);
 7d1:	4d 8b 41 08          	mov    0x8(%r9),%r8
 7d5:	49 8b 09             	mov    (%r9),%rcx
 7d8:	4c 89 ca             	mov    %r9,%rdx
 7db:	4c 89 f6             	mov    %r14,%rsi
 7de:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
 7e5:	31 c0                	xor    %eax,%eax
 7e7:	e8 00 00 00 00       	callq  7ec <__do_page_fault+0x4ac>
bad_area(regs, error_code, address);
 7ec:	4c 89 f2             	mov    %r14,%rdx
 7ef:	48 89 de             	mov    %rbx,%rsi
 7f2:	4c 89 e7             	mov    %r12,%rdi
 7f5:	e8 00 00 00 00       	callq  7fa <__do_page_fault+0x4ba>
		return;
 7fa:	e9 03 fc ff ff       	jmpq   402 <__do_page_fault+0xc2>
	 * deadlock. Attempt to lock the address space, if we cannot we then
	 * validate the source. If this is invalid we can skip the address
	 * space check, thus avoiding the deadlock:
	 */
	if (unlikely(!down_read_trylock(&mm->mmap_sem))) {
		if ((error_code & PF_USER) == 0 &&
 7ff:	f6 c3 04             	test   $0x4,%bl
 802:	0f 85 03 fd ff ff    	jne    50b <__do_page_fault+0x1cb>
		    !search_exception_tables(regs->ip)) {
 808:	49 8b bc 24 80 00 00 	mov    0x80(%r12),%rdi
 80f:	00 
 810:	e8 00 00 00 00       	callq  815 <__do_page_fault+0x4d5>
	 * deadlock. Attempt to lock the address space, if we cannot we then
	 * validate the source. If this is invalid we can skip the address
	 * space check, thus avoiding the deadlock:
	 */
	if (unlikely(!down_read_trylock(&mm->mmap_sem))) {
		if ((error_code & PF_USER) == 0 &&
 815:	48 85 c0             	test   %rax,%rax
 818:	0f 85 ed fc ff ff    	jne    50b <__do_page_fault+0x1cb>
 81e:	e9 d1 fb ff ff       	jmpq   3f4 <__do_page_fault+0xb4>
	if (unlikely(fault & VM_FAULT_ERROR)) {
		mm_fault_error(regs, error_code, address, fault);
		return;
	}
	if(unlikely(fault& VM_FAULT_RSVD)){
			pgtable_bad(regs, error_code, address);
 823:	4c 89 f2             	mov    %r14,%rdx
 826:	48 89 de             	mov    %rbx,%rsi
 829:	4c 89 e7             	mov    %r12,%rdi
 82c:	44 89 85 04 ff ff ff 	mov    %r8d,-0xfc(%rbp)
 833:	89 85 08 ff ff ff    	mov    %eax,-0xf8(%rbp)
 839:	e8 00 00 00 00       	callq  83e <__do_page_fault+0x4fe>
 83e:	44 8b 85 04 ff ff ff 	mov    -0xfc(%rbp),%r8d
 845:	8b 85 08 ff ff ff    	mov    -0xf8(%rbp),%eax
 84b:	e9 8c fc ff ff       	jmpq   4dc <__do_page_fault+0x19c>
	 * This verifies that the fault happens in kernel space
	 * (error_code & 4) == 0, and that the fault was not a
	 * protection error (error_code & 9) == 0.
	 */
	if (unlikely(fault_in_kernel_space(address))) {
		if (!(error_code & (PF_RSVD | PF_USER | PF_PROT))) {
 850:	f6 c3 0d             	test   $0xd,%bl
 853:	75 10                	jne    865 <__do_page_fault+0x525>
			if (vmalloc_fault(address) >= 0)
 855:	4c 89 f7             	mov    %r14,%rdi
 858:	e8 a3 f7 ff ff       	callq  0 <vmalloc_fault>
 85d:	85 c0                	test   %eax,%eax
 85f:	0f 89 9d fb ff ff    	jns    402 <__do_page_fault+0xc2>
			if (kmemcheck_fault(regs, address, error_code))
				return;
		}

		/* Can handle a stale RO->RW TLB: */
		if (spurious_fault(error_code, address))
 865:	4c 89 f6             	mov    %r14,%rsi
 868:	48 89 df             	mov    %rbx,%rdi
 86b:	e8 9c f9 ff ff       	callq  20c <spurious_fault>
 870:	85 c0                	test   %eax,%eax
 872:	0f 85 8a fb ff ff    	jne    402 <__do_page_fault+0xc2>
static inline int __kprobes kprobes_fault(struct pt_regs *regs)
{
	int ret = 0;

	/* kprobe_running() needs smp_processor_id() */
	if (kprobes_built_in() && !user_mode_vm(regs)) {
 878:	41 f6 84 24 88 00 00 	testb  $0x3,0x88(%r12)
 87f:	00 03 
 881:	0f 85 6d fb ff ff    	jne    3f4 <__do_page_fault+0xb4>
 887:	65 48 8b 04 25 00 00 	mov    %gs:0x0,%rax
 88e:	00 00 
		preempt_disable();
		if (kprobe_running() && kprobe_fault_handler(regs, 14))
 890:	48 85 c0             	test   %rax,%rax
 893:	74 5a                	je     8ef <__do_page_fault+0x5af>
 895:	be 0e 00 00 00       	mov    $0xe,%esi
 89a:	4c 89 e7             	mov    %r12,%rdi
 89d:	e8 00 00 00 00       	callq  8a2 <__do_page_fault+0x562>
 8a2:	85 c0                	test   %eax,%eax
 8a4:	0f 95 c0             	setne  %al
 8a7:	0f b6 c0             	movzbl %al,%eax
		/* Can handle a stale RO->RW TLB: */
		if (spurious_fault(error_code, address))
			return;

		/* kprobes don't want to hook the spurious faults: */
		if (kprobes_fault(regs))
 8aa:	85 c0                	test   %eax,%eax
 8ac:	0f 85 50 fb ff ff    	jne    402 <__do_page_fault+0xc2>
 8b2:	e9 3d fb ff ff       	jmpq   3f4 <__do_page_fault+0xb4>
			if(handle_double_cache_mm_fault(mm,vma,address,flags)==0){
				return ;
			}
		}
		pgtable_bad(regs, error_code, address);*/
		flags|=FAULT_FLAG_RSVD;
 8b7:	41 81 cf 00 01 00 00 	or     $0x100,%r15d
 8be:	e9 1f fb ff ff       	jmpq   3e2 <__do_page_fault+0xa2>
 */
static inline int __kprobes
kmmio_fault(struct pt_regs *regs, unsigned long addr)
{
	if (unlikely(is_kmmio_active()))
		if (kmmio_handler(regs, addr) == 1)
 8c3:	4c 89 f6             	mov    %r14,%rsi
 8c6:	4c 89 e7             	mov    %r12,%rdi
 8c9:	e8 00 00 00 00       	callq  8ce <__do_page_fault+0x58e>
 8ce:	83 e8 01             	sub    $0x1,%eax
 8d1:	0f 85 c5 fa ff ff    	jne    39c <__do_page_fault+0x5c>
 8d7:	e9 26 fb ff ff       	jmpq   402 <__do_page_fault+0xc2>
			return 1;
		return 0;
	}

	/* read, present: */
	if (unlikely(error_code & PF_PROT)&&(!(error_code&PF_RSVD)))
 8dc:	48 83 bd 10 ff ff ff 	cmpq   $0x0,-0xf0(%rbp)
 8e3:	00 
 8e4:	0f 85 7f fc ff ff    	jne    569 <__do_page_fault+0x229>
 8ea:	e9 87 fc ff ff       	jmpq   576 <__do_page_fault+0x236>
	return 0;
}

static inline int __kprobes kprobes_fault(struct pt_regs *regs)
{
	int ret = 0;
 8ef:	31 c0                	xor    %eax,%eax
 8f1:	eb b7                	jmp    8aa <__do_page_fault+0x56a>
		return;
	}
	if (likely(vma->vm_start <= address))
		goto good_area;
	if (unlikely(!(vma->vm_flags & VM_GROWSDOWN))) {
		printk("bad not growndoan at %lx, vma=%lx start=%lx,end=%lx\n",address,vma,vma->vm_start,vma->vm_end);
 8f3:	4c 8b 40 08          	mov    0x8(%rax),%r8
 8f7:	48 89 c2             	mov    %rax,%rdx
 8fa:	4c 89 f6             	mov    %r14,%rsi
 8fd:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
 904:	e9 dc fe ff ff       	jmpq   7e5 <__do_page_fault+0x4a5>
		 * The large cushion allows instructions like enter
		 * and pusha to work. ("enter $65535, $31" pushes
		 * 32 pointers and then decrements %sp by 65535.)
		 */
		if (unlikely(address + 65536 + 32 * sizeof(unsigned long) < regs->sp)) {
				printk("bad not +655 at %lx, vma=%lx start=%lx,end=%lx\n",address,vma,vma->vm_start,vma->vm_end);
 909:	4d 8b 41 08          	mov    0x8(%r9),%r8
 90d:	4c 89 ca             	mov    %r9,%rdx
 910:	4c 89 f6             	mov    %r14,%rsi
 913:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
 91a:	e9 c6 fe ff ff       	jmpq   7e5 <__do_page_fault+0x4a5>
 91f:	90                   	nop

0000000000000920 <do_page_fault>:
	up_read(&mm->mmap_sem);
}

dotraplinkage void __kprobes
do_page_fault(struct pt_regs *regs, unsigned long error_code)
{
 920:	e8 00 00 00 00       	callq  925 <do_page_fault+0x5>
 925:	55                   	push   %rbp
 926:	48 89 e5             	mov    %rsp,%rbp
 929:	53                   	push   %rbx
 92a:	48 83 ec 10          	sub    $0x10,%rsp
 92e:	0f 1f 44 00 00       	nopl   0x0(%rax,%rax,1)
static inline enum ctx_state exception_enter(void)
{
	enum ctx_state prev_ctx;

	if (!static_key_false(&context_tracking_enabled))
		return 0;
 933:	31 db                	xor    %ebx,%ebx
	enum ctx_state prev_state;

	prev_state = exception_enter();
	__do_page_fault(regs, error_code);
 935:	e8 06 fa ff ff       	callq  340 <__do_page_fault>
 93a:	0f 1f 44 00 00       	nopl   0x0(%rax,%rax,1)
	exception_exit(prev_state);
}
 93f:	48 83 c4 10          	add    $0x10,%rsp
 943:	5b                   	pop    %rbx
 944:	5d                   	pop    %rbp
 945:	c3                   	retq   
 946:	66 2e 0f 1f 84 00 00 	nopw   %cs:0x0(%rax,%rax,1)
 94d:	00 00 00 
 950:	48 89 75 e8          	mov    %rsi,-0x18(%rbp)
 954:	48 89 7d f0          	mov    %rdi,-0x10(%rbp)

	prev_ctx = this_cpu_read(context_tracking.state);
 958:	65 8b 1c 25 00 00 00 	mov    %gs:0x0,%ebx
 95f:	00 
	context_tracking_user_exit();
 960:	e8 00 00 00 00       	callq  965 <do_page_fault+0x45>
 965:	48 8b 7d f0          	mov    -0x10(%rbp),%rdi
 969:	48 8b 75 e8          	mov    -0x18(%rbp),%rsi
 96d:	eb c6                	jmp    935 <do_page_fault+0x15>
 96f:	90                   	nop
}

static inline void exception_exit(enum ctx_state prev_ctx)
{
	if (static_key_false(&context_tracking_enabled)) {
		if (prev_ctx == IN_USER)
 970:	83 fb 01             	cmp    $0x1,%ebx
 973:	75 ca                	jne    93f <do_page_fault+0x1f>
			context_tracking_user_enter();
 975:	e8 00 00 00 00       	callq  97a <do_page_fault+0x5a>
 97a:	48 83 c4 10          	add    $0x10,%rsp
 97e:	5b                   	pop    %rbx
 97f:	5d                   	pop    %rbp
 980:	c3                   	retq   
 981:	66 66 66 66 66 66 2e 	data32 data32 data32 data32 data32 nopw %cs:0x0(%rax,%rax,1)
 988:	0f 1f 84 00 00 00 00 
 98f:	00 

0000000000000990 <trace_do_page_fault>:
		trace_page_fault_kernel(read_cr2(), regs, error_code);
}

dotraplinkage void __kprobes
trace_do_page_fault(struct pt_regs *regs, unsigned long error_code)
{
 990:	e8 00 00 00 00       	callq  995 <trace_do_page_fault+0x5>
 995:	55                   	push   %rbp
 996:	48 89 e5             	mov    %rsp,%rbp
 999:	41 57                	push   %r15
 99b:	41 56                	push   %r14
 99d:	41 55                	push   %r13
 99f:	41 54                	push   %r12
 9a1:	49 89 f4             	mov    %rsi,%r12
 9a4:	53                   	push   %rbx
 9a5:	48 89 fb             	mov    %rdi,%rbx
 9a8:	0f 1f 44 00 00       	nopl   0x0(%rax,%rax,1)
static inline enum ctx_state exception_enter(void)
{
	enum ctx_state prev_ctx;

	if (!static_key_false(&context_tracking_enabled))
		return 0;
 9ad:	45 31 f6             	xor    %r14d,%r14d
}

static void trace_page_fault_entries(struct pt_regs *regs,
				     unsigned long error_code)
{
	if (user_mode(regs))
 9b0:	f6 83 88 00 00 00 03 	testb  $0x3,0x88(%rbx)
 9b7:	75 46                	jne    9ff <trace_do_page_fault+0x6f>
	PVOP_VCALL1(pv_cpu_ops.write_cr0, x);
}

static inline unsigned long read_cr2(void)
{
	return PVOP_CALL0(unsigned long, pv_mmu_ops.read_cr2);
 9b9:	ff 14 25 00 00 00 00 	callq  *0x0
 9c0:	49 89 c5             	mov    %rax,%r13
 9c3:	0f 1f 44 00 00       	nopl   0x0(%rax,%rax,1)
{
	enum ctx_state prev_state;

	prev_state = exception_enter();
	trace_page_fault_entries(regs, error_code);
	__do_page_fault(regs, error_code);
 9c8:	4c 89 e6             	mov    %r12,%rsi
 9cb:	48 89 df             	mov    %rbx,%rdi
 9ce:	e8 6d f9 ff ff       	callq  340 <__do_page_fault>
 9d3:	0f 1f 44 00 00       	nopl   0x0(%rax,%rax,1)
	exception_exit(prev_state);
}
 9d8:	5b                   	pop    %rbx
 9d9:	41 5c                	pop    %r12
 9db:	41 5d                	pop    %r13
 9dd:	41 5e                	pop    %r14
 9df:	41 5f                	pop    %r15
 9e1:	5d                   	pop    %rbp
 9e2:	c3                   	retq   
 9e3:	0f 1f 44 00 00       	nopl   0x0(%rax,%rax,1)

	prev_ctx = this_cpu_read(context_tracking.state);
 9e8:	65 44 8b 34 25 00 00 	mov    %gs:0x0,%r14d
 9ef:	00 00 
	context_tracking_user_exit();
 9f1:	e8 00 00 00 00       	callq  9f6 <trace_do_page_fault+0x66>
}

static void trace_page_fault_entries(struct pt_regs *regs,
				     unsigned long error_code)
{
	if (user_mode(regs))
 9f6:	f6 83 88 00 00 00 03 	testb  $0x3,0x88(%rbx)
 9fd:	74 ba                	je     9b9 <trace_do_page_fault+0x29>
 9ff:	ff 14 25 00 00 00 00 	callq  *0x0
 a06:	49 89 c5             	mov    %rax,%r13
 a09:	0f 1f 44 00 00       	nopl   0x0(%rax,%rax,1)
 a0e:	eb b8                	jmp    9c8 <trace_do_page_fault+0x38>
}

static inline void exception_exit(enum ctx_state prev_ctx)
{
	if (static_key_false(&context_tracking_enabled)) {
		if (prev_ctx == IN_USER)
 a10:	41 83 fe 01          	cmp    $0x1,%r14d
 a14:	75 c2                	jne    9d8 <trace_do_page_fault+0x48>
			context_tracking_user_enter();
 a16:	e8 00 00 00 00       	callq  a1b <trace_do_page_fault+0x8b>

	prev_state = exception_enter();
	trace_page_fault_entries(regs, error_code);
	__do_page_fault(regs, error_code);
	exception_exit(prev_state);
}
 a1b:	5b                   	pop    %rbx
 a1c:	41 5c                	pop    %r12
 a1e:	41 5d                	pop    %r13
 a20:	41 5e                	pop    %r14
 a22:	41 5f                	pop    %r15
 a24:	5d                   	pop    %rbp
 a25:	c3                   	retq   
 a26:	66 2e 0f 1f 84 00 00 	nopw   %cs:0x0(%rax,%rax,1)
 a2d:	00 00 00 
	TP_ARGS(address, regs, error_code),			\
	trace_irq_vector_regfunc,				\
	trace_irq_vector_unregfunc);

DEFINE_PAGE_FAULT_EVENT(page_fault_user);
DEFINE_PAGE_FAULT_EVENT(page_fault_kernel);
 a30:	4c 8b 3d 00 00 00 00 	mov    0x0(%rip),%r15        # a37 <trace_do_page_fault+0xa7>
 a37:	4d 85 ff             	test   %r15,%r15
 a3a:	74 1f                	je     a5b <trace_do_page_fault+0xcb>
 a3c:	49 8b 07             	mov    (%r15),%rax
 a3f:	90                   	nop
 a40:	49 8b 7f 08          	mov    0x8(%r15),%rdi
 a44:	49 83 c7 10          	add    $0x10,%r15
 a48:	4c 89 e1             	mov    %r12,%rcx
 a4b:	48 89 da             	mov    %rbx,%rdx
 a4e:	4c 89 ee             	mov    %r13,%rsi
 a51:	ff d0                	callq  *%rax
 a53:	49 8b 07             	mov    (%r15),%rax
 a56:	48 85 c0             	test   %rax,%rax
 a59:	75 e5                	jne    a40 <trace_do_page_fault+0xb0>

/* Used by lockdep and tracing: cannot be traced, cannot call lockdep. */
static inline notrace void rcu_read_unlock_sched_notrace(void)
{
	__release(RCU_SCHED);
	preempt_enable_notrace();
 a5b:	e9 68 ff ff ff       	jmpq   9c8 <trace_do_page_fault+0x38>
		 unsigned long error_code),			\
	TP_ARGS(address, regs, error_code),			\
	trace_irq_vector_regfunc,				\
	trace_irq_vector_unregfunc);

DEFINE_PAGE_FAULT_EVENT(page_fault_user);
 a60:	4c 8b 3d 00 00 00 00 	mov    0x0(%rip),%r15        # a67 <trace_do_page_fault+0xd7>
 a67:	4d 85 ff             	test   %r15,%r15
 a6a:	74 ef                	je     a5b <trace_do_page_fault+0xcb>
 a6c:	49 8b 07             	mov    (%r15),%rax
 a6f:	90                   	nop
 a70:	49 8b 7f 08          	mov    0x8(%r15),%rdi
 a74:	49 83 c7 10          	add    $0x10,%r15
 a78:	4c 89 e1             	mov    %r12,%rcx
 a7b:	48 89 da             	mov    %rbx,%rdx
 a7e:	4c 89 ee             	mov    %r13,%rsi
 a81:	ff d0                	callq  *%rax
 a83:	49 8b 07             	mov    (%r15),%rax
 a86:	48 85 c0             	test   %rax,%rax
 a89:	75 e5                	jne    a70 <trace_do_page_fault+0xe0>
 a8b:	e9 38 ff ff ff       	jmpq   9c8 <trace_do_page_fault+0x38>

Disassembly of section .init.text:

0000000000000000 <ftrace_define_fields_x86_exceptions>:
   0:	55                   	push   %rbp
   1:	45 31 c9             	xor    %r9d,%r9d
   4:	41 b8 08 00 00 00    	mov    $0x8,%r8d
   a:	48 c7 c2 00 00 00 00 	mov    $0x0,%rdx
  11:	48 c7 c6 00 00 00 00 	mov    $0x0,%rsi
  18:	48 89 e5             	mov    %rsp,%rbp
  1b:	53                   	push   %rbx
  1c:	48 89 fb             	mov    %rdi,%rbx
  1f:	51                   	push   %rcx
  20:	b9 08 00 00 00       	mov    $0x8,%ecx
  25:	c7 04 24 00 00 00 00 	movl   $0x0,(%rsp)
  2c:	e8 00 00 00 00       	callq  31 <ftrace_define_fields_x86_exceptions+0x31>
  31:	85 c0                	test   %eax,%eax
  33:	75 5a                	jne    8f <ftrace_define_fields_x86_exceptions+0x8f>
  35:	45 31 c9             	xor    %r9d,%r9d
  38:	c7 04 24 00 00 00 00 	movl   $0x0,(%rsp)
  3f:	41 b8 08 00 00 00    	mov    $0x8,%r8d
  45:	b9 10 00 00 00       	mov    $0x10,%ecx
  4a:	48 c7 c2 00 00 00 00 	mov    $0x0,%rdx
  51:	48 c7 c6 00 00 00 00 	mov    $0x0,%rsi
  58:	48 89 df             	mov    %rbx,%rdi
  5b:	e8 00 00 00 00       	callq  60 <ftrace_define_fields_x86_exceptions+0x60>
  60:	85 c0                	test   %eax,%eax
  62:	75 2b                	jne    8f <ftrace_define_fields_x86_exceptions+0x8f>
  64:	c7 04 24 00 00 00 00 	movl   $0x0,(%rsp)
  6b:	45 31 c9             	xor    %r9d,%r9d
  6e:	41 b8 08 00 00 00    	mov    $0x8,%r8d
  74:	b9 18 00 00 00       	mov    $0x18,%ecx
  79:	48 c7 c2 00 00 00 00 	mov    $0x0,%rdx
  80:	48 c7 c6 00 00 00 00 	mov    $0x0,%rsi
  87:	48 89 df             	mov    %rbx,%rdi
  8a:	e8 00 00 00 00       	callq  8f <ftrace_define_fields_x86_exceptions+0x8f>
  8f:	5a                   	pop    %rdx
  90:	5b                   	pop    %rbx
  91:	5d                   	pop    %rbp
  92:	c3                   	retq   
