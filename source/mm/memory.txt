
memory.o:     file format elf64-x86-64


Disassembly of section .text:

0000000000000000 <pte_offset_kernel>:
{
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
}

static inline pte_t *pte_offset_kernel(pmd_t *pmd, unsigned long address)
{
       0:	55                   	push   %rbp

	if (sizeof(pmdval_t) > sizeof(long))
		ret =  PVOP_CALLEE2(pmdval_t, pv_mmu_ops.pmd_val,
				    pmd.pmd, (u64)pmd.pmd >> 32);
	else
		ret =  PVOP_CALLEE1(pmdval_t, pv_mmu_ops.pmd_val,
       1:	48 8b 3f             	mov    (%rdi),%rdi
       4:	48 89 e5             	mov    %rsp,%rbp
       7:	ff 14 25 00 00 00 00 	callq  *0x0
 * this function returns the index of the entry in the pte page which would
 * control the given virtual address
 */
static inline unsigned long pte_index(unsigned long address)
{
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
       e:	48 c1 ee 09          	shr    $0x9,%rsi
	return (unsigned long)native_pmd_val(pmd) == 0;
}

static inline unsigned long pmd_page_vaddr(pmd_t pmd)
{
	return (unsigned long)__va(pmd_val(pmd) & PTE_PFN_MASK);
      12:	48 ba 00 00 00 00 00 	movabs $0xffff880000000000,%rdx
      19:	88 ff ff 
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
}

static inline pte_t *pte_offset_kernel(pmd_t *pmd, unsigned long address)
{
	return (pte_t *)pmd_page_vaddr(*pmd) + pte_index(address);
      1c:	81 e6 f8 0f 00 00    	and    $0xff8,%esi
	return (unsigned long)native_pmd_val(pmd) == 0;
}

static inline unsigned long pmd_page_vaddr(pmd_t pmd)
{
	return (unsigned long)__va(pmd_val(pmd) & PTE_PFN_MASK);
      22:	48 01 d6             	add    %rdx,%rsi
      25:	48 ba 00 f0 ff ff ff 	movabs $0x3ffffffff000,%rdx
      2c:	3f 00 00 
      2f:	48 21 d0             	and    %rdx,%rax
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
}

static inline pte_t *pte_offset_kernel(pmd_t *pmd, unsigned long address)
{
	return (pte_t *)pmd_page_vaddr(*pmd) + pte_index(address);
      32:	48 01 f0             	add    %rsi,%rax
}
      35:	5d                   	pop    %rbp
      36:	c3                   	retq   
      37:	66 0f 1f 84 00 00 00 	nopw   0x0(%rax,%rax,1)
      3e:	00 00 

0000000000000040 <page_dup_rmap>:
	atomic_inc(&page->_mapcount);
	inc_page_counter_by_ns(page,ns_of_pid(task_pid(current)));
}
EXPORT_SYMBOL(get_huge_page_tail);/*mm.h*/
void page_dup_rmap(struct page *page)
{
      40:	e8 00 00 00 00       	callq  45 <page_dup_rmap+0x5>
      45:	55                   	push   %rbp
      46:	48 89 e5             	mov    %rsp,%rbp
 *
 * Atomically increments @v by 1.
 */
static inline void atomic_inc(atomic_t *v)
{
	asm volatile(LOCK_PREFIX "incl %0"
      49:	f0 ff 47 18          	lock incl 0x18(%rdi)
	atomic_inc(&page->_mapcount);
	//	inc_page_counter_by_ns(page,ns_of_pid(task_pid(current)));

}
      4d:	5d                   	pop    %rbp
      4e:	c3                   	retq   
      4f:	90                   	nop

0000000000000050 <__get_page_tail_foll>:
DEFINE_PER_CPU(unsigned long long,global_ins_count)=0;

static int handle_double_cache_pte_fault(struct mm_struct *mm, struct vm_area_struct *vma,unsigned long address, pte_t *page_table, pmd_t *pmd, spinlock_t *ptl, pte_t orig_pte,bool invalid);
void __get_page_tail_foll(struct page *page,
			bool get_page_head)
{
      50:	e8 00 00 00 00       	callq  55 <__get_page_tail_foll+0x5>
      55:	55                   	push   %rbp
	 * page_cache_get_speculative()) on tail pages.
	 */
	VM_BUG_ON(atomic_read(&page->first_page->_count) <= 0);
	VM_BUG_ON(atomic_read(&page->_count) != 0);
	VM_BUG_ON(page_mapcount(page) < 0);
	if (get_page_head){
      56:	40 84 f6             	test   %sil,%sil
DEFINE_PER_CPU(unsigned long long,global_ins_count)=0;

static int handle_double_cache_pte_fault(struct mm_struct *mm, struct vm_area_struct *vma,unsigned long address, pte_t *page_table, pmd_t *pmd, spinlock_t *ptl, pte_t orig_pte,bool invalid);
void __get_page_tail_foll(struct page *page,
			bool get_page_head)
{
      59:	48 89 e5             	mov    %rsp,%rbp
	 * page_cache_get_speculative()) on tail pages.
	 */
	VM_BUG_ON(atomic_read(&page->first_page->_count) <= 0);
	VM_BUG_ON(atomic_read(&page->_count) != 0);
	VM_BUG_ON(page_mapcount(page) < 0);
	if (get_page_head){
      5c:	74 08                	je     66 <__get_page_tail_foll+0x16>
		atomic_inc(&page->first_page->_count);
      5e:	48 8b 47 30          	mov    0x30(%rdi),%rax
      62:	f0 ff 40 1c          	lock incl 0x1c(%rax)
      66:	f0 ff 47 18          	lock incl 0x18(%rdi)

DECLARE_PER_CPU(struct task_struct *, current_task);

static __always_inline struct task_struct *get_current(void)
{
	return this_cpu_read_stable(current_task);
      6a:	65 48 8b 04 25 00 00 	mov    %gs:0x0,%rax
      71:	00 00 
}
#endif

static inline struct pid *task_pid(struct task_struct *task)
{
	return task->pids[PIDTYPE_PID].pid;
      73:	48 8b 80 60 03 00 00 	mov    0x360(%rax),%rax
 * 	the resulting NULL pid-ns.
 */
static inline struct pid_namespace *ns_of_pid(struct pid *pid)
{
	struct pid_namespace *ns = NULL;
	if (pid)
      7a:	48 85 c0             	test   %rax,%rax
      7d:	74 19                	je     98 <__get_page_tail_foll+0x48>
		ns = pid->numbers[pid->level].ns;
      7f:	8b 50 04             	mov    0x4(%rax),%edx
      82:	48 c1 e2 05          	shl    $0x5,%rdx
      86:	48 8b 74 10 38       	mov    0x38(%rax,%rdx,1),%rsi
	}
	atomic_inc(&page->_mapcount);
	inc_page_counter_by_ns(page,ns_of_pid(task_pid(current)));
      8b:	e8 00 00 00 00       	callq  90 <__get_page_tail_foll+0x40>
}
      90:	5d                   	pop    %rbp
      91:	c3                   	retq   
      92:	66 0f 1f 44 00 00    	nopw   0x0(%rax,%rax,1)
 * 	is expected to be non-NULL. If @pid is NULL, caller should handle
 * 	the resulting NULL pid-ns.
 */
static inline struct pid_namespace *ns_of_pid(struct pid *pid)
{
	struct pid_namespace *ns = NULL;
      98:	31 f6                	xor    %esi,%esi
	VM_BUG_ON(page_mapcount(page) < 0);
	if (get_page_head){
		atomic_inc(&page->first_page->_count);
	}
	atomic_inc(&page->_mapcount);
	inc_page_counter_by_ns(page,ns_of_pid(task_pid(current)));
      9a:	e8 00 00 00 00       	callq  9f <__get_page_tail_foll+0x4f>
}
      9f:	5d                   	pop    %rbp
      a0:	c3                   	retq   
      a1:	66 66 66 66 66 66 2e 	data32 data32 data32 data32 data32 nopw %cs:0x0(%rax,%rax,1)
      a8:	0f 1f 84 00 00 00 00 
      af:	00 

00000000000000b0 <get_huge_page_tail>:
EXPORT_SYMBOL(__get_page_tail_foll);/*internal.h*/
void get_huge_page_tail(struct page* page)
{
      b0:	e8 00 00 00 00       	callq  b5 <get_huge_page_tail+0x5>
      b5:	55                   	push   %rbp
      b6:	48 89 e5             	mov    %rsp,%rbp
      b9:	f0 ff 47 18          	lock incl 0x18(%rdi)
      bd:	65 48 8b 04 25 00 00 	mov    %gs:0x0,%rax
      c4:	00 00 
      c6:	48 8b 80 60 03 00 00 	mov    0x360(%rax),%rax
	if (pid)
      cd:	48 85 c0             	test   %rax,%rax
      d0:	74 16                	je     e8 <get_huge_page_tail+0x38>
		ns = pid->numbers[pid->level].ns;
      d2:	8b 50 04             	mov    0x4(%rax),%edx
      d5:	48 c1 e2 05          	shl    $0x5,%rdx
      d9:	48 8b 74 10 38       	mov    0x38(%rax,%rdx,1),%rsi
	 * from under us.
	 */
	VM_BUG_ON(page_mapcount(page) < 0);
	VM_BUG_ON(atomic_read(&page->_count) != 0);
	atomic_inc(&page->_mapcount);
	inc_page_counter_by_ns(page,ns_of_pid(task_pid(current)));
      de:	e8 00 00 00 00       	callq  e3 <get_huge_page_tail+0x33>
}
      e3:	5d                   	pop    %rbp
      e4:	c3                   	retq   
      e5:	0f 1f 00             	nopl   (%rax)
 * 	is expected to be non-NULL. If @pid is NULL, caller should handle
 * 	the resulting NULL pid-ns.
 */
static inline struct pid_namespace *ns_of_pid(struct pid *pid)
{
	struct pid_namespace *ns = NULL;
      e8:	31 f6                	xor    %esi,%esi
	 * from under us.
	 */
	VM_BUG_ON(page_mapcount(page) < 0);
	VM_BUG_ON(atomic_read(&page->_count) != 0);
	atomic_inc(&page->_mapcount);
	inc_page_counter_by_ns(page,ns_of_pid(task_pid(current)));
      ea:	e8 00 00 00 00       	callq  ef <get_huge_page_tail+0x3f>
}
      ef:	5d                   	pop    %rbp
      f0:	c3                   	retq   
      f1:	66 66 66 66 66 66 2e 	data32 data32 data32 data32 data32 nopw %cs:0x0(%rax,%rax,1)
      f8:	0f 1f 84 00 00 00 00 
      ff:	00 

0000000000000100 <get_page>:
}

static __always_inline int constant_test_bit(long nr, const volatile unsigned long *addr)
{
	return ((1UL << (nr & (BITS_PER_LONG-1))) &
		(addr[nr >> _BITOPS_LONG_SHIFT])) != 0;
     100:	48 8b 07             	mov    (%rdi),%rax
}
void get_huge_page_tail(struct page *page);/*ziqiao*/
extern bool __get_page_tail(struct page *page);

static inline void get_page(struct page *page)
{
     103:	55                   	push   %rbp
     104:	48 89 e5             	mov    %rsp,%rbp
     107:	53                   	push   %rbx
     108:	48 89 fb             	mov    %rdi,%rbx
	if (unlikely(PageTail(page)))
     10b:	f6 c4 80             	test   $0x80,%ah
     10e:	75 07                	jne    117 <get_page+0x17>
     110:	f0 ff 43 1c          	lock incl 0x1c(%rbx)
	 * Getting a normal page or the head of a compound page
	 * requires to already have an elevated page->_count.
	 */
	VM_BUG_ON(atomic_read(&page->_count) <= 0);
	atomic_inc(&page->_count);
}
     114:	5b                   	pop    %rbx
     115:	5d                   	pop    %rbp
     116:	c3                   	retq   
extern bool __get_page_tail(struct page *page);

static inline void get_page(struct page *page)
{
	if (unlikely(PageTail(page)))
		if (likely(__get_page_tail(page)))
     117:	e8 00 00 00 00       	callq  11c <get_page+0x1c>
     11c:	84 c0                	test   %al,%al
     11e:	66 90                	xchg   %ax,%ax
     120:	75 f2                	jne    114 <get_page+0x14>
     122:	eb ec                	jmp    110 <get_page+0x10>
     124:	66 66 66 2e 0f 1f 84 	data32 data32 nopw %cs:0x0(%rax,%rax,1)
     12b:	00 00 00 00 00 

0000000000000130 <pte_lockptr.isra.16>:
{
	return &page->ptl;
}
#endif /* ALLOC_SPLIT_PTLOCKS */

static inline spinlock_t *pte_lockptr(struct mm_struct *mm, pmd_t *pmd)
     130:	55                   	push   %rbp
     131:	48 8b 3f             	mov    (%rdi),%rdi
     134:	48 89 e5             	mov    %rsp,%rbp
     137:	ff 14 25 00 00 00 00 	callq  *0x0
{
	return ptlock_ptr(pmd_page(*pmd));
     13e:	48 c1 e0 12          	shl    $0x12,%rax
     142:	48 ba 00 00 00 00 00 	movabs $0xffffea0000000000,%rdx
     149:	ea ff ff 
     14c:	48 c1 e8 1e          	shr    $0x1e,%rax
     150:	48 c1 e0 06          	shl    $0x6,%rax
     154:	48 8b 44 10 30       	mov    0x30(%rax,%rdx,1),%rax
}
     159:	5d                   	pop    %rbp
     15a:	c3                   	retq   
     15b:	0f 1f 44 00 00       	nopl   0x0(%rax,%rax,1)

0000000000000160 <add_mm_counter_fast>:
	}
	current->rss_stat.events = 0;
}

static void add_mm_counter_fast(struct mm_struct *mm, int member, int val)
{
     160:	e8 00 00 00 00       	callq  165 <add_mm_counter_fast+0x5>
     165:	55                   	push   %rbp
     166:	48 63 f6             	movslq %esi,%rsi
     169:	65 48 8b 04 25 00 00 	mov    %gs:0x0,%rax
     170:	00 00 
	struct task_struct *task = current;

	if (likely(task->mm == mm))
     172:	48 39 b8 a8 02 00 00 	cmp    %rdi,0x2a8(%rax)
	}
	current->rss_stat.events = 0;
}

static void add_mm_counter_fast(struct mm_struct *mm, int member, int val)
{
     179:	48 89 e5             	mov    %rsp,%rbp
	struct task_struct *task = current;

	if (likely(task->mm == mm))
     17c:	75 09                	jne    187 <add_mm_counter_fast+0x27>
	  task->rss_stat.count[member] += val;
     17e:	01 94 b0 bc 02 00 00 	add    %edx,0x2bc(%rax,%rsi,4)
	else
	  add_mm_counter(mm, member, val);
}
     185:	5d                   	pop    %rbp
     186:	c3                   	retq   
	struct task_struct *task = current;

	if (likely(task->mm == mm))
	  task->rss_stat.count[member] += val;
	else
	  add_mm_counter(mm, member, val);
     187:	48 63 d2             	movslq %edx,%rdx
 *
 * Atomically adds @i to @v.
 */
static inline void atomic64_add(long i, atomic64_t *v)
{
	asm volatile(LOCK_PREFIX "addq %1,%0"
     18a:	f0 48 01 94 f7 c8 02 	lock add %rdx,0x2c8(%rdi,%rsi,8)
     191:	00 00 
}
     193:	5d                   	pop    %rbp
     194:	c3                   	retq   
     195:	66 66 2e 0f 1f 84 00 	data32 nopw %cs:0x0(%rax,%rax,1)
     19c:	00 00 00 00 

00000000000001a0 <pte_to_swp_entry>:
	return pte.pte;
}

static inline pteval_t pte_flags(pte_t pte)
{
	return native_pte_val(pte) & PTE_FLAGS_MASK;
     1a0:	48 b8 ff 0f 00 00 00 	movabs $0xffffc00000000fff,%rax
     1a7:	c0 ff ff 
     1aa:	48 21 f8             	and    %rdi,%rax
	return ret;
}
static inline swp_entry_t pte_to_swp_entry(pte_t pte)
{
	swp_entry_t arch_entry;
	BUG_ON(pte_file(pte));
     1ad:	a8 40                	test   $0x40,%al
     1af:	75 43                	jne    1f4 <pte_to_swp_entry+0x54>

static inline pte_t pte_clear_flags(pte_t pte, pteval_t clear)
{
	pteval_t v = native_pte_val(pte);

	return native_make_pte(v & ~clear);
     1b1:	48 89 fa             	mov    %rdi,%rdx
     1b4:	80 e2 7f             	and    $0x7f,%dl
     1b7:	a8 80                	test   $0x80,%al
     1b9:	48 0f 45 fa          	cmovne %rdx,%rdi
     1bd:	48 ba 00 00 00 00 00 	movabs $0x4000000000000,%rdx
     1c4:	00 04 00 
     1c7:	48 89 f8             	mov    %rdi,%rax
     1ca:	48 83 e0 ef          	and    $0xffffffffffffffef,%rax
     1ce:	48 85 d7             	test   %rdx,%rdi
     1d1:	48 0f 45 f8          	cmovne %rax,%rdi

	if (sizeof(pteval_t) > sizeof(long))
		ret = PVOP_CALLEE2(pteval_t, pv_mmu_ops.pte_val,
				   pte.pte, (u64)pte.pte >> 32);
	else
		ret = PVOP_CALLEE1(pteval_t, pv_mmu_ops.pte_val,
     1d5:	ff 14 25 00 00 00 00 	callq  *0x0
	if (pte_swp_soft_dirty(pte))	
	  pte = pte_swp_clear_soft_dirty(pte);
	if(	pte.pte & _PAGE_NCACHE)
	  pte= pte_clear_flags(pte, _PAGE_CACHE_UC_MINUS);
	arch_entry = __pte_to_swp_entry(pte);
	return swp_entry(__swp_type(arch_entry), __swp_offset(arch_entry));
     1dc:	48 89 c2             	mov    %rax,%rdx
     1df:	48 c1 e8 09          	shr    $0x9,%rax
     1e3:	48 d1 ea             	shr    %rdx
     1e6:	83 e2 1f             	and    $0x1f,%edx
 */
static inline swp_entry_t swp_entry(unsigned long type, pgoff_t offset)
{
	swp_entry_t ret;

	ret.val = (type << SWP_TYPE_SHIFT(ret)) |
     1e9:	48 c1 e2 39          	shl    $0x39,%rdx
     1ed:	48 09 c2             	or     %rax,%rdx
	  pte = pte_swp_clear_soft_dirty(pte);
	if(	pte.pte & _PAGE_NCACHE)
	  pte= pte_clear_flags(pte, _PAGE_CACHE_UC_MINUS);
	arch_entry = __pte_to_swp_entry(pte);
	return swp_entry(__swp_type(arch_entry), __swp_offset(arch_entry));
}
     1f0:	48 89 d0             	mov    %rdx,%rax
     1f3:	c3                   	retq   
	ret.val = (type << SWP_TYPE_SHIFT(ret)) |
			(offset & SWP_OFFSET_MASK(ret));
	return ret;
}
static inline swp_entry_t pte_to_swp_entry(pte_t pte)
{
     1f4:	55                   	push   %rbp
     1f5:	48 89 e5             	mov    %rsp,%rbp
     1f8:	e8 00 00 00 00       	callq  1fd <pte_to_swp_entry+0x5d>
     1fd:	0f 1f 00             	nopl   (%rax)

0000000000000200 <do_file_page_set.isra.55>:
	//mutex_unlock(&mapping->i_mmap_mutex);
	return ret;
}


static int do_file_page_set(struct page* page,struct pid_namespace* pid_ns_ref, pteval_t flags,int count){
     200:	e8 00 00 00 00       	callq  205 <do_file_page_set.isra.55+0x5>
     205:	55                   	push   %rbp
     206:	48 89 e5             	mov    %rsp,%rbp
     209:	41 57                	push   %r15
     20b:	41 56                	push   %r14
     20d:	49 89 fe             	mov    %rdi,%r14
     210:	41 55                	push   %r13
     212:	49 89 f5             	mov    %rsi,%r13
     215:	41 54                	push   %r12
     217:	53                   	push   %rbx
     218:	48 83 ec 28          	sub    $0x28,%rsp
	struct address_space *mapping = page->mapping;
     21c:	4c 8b 67 08          	mov    0x8(%rdi),%r12
	pgoff_t pgoff = page->index << (PAGE_CACHE_SHIFT - PAGE_SHIFT);
     220:	48 8b 5f 10          	mov    0x10(%rdi),%rbx
	//mutex_unlock(&mapping->i_mmap_mutex);
	return ret;
}


static int do_file_page_set(struct page* page,struct pid_namespace* pid_ns_ref, pteval_t flags,int count){
     224:	48 89 55 c8          	mov    %rdx,-0x38(%rbp)
}
static inline void i_mmap_unlock_write(struct address_space *mapping){
 up_write(&mapping->i_mmap_rwsem);
}
static inline void i_mmap_lock_read(struct address_space *mapping){
 down_read(&mapping->i_mmap_rwsem);
     228:	4d 8d 7c 24 50       	lea    0x50(%r12),%r15
     22d:	4c 89 ff             	mov    %r15,%rdi
     230:	e8 00 00 00 00       	callq  235 <do_file_page_set.isra.55+0x35>
	//spin_lock(&mapping->i_mmap_spinlock);
#else

	i_mmap_lock_read(mapping);
#endif
	vma_interval_tree_foreach(vma, &mapping->i_mmap, pgoff, pgoff) {
     235:	49 8d 7c 24 38       	lea    0x38(%r12),%rdi
     23a:	48 89 da             	mov    %rbx,%rdx
     23d:	48 89 de             	mov    %rbx,%rsi
     240:	e8 00 00 00 00       	callq  245 <do_file_page_set.isra.55+0x45>
     245:	48 85 c0             	test   %rax,%rax
     248:	48 89 c1             	mov    %rax,%rcx
	struct vm_area_struct *vma;
	struct mm_struct * mm;
	struct task_struct * task;
	unsigned long address;
	pte_t* pte,pte_entry; 
	int ret=0;
     24b:	c7 45 d4 00 00 00 00 	movl   $0x0,-0x2c(%rbp)
	//spin_lock(&mapping->i_mmap_spinlock);
#else

	i_mmap_lock_read(mapping);
#endif
	vma_interval_tree_foreach(vma, &mapping->i_mmap, pgoff, pgoff) {
     252:	0f 84 f8 00 00 00    	je     350 <do_file_page_set.isra.55+0x150>
		if(vma&&(mm=vma->vm_mm)&&(mm->def_flags&VM_NCACHE)&&(task=mm->owner)&&(ns_of_pid(task_pid(task))==pid_ns_ref)){
     258:	49 bc 00 00 00 00 02 	movabs $0x200000000,%r12
     25f:	00 00 00 
     262:	eb 2f                	jmp    293 <do_file_page_set.isra.55+0x93>
     264:	0f 1f 40 00          	nopl   0x0(%rax)
	if (pid)
		ns = pid->numbers[pid->level].ns;
     268:	8b 70 04             	mov    0x4(%rax),%esi
     26b:	48 c1 e6 05          	shl    $0x5,%rsi
     26f:	48 8b 44 30 38       	mov    0x38(%rax,%rsi,1),%rax
     274:	49 39 c5             	cmp    %rax,%r13
     277:	74 4f                	je     2c8 <do_file_page_set.isra.55+0xc8>
	//spin_lock(&mapping->i_mmap_spinlock);
#else

	i_mmap_lock_read(mapping);
#endif
	vma_interval_tree_foreach(vma, &mapping->i_mmap, pgoff, pgoff) {
     279:	48 89 cf             	mov    %rcx,%rdi
     27c:	48 89 da             	mov    %rbx,%rdx
     27f:	48 89 de             	mov    %rbx,%rsi
     282:	e8 00 00 00 00       	callq  287 <do_file_page_set.isra.55+0x87>
     287:	48 85 c0             	test   %rax,%rax
     28a:	48 89 c1             	mov    %rax,%rcx
     28d:	0f 84 bd 00 00 00    	je     350 <do_file_page_set.isra.55+0x150>
		if(vma&&(mm=vma->vm_mm)&&(mm->def_flags&VM_NCACHE)&&(task=mm->owner)&&(ns_of_pid(task_pid(task))==pid_ns_ref)){
     293:	48 8b 51 40          	mov    0x40(%rcx),%rdx
     297:	48 85 d2             	test   %rdx,%rdx
     29a:	74 dd                	je     279 <do_file_page_set.isra.55+0x79>
     29c:	4c 85 a2 f8 00 00 00 	test   %r12,0xf8(%rdx)
     2a3:	74 d4                	je     279 <do_file_page_set.isra.55+0x79>
     2a5:	48 8b 82 90 03 00 00 	mov    0x390(%rdx),%rax
     2ac:	48 85 c0             	test   %rax,%rax
     2af:	74 c8                	je     279 <do_file_page_set.isra.55+0x79>
     2b1:	48 8b 80 60 03 00 00 	mov    0x360(%rax),%rax
 * 	the resulting NULL pid-ns.
 */
static inline struct pid_namespace *ns_of_pid(struct pid *pid)
{
	struct pid_namespace *ns = NULL;
	if (pid)
     2b8:	48 85 c0             	test   %rax,%rax
     2bb:	75 ab                	jne    268 <do_file_page_set.isra.55+0x68>
 * 	is expected to be non-NULL. If @pid is NULL, caller should handle
 * 	the resulting NULL pid-ns.
 */
static inline struct pid_namespace *ns_of_pid(struct pid *pid)
{
	struct pid_namespace *ns = NULL;
     2bd:	31 c0                	xor    %eax,%eax
     2bf:	49 39 c5             	cmp    %rax,%r13
     2c2:	75 b5                	jne    279 <do_file_page_set.isra.55+0x79>
     2c4:	0f 1f 40 00          	nopl   0x0(%rax)
				address = vma_to_address(page, vma);
     2c8:	48 89 ce             	mov    %rcx,%rsi
     2cb:	4c 89 f7             	mov    %r14,%rdi
     2ce:	48 89 4d c0          	mov    %rcx,-0x40(%rbp)
     2d2:	48 89 55 b8          	mov    %rdx,-0x48(%rbp)
     2d6:	e8 00 00 00 00       	callq  2db <do_file_page_set.isra.55+0xdb>
				spinlock_t *ptl;
			//	if(!follow_pte(mm,address,&pte,&ptl)){
					pte=find_pte(mm,address);
     2db:	48 8b 55 b8          	mov    -0x48(%rbp),%rdx
     2df:	48 89 c6             	mov    %rax,%rsi

	i_mmap_lock_read(mapping);
#endif
	vma_interval_tree_foreach(vma, &mapping->i_mmap, pgoff, pgoff) {
		if(vma&&(mm=vma->vm_mm)&&(mm->def_flags&VM_NCACHE)&&(task=mm->owner)&&(ns_of_pid(task_pid(task))==pid_ns_ref)){
				address = vma_to_address(page, vma);
     2e2:	48 89 45 b0          	mov    %rax,-0x50(%rbp)
				spinlock_t *ptl;
			//	if(!follow_pte(mm,address,&pte,&ptl)){
					pte=find_pte(mm,address);
     2e6:	48 89 d7             	mov    %rdx,%rdi
     2e9:	e8 00 00 00 00       	callq  2ee <do_file_page_set.isra.55+0xee>
					if(pte&&pte_present(*pte))
     2ee:	48 85 c0             	test   %rax,%rax
     2f1:	48 8b 4d c0          	mov    -0x40(%rbp),%rcx
     2f5:	74 82                	je     279 <do_file_page_set.isra.55+0x79>
     2f7:	48 8b 10             	mov    (%rax),%rdx
     2fa:	f7 c2 01 01 00 00    	test   $0x101,%edx
     300:	0f 84 73 ff ff ff    	je     279 <do_file_page_set.isra.55+0x79>
					  if(((pte->pte&flags)!=flags)){
     306:	48 8b 7d c8          	mov    -0x38(%rbp),%rdi
     30a:	48 89 fe             	mov    %rdi,%rsi
     30d:	48 21 d6             	and    %rdx,%rsi
     310:	48 39 f7             	cmp    %rsi,%rdi
     313:	0f 84 60 ff ff ff    	je     279 <do_file_page_set.isra.55+0x79>
						  ret++;
						  pte->pte|=flags;
						  flush_tlb_page(vma,address);
     319:	48 8b 75 b0          	mov    -0x50(%rbp),%rsi
			//	if(!follow_pte(mm,address,&pte,&ptl)){
					pte=find_pte(mm,address);
					if(pte&&pte_present(*pte))
					  if(((pte->pte&flags)!=flags)){
						  ret++;
						  pte->pte|=flags;
     31d:	48 09 fa             	or     %rdi,%rdx
						  flush_tlb_page(vma,address);
     320:	48 89 cf             	mov    %rcx,%rdi
			//	if(!follow_pte(mm,address,&pte,&ptl)){
					pte=find_pte(mm,address);
					if(pte&&pte_present(*pte))
					  if(((pte->pte&flags)!=flags)){
						  ret++;
						  pte->pte|=flags;
     323:	48 89 10             	mov    %rdx,(%rax)
				spinlock_t *ptl;
			//	if(!follow_pte(mm,address,&pte,&ptl)){
					pte=find_pte(mm,address);
					if(pte&&pte_present(*pte))
					  if(((pte->pte&flags)!=flags)){
						  ret++;
     326:	83 45 d4 01          	addl   $0x1,-0x2c(%rbp)
						  pte->pte|=flags;
						  flush_tlb_page(vma,address);
     32a:	e8 00 00 00 00       	callq  32f <do_file_page_set.isra.55+0x12f>
						  if(ret>= get_page_counter_in_ns(page,pid_ns_ref))  		
     32f:	4c 89 ee             	mov    %r13,%rsi
     332:	4c 89 f7             	mov    %r14,%rdi
     335:	e8 00 00 00 00       	callq  33a <do_file_page_set.isra.55+0x13a>
     33a:	39 45 d4             	cmp    %eax,-0x2c(%rbp)
     33d:	48 8b 4d c0          	mov    -0x40(%rbp),%rcx
     341:	0f 8c 32 ff ff ff    	jl     279 <do_file_page_set.isra.55+0x79>
     347:	66 0f 1f 84 00 00 00 	nopw   0x0(%rax,%rax,1)
     34e:	00 00 
static inline int i_mmap_trylock_read(struct address_space *mapping){
 return down_read_trylock(&mapping->i_mmap_rwsem);
}

static inline void i_mmap_unlock_read(struct address_space *mapping){
 up_read(&mapping->i_mmap_rwsem);
     350:	4c 89 ff             	mov    %r15,%rdi
     353:	e8 00 00 00 00       	callq  358 <do_file_page_set.isra.55+0x158>
unlock:	

	i_mmap_unlock_read(mapping);

	return ret;
}
     358:	8b 45 d4             	mov    -0x2c(%rbp),%eax
     35b:	48 83 c4 28          	add    $0x28,%rsp
     35f:	5b                   	pop    %rbx
     360:	41 5c                	pop    %r12
     362:	41 5d                	pop    %r13
     364:	41 5e                	pop    %r14
     366:	41 5f                	pop    %r15
     368:	5d                   	pop    %rbp
     369:	c3                   	retq   
     36a:	66 0f 1f 44 00 00    	nopw   0x0(%rax,%rax,1)

0000000000000370 <do_ksm_page_set.isra.56>:
		}
	}
unlock:	page_unlock_anon_vma_read(anon_vma);
		return ret;
}
static int do_ksm_page_set(struct page* page,struct pid_namespace* pid_ns_ref, pteval_t flags,int count){
     370:	e8 00 00 00 00       	callq  375 <do_ksm_page_set.isra.56+0x5>
     375:	55                   	push   %rbp
	struct task_struct * task;
	unsigned long address;
	int ret=0;
	stable_node = page_stable_node(page);
	if (!stable_node)
	  return -1;
     376:	b8 ff ff ff ff       	mov    $0xffffffff,%eax
		}
	}
unlock:	page_unlock_anon_vma_read(anon_vma);
		return ret;
}
static int do_ksm_page_set(struct page* page,struct pid_namespace* pid_ns_ref, pteval_t flags,int count){
     37b:	48 89 e5             	mov    %rsp,%rbp
     37e:	41 57                	push   %r15
     380:	41 56                	push   %r14
     382:	41 55                	push   %r13
     384:	41 54                	push   %r12
     386:	53                   	push   %rbx
     387:	48 83 ec 30          	sub    $0x30,%rsp
     38b:	48 89 55 a8          	mov    %rdx,-0x58(%rbp)
 * is found in VM_MERGEABLE vmas.  It's a PageAnon page, pointing not to any
 * anon_vma, but to that page's node of the stable tree.
 */
static inline int PageKsm(struct page *page)
{
	return ((unsigned long)page->mapping & PAGE_MAPPING_FLAGS) ==
     38f:	48 8b 57 08          	mov    0x8(%rdi),%rdx
     393:	48 89 7d c0          	mov    %rdi,-0x40(%rbp)
     397:	48 89 d1             	mov    %rdx,%rcx
     39a:	83 e1 03             	and    $0x3,%ecx
				(PAGE_MAPPING_ANON | PAGE_MAPPING_KSM);
}

static inline struct stable_node *page_stable_node(struct page *page)
{
	return PageKsm(page) ? page_rmapping(page) : NULL;
     39d:	48 83 f9 03          	cmp    $0x3,%rcx
     3a1:	74 15                	je     3b8 <do_ksm_page_set.isra.56+0x48>
	}
	return ret;
unlock:	
	anon_vma_unlock_read(anon_vma);
	return ret;
}
     3a3:	48 83 c4 30          	add    $0x30,%rsp
     3a7:	5b                   	pop    %rbx
     3a8:	41 5c                	pop    %r12
     3aa:	41 5d                	pop    %r13
     3ac:	41 5e                	pop    %r14
     3ae:	41 5f                	pop    %r15
     3b0:	5d                   	pop    %rbp
     3b1:	c3                   	retq   
     3b2:	66 0f 1f 44 00 00    	nopw   0x0(%rax,%rax,1)
	struct mm_struct * mm;
	struct task_struct * task;
	unsigned long address;
	int ret=0;
	stable_node = page_stable_node(page);
	if (!stable_node)
     3b8:	48 83 e2 fc          	and    $0xfffffffffffffffc,%rdx
     3bc:	74 e5                	je     3a3 <do_ksm_page_set.isra.56+0x33>
	  return -1;
	hlist_for_each_entry(rmap_item, &stable_node->hlist, hlist) {
     3be:	4c 8b 72 18          	mov    0x18(%rdx),%r14
     3c2:	4d 85 f6             	test   %r14,%r14
     3c5:	0f 84 76 01 00 00    	je     541 <do_ksm_page_set.isra.56+0x1d1>
     3cb:	49 89 f4             	mov    %rsi,%r12
     3ce:	49 83 ee 30          	sub    $0x30,%r14
     3d2:	c7 45 b4 00 00 00 00 	movl   $0x0,-0x4c(%rbp)
		anon_vma_lock_read(anon_vma);
		anon_vma_interval_tree_foreach(vmac, &anon_vma->rb_root,
					0, ULONG_MAX) {
			vma = vmac->vma;
			if((mm=vma->vm_mm))
			  if((mm->def_flags&VM_NCACHE)&&(task=mm->owner))
     3d9:	48 bb 00 00 00 00 02 	movabs $0x200000000,%rbx
     3e0:	00 00 00 
     3e3:	0f 1f 44 00 00       	nopl   0x0(%rax,%rax,1)
	int ret=0;
	stable_node = page_stable_node(page);
	if (!stable_node)
	  return -1;
	hlist_for_each_entry(rmap_item, &stable_node->hlist, hlist) {
		anon_vma = rmap_item->anon_vma;
     3e8:	4d 8b 6e 08          	mov    0x8(%r14),%r13
	up_write(&anon_vma->root->rwsem);
}

static inline void anon_vma_lock_read(struct anon_vma *anon_vma)
{
	down_read(&anon_vma->root->rwsem);
     3ec:	49 8b 45 00          	mov    0x0(%r13),%rax
     3f0:	48 8d 78 08          	lea    0x8(%rax),%rdi
     3f4:	e8 00 00 00 00       	callq  3f9 <do_ksm_page_set.isra.56+0x89>
		anon_vma_lock_read(anon_vma);
		anon_vma_interval_tree_foreach(vmac, &anon_vma->rb_root,
     3f9:	49 8d 7d 40          	lea    0x40(%r13),%rdi
     3fd:	31 f6                	xor    %esi,%esi
     3ff:	48 c7 c2 ff ff ff ff 	mov    $0xffffffffffffffff,%rdx
     406:	e8 00 00 00 00       	callq  40b <do_ksm_page_set.isra.56+0x9b>
     40b:	48 85 c0             	test   %rax,%rax
     40e:	49 89 c7             	mov    %rax,%r15
     411:	75 3b                	jne    44e <do_ksm_page_set.isra.56+0xde>
     413:	e9 f8 00 00 00       	jmpq   510 <do_ksm_page_set.isra.56+0x1a0>
     418:	0f 1f 84 00 00 00 00 	nopl   0x0(%rax,%rax,1)
     41f:	00 
	if (pid)
		ns = pid->numbers[pid->level].ns;
     420:	8b 70 04             	mov    0x4(%rax),%esi
     423:	48 c1 e6 05          	shl    $0x5,%rsi
     427:	48 8b 44 30 38       	mov    0x38(%rax,%rsi,1),%rax
					0, ULONG_MAX) {
			vma = vmac->vma;
			if((mm=vma->vm_mm))
			  if((mm->def_flags&VM_NCACHE)&&(task=mm->owner))
				if(ns_of_pid(task_pid(task))==pid_ns_ref){
     42c:	49 39 c4             	cmp    %rax,%r12
     42f:	74 57                	je     488 <do_ksm_page_set.isra.56+0x118>
	if (!stable_node)
	  return -1;
	hlist_for_each_entry(rmap_item, &stable_node->hlist, hlist) {
		anon_vma = rmap_item->anon_vma;
		anon_vma_lock_read(anon_vma);
		anon_vma_interval_tree_foreach(vmac, &anon_vma->rb_root,
     431:	31 f6                	xor    %esi,%esi
     433:	4c 89 ff             	mov    %r15,%rdi
     436:	48 c7 c2 ff ff ff ff 	mov    $0xffffffffffffffff,%rdx
     43d:	e8 00 00 00 00       	callq  442 <do_ksm_page_set.isra.56+0xd2>
     442:	48 85 c0             	test   %rax,%rax
     445:	49 89 c7             	mov    %rax,%r15
     448:	0f 84 c2 00 00 00    	je     510 <do_ksm_page_set.isra.56+0x1a0>
					0, ULONG_MAX) {
			vma = vmac->vma;
     44e:	4d 8b 07             	mov    (%r15),%r8
			if((mm=vma->vm_mm))
     451:	49 8b 50 40          	mov    0x40(%r8),%rdx
     455:	48 85 d2             	test   %rdx,%rdx
     458:	74 d7                	je     431 <do_ksm_page_set.isra.56+0xc1>
			  if((mm->def_flags&VM_NCACHE)&&(task=mm->owner))
     45a:	48 85 9a f8 00 00 00 	test   %rbx,0xf8(%rdx)
     461:	74 ce                	je     431 <do_ksm_page_set.isra.56+0xc1>
     463:	48 8b 82 90 03 00 00 	mov    0x390(%rdx),%rax
     46a:	48 85 c0             	test   %rax,%rax
     46d:	74 c2                	je     431 <do_ksm_page_set.isra.56+0xc1>
     46f:	48 8b 80 60 03 00 00 	mov    0x360(%rax),%rax
 * 	the resulting NULL pid-ns.
 */
static inline struct pid_namespace *ns_of_pid(struct pid *pid)
{
	struct pid_namespace *ns = NULL;
	if (pid)
     476:	48 85 c0             	test   %rax,%rax
     479:	75 a5                	jne    420 <do_ksm_page_set.isra.56+0xb0>
 * 	is expected to be non-NULL. If @pid is NULL, caller should handle
 * 	the resulting NULL pid-ns.
 */
static inline struct pid_namespace *ns_of_pid(struct pid *pid)
{
	struct pid_namespace *ns = NULL;
     47b:	31 c0                	xor    %eax,%eax
				if(ns_of_pid(task_pid(task))==pid_ns_ref){
     47d:	49 39 c4             	cmp    %rax,%r12
     480:	75 af                	jne    431 <do_ksm_page_set.isra.56+0xc1>
     482:	66 0f 1f 44 00 00    	nopw   0x0(%rax,%rax,1)
					address= vma_to_address(page, vma);
     488:	48 8b 7d c0          	mov    -0x40(%rbp),%rdi
     48c:	4c 89 c6             	mov    %r8,%rsi
     48f:	48 89 55 d0          	mov    %rdx,-0x30(%rbp)
     493:	4c 89 45 c8          	mov    %r8,-0x38(%rbp)
     497:	e8 00 00 00 00       	callq  49c <do_ksm_page_set.isra.56+0x12c>
					spinlock_t *ptl;
			//		if(!follow_pte(mm,address,&pte,&ptl)){

						pte=find_pte(mm,address);
     49c:	48 8b 55 d0          	mov    -0x30(%rbp),%rdx
     4a0:	48 89 c6             	mov    %rax,%rsi
					0, ULONG_MAX) {
			vma = vmac->vma;
			if((mm=vma->vm_mm))
			  if((mm->def_flags&VM_NCACHE)&&(task=mm->owner))
				if(ns_of_pid(task_pid(task))==pid_ns_ref){
					address= vma_to_address(page, vma);
     4a3:	48 89 45 b8          	mov    %rax,-0x48(%rbp)
					spinlock_t *ptl;
			//		if(!follow_pte(mm,address,&pte,&ptl)){

						pte=find_pte(mm,address);
     4a7:	48 89 d7             	mov    %rdx,%rdi
     4aa:	e8 00 00 00 00       	callq  4af <do_ksm_page_set.isra.56+0x13f>
						if(pte&&pte_present(*pte)){
     4af:	48 85 c0             	test   %rax,%rax
     4b2:	0f 84 79 ff ff ff    	je     431 <do_ksm_page_set.isra.56+0xc1>
     4b8:	48 8b 10             	mov    (%rax),%rdx
     4bb:	4c 8b 45 c8          	mov    -0x38(%rbp),%r8
     4bf:	f7 c2 01 01 00 00    	test   $0x101,%edx
     4c5:	0f 84 66 ff ff ff    	je     431 <do_ksm_page_set.isra.56+0xc1>
							if(((pte->pte&flags)!=flags)){													
     4cb:	48 8b 4d a8          	mov    -0x58(%rbp),%rcx
     4cf:	48 89 ce             	mov    %rcx,%rsi
     4d2:	48 21 d6             	and    %rdx,%rsi
     4d5:	48 39 f1             	cmp    %rsi,%rcx
     4d8:	0f 84 53 ff ff ff    	je     431 <do_ksm_page_set.isra.56+0xc1>
								ret++;
								pte->pte|=flags;
								flush_tlb_page(vma,address);
     4de:	48 8b 75 b8          	mov    -0x48(%rbp),%rsi

						pte=find_pte(mm,address);
						if(pte&&pte_present(*pte)){
							if(((pte->pte&flags)!=flags)){													
								ret++;
								pte->pte|=flags;
     4e2:	48 09 ca             	or     %rcx,%rdx
								flush_tlb_page(vma,address);
     4e5:	4c 89 c7             	mov    %r8,%rdi

						pte=find_pte(mm,address);
						if(pte&&pte_present(*pte)){
							if(((pte->pte&flags)!=flags)){													
								ret++;
								pte->pte|=flags;
     4e8:	48 89 10             	mov    %rdx,(%rax)
			//		if(!follow_pte(mm,address,&pte,&ptl)){

						pte=find_pte(mm,address);
						if(pte&&pte_present(*pte)){
							if(((pte->pte&flags)!=flags)){													
								ret++;
     4eb:	83 45 b4 01          	addl   $0x1,-0x4c(%rbp)
								pte->pte|=flags;
								flush_tlb_page(vma,address);
     4ef:	e8 00 00 00 00       	callq  4f4 <do_ksm_page_set.isra.56+0x184>
								if(ret>=get_page_counter_in_ns(page,pid_ns_ref))
     4f4:	48 8b 7d c0          	mov    -0x40(%rbp),%rdi
     4f8:	4c 89 e6             	mov    %r12,%rsi
     4fb:	e8 00 00 00 00       	callq  500 <do_ksm_page_set.isra.56+0x190>
     500:	39 45 b4             	cmp    %eax,-0x4c(%rbp)
     503:	0f 8c 28 ff ff ff    	jl     431 <do_ksm_page_set.isra.56+0xc1>
     509:	0f 1f 80 00 00 00 00 	nopl   0x0(%rax)
}

static inline void anon_vma_unlock_read(struct anon_vma *anon_vma)
{
	up_read(&anon_vma->root->rwsem);
     510:	49 8b 7d 00          	mov    0x0(%r13),%rdi
     514:	48 83 c7 08          	add    $0x8,%rdi
     518:	e8 00 00 00 00       	callq  51d <do_ksm_page_set.isra.56+0x1ad>
	unsigned long address;
	int ret=0;
	stable_node = page_stable_node(page);
	if (!stable_node)
	  return -1;
	hlist_for_each_entry(rmap_item, &stable_node->hlist, hlist) {
     51d:	4d 8b 76 30          	mov    0x30(%r14),%r14
     521:	4d 85 f6             	test   %r14,%r14
     524:	74 09                	je     52f <do_ksm_page_set.isra.56+0x1bf>
     526:	49 83 ee 30          	sub    $0x30,%r14
     52a:	e9 b9 fe ff ff       	jmpq   3e8 <do_ksm_page_set.isra.56+0x78>
     52f:	8b 45 b4             	mov    -0x4c(%rbp),%eax
	}
	return ret;
unlock:	
	anon_vma_unlock_read(anon_vma);
	return ret;
}
     532:	48 83 c4 30          	add    $0x30,%rsp
     536:	5b                   	pop    %rbx
     537:	41 5c                	pop    %r12
     539:	41 5d                	pop    %r13
     53b:	41 5e                	pop    %r14
     53d:	41 5f                	pop    %r15
     53f:	5d                   	pop    %rbp
     540:	c3                   	retq   
	unsigned long address;
	int ret=0;
	stable_node = page_stable_node(page);
	if (!stable_node)
	  return -1;
	hlist_for_each_entry(rmap_item, &stable_node->hlist, hlist) {
     541:	31 c0                	xor    %eax,%eax
     543:	e9 5b fe ff ff       	jmpq   3a3 <do_ksm_page_set.isra.56+0x33>
     548:	0f 1f 84 00 00 00 00 	nopl   0x0(%rax,%rax,1)
     54f:	00 

0000000000000550 <do_anon_page_set.isra.57>:
		}
	}
unlock:	page_unlock_anon_vma_read(anon_vma);
		return ret;
}
static int do_anon_page_set(struct page* page,struct pid_namespace* pid_ns_ref, pteval_t flags,int count){
     550:	e8 00 00 00 00       	callq  555 <do_anon_page_set.isra.57+0x5>
     555:	55                   	push   %rbp
     556:	48 89 e5             	mov    %rsp,%rbp
     559:	41 57                	push   %r15
	struct mm_struct* mm;
	struct task_struct * task;
	int ret=0;
	anon_vma=page_lock_anon_vma_read(page);
	if (!anon_vma){
		return ret;
     55b:	45 31 ff             	xor    %r15d,%r15d
		}
	}
unlock:	page_unlock_anon_vma_read(anon_vma);
		return ret;
}
static int do_anon_page_set(struct page* page,struct pid_namespace* pid_ns_ref, pteval_t flags,int count){
     55e:	41 56                	push   %r14
     560:	41 55                	push   %r13
     562:	49 89 fd             	mov    %rdi,%r13
     565:	41 54                	push   %r12
     567:	49 89 f4             	mov    %rsi,%r12
     56a:	53                   	push   %rbx
     56b:	48 83 ec 28          	sub    $0x28,%rsp
     56f:	48 89 55 b8          	mov    %rdx,-0x48(%rbp)
	pgoff_t pgoff;
	struct vm_area_struct *vma;
	struct mm_struct* mm;
	struct task_struct * task;
	int ret=0;
	anon_vma=page_lock_anon_vma_read(page);
     573:	e8 00 00 00 00       	callq  578 <do_anon_page_set.isra.57+0x28>
	if (!anon_vma){
     578:	48 85 c0             	test   %rax,%rax
	pgoff_t pgoff;
	struct vm_area_struct *vma;
	struct mm_struct* mm;
	struct task_struct * task;
	int ret=0;
	anon_vma=page_lock_anon_vma_read(page);
     57b:	49 89 c6             	mov    %rax,%r14
	if (!anon_vma){
     57e:	0f 84 24 01 00 00    	je     6a8 <do_anon_page_set.isra.57+0x158>
		return ret;
	}
	pgoff=page->index<<(PAGE_CACHE_SHIFT-PAGE_SHIFT);
     584:	49 8b 5d 10          	mov    0x10(%r13),%rbx
	anon_vma_interval_tree_foreach(avc,&anon_vma->rb_root,pgoff,pgoff){
     588:	48 8d 78 40          	lea    0x40(%rax),%rdi
     58c:	48 89 da             	mov    %rbx,%rdx
     58f:	48 89 de             	mov    %rbx,%rsi
     592:	e8 00 00 00 00       	callq  597 <do_anon_page_set.isra.57+0x47>
     597:	48 85 c0             	test   %rax,%rax
     59a:	48 89 c1             	mov    %rax,%rcx
     59d:	75 34                	jne    5d3 <do_anon_page_set.isra.57+0x83>
     59f:	e9 fc 00 00 00       	jmpq   6a0 <do_anon_page_set.isra.57+0x150>
     5a4:	0f 1f 40 00          	nopl   0x0(%rax)
	if (pid)
		ns = pid->numbers[pid->level].ns;
     5a8:	8b 70 04             	mov    0x4(%rax),%esi
     5ab:	48 c1 e6 05          	shl    $0x5,%rsi
     5af:	48 8b 44 30 38       	mov    0x38(%rax,%rsi,1),%rax
		vma=avc->vma;
		if(vma&&(mm=vma->vm_mm)&&(task=mm->owner)){
			if(ns_of_pid(task_pid(task))==pid_ns_ref){
     5b4:	49 39 c4             	cmp    %rax,%r12
     5b7:	74 4f                	je     608 <do_anon_page_set.isra.57+0xb8>
	anon_vma=page_lock_anon_vma_read(page);
	if (!anon_vma){
		return ret;
	}
	pgoff=page->index<<(PAGE_CACHE_SHIFT-PAGE_SHIFT);
	anon_vma_interval_tree_foreach(avc,&anon_vma->rb_root,pgoff,pgoff){
     5b9:	48 89 cf             	mov    %rcx,%rdi
     5bc:	48 89 da             	mov    %rbx,%rdx
     5bf:	48 89 de             	mov    %rbx,%rsi
     5c2:	e8 00 00 00 00       	callq  5c7 <do_anon_page_set.isra.57+0x77>
     5c7:	48 85 c0             	test   %rax,%rax
     5ca:	48 89 c1             	mov    %rax,%rcx
     5cd:	0f 84 cd 00 00 00    	je     6a0 <do_anon_page_set.isra.57+0x150>
		vma=avc->vma;
     5d3:	48 8b 11             	mov    (%rcx),%rdx
		if(vma&&(mm=vma->vm_mm)&&(task=mm->owner)){
     5d6:	48 85 d2             	test   %rdx,%rdx
     5d9:	74 de                	je     5b9 <do_anon_page_set.isra.57+0x69>
     5db:	4c 8b 42 40          	mov    0x40(%rdx),%r8
     5df:	4d 85 c0             	test   %r8,%r8
     5e2:	74 d5                	je     5b9 <do_anon_page_set.isra.57+0x69>
     5e4:	49 8b 80 90 03 00 00 	mov    0x390(%r8),%rax
     5eb:	48 85 c0             	test   %rax,%rax
     5ee:	74 c9                	je     5b9 <do_anon_page_set.isra.57+0x69>
     5f0:	48 8b 80 60 03 00 00 	mov    0x360(%rax),%rax
 * 	the resulting NULL pid-ns.
 */
static inline struct pid_namespace *ns_of_pid(struct pid *pid)
{
	struct pid_namespace *ns = NULL;
	if (pid)
     5f7:	48 85 c0             	test   %rax,%rax
     5fa:	75 ac                	jne    5a8 <do_anon_page_set.isra.57+0x58>
 * 	is expected to be non-NULL. If @pid is NULL, caller should handle
 * 	the resulting NULL pid-ns.
 */
static inline struct pid_namespace *ns_of_pid(struct pid *pid)
{
	struct pid_namespace *ns = NULL;
     5fc:	31 c0                	xor    %eax,%eax
			if(ns_of_pid(task_pid(task))==pid_ns_ref){
     5fe:	49 39 c4             	cmp    %rax,%r12
     601:	75 b6                	jne    5b9 <do_anon_page_set.isra.57+0x69>
     603:	0f 1f 44 00 00       	nopl   0x0(%rax,%rax,1)
				address = vma_to_address(page, vma);
     608:	48 89 d6             	mov    %rdx,%rsi
     60b:	4c 89 ef             	mov    %r13,%rdi
     60e:	48 89 4d c8          	mov    %rcx,-0x38(%rbp)
     612:	4c 89 45 d0          	mov    %r8,-0x30(%rbp)
     616:	48 89 55 c0          	mov    %rdx,-0x40(%rbp)
     61a:	e8 00 00 00 00       	callq  61f <do_anon_page_set.isra.57+0xcf>
				spinlock_t *ptl;
				//if(find_pte(mm,address,&pte,&ptl)){
					pte=find_pte(mm,address);
     61f:	4c 8b 45 d0          	mov    -0x30(%rbp),%r8
     623:	48 89 c6             	mov    %rax,%rsi
	pgoff=page->index<<(PAGE_CACHE_SHIFT-PAGE_SHIFT);
	anon_vma_interval_tree_foreach(avc,&anon_vma->rb_root,pgoff,pgoff){
		vma=avc->vma;
		if(vma&&(mm=vma->vm_mm)&&(task=mm->owner)){
			if(ns_of_pid(task_pid(task))==pid_ns_ref){
				address = vma_to_address(page, vma);
     626:	48 89 45 b0          	mov    %rax,-0x50(%rbp)
				spinlock_t *ptl;
				//if(find_pte(mm,address,&pte,&ptl)){
					pte=find_pte(mm,address);
     62a:	4c 89 c7             	mov    %r8,%rdi
     62d:	e8 00 00 00 00       	callq  632 <do_anon_page_set.isra.57+0xe2>
					if(pte&&pte_present(*pte))
     632:	48 85 c0             	test   %rax,%rax
     635:	48 8b 4d c8          	mov    -0x38(%rbp),%rcx
     639:	0f 84 7a ff ff ff    	je     5b9 <do_anon_page_set.isra.57+0x69>
     63f:	48 8b 30             	mov    (%rax),%rsi
     642:	48 8b 55 c0          	mov    -0x40(%rbp),%rdx
     646:	f7 c6 01 01 00 00    	test   $0x101,%esi
     64c:	0f 84 67 ff ff ff    	je     5b9 <do_anon_page_set.isra.57+0x69>
					  if(((pte->pte&flags)!=flags)){
     652:	4c 8b 4d b8          	mov    -0x48(%rbp),%r9
     656:	4c 89 cf             	mov    %r9,%rdi
     659:	48 21 f7             	and    %rsi,%rdi
     65c:	49 39 f9             	cmp    %rdi,%r9
     65f:	0f 84 54 ff ff ff    	je     5b9 <do_anon_page_set.isra.57+0x69>
						  //printk("PAGE_ACCESSED in anon");
						  ret=-1;
						  goto unlock;// cannot marked as UC...
						  }*/
						  ret++;
						  pte->pte|=flags;
     665:	4c 09 ce             	or     %r9,%rsi
						  flush_tlb_page(vma,address);
     668:	48 89 d7             	mov    %rdx,%rdi
     66b:	48 89 4d d0          	mov    %rcx,-0x30(%rbp)
						  //printk("PAGE_ACCESSED in anon");
						  ret=-1;
						  goto unlock;// cannot marked as UC...
						  }*/
						  ret++;
						  pte->pte|=flags;
     66f:	48 89 30             	mov    %rsi,(%rax)
						  flush_tlb_page(vma,address);
     672:	48 8b 75 b0          	mov    -0x50(%rbp),%rsi
						  /*		if(pte->pte|_PAGE_ACCESSED){
						  //printk("PAGE_ACCESSED in anon");
						  ret=-1;
						  goto unlock;// cannot marked as UC...
						  }*/
						  ret++;
     676:	41 83 c7 01          	add    $0x1,%r15d
						  pte->pte|=flags;
						  flush_tlb_page(vma,address);
     67a:	e8 00 00 00 00       	callq  67f <do_anon_page_set.isra.57+0x12f>
						  if(ret>= get_page_counter_in_ns(page,pid_ns_ref))
     67f:	4c 89 e6             	mov    %r12,%rsi
     682:	4c 89 ef             	mov    %r13,%rdi
     685:	e8 00 00 00 00       	callq  68a <do_anon_page_set.isra.57+0x13a>
     68a:	41 39 c7             	cmp    %eax,%r15d
     68d:	48 8b 4d d0          	mov    -0x30(%rbp),%rcx
     691:	0f 8c 22 ff ff ff    	jl     5b9 <do_anon_page_set.isra.57+0x69>
     697:	66 0f 1f 84 00 00 00 	nopw   0x0(%rax,%rax,1)
     69e:	00 00 
					//pte_unmap_unlock(pte, ptl);
			//	}
			}
		}
	}
unlock:	page_unlock_anon_vma_read(anon_vma);
     6a0:	4c 89 f7             	mov    %r14,%rdi
     6a3:	e8 00 00 00 00       	callq  6a8 <do_anon_page_set.isra.57+0x158>
		return ret;
	}
     6a8:	48 83 c4 28          	add    $0x28,%rsp
     6ac:	44 89 f8             	mov    %r15d,%eax
     6af:	5b                   	pop    %rbx
     6b0:	41 5c                	pop    %r12
     6b2:	41 5d                	pop    %r13
     6b4:	41 5e                	pop    %r14
     6b6:	41 5f                	pop    %r15
     6b8:	5d                   	pop    %rbp
     6b9:	c3                   	retq   
     6ba:	66 0f 1f 44 00 00    	nopw   0x0(%rax,%rax,1)

00000000000006c0 <tlb_flush_mmu.part.59>:
#ifdef CONFIG_HAVE_RCU_TABLE_FREE
	tlb->batch = NULL;
#endif
}

void tlb_flush_mmu(struct mmu_gather *tlb)
     6c0:	e8 00 00 00 00       	callq  6c5 <tlb_flush_mmu.part.59+0x5>
     6c5:	55                   	push   %rbp
     6c6:	48 89 e5             	mov    %rsp,%rbp
     6c9:	41 55                	push   %r13
     6cb:	41 54                	push   %r12
     6cd:	49 89 fc             	mov    %rdi,%r12
     6d0:	53                   	push   %rbx
{
	struct mmu_gather_batch *batch;
	if (!tlb->need_flush)
	  return;
	tlb->need_flush = 0;
     6d1:	0f b6 47 18          	movzbl 0x18(%rdi),%eax
     6d5:	89 c2                	mov    %eax,%edx
     6d7:	83 e2 fe             	and    $0xfffffffe,%edx
	tlb_flush(tlb);
     6da:	a8 06                	test   $0x6,%al
void tlb_flush_mmu(struct mmu_gather *tlb)
{
	struct mmu_gather_batch *batch;
	if (!tlb->need_flush)
	  return;
	tlb->need_flush = 0;
     6dc:	88 57 18             	mov    %dl,0x18(%rdi)
	tlb_flush(tlb);
     6df:	75 4f                	jne    730 <tlb_flush_mmu.part.59+0x70>
     6e1:	48 8b 57 10          	mov    0x10(%rdi),%rdx
     6e5:	48 8b 77 08          	mov    0x8(%rdi),%rsi
     6e9:	31 c9                	xor    %ecx,%ecx
     6eb:	48 8b 3f             	mov    (%rdi),%rdi
     6ee:	e8 00 00 00 00       	callq  6f3 <tlb_flush_mmu.part.59+0x33>
#ifdef CONFIG_HAVE_RCU_TABLE_FREE
	tlb_table_flush(tlb);
#endif

	for (batch = &tlb->local; batch; batch = batch->next) {
     6f3:	4d 8d 6c 24 28       	lea    0x28(%r12),%r13
     6f8:	4d 85 ed             	test   %r13,%r13
     6fb:	4c 89 eb             	mov    %r13,%rbx
     6fe:	74 1b                	je     71b <tlb_flush_mmu.part.59+0x5b>
		free_pages_and_swap_cache(batch->pages, batch->nr);
     700:	8b 73 08             	mov    0x8(%rbx),%esi
     703:	48 8d 7b 10          	lea    0x10(%rbx),%rdi
     707:	e8 00 00 00 00       	callq  70c <tlb_flush_mmu.part.59+0x4c>
		batch->nr = 0;
     70c:	c7 43 08 00 00 00 00 	movl   $0x0,0x8(%rbx)
	tlb_flush(tlb);
#ifdef CONFIG_HAVE_RCU_TABLE_FREE
	tlb_table_flush(tlb);
#endif

	for (batch = &tlb->local; batch; batch = batch->next) {
     713:	48 8b 1b             	mov    (%rbx),%rbx
     716:	48 85 db             	test   %rbx,%rbx
     719:	75 e5                	jne    700 <tlb_flush_mmu.part.59+0x40>
		free_pages_and_swap_cache(batch->pages, batch->nr);
		batch->nr = 0;
	}
	tlb->active = &tlb->local;
     71b:	4d 89 6c 24 20       	mov    %r13,0x20(%r12)
}
     720:	5b                   	pop    %rbx
     721:	41 5c                	pop    %r12
     723:	41 5d                	pop    %r13
     725:	5d                   	pop    %rbp
     726:	c3                   	retq   
     727:	66 0f 1f 84 00 00 00 	nopw   0x0(%rax,%rax,1)
     72e:	00 00 
{
	struct mmu_gather_batch *batch;
	if (!tlb->need_flush)
	  return;
	tlb->need_flush = 0;
	tlb_flush(tlb);
     730:	48 8b 3f             	mov    (%rdi),%rdi
     733:	31 c9                	xor    %ecx,%ecx
     735:	48 c7 c2 ff ff ff ff 	mov    $0xffffffffffffffff,%rdx
     73c:	31 f6                	xor    %esi,%esi
     73e:	e8 00 00 00 00       	callq  743 <tlb_flush_mmu.part.59+0x83>
     743:	eb ae                	jmp    6f3 <tlb_flush_mmu.part.59+0x33>
     745:	66 66 2e 0f 1f 84 00 	data32 nopw %cs:0x0(%rax,%rax,1)
     74c:	00 00 00 00 

0000000000000750 <lock_page>:
 * This operation is atomic and cannot be reordered.
 * It also implies a memory barrier.
 */
static inline int test_and_set_bit(long nr, volatile unsigned long *addr)
{
	GEN_BINARY_RMWcc(LOCK_PREFIX "bts", *addr, "Ir", nr, "%0", "c");
     750:	f0 0f ba 2f 00       	lock btsl $0x0,(%rdi)
     755:	72 09                	jb     760 <lock_page+0x10>
     757:	c3                   	retq   
     758:	0f 1f 84 00 00 00 00 	nopl   0x0(%rax,%rax,1)
     75f:	00 

/*
 * lock_page may only be called if we have the page's inode pinned.
 */
static inline void lock_page(struct page *page)
{
     760:	55                   	push   %rbp
     761:	48 89 e5             	mov    %rsp,%rbp
	might_sleep();
	if (!trylock_page(page))
		__lock_page(page);
     764:	e8 00 00 00 00       	callq  769 <lock_page+0x19>
}
     769:	5d                   	pop    %rbp
     76a:	c3                   	retq   
     76b:	0f 1f 44 00 00       	nopl   0x0(%rax,%rax,1)

0000000000000770 <print_bad_pte>:
 *
 * The calling function must still handle the error.
 */
static void print_bad_pte(struct vm_area_struct *vma, unsigned long addr,
			pte_t pte, struct page *page)
{
     770:	e8 00 00 00 00       	callq  775 <print_bad_pte+0x5>
     775:	55                   	push   %rbp
	pgd_t *pgd = pgd_offset(vma->vm_mm, addr);
     776:	48 89 f0             	mov    %rsi,%rax
     779:	48 c1 e8 27          	shr    $0x27,%rax
 *
 * The calling function must still handle the error.
 */
static void print_bad_pte(struct vm_area_struct *vma, unsigned long addr,
			pte_t pte, struct page *page)
{
     77d:	48 89 e5             	mov    %rsp,%rbp
     780:	41 57                	push   %r15
	pgd_t *pgd = pgd_offset(vma->vm_mm, addr);
     782:	25 ff 01 00 00       	and    $0x1ff,%eax
 *
 * The calling function must still handle the error.
 */
static void print_bad_pte(struct vm_area_struct *vma, unsigned long addr,
			pte_t pte, struct page *page)
{
     787:	41 56                	push   %r14
     789:	41 55                	push   %r13
     78b:	49 89 cd             	mov    %rcx,%r13
     78e:	41 54                	push   %r12
     790:	49 89 fc             	mov    %rdi,%r12
     793:	53                   	push   %rbx
     794:	48 89 f3             	mov    %rsi,%rbx
     797:	48 83 ec 08          	sub    $0x8,%rsp
	pgd_t *pgd = pgd_offset(vma->vm_mm, addr);
     79b:	48 8b 4f 40          	mov    0x40(%rdi),%rcx
     79f:	48 8b 49 40          	mov    0x40(%rcx),%rcx

	if (sizeof(pgdval_t) > sizeof(long))
		ret =  PVOP_CALLEE2(pgdval_t, pv_mmu_ops.pgd_val,
				    pgd.pgd, (u64)pgd.pgd >> 32);
	else
		ret =  PVOP_CALLEE1(pgdval_t, pv_mmu_ops.pgd_val,
     7a3:	48 8b 3c c1          	mov    (%rcx,%rax,8),%rdi
     7a7:	ff 14 25 00 00 00 00 	callq  *0x0
#define pgd_page(pgd)		pfn_to_page(pgd_val(pgd) >> PAGE_SHIFT)

/* to find an entry in a page-table-directory. */
static inline unsigned long pud_index(unsigned long address)
{
	return (address >> PUD_SHIFT) & (PTRS_PER_PUD - 1);
     7ae:	49 89 f0             	mov    %rsi,%r8
	return pgd_flags(pgd) & _PAGE_PRESENT;
}

static inline unsigned long pgd_page_vaddr(pgd_t pgd)
{
	return (unsigned long)__va((unsigned long)pgd_val(pgd) & PTE_PFN_MASK);
     7b1:	48 b9 00 f0 ff ff ff 	movabs $0x3ffffffff000,%rcx
     7b8:	3f 00 00 

	if (sizeof(pudval_t) > sizeof(long))
		ret =  PVOP_CALLEE2(pudval_t, pv_mmu_ops.pud_val,
				    pud.pud, (u64)pud.pud >> 32);
	else
		ret =  PVOP_CALLEE1(pudval_t, pv_mmu_ops.pud_val,
     7bb:	48 be 00 00 00 00 00 	movabs $0xffff880000000000,%rsi
     7c2:	88 ff ff 
#define pgd_page(pgd)		pfn_to_page(pgd_val(pgd) >> PAGE_SHIFT)

/* to find an entry in a page-table-directory. */
static inline unsigned long pud_index(unsigned long address)
{
	return (address >> PUD_SHIFT) & (PTRS_PER_PUD - 1);
     7c5:	49 c1 e8 1b          	shr    $0x1b,%r8
	return pgd_flags(pgd) & _PAGE_PRESENT;
}

static inline unsigned long pgd_page_vaddr(pgd_t pgd)
{
	return (unsigned long)__va((unsigned long)pgd_val(pgd) & PTE_PFN_MASK);
     7c9:	48 21 c8             	and    %rcx,%rax
     7cc:	41 81 e0 f8 0f 00 00 	and    $0xff8,%r8d
     7d3:	4c 01 c0             	add    %r8,%rax
     7d6:	48 8b 3c 30          	mov    (%rax,%rsi,1),%rdi
     7da:	ff 14 25 00 00 00 00 	callq  *0x0

	/*
	 * Allow a burst of 60 reports, then keep quiet for that minute;
	 * or allow a steady drip of one report per second.
	 */
	if (nr_shown == 60) {
     7e1:	48 8b 35 00 00 00 00 	mov    0x0(%rip),%rsi        # 7e8 <print_bad_pte+0x78>
     7e8:	49 89 c7             	mov    %rax,%r15
     7eb:	48 83 fe 3c          	cmp    $0x3c,%rsi
     7ef:	75 2b                	jne    81c <print_bad_pte+0xac>
		if (time_before(jiffies, resume)) {
     7f1:	48 8b 0d 00 00 00 00 	mov    0x0(%rip),%rcx        # 7f8 <print_bad_pte+0x88>
     7f8:	48 3b 0d 00 00 00 00 	cmp    0x0(%rip),%rcx        # 7ff <print_bad_pte+0x8f>
     7ff:	0f 89 22 01 00 00    	jns    927 <print_bad_pte+0x1b7>
			nr_unshown++;
     805:	48 83 05 00 00 00 00 	addq   $0x1,0x0(%rip)        # 80d <print_bad_pte+0x9d>
     80c:	01 
	if (vma->vm_file)
	  printk(KERN_ALERT "vma->vm_file->f_op->mmap: %pSR\n",
				  vma->vm_file->f_op->mmap);
	dump_stack();
	add_taint(TAINT_BAD_PAGE, LOCKDEP_NOW_UNRELIABLE);
}
     80d:	48 83 c4 08          	add    $0x8,%rsp
     811:	5b                   	pop    %rbx
     812:	41 5c                	pop    %r12
     814:	41 5d                	pop    %r13
     816:	41 5e                	pop    %r14
     818:	41 5f                	pop    %r15
     81a:	5d                   	pop    %rbp
     81b:	c3                   	retq   
						nr_unshown);
			nr_unshown = 0;
		}
		nr_shown = 0;
	}
	if (nr_shown++ == 0)
     81c:	48 8d 46 01          	lea    0x1(%rsi),%rax
     820:	48 85 f6             	test   %rsi,%rsi
     823:	48 89 05 00 00 00 00 	mov    %rax,0x0(%rip)        # 82a <print_bad_pte+0xba>
     82a:	0f 84 0e 01 00 00    	je     93e <print_bad_pte+0x1ce>
	  resume = jiffies + 60 * HZ;

	mapping = vma->vm_file ? vma->vm_file->f_mapping : NULL;
     830:	49 8b 84 24 a8 00 00 	mov    0xa8(%r12),%rax
     837:	00 
     838:	48 85 c0             	test   %rax,%rax
     83b:	0f 84 16 01 00 00    	je     957 <print_bad_pte+0x1e7>
     841:	48 8b 80 f0 00 00 00 	mov    0xf0(%rax),%rax
     848:	48 89 45 d0          	mov    %rax,-0x30(%rbp)
					unsigned long address)
{
	pgoff_t pgoff;
	if (unlikely(is_vm_hugetlb_page(vma)))
		return linear_hugepage_index(vma, address);
	pgoff = (address - vma->vm_start) >> PAGE_SHIFT;
     84c:	49 89 de             	mov    %rbx,%r14
 * this macro returns the index of the entry in the pmd page which would
 * control the given virtual address
 */
static inline unsigned long pmd_index(unsigned long address)
{
	return (address >> PMD_SHIFT) & (PTRS_PER_PMD - 1);
     84f:	48 89 d8             	mov    %rbx,%rax
     852:	4d 2b 34 24          	sub    (%r12),%r14
     856:	48 c1 e8 12          	shr    $0x12,%rax
	return pud_flags(pud) & _PAGE_PRESENT;
}

static inline unsigned long pud_page_vaddr(pud_t pud)
{
	return (unsigned long)__va((unsigned long)pud_val(pud) & PTE_PFN_MASK);
     85a:	48 b9 00 f0 ff ff ff 	movabs $0x3ffffffff000,%rcx
     861:	3f 00 00 
     864:	49 21 cf             	and    %rcx,%r15
     867:	25 f8 0f 00 00       	and    $0xff8,%eax

	if (sizeof(pmdval_t) > sizeof(long))
		ret =  PVOP_CALLEE2(pmdval_t, pv_mmu_ops.pmd_val,
				    pmd.pmd, (u64)pmd.pmd >> 32);
	else
		ret =  PVOP_CALLEE1(pmdval_t, pv_mmu_ops.pmd_val,
     86c:	48 b9 00 00 00 00 00 	movabs $0xffff880000000000,%rcx
     873:	88 ff ff 
     876:	4c 01 f8             	add    %r15,%rax
     879:	49 c1 ee 0c          	shr    $0xc,%r14
     87d:	48 8b 3c 08          	mov    (%rax,%rcx,1),%rdi
	pgoff += vma->vm_pgoff;
     881:	4d 03 b4 24 a0 00 00 	add    0xa0(%r12),%r14
     888:	00 
     889:	ff 14 25 00 00 00 00 	callq  *0x0
     890:	48 89 c1             	mov    %rax,%rcx

	if (sizeof(pteval_t) > sizeof(long))
		ret = PVOP_CALLEE2(pteval_t, pv_mmu_ops.pte_val,
				   pte.pte, (u64)pte.pte >> 32);
	else
		ret = PVOP_CALLEE1(pteval_t, pv_mmu_ops.pte_val,
     893:	48 89 d7             	mov    %rdx,%rdi
     896:	ff 14 25 00 00 00 00 	callq  *0x0
     89d:	65 48 8b 34 25 00 00 	mov    %gs:0x0,%rsi
     8a4:	00 00 
     8a6:	48 89 c2             	mov    %rax,%rdx
	index = linear_page_index(vma, addr);

	printk(KERN_ALERT
				"BUG: Bad page map in process %s  pte:%08llx pmd:%08llx\n",
				current->comm,
     8a9:	48 81 c6 d0 04 00 00 	add    $0x4d0,%rsi
	  resume = jiffies + 60 * HZ;

	mapping = vma->vm_file ? vma->vm_file->f_mapping : NULL;
	index = linear_page_index(vma, addr);

	printk(KERN_ALERT
     8b0:	31 c0                	xor    %eax,%eax
     8b2:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
     8b9:	e8 00 00 00 00       	callq  8be <print_bad_pte+0x14e>
				"BUG: Bad page map in process %s  pte:%08llx pmd:%08llx\n",
				current->comm,
				(long long)pte_val(pte), (long long)pmd_val(*pmd));
	if (page)
     8be:	4d 85 ed             	test   %r13,%r13
     8c1:	74 08                	je     8cb <print_bad_pte+0x15b>
	  dump_page(page);
     8c3:	4c 89 ef             	mov    %r13,%rdi
     8c6:	e8 00 00 00 00       	callq  8cb <print_bad_pte+0x15b>
	printk(KERN_ALERT
     8cb:	49 8b 8c 24 90 00 00 	mov    0x90(%r12),%rcx
     8d2:	00 
     8d3:	49 8b 54 24 50       	mov    0x50(%r12),%rdx
     8d8:	31 c0                	xor    %eax,%eax
     8da:	4c 8b 45 d0          	mov    -0x30(%rbp),%r8
     8de:	4d 89 f1             	mov    %r14,%r9
     8e1:	48 89 de             	mov    %rbx,%rsi
     8e4:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
     8eb:	e8 00 00 00 00       	callq  8f0 <print_bad_pte+0x180>
				"addr:%p vm_flags:%08lx anon_vma:%p mapping:%p index:%lx\n",
				(void *)addr, vma->vm_flags, vma->anon_vma, mapping, index);
	/*
	 * Choose text because data symbols depend on CONFIG_KALLSYMS_ALL=y
	 */
	if (vma->vm_ops)
     8f0:	49 8b 84 24 98 00 00 	mov    0x98(%r12),%rax
     8f7:	00 
     8f8:	48 85 c0             	test   %rax,%rax
     8fb:	0f 85 a1 00 00 00    	jne    9a2 <print_bad_pte+0x232>
	  printk(KERN_ALERT "vma->vm_ops->fault: %pSR\n",
				  vma->vm_ops->fault);
	if (vma->vm_file)
     901:	49 8b 84 24 a8 00 00 	mov    0xa8(%r12),%rax
     908:	00 
     909:	48 85 c0             	test   %rax,%rax
     90c:	75 79                	jne    987 <print_bad_pte+0x217>
	  printk(KERN_ALERT "vma->vm_file->f_op->mmap: %pSR\n",
				  vma->vm_file->f_op->mmap);
	dump_stack();
     90e:	e8 00 00 00 00       	callq  913 <print_bad_pte+0x1a3>
	add_taint(TAINT_BAD_PAGE, LOCKDEP_NOW_UNRELIABLE);
     913:	be 01 00 00 00       	mov    $0x1,%esi
     918:	bf 05 00 00 00       	mov    $0x5,%edi
     91d:	e8 00 00 00 00       	callq  922 <print_bad_pte+0x1b2>
     922:	e9 e6 fe ff ff       	jmpq   80d <print_bad_pte+0x9d>
	if (nr_shown == 60) {
		if (time_before(jiffies, resume)) {
			nr_unshown++;
			return;
		}
		if (nr_unshown) {
     927:	48 8b 35 00 00 00 00 	mov    0x0(%rip),%rsi        # 92e <print_bad_pte+0x1be>
     92e:	48 85 f6             	test   %rsi,%rsi
     931:	75 31                	jne    964 <print_bad_pte+0x1f4>
						nr_unshown);
			nr_unshown = 0;
		}
		nr_shown = 0;
	}
	if (nr_shown++ == 0)
     933:	48 c7 05 00 00 00 00 	movq   $0x1,0x0(%rip)        # 93e <print_bad_pte+0x1ce>
     93a:	01 00 00 00 
	  resume = jiffies + 60 * HZ;
     93e:	48 8b 05 00 00 00 00 	mov    0x0(%rip),%rax        # 945 <print_bad_pte+0x1d5>
     945:	48 05 98 3a 00 00    	add    $0x3a98,%rax
     94b:	48 89 05 00 00 00 00 	mov    %rax,0x0(%rip)        # 952 <print_bad_pte+0x1e2>
     952:	e9 d9 fe ff ff       	jmpq   830 <print_bad_pte+0xc0>

	mapping = vma->vm_file ? vma->vm_file->f_mapping : NULL;
     957:	48 c7 45 d0 00 00 00 	movq   $0x0,-0x30(%rbp)
     95e:	00 
     95f:	e9 e8 fe ff ff       	jmpq   84c <print_bad_pte+0xdc>
		if (time_before(jiffies, resume)) {
			nr_unshown++;
			return;
		}
		if (nr_unshown) {
			printk(KERN_ALERT
     964:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
     96b:	31 c0                	xor    %eax,%eax
     96d:	48 89 55 d0          	mov    %rdx,-0x30(%rbp)
     971:	e8 00 00 00 00       	callq  976 <print_bad_pte+0x206>
						"BUG: Bad page map: %lu messages suppressed\n",
						nr_unshown);
			nr_unshown = 0;
     976:	48 c7 05 00 00 00 00 	movq   $0x0,0x0(%rip)        # 981 <print_bad_pte+0x211>
     97d:	00 00 00 00 
     981:	48 8b 55 d0          	mov    -0x30(%rbp),%rdx
     985:	eb ac                	jmp    933 <print_bad_pte+0x1c3>
	 */
	if (vma->vm_ops)
	  printk(KERN_ALERT "vma->vm_ops->fault: %pSR\n",
				  vma->vm_ops->fault);
	if (vma->vm_file)
	  printk(KERN_ALERT "vma->vm_file->f_op->mmap: %pSR\n",
     987:	48 8b 40 28          	mov    0x28(%rax),%rax
     98b:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
     992:	48 8b 70 50          	mov    0x50(%rax),%rsi
     996:	31 c0                	xor    %eax,%eax
     998:	e8 00 00 00 00       	callq  99d <print_bad_pte+0x22d>
     99d:	e9 6c ff ff ff       	jmpq   90e <print_bad_pte+0x19e>
				(void *)addr, vma->vm_flags, vma->anon_vma, mapping, index);
	/*
	 * Choose text because data symbols depend on CONFIG_KALLSYMS_ALL=y
	 */
	if (vma->vm_ops)
	  printk(KERN_ALERT "vma->vm_ops->fault: %pSR\n",
     9a2:	48 8b 70 10          	mov    0x10(%rax),%rsi
     9a6:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
     9ad:	31 c0                	xor    %eax,%eax
     9af:	e8 00 00 00 00       	callq  9b4 <print_bad_pte+0x244>
     9b4:	e9 48 ff ff ff       	jmpq   901 <print_bad_pte+0x191>
     9b9:	0f 1f 80 00 00 00 00 	nopl   0x0(%rax)

00000000000009c0 <__follow_pte.isra.40>:
	return 0;
}

#endif	/* __HAVE_ARCH_GATE_AREA */

static int __follow_pte(struct mm_struct *mm, unsigned long address,
     9c0:	e8 00 00 00 00       	callq  9c5 <__follow_pte.isra.40+0x5>
	pgd_t *pgd;
	pud_t *pud;
	pmd_t *pmd;
	pte_t *ptep;

	pgd = pgd_offset(mm, address);
     9c5:	48 89 f0             	mov    %rsi,%rax
     9c8:	48 c1 e8 27          	shr    $0x27,%rax
     9cc:	25 ff 01 00 00       	and    $0x1ff,%eax
     9d1:	48 8b 3c c7          	mov    (%rdi,%rax,8),%rdi
	if (pgd_none(*pgd) || unlikely(pgd_bad(*pgd)))
     9d5:	48 85 ff             	test   %rdi,%rdi
     9d8:	0f 84 52 01 00 00    	je     b30 <__follow_pte.isra.40+0x170>
	return 0;
}

#endif	/* __HAVE_ARCH_GATE_AREA */

static int __follow_pte(struct mm_struct *mm, unsigned long address,
     9de:	55                   	push   %rbp
	return (pud_t *)pgd_page_vaddr(*pgd) + pud_index(address);
}

static inline int pgd_bad(pgd_t pgd)
{
	return (pgd_flags(pgd) & ~_PAGE_USER) != _KERNPG_TABLE;
     9df:	48 89 f8             	mov    %rdi,%rax
     9e2:	48 89 e5             	mov    %rsp,%rbp
     9e5:	41 55                	push   %r13
     9e7:	41 54                	push   %r12
     9e9:	49 89 cc             	mov    %rcx,%r12
     9ec:	48 b9 fb 0f 00 00 00 	movabs $0xffffc00000000ffb,%rcx
     9f3:	c0 ff ff 
     9f6:	48 21 c8             	and    %rcx,%rax
	pud_t *pud;
	pmd_t *pmd;
	pte_t *ptep;

	pgd = pgd_offset(mm, address);
	if (pgd_none(*pgd) || unlikely(pgd_bad(*pgd)))
     9f9:	48 83 f8 63          	cmp    $0x63,%rax
	return 0;
}

#endif	/* __HAVE_ARCH_GATE_AREA */

static int __follow_pte(struct mm_struct *mm, unsigned long address,
     9fd:	53                   	push   %rbx
	pud_t *pud;
	pmd_t *pmd;
	pte_t *ptep;

	pgd = pgd_offset(mm, address);
	if (pgd_none(*pgd) || unlikely(pgd_bad(*pgd)))
     9fe:	0f 85 24 01 00 00    	jne    b28 <__follow_pte.isra.40+0x168>
     a04:	48 89 d3             	mov    %rdx,%rbx

	if (sizeof(pgdval_t) > sizeof(long))
		ret =  PVOP_CALLEE2(pgdval_t, pv_mmu_ops.pgd_val,
				    pgd.pgd, (u64)pgd.pgd >> 32);
	else
		ret =  PVOP_CALLEE1(pgdval_t, pv_mmu_ops.pgd_val,
     a07:	ff 14 25 00 00 00 00 	callq  *0x0
#define pgd_page(pgd)		pfn_to_page(pgd_val(pgd) >> PAGE_SHIFT)

/* to find an entry in a page-table-directory. */
static inline unsigned long pud_index(unsigned long address)
{
	return (address >> PUD_SHIFT) & (PTRS_PER_PUD - 1);
     a0e:	48 89 f7             	mov    %rsi,%rdi
	return pgd_flags(pgd) & _PAGE_PRESENT;
}

static inline unsigned long pgd_page_vaddr(pgd_t pgd)
{
	return (unsigned long)__va((unsigned long)pgd_val(pgd) & PTE_PFN_MASK);
     a11:	48 ba 00 f0 ff ff ff 	movabs $0x3ffffffff000,%rdx
     a18:	3f 00 00 
     a1b:	49 b8 00 00 00 00 00 	movabs $0xffff880000000000,%r8
     a22:	88 ff ff 
#define pgd_page(pgd)		pfn_to_page(pgd_val(pgd) >> PAGE_SHIFT)

/* to find an entry in a page-table-directory. */
static inline unsigned long pud_index(unsigned long address)
{
	return (address >> PUD_SHIFT) & (PTRS_PER_PUD - 1);
     a25:	48 c1 ef 1b          	shr    $0x1b,%rdi
	return pgd_flags(pgd) & _PAGE_PRESENT;
}

static inline unsigned long pgd_page_vaddr(pgd_t pgd)
{
	return (unsigned long)__va((unsigned long)pgd_val(pgd) & PTE_PFN_MASK);
     a29:	48 21 d0             	and    %rdx,%rax
     a2c:	81 e7 f8 0f 00 00    	and    $0xff8,%edi
     a32:	48 01 c7             	add    %rax,%rdi
     a35:	4a 8b 3c 07          	mov    (%rdi,%r8,1),%rdi
	  goto out;

	pud = pud_offset(pgd, address);
	if (pud_none(*pud) || unlikely(pud_bad(*pud)))
     a39:	48 85 ff             	test   %rdi,%rdi
     a3c:	0f 84 e6 00 00 00    	je     b28 <__follow_pte.isra.40+0x168>
     a42:	48 b8 98 0f 00 00 00 	movabs $0xffffc00000000f98,%rax
     a49:	c0 ff ff 
     a4c:	48 85 c7             	test   %rax,%rdi
     a4f:	0f 85 d3 00 00 00    	jne    b28 <__follow_pte.isra.40+0x168>

	if (sizeof(pudval_t) > sizeof(long))
		ret =  PVOP_CALLEE2(pudval_t, pv_mmu_ops.pud_val,
				    pud.pud, (u64)pud.pud >> 32);
	else
		ret =  PVOP_CALLEE1(pudval_t, pv_mmu_ops.pud_val,
     a55:	ff 14 25 00 00 00 00 	callq  *0x0
 * this macro returns the index of the entry in the pmd page which would
 * control the given virtual address
 */
static inline unsigned long pmd_index(unsigned long address)
{
	return (address >> PMD_SHIFT) & (PTRS_PER_PMD - 1);
     a5c:	48 89 f7             	mov    %rsi,%rdi
	return pud_flags(pud) & _PAGE_PRESENT;
}

static inline unsigned long pud_page_vaddr(pud_t pud)
{
	return (unsigned long)__va((unsigned long)pud_val(pud) & PTE_PFN_MASK);
     a5f:	48 21 d0             	and    %rdx,%rax
 * this macro returns the index of the entry in the pmd page which would
 * control the given virtual address
 */
static inline unsigned long pmd_index(unsigned long address)
{
	return (address >> PMD_SHIFT) & (PTRS_PER_PMD - 1);
     a62:	48 c1 ef 12          	shr    $0x12,%rdi
#define pud_page(pud)		pfn_to_page(pud_val(pud) >> PAGE_SHIFT)

/* Find an entry in the second-level page table.. */
static inline pmd_t *pmd_offset(pud_t *pud, unsigned long address)
{
	return (pmd_t *)pud_page_vaddr(*pud) + pmd_index(address);
     a66:	81 e7 f8 0f 00 00    	and    $0xff8,%edi
	return pud_flags(pud) & _PAGE_PRESENT;
}

static inline unsigned long pud_page_vaddr(pud_t pud)
{
	return (unsigned long)__va((unsigned long)pud_val(pud) & PTE_PFN_MASK);
     a6c:	49 01 f8             	add    %rdi,%r8
#define pud_page(pud)		pfn_to_page(pud_val(pud) >> PAGE_SHIFT)

/* Find an entry in the second-level page table.. */
static inline pmd_t *pmd_offset(pud_t *pud, unsigned long address)
{
	return (pmd_t *)pud_page_vaddr(*pud) + pmd_index(address);
     a6f:	49 01 c0             	add    %rax,%r8
     a72:	49 8b 38             	mov    (%r8),%rdi
	  goto out;

	pmd = pmd_offset(pud, address);
	VM_BUG_ON(pmd_trans_huge(*pmd));
	if (pmd_none(*pmd) || unlikely(pmd_bad(*pmd)))
     a75:	48 85 ff             	test   %rdi,%rdi
     a78:	0f 84 aa 00 00 00    	je     b28 <__follow_pte.isra.40+0x168>

static inline int pmd_bad(pmd_t pmd)
{
#ifdef CONFIG_NUMA_BALANCING
	/* pmd_numa check */
	if ((pmd_flags(pmd) & (_PAGE_NUMA|_PAGE_PRESENT)) == _PAGE_NUMA)
     a7e:	48 89 f8             	mov    %rdi,%rax
     a81:	25 01 01 00 00       	and    $0x101,%eax
     a86:	48 3d 00 01 00 00    	cmp    $0x100,%rax
     a8c:	74 0d                	je     a9b <__follow_pte.isra.40+0xdb>
		return 0;
#endif
	return (pmd_flags(pmd) & ~_PAGE_USER) != _KERNPG_TABLE;
     a8e:	48 21 f9             	and    %rdi,%rcx
     a91:	48 83 f9 63          	cmp    $0x63,%rcx
     a95:	0f 85 8d 00 00 00    	jne    b28 <__follow_pte.isra.40+0x168>

	if (sizeof(pmdval_t) > sizeof(long))
		ret =  PVOP_CALLEE2(pmdval_t, pv_mmu_ops.pmd_val,
				    pmd.pmd, (u64)pmd.pmd >> 32);
	else
		ret =  PVOP_CALLEE1(pmdval_t, pv_mmu_ops.pmd_val,
     a9b:	ff 14 25 00 00 00 00 	callq  *0x0
}
#endif /* ALLOC_SPLIT_PTLOCKS */

static inline spinlock_t *pte_lockptr(struct mm_struct *mm, pmd_t *pmd)
{
	return ptlock_ptr(pmd_page(*pmd));
     aa2:	48 b9 00 f0 ff ff ff 	movabs $0x3ffffffff000,%rcx
     aa9:	3f 00 00 
     aac:	48 ba 00 00 00 00 00 	movabs $0xffffea0000000000,%rdx
     ab3:	ea ff ff 
     ab6:	49 8b 38             	mov    (%r8),%rdi
     ab9:	48 21 c8             	and    %rcx,%rax
     abc:	48 c1 e8 06          	shr    $0x6,%rax
     ac0:	48 8b 54 10 30       	mov    0x30(%rax,%rdx,1),%rdx
     ac5:	ff 14 25 00 00 00 00 	callq  *0x0
 * this function returns the index of the entry in the pte page which would
 * control the given virtual address
 */
static inline unsigned long pte_index(unsigned long address)
{
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
     acc:	48 c1 ee 09          	shr    $0x9,%rsi
	return (unsigned long)native_pmd_val(pmd) == 0;
}

static inline unsigned long pmd_page_vaddr(pmd_t pmd)
{
	return (unsigned long)__va(pmd_val(pmd) & PTE_PFN_MASK);
     ad0:	49 bd 00 00 00 00 00 	movabs $0xffff880000000000,%r13
     ad7:	88 ff ff 
     ada:	48 21 c8             	and    %rcx,%rax
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
}

static inline pte_t *pte_offset_kernel(pmd_t *pmd, unsigned long address)
{
	return (pte_t *)pmd_page_vaddr(*pmd) + pte_index(address);
     add:	81 e6 f8 0f 00 00    	and    $0xff8,%esi

	/* We cannot handle huge page PFN maps. Luckily they don't exist. */
	if (pmd_huge(*pmd))
	  goto out;

	ptep = pte_offset_map_lock(mm, pmd, address, ptlp);
     ae3:	49 89 14 24          	mov    %rdx,(%r12)
	raw_spin_lock_init(&(_lock)->rlock);		\
} while (0)

static inline void spin_lock(spinlock_t *lock)
{
	raw_spin_lock(&lock->rlock);
     ae7:	48 89 d7             	mov    %rdx,%rdi
	return (unsigned long)native_pmd_val(pmd) == 0;
}

static inline unsigned long pmd_page_vaddr(pmd_t pmd)
{
	return (unsigned long)__va(pmd_val(pmd) & PTE_PFN_MASK);
     aea:	4c 01 ee             	add    %r13,%rsi
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
}

static inline pte_t *pte_offset_kernel(pmd_t *pmd, unsigned long address)
{
	return (pte_t *)pmd_page_vaddr(*pmd) + pte_index(address);
     aed:	4c 8d 2c 06          	lea    (%rsi,%rax,1),%r13
     af1:	e8 00 00 00 00       	callq  af6 <__follow_pte.isra.40+0x136>
	if (!ptep)
     af6:	4d 85 ed             	test   %r13,%r13
     af9:	74 2d                	je     b28 <__follow_pte.isra.40+0x168>
	  goto out;
	if (!pte_present(*ptep))
     afb:	49 f7 45 00 01 01 00 	testq  $0x101,0x0(%r13)
     b02:	00 
     b03:	74 13                	je     b18 <__follow_pte.isra.40+0x158>
	  goto unlock;
	*ptepp = ptep;
     b05:	4c 89 2b             	mov    %r13,(%rbx)
	return 0;
     b08:	31 c0                	xor    %eax,%eax
unlock:
	pte_unmap_unlock(ptep, *ptlp);
out:
	return -EINVAL;
}
     b0a:	5b                   	pop    %rbx
     b0b:	41 5c                	pop    %r12
     b0d:	41 5d                	pop    %r13
     b0f:	5d                   	pop    %rbp
     b10:	c3                   	retq   
     b11:	0f 1f 80 00 00 00 00 	nopl   0x0(%rax)
	raw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \
} while (0)

static inline void spin_unlock(spinlock_t *lock)
{
	raw_spin_unlock(&lock->rlock);
     b18:	49 8b 3c 24          	mov    (%r12),%rdi
     b1c:	e8 00 00 00 00       	callq  b21 <__follow_pte.isra.40+0x161>
     b21:	0f 1f 80 00 00 00 00 	nopl   0x0(%rax)
	*ptepp = ptep;
	return 0;
unlock:
	pte_unmap_unlock(ptep, *ptlp);
out:
	return -EINVAL;
     b28:	b8 ea ff ff ff       	mov    $0xffffffea,%eax
     b2d:	eb db                	jmp    b0a <__follow_pte.isra.40+0x14a>
     b2f:	90                   	nop
     b30:	b8 ea ff ff ff       	mov    $0xffffffea,%eax
}
     b35:	c3                   	retq   
     b36:	66 2e 0f 1f 84 00 00 	nopw   %cs:0x0(%rax,%rax,1)
     b3d:	00 00 00 

0000000000000b40 <follow_pfn>:
 *
 * Returns zero and the pfn at @pfn on success, -ve otherwise.
 */
int follow_pfn(struct vm_area_struct *vma, unsigned long address,
			unsigned long *pfn)
{
     b40:	e8 00 00 00 00       	callq  b45 <follow_pfn+0x5>
     b45:	55                   	push   %rbp
     b46:	48 89 e5             	mov    %rsp,%rbp
     b49:	41 54                	push   %r12
     b4b:	53                   	push   %rbx
	int ret = -EINVAL;
	spinlock_t *ptl;
	pte_t *ptep;

	if (!(vma->vm_flags & (VM_IO | VM_PFNMAP)))
	  return ret;
     b4c:	bb ea ff ff ff       	mov    $0xffffffea,%ebx
 *
 * Returns zero and the pfn at @pfn on success, -ve otherwise.
 */
int follow_pfn(struct vm_area_struct *vma, unsigned long address,
			unsigned long *pfn)
{
     b51:	48 83 ec 10          	sub    $0x10,%rsp
	int ret = -EINVAL;
	spinlock_t *ptl;
	pte_t *ptep;

	if (!(vma->vm_flags & (VM_IO | VM_PFNMAP)))
     b55:	48 f7 47 50 00 44 00 	testq  $0x4400,0x50(%rdi)
     b5c:	00 
     b5d:	74 41                	je     ba0 <follow_pfn+0x60>
	pte_unmap_unlock(ptep, *ptlp);
out:
	return -EINVAL;
}

static inline int follow_pte(struct mm_struct *mm, unsigned long address,
     b5f:	48 8b 47 40          	mov    0x40(%rdi),%rax
     b63:	49 89 d4             	mov    %rdx,%r12
			pte_t **ptepp, spinlock_t **ptlp)
{
	int res;

	/* (void) is needed to make gcc happy */
	(void) __cond_lock(*ptlp,
     b66:	48 8d 4d e0          	lea    -0x20(%rbp),%rcx
     b6a:	48 8d 55 e8          	lea    -0x18(%rbp),%rdx
     b6e:	48 8b 78 40          	mov    0x40(%rax),%rdi
     b72:	e8 49 fe ff ff       	callq  9c0 <__follow_pte.isra.40>

	if (!(vma->vm_flags & (VM_IO | VM_PFNMAP)))
	  return ret;

	ret = follow_pte(vma->vm_mm, address, &ptep, &ptl);
	if (ret)
     b77:	85 c0                	test   %eax,%eax
			pte_t **ptepp, spinlock_t **ptlp)
{
	int res;

	/* (void) is needed to make gcc happy */
	(void) __cond_lock(*ptlp,
     b79:	89 c3                	mov    %eax,%ebx

	if (!(vma->vm_flags & (VM_IO | VM_PFNMAP)))
	  return ret;

	ret = follow_pte(vma->vm_mm, address, &ptep, &ptl);
	if (ret)
     b7b:	75 23                	jne    ba0 <follow_pfn+0x60>
     b7d:	48 8b 45 e8          	mov    -0x18(%rbp),%rax

	if (sizeof(pteval_t) > sizeof(long))
		ret = PVOP_CALLEE2(pteval_t, pv_mmu_ops.pte_val,
				   pte.pte, (u64)pte.pte >> 32);
	else
		ret = PVOP_CALLEE1(pteval_t, pv_mmu_ops.pte_val,
     b81:	48 8b 38             	mov    (%rax),%rdi
     b84:	ff 14 25 00 00 00 00 	callq  *0x0
	return pte_flags(pte) & _PAGE_SPECIAL;
}

static inline unsigned long pte_pfn(pte_t pte)
{
	return (pte_val(pte) & PTE_PFN_MASK) >> PAGE_SHIFT;
     b8b:	48 c1 e0 12          	shl    $0x12,%rax
     b8f:	48 8b 7d e0          	mov    -0x20(%rbp),%rdi
     b93:	48 c1 e8 1e          	shr    $0x1e,%rax
     b97:	49 89 04 24          	mov    %rax,(%r12)
     b9b:	e8 00 00 00 00       	callq  ba0 <follow_pfn+0x60>
	  return ret;
	*pfn = pte_pfn(*ptep);
	pte_unmap_unlock(ptep, ptl);
	return 0;
}
     ba0:	48 83 c4 10          	add    $0x10,%rsp
     ba4:	89 d8                	mov    %ebx,%eax
     ba6:	5b                   	pop    %rbx
     ba7:	41 5c                	pop    %r12
     ba9:	5d                   	pop    %rbp
     baa:	c3                   	retq   
     bab:	0f 1f 44 00 00       	nopl   0x0(%rax,%rax,1)

0000000000000bb0 <do_anonymous_page.isra.47>:
/*
 * We enter with non-exclusive mmap_sem (to exclude vma changes,
 * but allow concurrent faults), and pte mapped but not yet locked.
 * We return with mmap_sem still held, but pte unmapped and unlocked.
 */
static int do_anonymous_page(struct mm_struct *mm, struct vm_area_struct *vma,
     bb0:	e8 00 00 00 00       	callq  bb5 <do_anonymous_page.isra.47+0x5>
     bb5:	55                   	push   %rbp
     bb6:	48 89 e5             	mov    %rsp,%rbp
     bb9:	41 57                	push   %r15
     bbb:	49 89 cf             	mov    %rcx,%r15
     bbe:	41 56                	push   %r14
     bc0:	41 55                	push   %r13
     bc2:	41 54                	push   %r12
     bc4:	49 89 d4             	mov    %rdx,%r12
     bc7:	53                   	push   %rbx
     bc8:	48 89 f3             	mov    %rsi,%rbx
     bcb:	48 83 ec 20          	sub    $0x20,%rsp
 * doesn't hit another vma.
 */
static inline int check_stack_guard_page(struct vm_area_struct *vma, unsigned long address)
{
	address &= PAGE_MASK;
	if ((vma->vm_flags & VM_GROWSDOWN) && address == vma->vm_start) {
     bcf:	f6 46 51 01          	testb  $0x1,0x51(%rsi)
/*
 * We enter with non-exclusive mmap_sem (to exclude vma changes,
 * but allow concurrent faults), and pte mapped but not yet locked.
 * We return with mmap_sem still held, but pte unmapped and unlocked.
 */
static int do_anonymous_page(struct mm_struct *mm, struct vm_area_struct *vma,
     bd3:	48 89 7d d0          	mov    %rdi,-0x30(%rbp)
 * doesn't hit another vma.
 */
static inline int check_stack_guard_page(struct vm_area_struct *vma, unsigned long address)
{
	address &= PAGE_MASK;
	if ((vma->vm_flags & VM_GROWSDOWN) && address == vma->vm_start) {
     bd7:	74 13                	je     bec <do_anonymous_page.isra.47+0x3c>
 * except we must first make sure that 'address{-|+}PAGE_SIZE'
 * doesn't hit another vma.
 */
static inline int check_stack_guard_page(struct vm_area_struct *vma, unsigned long address)
{
	address &= PAGE_MASK;
     bd9:	48 89 d6             	mov    %rdx,%rsi
     bdc:	48 81 e6 00 f0 ff ff 	and    $0xfffffffffffff000,%rsi
	if ((vma->vm_flags & VM_GROWSDOWN) && address == vma->vm_start) {
     be3:	48 3b 33             	cmp    (%rbx),%rsi
     be6:	0f 84 3c 02 00 00    	je     e28 <do_anonymous_page.isra.47+0x278>
	/* Check if we need to add a guard page to the stack */
	if (check_stack_guard_page(vma, address) < 0)
	  return VM_FAULT_SIGBUS;

	/* Use the zero-page for reads */
	if (!(flags & FAULT_FLAG_WRITE)) {
     bec:	41 83 e0 01          	and    $0x1,%r8d
     bf0:	0f 84 7a 01 00 00    	je     d70 <do_anonymous_page.isra.47+0x1c0>
		  goto unlock;
		goto setpte;
	}

	/* Allocate our own private page. */
	if (unlikely(anon_vma_prepare(vma)))
     bf6:	48 89 df             	mov    %rbx,%rdi
     bf9:	e8 00 00 00 00       	callq  bfe <do_anonymous_page.isra.47+0x4e>
     bfe:	85 c0                	test   %eax,%eax
     c00:	0f 85 62 02 00 00    	jne    e68 <do_anonymous_page.isra.47+0x2b8>
 */
static inline struct page *
alloc_zeroed_user_highpage_movable(struct vm_area_struct *vma,
					unsigned long vaddr)
{
	return __alloc_zeroed_user_highpage(__GFP_MOVABLE, vma, vaddr);
     c06:	31 f6                	xor    %esi,%esi
     c08:	4c 89 e1             	mov    %r12,%rcx
     c0b:	48 89 da             	mov    %rbx,%rdx
     c0e:	bf da 80 02 00       	mov    $0x280da,%edi
     c13:	65 44 8b 04 25 00 00 	mov    %gs:0x0,%r8d
     c1a:	00 00 
     c1c:	e8 00 00 00 00       	callq  c21 <do_anonymous_page.isra.47+0x71>
	  goto oom;
	page = alloc_zeroed_user_highpage_movable(vma, address);
	if (!page)
     c21:	48 85 c0             	test   %rax,%rax
     c24:	49 89 c6             	mov    %rax,%r14
     c27:	0f 84 3b 02 00 00    	je     e68 <do_anonymous_page.isra.47+0x2b8>
 * If it's called on the same region of memory simultaneously, the effect
 * may be that only one operation succeeds.
 */
static inline void __set_bit(long nr, volatile unsigned long *addr)
{
	asm volatile("bts %1,%0" : ADDR : "Ir" (nr) : "memory");
     c2d:	0f ba 28 03          	btsl   $0x3,(%rax)
	 * preceeding stores to the page contents become visible before
	 * the set_pte_at() write.
	 */
	__SetPageUptodate(page);

	if (mem_cgroup_newpage_charge(page, mm, GFP_KERNEL))
     c31:	48 8b 75 d0          	mov    -0x30(%rbp),%rsi
     c35:	ba d0 00 00 00       	mov    $0xd0,%edx
     c3a:	48 89 c7             	mov    %rax,%rdi
     c3d:	e8 00 00 00 00       	callq  c42 <do_anonymous_page.isra.47+0x92>
     c42:	85 c0                	test   %eax,%eax
     c44:	41 89 c5             	mov    %eax,%r13d
     c47:	0f 85 13 02 00 00    	jne    e60 <do_anonymous_page.isra.47+0x2b0>
	  goto oom_free_page;

	entry = mk_pte(page, vma->vm_page_prot);
     c4d:	48 8b 53 48          	mov    0x48(%rbx),%rdx
     c51:	48 b8 00 00 00 00 00 	movabs $0x4000000000000,%rax
     c58:	00 04 00 
     c5b:	48 21 d0             	and    %rdx,%rax
     c5e:	48 89 c1             	mov    %rax,%rcx
     c61:	0f 84 29 02 00 00    	je     e90 <do_anonymous_page.isra.47+0x2e0>
}

static __always_inline int constant_test_bit(long nr, const volatile unsigned long *addr)
{
	return ((1UL << (nr & (BITS_PER_LONG-1))) &
		(addr[nr >> _BITOPS_LONG_SHIFT])) != 0;
     c67:	49 8b 06             	mov    (%r14),%rax
     c6a:	48 b9 00 00 00 00 00 	movabs $0x8000000000000,%rcx
     c71:	00 08 00 
     c74:	a9 00 00 00 02       	test   $0x2000000,%eax
     c79:	b8 00 00 00 00       	mov    $0x0,%eax
     c7e:	48 0f 44 c8          	cmove  %rax,%rcx
     c82:	48 89 d0             	mov    %rdx,%rax
     c85:	48 83 c8 10          	or     $0x10,%rax
     c89:	48 09 c8             	or     %rcx,%rax
     c8c:	49 b8 00 00 00 00 00 	movabs $0x160000000000,%r8
     c93:	16 00 00 
static inline pgprotval_t massage_pgprot(pgprot_t pgprot)
{
	pgprotval_t protval = pgprot_val(pgprot);

	if (protval & _PAGE_PRESENT)
	  protval &= __supported_pte_mask;
     c96:	48 89 c7             	mov    %rax,%rdi
     c99:	48 23 3d 00 00 00 00 	and    0x0(%rip),%rdi        # ca0 <do_anonymous_page.isra.47+0xf0>
     ca0:	4b 8d 34 06          	lea    (%r14,%r8,1),%rsi
     ca4:	48 c1 fe 06          	sar    $0x6,%rsi
	return protval;
}

static inline pte_t pfn_pte(unsigned long page_nr, pgprot_t pgprot)
{
	return __pte(((phys_addr_t)page_nr << PAGE_SHIFT) |
     ca8:	48 c1 e6 0c          	shl    $0xc,%rsi
static inline pgprotval_t massage_pgprot(pgprot_t pgprot)
{
	pgprotval_t protval = pgprot_val(pgprot);

	if (protval & _PAGE_PRESENT)
	  protval &= __supported_pte_mask;
     cac:	a8 01                	test   $0x1,%al
     cae:	48 0f 44 f8          	cmove  %rax,%rdi
	return protval;
}

static inline pte_t pfn_pte(unsigned long page_nr, pgprot_t pgprot)
{
	return __pte(((phys_addr_t)page_nr << PAGE_SHIFT) |
     cb2:	48 09 f7             	or     %rsi,%rdi
	if (sizeof(pteval_t) > sizeof(long))
		ret = PVOP_CALLEE2(pteval_t,
				   pv_mmu_ops.make_pte,
				   val, (u64)val >> 32);
	else
		ret = PVOP_CALLEE1(pteval_t,
     cb5:	ff 14 25 00 00 00 00 	callq  *0x0

static inline pte_t pte_set_flags(pte_t pte, pteval_t set)
{
	pteval_t v = native_pte_val(pte);

	return native_make_pte(v | set);
     cbc:	48 89 c2             	mov    %rax,%rdx

	if (sizeof(pmdval_t) > sizeof(long))
		ret =  PVOP_CALLEE2(pmdval_t, pv_mmu_ops.pmd_val,
				    pmd.pmd, (u64)pmd.pmd >> 32);
	else
		ret =  PVOP_CALLEE1(pmdval_t, pv_mmu_ops.pmd_val,
     cbf:	49 8b 3f             	mov    (%r15),%rdi
     cc2:	48 81 ca 42 08 00 00 	or     $0x842,%rdx
     cc9:	f6 43 50 02          	testb  $0x2,0x50(%rbx)
     ccd:	48 0f 45 c2          	cmovne %rdx,%rax
     cd1:	48 89 45 c0          	mov    %rax,-0x40(%rbp)
     cd5:	ff 14 25 00 00 00 00 	callq  *0x0
     cdc:	48 b9 00 f0 ff ff ff 	movabs $0x3ffffffff000,%rcx
     ce3:	3f 00 00 
     ce6:	49 8b 3f             	mov    (%r15),%rdi
     ce9:	48 21 c8             	and    %rcx,%rax
     cec:	48 c1 e8 06          	shr    $0x6,%rax
     cf0:	4c 29 c0             	sub    %r8,%rax
     cf3:	48 8b 40 30          	mov    0x30(%rax),%rax
     cf7:	48 89 c6             	mov    %rax,%rsi
     cfa:	48 89 45 b8          	mov    %rax,-0x48(%rbp)
     cfe:	ff 14 25 00 00 00 00 	callq  *0x0
 * this function returns the index of the entry in the pte page which would
 * control the given virtual address
 */
static inline unsigned long pte_index(unsigned long address)
{
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
     d05:	4c 89 e2             	mov    %r12,%rdx
	return (unsigned long)native_pmd_val(pmd) == 0;
}

static inline unsigned long pmd_page_vaddr(pmd_t pmd)
{
	return (unsigned long)__va(pmd_val(pmd) & PTE_PFN_MASK);
     d08:	49 bf 00 00 00 00 00 	movabs $0xffff880000000000,%r15
     d0f:	88 ff ff 
     d12:	48 21 c8             	and    %rcx,%rax
 * this function returns the index of the entry in the pte page which would
 * control the given virtual address
 */
static inline unsigned long pte_index(unsigned long address)
{
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
     d15:	48 c1 ea 09          	shr    $0x9,%rdx
	raw_spin_lock_init(&(_lock)->rlock);		\
} while (0)

static inline void spin_lock(spinlock_t *lock)
{
	raw_spin_lock(&lock->rlock);
     d19:	48 89 f7             	mov    %rsi,%rdi
}

static inline pte_t *pte_offset_kernel(pmd_t *pmd, unsigned long address)
{
	return (pte_t *)pmd_page_vaddr(*pmd) + pte_index(address);
     d1c:	81 e2 f8 0f 00 00    	and    $0xff8,%edx
	return (unsigned long)native_pmd_val(pmd) == 0;
}

static inline unsigned long pmd_page_vaddr(pmd_t pmd)
{
	return (unsigned long)__va(pmd_val(pmd) & PTE_PFN_MASK);
     d22:	49 01 d7             	add    %rdx,%r15
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
}

static inline pte_t *pte_offset_kernel(pmd_t *pmd, unsigned long address)
{
	return (pte_t *)pmd_page_vaddr(*pmd) + pte_index(address);
     d25:	4c 01 f8             	add    %r15,%rax
     d28:	49 89 f7             	mov    %rsi,%r15
     d2b:	48 89 45 c8          	mov    %rax,-0x38(%rbp)
     d2f:	e8 00 00 00 00       	callq  d34 <do_anonymous_page.isra.47+0x184>
	if (vma->vm_flags & VM_WRITE)
	  entry = pte_mkwrite(pte_mkdirty(entry));

	page_table = pte_offset_map_lock(mm, pmd, address, &ptl);
	if (!pte_none(*page_table))
     d34:	48 8b 45 c8          	mov    -0x38(%rbp),%rax
     d38:	48 83 38 00          	cmpq   $0x0,(%rax)
     d3c:	0f 84 6e 01 00 00    	je     eb0 <do_anonymous_page.isra.47+0x300>
	}
*/
	pte_unmap_unlock(page_table, ptl);
	return 0;
release:
	mem_cgroup_uncharge_page(page);
     d42:	4c 89 f7             	mov    %r14,%rdi
     d45:	e8 00 00 00 00       	callq  d4a <do_anonymous_page.isra.47+0x19a>
	page_cache_release(page);
     d4a:	4c 89 f7             	mov    %r14,%rdi
     d4d:	e8 00 00 00 00       	callq  d52 <do_anonymous_page.isra.47+0x1a2>
	raw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \
} while (0)

static inline void spin_unlock(spinlock_t *lock)
{
	raw_spin_unlock(&lock->rlock);
     d52:	4c 89 ff             	mov    %r15,%rdi
	return handle_double_cache_pte_fault(mm, vma, address,
	page_table, pmd, ptl, entry,false);
	}
*/
	pte_unmap_unlock(page_table, ptl);
	return 0;
     d55:	45 31 ed             	xor    %r13d,%r13d
     d58:	e8 00 00 00 00       	callq  d5d <do_anonymous_page.isra.47+0x1ad>
	goto unlock;
oom_free_page:
	page_cache_release(page);
oom:
	return VM_FAULT_OOM;
}
     d5d:	48 83 c4 20          	add    $0x20,%rsp
     d61:	44 89 e8             	mov    %r13d,%eax
     d64:	5b                   	pop    %rbx
     d65:	41 5c                	pop    %r12
     d67:	41 5d                	pop    %r13
     d69:	41 5e                	pop    %r14
     d6b:	41 5f                	pop    %r15
     d6d:	5d                   	pop    %rbp
     d6e:	c3                   	retq   
     d6f:	90                   	nop
     d70:	48 8b 53 48          	mov    0x48(%rbx),%rdx
	return protval;
}

static inline pte_t pfn_pte(unsigned long page_nr, pgprot_t pgprot)
{
	return __pte(((phys_addr_t)page_nr << PAGE_SHIFT) |
     d74:	48 8b 05 00 00 00 00 	mov    0x0(%rip),%rax        # d7b <do_anonymous_page.isra.47+0x1cb>
static inline pgprotval_t massage_pgprot(pgprot_t pgprot)
{
	pgprotval_t protval = pgprot_val(pgprot);

	if (protval & _PAGE_PRESENT)
	  protval &= __supported_pte_mask;
     d7b:	48 89 d7             	mov    %rdx,%rdi
     d7e:	48 23 3d 00 00 00 00 	and    0x0(%rip),%rdi        # d85 <do_anonymous_page.isra.47+0x1d5>
	return protval;
}

static inline pte_t pfn_pte(unsigned long page_nr, pgprot_t pgprot)
{
	return __pte(((phys_addr_t)page_nr << PAGE_SHIFT) |
     d85:	48 c1 e0 0c          	shl    $0xc,%rax
static inline pgprotval_t massage_pgprot(pgprot_t pgprot)
{
	pgprotval_t protval = pgprot_val(pgprot);

	if (protval & _PAGE_PRESENT)
	  protval &= __supported_pte_mask;
     d89:	f6 c2 01             	test   $0x1,%dl
     d8c:	48 0f 44 fa          	cmove  %rdx,%rdi
	return protval;
}

static inline pte_t pfn_pte(unsigned long page_nr, pgprot_t pgprot)
{
	return __pte(((phys_addr_t)page_nr << PAGE_SHIFT) |
     d90:	48 09 c7             	or     %rax,%rdi
	if (sizeof(pteval_t) > sizeof(long))
		ret = PVOP_CALLEE2(pteval_t,
				   pv_mmu_ops.make_pte,
				   val, (u64)val >> 32);
	else
		ret = PVOP_CALLEE1(pteval_t,
     d93:	ff 14 25 00 00 00 00 	callq  *0x0
     d9a:	48 89 c3             	mov    %rax,%rbx

	if (sizeof(pmdval_t) > sizeof(long))
		ret =  PVOP_CALLEE2(pmdval_t, pv_mmu_ops.pmd_val,
				    pmd.pmd, (u64)pmd.pmd >> 32);
	else
		ret =  PVOP_CALLEE1(pmdval_t, pv_mmu_ops.pmd_val,
     d9d:	49 8b 3f             	mov    (%r15),%rdi
     da0:	ff 14 25 00 00 00 00 	callq  *0x0
     da7:	48 b9 00 f0 ff ff ff 	movabs $0x3ffffffff000,%rcx
     dae:	3f 00 00 
     db1:	48 ba 00 00 00 00 00 	movabs $0xffffea0000000000,%rdx
     db8:	ea ff ff 
     dbb:	49 8b 3f             	mov    (%r15),%rdi
     dbe:	48 21 c8             	and    %rcx,%rax
     dc1:	48 c1 e8 06          	shr    $0x6,%rax
     dc5:	48 8b 74 10 30       	mov    0x30(%rax,%rdx,1),%rsi
     dca:	ff 14 25 00 00 00 00 	callq  *0x0
 * this function returns the index of the entry in the pte page which would
 * control the given virtual address
 */
static inline unsigned long pte_index(unsigned long address)
{
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
     dd1:	4c 89 e2             	mov    %r12,%rdx
	return (unsigned long)native_pmd_val(pmd) == 0;
}

static inline unsigned long pmd_page_vaddr(pmd_t pmd)
{
	return (unsigned long)__va(pmd_val(pmd) & PTE_PFN_MASK);
     dd4:	49 bd 00 00 00 00 00 	movabs $0xffff880000000000,%r13
     ddb:	88 ff ff 
     dde:	48 21 c8             	and    %rcx,%rax
 * this function returns the index of the entry in the pte page which would
 * control the given virtual address
 */
static inline unsigned long pte_index(unsigned long address)
{
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
     de1:	48 c1 ea 09          	shr    $0x9,%rdx
	raw_spin_lock_init(&(_lock)->rlock);		\
} while (0)

static inline void spin_lock(spinlock_t *lock)
{
	raw_spin_lock(&lock->rlock);
     de5:	48 89 f7             	mov    %rsi,%rdi
     de8:	49 89 f7             	mov    %rsi,%r15
}

static inline pte_t *pte_offset_kernel(pmd_t *pmd, unsigned long address)
{
	return (pte_t *)pmd_page_vaddr(*pmd) + pte_index(address);
     deb:	81 e2 f8 0f 00 00    	and    $0xff8,%edx
	return (unsigned long)native_pmd_val(pmd) == 0;
}

static inline unsigned long pmd_page_vaddr(pmd_t pmd)
{
	return (unsigned long)__va(pmd_val(pmd) & PTE_PFN_MASK);
     df1:	49 01 d5             	add    %rdx,%r13
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
}

static inline pte_t *pte_offset_kernel(pmd_t *pmd, unsigned long address)
{
	return (pte_t *)pmd_page_vaddr(*pmd) + pte_index(address);
     df4:	49 01 c5             	add    %rax,%r13
     df7:	e8 00 00 00 00       	callq  dfc <do_anonymous_page.isra.47+0x24c>
	/* Use the zero-page for reads */
	if (!(flags & FAULT_FLAG_WRITE)) {
		entry = pte_mkspecial(pfn_pte(my_zero_pfn(address),
						vma->vm_page_prot));
		page_table = pte_offset_map_lock(mm, pmd, address, &ptl);
		if (!pte_none(*page_table))
     dfc:	49 83 7d 00 00       	cmpq   $0x0,0x0(%r13)
     e01:	0f 85 4b ff ff ff    	jne    d52 <do_anonymous_page.isra.47+0x1a2>

static inline pte_t pte_set_flags(pte_t pte, pteval_t set)
{
	pteval_t v = native_pte_val(pte);

	return native_make_pte(v | set);
     e07:	48 89 d9             	mov    %rbx,%rcx
{
	if (sizeof(pteval_t) > sizeof(long))
		/* 5 arg words */
		pv_mmu_ops.set_pte_at(mm, addr, ptep, pte);
	else
		PVOP_VCALL4(pv_mmu_ops.set_pte_at, mm, addr, ptep, pte.pte);
     e0a:	48 8b 7d d0          	mov    -0x30(%rbp),%rdi
     e0e:	4c 89 e6             	mov    %r12,%rsi
     e11:	80 cd 02             	or     $0x2,%ch
     e14:	4c 89 ea             	mov    %r13,%rdx
     e17:	ff 14 25 00 00 00 00 	callq  *0x0
     e1e:	e9 2f ff ff ff       	jmpq   d52 <do_anonymous_page.isra.47+0x1a2>
     e23:	0f 1f 44 00 00       	nopl   0x0(%rax,%rax,1)
 */
static inline int check_stack_guard_page(struct vm_area_struct *vma, unsigned long address)
{
	address &= PAGE_MASK;
	if ((vma->vm_flags & VM_GROWSDOWN) && address == vma->vm_start) {
		struct vm_area_struct *prev = vma->vm_prev;
     e28:	48 8b 43 18          	mov    0x18(%rbx),%rax
		 * Is there a mapping abutting this one below?
		 *
		 * That's only ok if it's the same stack mapping
		 * that has gotten split..
		 */
		if (prev && prev->vm_end == address)
     e2c:	48 85 c0             	test   %rax,%rax
     e2f:	74 06                	je     e37 <do_anonymous_page.isra.47+0x287>
     e31:	48 3b 70 08          	cmp    0x8(%rax),%rsi
     e35:	74 41                	je     e78 <do_anonymous_page.isra.47+0x2c8>
		  return prev->vm_flags & VM_GROWSDOWN ? 0 : -ENOMEM;

		expand_downwards(vma, address - PAGE_SIZE);
     e37:	48 81 ee 00 10 00 00 	sub    $0x1000,%rsi
     e3e:	48 89 df             	mov    %rbx,%rdi
     e41:	44 89 45 c8          	mov    %r8d,-0x38(%rbp)
     e45:	e8 00 00 00 00       	callq  e4a <do_anonymous_page.isra.47+0x29a>
     e4a:	44 8b 45 c8          	mov    -0x38(%rbp),%r8d
	/* Check if we need to add a guard page to the stack */
	if (check_stack_guard_page(vma, address) < 0)
	  return VM_FAULT_SIGBUS;

	/* Use the zero-page for reads */
	if (!(flags & FAULT_FLAG_WRITE)) {
     e4e:	41 83 e0 01          	and    $0x1,%r8d
     e52:	0f 84 18 ff ff ff    	je     d70 <do_anonymous_page.isra.47+0x1c0>
     e58:	e9 99 fd ff ff       	jmpq   bf6 <do_anonymous_page.isra.47+0x46>
     e5d:	0f 1f 00             	nopl   (%rax)
release:
	mem_cgroup_uncharge_page(page);
	page_cache_release(page);
	goto unlock;
oom_free_page:
	page_cache_release(page);
     e60:	4c 89 f7             	mov    %r14,%rdi
     e63:	e8 00 00 00 00       	callq  e68 <do_anonymous_page.isra.47+0x2b8>
oom:
	return VM_FAULT_OOM;
     e68:	41 bd 01 00 00 00    	mov    $0x1,%r13d
     e6e:	e9 ea fe ff ff       	jmpq   d5d <do_anonymous_page.isra.47+0x1ad>
     e73:	0f 1f 44 00 00       	nopl   0x0(%rax,%rax,1)
		 *
		 * That's only ok if it's the same stack mapping
		 * that has gotten split..
		 */
		if (prev && prev->vm_end == address)
		  return prev->vm_flags & VM_GROWSDOWN ? 0 : -ENOMEM;
     e78:	f6 40 51 01          	testb  $0x1,0x51(%rax)

	pte_unmap(page_table);

	/* Check if we need to add a guard page to the stack */
	if (check_stack_guard_page(vma, address) < 0)
	  return VM_FAULT_SIGBUS;
     e7c:	41 bd 02 00 00 00    	mov    $0x2,%r13d
		 *
		 * That's only ok if it's the same stack mapping
		 * that has gotten split..
		 */
		if (prev && prev->vm_end == address)
		  return prev->vm_flags & VM_GROWSDOWN ? 0 : -ENOMEM;
     e82:	0f 84 d5 fe ff ff    	je     d5d <do_anonymous_page.isra.47+0x1ad>
     e88:	e9 5f fd ff ff       	jmpq   bec <do_anonymous_page.isra.47+0x3c>
     e8d:	0f 1f 00             	nopl   (%rax)
     e90:	49 8b 36             	mov    (%r14),%rsi
	__SetPageUptodate(page);

	if (mem_cgroup_newpage_charge(page, mm, GFP_KERNEL))
	  goto oom_free_page;

	entry = mk_pte(page, vma->vm_page_prot);
     e93:	48 b8 00 00 00 00 00 	movabs $0x8000000000000,%rax
     e9a:	00 08 00 
     e9d:	f7 c6 00 00 00 02    	test   $0x2000000,%esi
     ea3:	48 0f 44 c1          	cmove  %rcx,%rax
     ea7:	48 09 d0             	or     %rdx,%rax
     eaa:	e9 dd fd ff ff       	jmpq   c8c <do_anonymous_page.isra.47+0xdc>
     eaf:	90                   	nop

	page_table = pte_offset_map_lock(mm, pmd, address, &ptl);
	if (!pte_none(*page_table))
	  goto release;

	inc_mm_counter_fast(mm, MM_ANONPAGES);
     eb0:	4c 8b 7d d0          	mov    -0x30(%rbp),%r15
     eb4:	ba 01 00 00 00       	mov    $0x1,%edx
     eb9:	be 01 00 00 00       	mov    $0x1,%esi
     ebe:	4c 89 ff             	mov    %r15,%rdi
     ec1:	e8 9a f2 ff ff       	callq  160 <add_mm_counter_fast>
     ec6:	4c 89 ff             	mov    %r15,%rdi
     ec9:	4c 89 e6             	mov    %r12,%rsi
     ecc:	48 8b 55 c8          	mov    -0x38(%rbp),%rdx
     ed0:	48 8b 4d c0          	mov    -0x40(%rbp),%rcx
     ed4:	ff 14 25 00 00 00 00 	callq  *0x0
	raw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \
} while (0)

static inline void spin_unlock(spinlock_t *lock)
{
	raw_spin_unlock(&lock->rlock);
     edb:	48 8b 7d b8          	mov    -0x48(%rbp),%rdi
     edf:	e8 00 00 00 00       	callq  ee4 <do_anonymous_page.isra.47+0x334>
	set_pte_at(mm, address, page_table, entry);

	/* No need to invalidate - it was non-present before */
	update_mmu_cache(vma, address, page_table);
	pte_unmap_unlock(page_table, ptl);
	page_add_new_anon_rmap(page, vma, address);
     ee4:	4c 89 e2             	mov    %r12,%rdx
     ee7:	48 89 de             	mov    %rbx,%rsi
     eea:	4c 89 f7             	mov    %r14,%rdi
     eed:	e8 00 00 00 00       	callq  ef2 <do_anonymous_page.isra.47+0x342>
     ef2:	e9 66 fe ff ff       	jmpq   d5d <do_anonymous_page.isra.47+0x1ad>
     ef7:	66 0f 1f 84 00 00 00 	nopw   0x0(%rax,%rax,1)
     efe:	00 00 

0000000000000f00 <__do_fault>:
 * We return with mmap_sem still held, but pte unmapped and unlocked.
 */
static int __do_fault(struct mm_struct *mm, struct vm_area_struct *vma,
			unsigned long address, pmd_t *pmd,
			pgoff_t pgoff, unsigned int flags, pte_t orig_pte)
{
     f00:	e8 00 00 00 00       	callq  f05 <__do_fault+0x5>
     f05:	55                   	push   %rbp

	/*
	 * If we do COW later, allocate page befor taking lock_page()
	 * on the file cache page. This will reduce lock holding time.
	 */
	if ((flags & FAULT_FLAG_WRITE) && !(vma->vm_flags & VM_SHARED)) {
     f06:	44 89 c8             	mov    %r9d,%eax
 * We return with mmap_sem still held, but pte unmapped and unlocked.
 */
static int __do_fault(struct mm_struct *mm, struct vm_area_struct *vma,
			unsigned long address, pmd_t *pmd,
			pgoff_t pgoff, unsigned int flags, pte_t orig_pte)
{
     f09:	48 89 e5             	mov    %rsp,%rbp
     f0c:	41 57                	push   %r15
     f0e:	49 89 d7             	mov    %rdx,%r15
     f11:	41 56                	push   %r14
     f13:	49 89 ce             	mov    %rcx,%r14
     f16:	41 55                	push   %r13
     f18:	4d 89 c5             	mov    %r8,%r13
     f1b:	41 54                	push   %r12
     f1d:	49 89 f4             	mov    %rsi,%r12
     f20:	53                   	push   %rbx
     f21:	44 89 cb             	mov    %r9d,%ebx
     f24:	48 83 ec 48          	sub    $0x48,%rsp

	/*
	 * If we do COW later, allocate page befor taking lock_page()
	 * on the file cache page. This will reduce lock holding time.
	 */
	if ((flags & FAULT_FLAG_WRITE) && !(vma->vm_flags & VM_SHARED)) {
     f28:	83 e0 01             	and    $0x1,%eax
 * We return with mmap_sem still held, but pte unmapped and unlocked.
 */
static int __do_fault(struct mm_struct *mm, struct vm_area_struct *vma,
			unsigned long address, pmd_t *pmd,
			pgoff_t pgoff, unsigned int flags, pte_t orig_pte)
{
     f2b:	48 89 7d a8          	mov    %rdi,-0x58(%rbp)

	/*
	 * If we do COW later, allocate page befor taking lock_page()
	 * on the file cache page. This will reduce lock holding time.
	 */
	if ((flags & FAULT_FLAG_WRITE) && !(vma->vm_flags & VM_SHARED)) {
     f2f:	89 45 b4             	mov    %eax,-0x4c(%rbp)
		if (mem_cgroup_newpage_charge(cow_page, mm, GFP_KERNEL)) {
			page_cache_release(cow_page);
			return VM_FAULT_OOM;
		}
	} else
	  cow_page = NULL;
     f32:	48 c7 45 90 00 00 00 	movq   $0x0,-0x70(%rbp)
     f39:	00 

	/*
	 * If we do COW later, allocate page befor taking lock_page()
	 * on the file cache page. This will reduce lock holding time.
	 */
	if ((flags & FAULT_FLAG_WRITE) && !(vma->vm_flags & VM_SHARED)) {
     f3a:	74 0a                	je     f46 <__do_fault+0x46>
     f3c:	f6 46 50 08          	testb  $0x8,0x50(%rsi)
     f40:	0f 84 ca 02 00 00    	je     1210 <__do_fault+0x310>
			return VM_FAULT_OOM;
		}
	} else
	  cow_page = NULL;

	vmf.virtual_address = (void __user *)(address & PAGE_MASK);
     f46:	4c 89 f8             	mov    %r15,%rax
	vmf.pgoff = pgoff;
     f49:	4c 89 6d c0          	mov    %r13,-0x40(%rbp)
	vmf.flags = flags;
     f4d:	89 5d b8             	mov    %ebx,-0x48(%rbp)
			return VM_FAULT_OOM;
		}
	} else
	  cow_page = NULL;

	vmf.virtual_address = (void __user *)(address & PAGE_MASK);
     f50:	48 25 00 f0 ff ff    	and    $0xfffffffffffff000,%rax
	vmf.pgoff = pgoff;
	vmf.flags = flags;
	vmf.page = NULL;
     f56:	48 c7 45 d0 00 00 00 	movq   $0x0,-0x30(%rbp)
     f5d:	00 

	ret = vma->vm_ops->fault(vma, &vmf);
     f5e:	48 8d 75 b8          	lea    -0x48(%rbp),%rsi
			return VM_FAULT_OOM;
		}
	} else
	  cow_page = NULL;

	vmf.virtual_address = (void __user *)(address & PAGE_MASK);
     f62:	48 89 45 c8          	mov    %rax,-0x38(%rbp)
	vmf.pgoff = pgoff;
	vmf.flags = flags;
	vmf.page = NULL;

	ret = vma->vm_ops->fault(vma, &vmf);
     f66:	49 8b 84 24 98 00 00 	mov    0x98(%r12),%rax
     f6d:	00 
     f6e:	4c 89 e7             	mov    %r12,%rdi
     f71:	ff 50 10             	callq  *0x10(%rax)
	if (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE |
     f74:	a9 33 0d 00 00       	test   $0xd33,%eax
	vmf.virtual_address = (void __user *)(address & PAGE_MASK);
	vmf.pgoff = pgoff;
	vmf.flags = flags;
	vmf.page = NULL;

	ret = vma->vm_ops->fault(vma, &vmf);
     f79:	41 89 c5             	mov    %eax,%r13d
	if (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE |
     f7c:	0f 85 0e 04 00 00    	jne    1390 <__do_fault+0x490>
						VM_FAULT_RETRY)))
	  goto uncharge_out;

	if (unlikely(PageHWPoison(vmf.page))) {
     f82:	48 8b 7d d0          	mov    -0x30(%rbp),%rdi
     f86:	48 8b 07             	mov    (%rdi),%rax
     f89:	a9 00 00 80 00       	test   $0x800000,%eax
     f8e:	0f 85 e5 03 00 00    	jne    1379 <__do_fault+0x479>

	/*
	 * For consistency in subsequent calls, make the faulted page always
	 * locked.
	 */
	if (unlikely(!(ret & VM_FAULT_LOCKED)))
     f94:	41 f7 c5 00 02 00 00 	test   $0x200,%r13d
     f9b:	0f 84 33 04 00 00    	je     13d4 <__do_fault+0x4d4>
     fa1:	48 89 fb             	mov    %rdi,%rbx
     fa4:	48 8b 03             	mov    (%rbx),%rax
	  VM_BUG_ON(!PageLocked(vmf.page));

	page = vmf.page;

	/* Mark the page as used on fault. */
	if (PageReadaheadUnused(page))
     fa7:	a9 00 00 00 01       	test   $0x1000000,%eax
     fac:	0f 85 d6 01 00 00    	jne    1188 <__do_fault+0x288>
	  ClearPageReadaheadUnused(page);

	/*
	 * Should we do an early C-O-W break?
	 */
	if (flags & FAULT_FLAG_WRITE) {
     fb2:	8b 7d b4             	mov    -0x4c(%rbp),%edi
     fb5:	85 ff                	test   %edi,%edi
     fb7:	0f 84 db 01 00 00    	je     1198 <__do_fault+0x298>
		if (!(vma->vm_flags & VM_SHARED)) {
     fbd:	41 f6 44 24 50 08    	testb  $0x8,0x50(%r12)
     fc3:	0f 84 a7 02 00 00    	je     1270 <__do_fault+0x370>
			/*
			 * If the page will be shareable, see if the backing
			 * address space wants to know that the page is about
			 * to become writable
			 */
			if (vma->vm_ops->page_mkwrite) {
     fc9:	49 8b 84 24 98 00 00 	mov    0x98(%r12),%rax
     fd0:	00 
     fd1:	48 83 78 18 00       	cmpq   $0x0,0x18(%rax)
     fd6:	0f 84 bc 01 00 00    	je     1198 <__do_fault+0x298>
				int tmp;

				unlock_page(page);
     fdc:	48 89 df             	mov    %rbx,%rdi
     fdf:	e8 00 00 00 00       	callq  fe4 <__do_fault+0xe4>
				vmf.flags = FAULT_FLAG_WRITE|FAULT_FLAG_MKWRITE;
				tmp = vma->vm_ops->page_mkwrite(vma, &vmf);
     fe4:	49 8b 84 24 98 00 00 	mov    0x98(%r12),%rax
     feb:	00 
			 */
			if (vma->vm_ops->page_mkwrite) {
				int tmp;

				unlock_page(page);
				vmf.flags = FAULT_FLAG_WRITE|FAULT_FLAG_MKWRITE;
     fec:	c7 45 b8 05 00 00 00 	movl   $0x5,-0x48(%rbp)
				tmp = vma->vm_ops->page_mkwrite(vma, &vmf);
     ff3:	48 8d 75 b8          	lea    -0x48(%rbp),%rsi
     ff7:	4c 89 e7             	mov    %r12,%rdi
     ffa:	ff 50 18             	callq  *0x18(%rax)
				if (unlikely(tmp &
     ffd:	a9 33 09 00 00       	test   $0x933,%eax
    1002:	0f 85 35 04 00 00    	jne    143d <__do_fault+0x53d>
								(VM_FAULT_ERROR | VM_FAULT_NOPAGE))) {
					ret = tmp;
					goto unwritable_page;
				}
				if (unlikely(!(tmp & VM_FAULT_LOCKED))) {
    1008:	f6 c4 02             	test   $0x2,%ah
    100b:	0f 84 01 04 00 00    	je     1412 <__do_fault+0x512>
						unlock_page(page);
						goto unwritable_page;
					}
				} else
				  VM_BUG_ON(!PageLocked(page));
				page_mkwrite = 1;
    1011:	c7 45 b0 01 00 00 00 	movl   $0x1,-0x50(%rbp)
	pte_t *page_table;
	spinlock_t *ptl;
	struct page *page;
	struct page *cow_page;
	pte_t entry;
	int anon = 0;
    1018:	c7 45 a4 00 00 00 00 	movl   $0x0,-0x5c(%rbp)

	if (sizeof(pmdval_t) > sizeof(long))
		ret =  PVOP_CALLEE2(pmdval_t, pv_mmu_ops.pmd_val,
				    pmd.pmd, (u64)pmd.pmd >> 32);
	else
		ret =  PVOP_CALLEE1(pmdval_t, pv_mmu_ops.pmd_val,
    101f:	49 8b 3e             	mov    (%r14),%rdi
    1022:	ff 14 25 00 00 00 00 	callq  *0x0
    1029:	48 b9 00 f0 ff ff ff 	movabs $0x3ffffffff000,%rcx
    1030:	3f 00 00 
    1033:	48 ba 00 00 00 00 00 	movabs $0xffffea0000000000,%rdx
    103a:	ea ff ff 
    103d:	49 8b 3e             	mov    (%r14),%rdi
    1040:	48 21 c8             	and    %rcx,%rax
    1043:	48 c1 e8 06          	shr    $0x6,%rax
    1047:	48 8b 44 10 30       	mov    0x30(%rax,%rdx,1),%rax
    104c:	48 89 c6             	mov    %rax,%rsi
    104f:	48 89 45 98          	mov    %rax,-0x68(%rbp)
    1053:	ff 14 25 00 00 00 00 	callq  *0x0
 * this function returns the index of the entry in the pte page which would
 * control the given virtual address
 */
static inline unsigned long pte_index(unsigned long address)
{
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
    105a:	4c 89 fa             	mov    %r15,%rdx
	return (unsigned long)native_pmd_val(pmd) == 0;
}

static inline unsigned long pmd_page_vaddr(pmd_t pmd)
{
	return (unsigned long)__va(pmd_val(pmd) & PTE_PFN_MASK);
    105d:	49 be 00 00 00 00 00 	movabs $0xffff880000000000,%r14
    1064:	88 ff ff 
    1067:	48 21 c8             	and    %rcx,%rax
 * this function returns the index of the entry in the pte page which would
 * control the given virtual address
 */
static inline unsigned long pte_index(unsigned long address)
{
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
    106a:	48 c1 ea 09          	shr    $0x9,%rdx
	raw_spin_lock_init(&(_lock)->rlock);		\
} while (0)

static inline void spin_lock(spinlock_t *lock)
{
	raw_spin_lock(&lock->rlock);
    106e:	48 89 f7             	mov    %rsi,%rdi
}

static inline pte_t *pte_offset_kernel(pmd_t *pmd, unsigned long address)
{
	return (pte_t *)pmd_page_vaddr(*pmd) + pte_index(address);
    1071:	81 e2 f8 0f 00 00    	and    $0xff8,%edx
	return (unsigned long)native_pmd_val(pmd) == 0;
}

static inline unsigned long pmd_page_vaddr(pmd_t pmd)
{
	return (unsigned long)__va(pmd_val(pmd) & PTE_PFN_MASK);
    1077:	49 01 d6             	add    %rdx,%r14
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
}

static inline pte_t *pte_offset_kernel(pmd_t *pmd, unsigned long address)
{
	return (pte_t *)pmd_page_vaddr(*pmd) + pte_index(address);
    107a:	49 01 c6             	add    %rax,%r14
    107d:	e8 00 00 00 00       	callq  1082 <__do_fault+0x182>
	 * an exclusive copy of the page, or this is a shared mapping,
	 * so we can make it writable and dirty to avoid having to
	 * handle that later.
	 */
	/* Only go through if we didn't race with anybody else... */
	if (likely(pte_same(*page_table, orig_pte))) {
    1082:	49 8b 06             	mov    (%r14),%rax
    1085:	48 39 45 10          	cmp    %rax,0x10(%rbp)
    1089:	0f 85 23 03 00 00    	jne    13b2 <__do_fault+0x4b2>
		flush_icache_page(vma, page);
		entry = mk_pte(page, vma->vm_page_prot);
    108f:	49 8b 54 24 48       	mov    0x48(%r12),%rdx
    1094:	48 b8 00 00 00 00 00 	movabs $0x4000000000000,%rax
    109b:	00 04 00 
    109e:	48 21 d0             	and    %rdx,%rax
    10a1:	48 89 c1             	mov    %rax,%rcx
    10a4:	0f 84 06 01 00 00    	je     11b0 <__do_fault+0x2b0>
    10aa:	48 8b 03             	mov    (%rbx),%rax
    10ad:	48 b9 00 00 00 00 00 	movabs $0x8000000000000,%rcx
    10b4:	00 08 00 
    10b7:	a9 00 00 00 02       	test   $0x2000000,%eax
    10bc:	b8 00 00 00 00       	mov    $0x0,%eax
    10c1:	48 0f 44 c8          	cmove  %rax,%rcx
    10c5:	48 89 d0             	mov    %rdx,%rax
    10c8:	48 83 c8 10          	or     $0x10,%rax
    10cc:	48 09 c8             	or     %rcx,%rax
static inline pgprotval_t massage_pgprot(pgprot_t pgprot)
{
	pgprotval_t protval = pgprot_val(pgprot);

	if (protval & _PAGE_PRESENT)
	  protval &= __supported_pte_mask;
    10cf:	48 89 c7             	mov    %rax,%rdi
    10d2:	48 ba 00 00 00 00 00 	movabs $0x160000000000,%rdx
    10d9:	16 00 00 
    10dc:	48 23 3d 00 00 00 00 	and    0x0(%rip),%rdi        # 10e3 <__do_fault+0x1e3>
    10e3:	48 01 da             	add    %rbx,%rdx
    10e6:	48 c1 fa 06          	sar    $0x6,%rdx
	return protval;
}

static inline pte_t pfn_pte(unsigned long page_nr, pgprot_t pgprot)
{
	return __pte(((phys_addr_t)page_nr << PAGE_SHIFT) |
    10ea:	48 c1 e2 0c          	shl    $0xc,%rdx
static inline pgprotval_t massage_pgprot(pgprot_t pgprot)
{
	pgprotval_t protval = pgprot_val(pgprot);

	if (protval & _PAGE_PRESENT)
	  protval &= __supported_pte_mask;
    10ee:	a8 01                	test   $0x1,%al
    10f0:	48 0f 44 f8          	cmove  %rax,%rdi
	return protval;
}

static inline pte_t pfn_pte(unsigned long page_nr, pgprot_t pgprot)
{
	return __pte(((phys_addr_t)page_nr << PAGE_SHIFT) |
    10f4:	48 09 d7             	or     %rdx,%rdi
	if (sizeof(pteval_t) > sizeof(long))
		ret = PVOP_CALLEE2(pteval_t,
				   pv_mmu_ops.make_pte,
				   val, (u64)val >> 32);
	else
		ret = PVOP_CALLEE1(pteval_t,
    10f7:	ff 14 25 00 00 00 00 	callq  *0x0
		if (flags & FAULT_FLAG_WRITE)
    10fe:	8b 75 b4             	mov    -0x4c(%rbp),%esi
    1101:	48 89 c1             	mov    %rax,%rcx
    1104:	85 f6                	test   %esi,%esi
    1106:	74 13                	je     111b <__do_fault+0x21b>
 * pte_mkwrite.  But get_user_pages can cause write faults for mappings
 * that do not have writing enabled, when used by access_process_vm.
 */
static inline pte_t maybe_mkwrite(pte_t pte, struct vm_area_struct *vma)
{
	if (likely(vma->vm_flags & VM_WRITE))
    1108:	41 f6 44 24 50 02    	testb  $0x2,0x50(%r12)
    110e:	0f 84 d1 02 00 00    	je     13e5 <__do_fault+0x4e5>

static inline pte_t pte_set_flags(pte_t pte, pteval_t set)
{
	pteval_t v = native_pte_val(pte);

	return native_make_pte(v | set);
    1114:	48 81 c9 42 08 00 00 	or     $0x842,%rcx
{
	if (sizeof(pteval_t) > sizeof(long))
		/* 5 arg words */
		pv_mmu_ops.set_pte_at(mm, addr, ptep, pte);
	else
		PVOP_VCALL4(pv_mmu_ops.set_pte_at, mm, addr, ptep, pte.pte);
    111b:	48 8b 7d a8          	mov    -0x58(%rbp),%rdi
    111f:	4c 89 fe             	mov    %r15,%rsi
    1122:	4c 89 f2             	mov    %r14,%rdx
    1125:	ff 14 25 00 00 00 00 	callq  *0x0
	raw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \
} while (0)

static inline void spin_unlock(spinlock_t *lock)
{
	raw_spin_unlock(&lock->rlock);
    112c:	48 8b 7d 98          	mov    -0x68(%rbp),%rdi
    1130:	e8 00 00 00 00       	callq  1135 <__do_fault+0x235>

		/* no need to invalidate: a not-present page won't be cached */
		update_mmu_cache(vma, address, page_table);
		pte_unmap_unlock(page_table, ptl);

		if (anon) {
    1135:	8b 4d a4             	mov    -0x5c(%rbp),%ecx
			inc_mm_counter_fast(mm, MM_ANONPAGES);
    1138:	ba 01 00 00 00       	mov    $0x1,%edx

		/* no need to invalidate: a not-present page won't be cached */
		update_mmu_cache(vma, address, page_table);
		pte_unmap_unlock(page_table, ptl);

		if (anon) {
    113d:	85 c9                	test   %ecx,%ecx
    113f:	0f 84 8b 00 00 00    	je     11d0 <__do_fault+0x2d0>
			inc_mm_counter_fast(mm, MM_ANONPAGES);
    1145:	48 8b 7d a8          	mov    -0x58(%rbp),%rdi
    1149:	be 01 00 00 00       	mov    $0x1,%esi
    114e:	e8 0d f0 ff ff       	callq  160 <add_mm_counter_fast>
			page_add_new_anon_rmap(page, vma, address);
    1153:	4c 89 fa             	mov    %r15,%rdx
    1156:	4c 89 e6             	mov    %r12,%rsi
    1159:	48 89 df             	mov    %rbx,%rdi
    115c:	e8 00 00 00 00       	callq  1161 <__do_fault+0x261>

		/* file_update_time outside page_lock */
		if (vma->vm_file && !page_mkwrite)
		  vma_file_update_time(vma);
	} else {
		unlock_page(vmf.page);
    1161:	48 8b 7d d0          	mov    -0x30(%rbp),%rdi
    1165:	e8 00 00 00 00       	callq  116a <__do_fault+0x26a>
		if (anon)
		  page_cache_release(vmf.page);
    116a:	48 8b 7d d0          	mov    -0x30(%rbp),%rdi
    116e:	e8 00 00 00 00       	callq  1173 <__do_fault+0x273>
	if (cow_page) {
		mem_cgroup_uncharge_page(cow_page);
		page_cache_release(cow_page);
	}
	return ret;
}
    1173:	48 83 c4 48          	add    $0x48,%rsp
    1177:	44 89 e8             	mov    %r13d,%eax
    117a:	5b                   	pop    %rbx
    117b:	41 5c                	pop    %r12
    117d:	41 5d                	pop    %r13
    117f:	41 5e                	pop    %r14
    1181:	41 5f                	pop    %r15
    1183:	5d                   	pop    %rbp
    1184:	c3                   	retq   
    1185:	0f 1f 00             	nopl   (%rax)
 */
static __always_inline void
clear_bit(long nr, volatile unsigned long *addr)
{
	if (IS_IMMEDIATE(nr)) {
		asm volatile(LOCK_PREFIX "andb %1,%0"
    1188:	f0 80 63 03 fe       	lock andb $0xfe,0x3(%rbx)
	  ClearPageReadaheadUnused(page);

	/*
	 * Should we do an early C-O-W break?
	 */
	if (flags & FAULT_FLAG_WRITE) {
    118d:	8b 7d b4             	mov    -0x4c(%rbp),%edi
    1190:	85 ff                	test   %edi,%edi
    1192:	0f 85 25 fe ff ff    	jne    fbd <__do_fault+0xbd>
	pte_t entry;
	int anon = 0;
	struct page *dirty_page = NULL;
	struct vm_fault vmf;
	int ret;
	int page_mkwrite = 0;
    1198:	c7 45 b0 00 00 00 00 	movl   $0x0,-0x50(%rbp)
	pte_t *page_table;
	spinlock_t *ptl;
	struct page *page;
	struct page *cow_page;
	pte_t entry;
	int anon = 0;
    119f:	c7 45 a4 00 00 00 00 	movl   $0x0,-0x5c(%rbp)
    11a6:	e9 74 fe ff ff       	jmpq   101f <__do_fault+0x11f>
    11ab:	0f 1f 44 00 00       	nopl   0x0(%rax,%rax,1)
}

static __always_inline int constant_test_bit(long nr, const volatile unsigned long *addr)
{
	return ((1UL << (nr & (BITS_PER_LONG-1))) &
		(addr[nr >> _BITOPS_LONG_SHIFT])) != 0;
    11b0:	48 8b 33             	mov    (%rbx),%rsi
	 * handle that later.
	 */
	/* Only go through if we didn't race with anybody else... */
	if (likely(pte_same(*page_table, orig_pte))) {
		flush_icache_page(vma, page);
		entry = mk_pte(page, vma->vm_page_prot);
    11b3:	48 b8 00 00 00 00 00 	movabs $0x8000000000000,%rax
    11ba:	00 08 00 
    11bd:	f7 c6 00 00 00 02    	test   $0x2000000,%esi
    11c3:	48 0f 44 c1          	cmove  %rcx,%rax
    11c7:	48 09 d0             	or     %rdx,%rax
    11ca:	e9 00 ff ff ff       	jmpq   10cf <__do_fault+0x1cf>
    11cf:	90                   	nop

		if (anon) {
			inc_mm_counter_fast(mm, MM_ANONPAGES);
			page_add_new_anon_rmap(page, vma, address);
		} else {
			inc_mm_counter_fast(mm, MM_FILEPAGES);
    11d0:	48 8b 7d a8          	mov    -0x58(%rbp),%rdi
    11d4:	31 f6                	xor    %esi,%esi
    11d6:	e8 85 ef ff ff       	callq  160 <add_mm_counter_fast>
			page_add_file_rmap(page);
    11db:	48 89 df             	mov    %rbx,%rdi
    11de:	e8 00 00 00 00       	callq  11e3 <__do_fault+0x2e3>
			inc_page_counter_in_ns(page,vma);
    11e3:	4c 89 e6             	mov    %r12,%rsi
    11e6:	48 89 df             	mov    %rbx,%rdi
    11e9:	e8 00 00 00 00       	callq  11ee <__do_fault+0x2ee>
			if (flags & FAULT_FLAG_WRITE) {
    11ee:	8b 55 b4             	mov    -0x4c(%rbp),%edx
    11f1:	85 d2                	test   %edx,%edx
    11f3:	0f 85 ef 00 00 00    	jne    12e8 <__do_fault+0x3e8>

		/* file_update_time outside page_lock */
		if (vma->vm_file && !page_mkwrite)
		  vma_file_update_time(vma);
	} else {
		unlock_page(vmf.page);
    11f9:	48 8b 7d d0          	mov    -0x30(%rbp),%rdi
    11fd:	e8 00 00 00 00       	callq  1202 <__do_fault+0x302>
    1202:	e9 6c ff ff ff       	jmpq   1173 <__do_fault+0x273>
    1207:	66 0f 1f 84 00 00 00 	nopw   0x0(%rax,%rax,1)
    120e:	00 00 
	 * If we do COW later, allocate page befor taking lock_page()
	 * on the file cache page. This will reduce lock holding time.
	 */
	if ((flags & FAULT_FLAG_WRITE) && !(vma->vm_flags & VM_SHARED)) {

		if (unlikely(anon_vma_prepare(vma)))
    1210:	48 89 f7             	mov    %rsi,%rdi
    1213:	e8 00 00 00 00       	callq  1218 <__do_fault+0x318>
    1218:	85 c0                	test   %eax,%eax
    121a:	75 46                	jne    1262 <__do_fault+0x362>
		  return VM_FAULT_OOM;

		cow_page = alloc_page_vma(GFP_HIGHUSER_MOVABLE, vma, address);
    121c:	31 f6                	xor    %esi,%esi
    121e:	4c 89 f9             	mov    %r15,%rcx
    1221:	4c 89 e2             	mov    %r12,%rdx
    1224:	bf da 00 02 00       	mov    $0x200da,%edi
    1229:	65 44 8b 04 25 00 00 	mov    %gs:0x0,%r8d
    1230:	00 00 
    1232:	e8 00 00 00 00       	callq  1237 <__do_fault+0x337>
		if (!cow_page)
    1237:	48 85 c0             	test   %rax,%rax
	if ((flags & FAULT_FLAG_WRITE) && !(vma->vm_flags & VM_SHARED)) {

		if (unlikely(anon_vma_prepare(vma)))
		  return VM_FAULT_OOM;

		cow_page = alloc_page_vma(GFP_HIGHUSER_MOVABLE, vma, address);
    123a:	48 89 45 90          	mov    %rax,-0x70(%rbp)
		if (!cow_page)
    123e:	74 22                	je     1262 <__do_fault+0x362>
		  return VM_FAULT_OOM;

		if (mem_cgroup_newpage_charge(cow_page, mm, GFP_KERNEL)) {
    1240:	48 8b 75 a8          	mov    -0x58(%rbp),%rsi
    1244:	ba d0 00 00 00       	mov    $0xd0,%edx
    1249:	48 89 c7             	mov    %rax,%rdi
    124c:	e8 00 00 00 00       	callq  1251 <__do_fault+0x351>
    1251:	85 c0                	test   %eax,%eax
    1253:	0f 84 ed fc ff ff    	je     f46 <__do_fault+0x46>
			page_cache_release(cow_page);
    1259:	48 8b 7d 90          	mov    -0x70(%rbp),%rdi
    125d:	e8 00 00 00 00       	callq  1262 <__do_fault+0x362>
			return VM_FAULT_OOM;
    1262:	41 bd 01 00 00 00    	mov    $0x1,%r13d
    1268:	e9 06 ff ff ff       	jmpq   1173 <__do_fault+0x273>
    126d:	0f 1f 00             	nopl   (%rax)
	 */
	if (flags & FAULT_FLAG_WRITE) {
		if (!(vma->vm_flags & VM_SHARED)) {
			page = cow_page;
			anon = 1;
			copy_user_highpage(page, vmf.page, address, vma);
    1270:	48 8b 75 d0          	mov    -0x30(%rbp),%rsi
 * The various preempt_count add/sub methods
 */

static __always_inline void __preempt_count_add(int val)
{
	__this_cpu_add_4(__preempt_count, val);
    1274:	65 ff 04 25 00 00 00 	incl   %gs:0x0
    127b:	00 
    127c:	65 ff 04 25 00 00 00 	incl   %gs:0x0
    1283:	00 
 */
#include <linux/vmstat.h>

static __always_inline void *lowmem_page_address(const struct page *page)
{
	return __va(PFN_PHYS(page_to_pfn(page)));
    1284:	48 8b 5d 90          	mov    -0x70(%rbp),%rbx
    1288:	48 b8 00 00 00 00 00 	movabs $0x160000000000,%rax
    128f:	16 00 00 
    1292:	48 bf 00 00 00 00 00 	movabs $0xffff880000000000,%rdi
    1299:	88 ff ff 
    129c:	48 01 c6             	add    %rax,%rsi
    129f:	48 c1 fe 06          	sar    $0x6,%rsi
    12a3:	48 01 d8             	add    %rbx,%rax
    12a6:	48 c1 e6 0c          	shl    $0xc,%rsi
    12aa:	48 c1 f8 06          	sar    $0x6,%rax
    12ae:	48 01 fe             	add    %rdi,%rsi
    12b1:	48 c1 e0 0c          	shl    $0xc,%rax
    12b5:	48 01 c7             	add    %rax,%rdi
}

static inline void copy_user_page(void *to, void *from, unsigned long vaddr,
				  struct page *topage)
{
	copy_page(to, from);
    12b8:	e8 00 00 00 00       	callq  12bd <__do_fault+0x3bd>
}

static __always_inline void __preempt_count_sub(int val)
{
	__this_cpu_add_4(__preempt_count, -val);
    12bd:	65 ff 0c 25 00 00 00 	decl   %gs:0x0
    12c4:	00 
    12c5:	65 ff 0c 25 00 00 00 	decl   %gs:0x0
    12cc:	00 
 * If it's called on the same region of memory simultaneously, the effect
 * may be that only one operation succeeds.
 */
static inline void __set_bit(long nr, volatile unsigned long *addr)
{
	asm volatile("bts %1,%0" : ADDR : "Ir" (nr) : "memory");
    12cd:	0f ba 2b 03          	btsl   $0x3,(%rbx)
	pte_t entry;
	int anon = 0;
	struct page *dirty_page = NULL;
	struct vm_fault vmf;
	int ret;
	int page_mkwrite = 0;
    12d1:	c7 45 b0 00 00 00 00 	movl   $0x0,-0x50(%rbp)
	 * Should we do an early C-O-W break?
	 */
	if (flags & FAULT_FLAG_WRITE) {
		if (!(vma->vm_flags & VM_SHARED)) {
			page = cow_page;
			anon = 1;
    12d8:	c7 45 a4 01 00 00 00 	movl   $0x1,-0x5c(%rbp)
    12df:	e9 3b fd ff ff       	jmpq   101f <__do_fault+0x11f>
    12e4:	0f 1f 40 00          	nopl   0x0(%rax)
}

static __always_inline int constant_test_bit(long nr, const volatile unsigned long *addr)
{
	return ((1UL << (nr & (BITS_PER_LONG-1))) &
		(addr[nr >> _BITOPS_LONG_SHIFT])) != 0;
    12e8:	48 8b 03             	mov    (%rbx),%rax
void get_huge_page_tail(struct page *page);/*ziqiao*/
extern bool __get_page_tail(struct page *page);

static inline void get_page(struct page *page)
{
	if (unlikely(PageTail(page)))
    12eb:	f6 c4 80             	test   $0x80,%ah
    12ee:	0f 85 fd 00 00 00    	jne    13f1 <__do_fault+0x4f1>
    12f4:	f0 ff 43 1c          	lock incl 0x1c(%rbx)

	}

	//pte_unmap_unlock(page_table, ptl);

	if (dirty_page) {
    12f8:	48 85 db             	test   %rbx,%rbx
    12fb:	0f 84 f8 fe ff ff    	je     11f9 <__do_fault+0x2f9>
		struct address_space *mapping = page->mapping;
		int dirtied = 0;

		if (set_page_dirty(dirty_page))
    1301:	48 89 df             	mov    %rbx,%rdi
	}

	//pte_unmap_unlock(page_table, ptl);

	if (dirty_page) {
		struct address_space *mapping = page->mapping;
    1304:	4c 8b 73 08          	mov    0x8(%rbx),%r14
		int dirtied = 0;

		if (set_page_dirty(dirty_page))
    1308:	e8 00 00 00 00       	callq  130d <__do_fault+0x40d>
		  dirtied = 1;
		unlock_page(dirty_page);
    130d:	48 89 df             	mov    %rbx,%rdi

	if (dirty_page) {
		struct address_space *mapping = page->mapping;
		int dirtied = 0;

		if (set_page_dirty(dirty_page))
    1310:	41 89 c7             	mov    %eax,%r15d
		  dirtied = 1;
		unlock_page(dirty_page);
    1313:	e8 00 00 00 00       	callq  1318 <__do_fault+0x418>
		put_page(dirty_page);
    1318:	48 89 df             	mov    %rbx,%rdi
    131b:	e8 00 00 00 00       	callq  1320 <__do_fault+0x420>

	if (dirty_page) {
		struct address_space *mapping = page->mapping;
		int dirtied = 0;

		if (set_page_dirty(dirty_page))
    1320:	31 c0                	xor    %eax,%eax
    1322:	45 85 ff             	test   %r15d,%r15d
    1325:	0f 95 c0             	setne  %al
		  dirtied = 1;
		unlock_page(dirty_page);
		put_page(dirty_page);
		if ((dirtied || page_mkwrite) && mapping) {
    1328:	0b 45 b0             	or     -0x50(%rbp),%eax
    132b:	74 0d                	je     133a <__do_fault+0x43a>
    132d:	4d 85 f6             	test   %r14,%r14
    1330:	74 08                	je     133a <__do_fault+0x43a>
			/*
			 * Some device drivers do not set page.mapping but still
			 * dirty their pages
			 */
			balance_dirty_pages_ratelimited(mapping);
    1332:	4c 89 f7             	mov    %r14,%rdi
    1335:	e8 00 00 00 00       	callq  133a <__do_fault+0x43a>
		}

		/* file_update_time outside page_lock */
		if (vma->vm_file && !page_mkwrite)
    133a:	49 8b bc 24 a8 00 00 	mov    0xa8(%r12),%rdi
    1341:	00 
    1342:	48 85 ff             	test   %rdi,%rdi
    1345:	0f 84 28 fe ff ff    	je     1173 <__do_fault+0x273>
    134b:	8b 45 b0             	mov    -0x50(%rbp),%eax
    134e:	85 c0                	test   %eax,%eax
    1350:	0f 85 1d fe ff ff    	jne    1173 <__do_fault+0x273>
 *
 * We enter with non-exclusive mmap_sem (to exclude vma changes,
 * but allow concurrent faults), and pte neither mapped nor locked.
 * We return with mmap_sem still held, but pte unmapped and unlocked.
 */
static int __do_fault(struct mm_struct *mm, struct vm_area_struct *vma,
    1356:	49 8b 9c 24 b0 00 00 	mov    0xb0(%r12),%rbx
    135d:	00 
static inline void vma_do_file_update_time(struct vm_area_struct *vma,
					   const char func[], int line)
{
	struct file *f = vma->vm_file, *pr = vma->vm_prfile;
	aufs_trace(f, pr, func, line, __func__);
	file_update_time(f);
    135e:	e8 00 00 00 00       	callq  1363 <__do_fault+0x463>
	if (f && pr)
    1363:	48 85 db             	test   %rbx,%rbx
    1366:	0f 84 07 fe ff ff    	je     1173 <__do_fault+0x273>
		file_update_time(pr);
    136c:	48 89 df             	mov    %rbx,%rdi
    136f:	e8 00 00 00 00       	callq  1374 <__do_fault+0x474>
    1374:	e9 fa fd ff ff       	jmpq   1173 <__do_fault+0x273>
	if (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE |
						VM_FAULT_RETRY)))
	  goto uncharge_out;

	if (unlikely(PageHWPoison(vmf.page))) {
		if (ret & VM_FAULT_LOCKED)
    1379:	44 89 e8             	mov    %r13d,%eax
		  unlock_page(vmf.page);
		ret = VM_FAULT_HWPOISON;
    137c:	41 bd 10 00 00 00    	mov    $0x10,%r13d
	if (unlikely(ret & (VM_FAULT_ERROR | VM_FAULT_NOPAGE |
						VM_FAULT_RETRY)))
	  goto uncharge_out;

	if (unlikely(PageHWPoison(vmf.page))) {
		if (ret & VM_FAULT_LOCKED)
    1382:	25 00 02 00 00       	and    $0x200,%eax
    1387:	85 c0                	test   %eax,%eax
    1389:	74 05                	je     1390 <__do_fault+0x490>
		  unlock_page(vmf.page);
    138b:	e8 00 00 00 00       	callq  1390 <__do_fault+0x490>
unwritable_page:
	page_cache_release(page);
	return ret;
uncharge_out:
	/* fs's fault handler get error */
	if (cow_page) {
    1390:	48 8b 5d 90          	mov    -0x70(%rbp),%rbx
    1394:	48 85 db             	test   %rbx,%rbx
    1397:	0f 84 d6 fd ff ff    	je     1173 <__do_fault+0x273>
		mem_cgroup_uncharge_page(cow_page);
    139d:	48 89 df             	mov    %rbx,%rdi
    13a0:	e8 00 00 00 00       	callq  13a5 <__do_fault+0x4a5>
		page_cache_release(cow_page);
    13a5:	48 89 df             	mov    %rbx,%rdi
    13a8:	e8 00 00 00 00       	callq  13ad <__do_fault+0x4ad>
    13ad:	e9 c1 fd ff ff       	jmpq   1173 <__do_fault+0x273>
		//	set_pte_at(mm, address, page_table, entry);

		/* no need to invalidate: a not-present page won't be cached */
		//	update_mmu_cache(vma, address, page_table);
	} else {
		if (cow_page)
    13b2:	48 8b 7d 90          	mov    -0x70(%rbp),%rdi
    13b6:	48 85 ff             	test   %rdi,%rdi
    13b9:	74 05                	je     13c0 <__do_fault+0x4c0>
		  mem_cgroup_uncharge_page(cow_page);
    13bb:	e8 00 00 00 00       	callq  13c0 <__do_fault+0x4c0>
		if (anon)
    13c0:	83 7d a4 00          	cmpl   $0x0,-0x5c(%rbp)
    13c4:	75 40                	jne    1406 <__do_fault+0x506>
    13c6:	48 8b 7d 98          	mov    -0x68(%rbp),%rdi
    13ca:	e8 00 00 00 00       	callq  13cf <__do_fault+0x4cf>
    13cf:	e9 8d fd ff ff       	jmpq   1161 <__do_fault+0x261>
	/*
	 * For consistency in subsequent calls, make the faulted page always
	 * locked.
	 */
	if (unlikely(!(ret & VM_FAULT_LOCKED)))
	  lock_page(vmf.page);
    13d4:	e8 77 f3 ff ff       	callq  750 <lock_page>
    13d9:	48 8b 5d d0          	mov    -0x30(%rbp),%rbx
    13dd:	0f 1f 00             	nopl   (%rax)
    13e0:	e9 bf fb ff ff       	jmpq   fa4 <__do_fault+0xa4>
    13e5:	48 81 c9 40 08 00 00 	or     $0x840,%rcx
    13ec:	e9 2a fd ff ff       	jmpq   111b <__do_fault+0x21b>
extern bool __get_page_tail(struct page *page);

static inline void get_page(struct page *page)
{
	if (unlikely(PageTail(page)))
		if (likely(__get_page_tail(page)))
    13f1:	48 89 df             	mov    %rbx,%rdi
    13f4:	e8 00 00 00 00       	callq  13f9 <__do_fault+0x4f9>
    13f9:	84 c0                	test   %al,%al
    13fb:	0f 85 f7 fe ff ff    	jne    12f8 <__do_fault+0x3f8>
    1401:	e9 ee fe ff ff       	jmpq   12f4 <__do_fault+0x3f4>
		//	update_mmu_cache(vma, address, page_table);
	} else {
		if (cow_page)
		  mem_cgroup_uncharge_page(cow_page);
		if (anon)
		  page_cache_release(page);
    1406:	48 89 df             	mov    %rbx,%rdi
    1409:	e8 00 00 00 00       	callq  140e <__do_fault+0x50e>
    140e:	66 90                	xchg   %ax,%ax
    1410:	eb b4                	jmp    13c6 <__do_fault+0x4c6>
								(VM_FAULT_ERROR | VM_FAULT_NOPAGE))) {
					ret = tmp;
					goto unwritable_page;
				}
				if (unlikely(!(tmp & VM_FAULT_LOCKED))) {
					lock_page(page);
    1412:	48 89 df             	mov    %rbx,%rdi
    1415:	e8 36 f3 ff ff       	callq  750 <lock_page>
					if (!page->mapping) {
    141a:	48 83 7b 08 00       	cmpq   $0x0,0x8(%rbx)
						unlock_page(page);
						goto unwritable_page;
					}
				} else
				  VM_BUG_ON(!PageLocked(page));
				page_mkwrite = 1;
    141f:	c7 45 b0 01 00 00 00 	movl   $0x1,-0x50(%rbp)
	pte_t *page_table;
	spinlock_t *ptl;
	struct page *page;
	struct page *cow_page;
	pte_t entry;
	int anon = 0;
    1426:	c7 45 a4 00 00 00 00 	movl   $0x0,-0x5c(%rbp)
					ret = tmp;
					goto unwritable_page;
				}
				if (unlikely(!(tmp & VM_FAULT_LOCKED))) {
					lock_page(page);
					if (!page->mapping) {
    142d:	0f 85 ec fb ff ff    	jne    101f <__do_fault+0x11f>
						ret = 0; /* retry the fault */
						unlock_page(page);
    1433:	48 89 df             	mov    %rbx,%rdi
    1436:	e8 00 00 00 00       	callq  143b <__do_fault+0x53b>
					goto unwritable_page;
				}
				if (unlikely(!(tmp & VM_FAULT_LOCKED))) {
					lock_page(page);
					if (!page->mapping) {
						ret = 0; /* retry the fault */
    143b:	31 c0                	xor    %eax,%eax
	}
*/
	return ret;

unwritable_page:
	page_cache_release(page);
    143d:	48 89 df             	mov    %rbx,%rdi
    1440:	89 45 b4             	mov    %eax,-0x4c(%rbp)
    1443:	e8 00 00 00 00       	callq  1448 <__do_fault+0x548>
	return ret;
    1448:	8b 45 b4             	mov    -0x4c(%rbp),%eax
    144b:	41 89 c5             	mov    %eax,%r13d
    144e:	e9 20 fd ff ff       	jmpq   1173 <__do_fault+0x273>
    1453:	66 66 66 66 2e 0f 1f 	data32 data32 data32 nopw %cs:0x0(%rax,%rax,1)
    145a:	84 00 00 00 00 00 

0000000000001460 <do_nonlinear_fault.isra.46>:
 *
 * We enter with non-exclusive mmap_sem (to exclude vma changes,
 * but allow concurrent faults), and pte mapped but not yet locked.
 * We return with mmap_sem still held, but pte unmapped and unlocked.
 */
static int do_nonlinear_fault(struct mm_struct *mm, struct vm_area_struct *vma,
    1460:	e8 00 00 00 00       	callq  1465 <do_nonlinear_fault.isra.46+0x5>
    1465:	55                   	push   %rbp
    1466:	49 89 f2             	mov    %rsi,%r10
			unsigned long address, pte_t *page_table, pmd_t *pmd,
			unsigned int flags, pte_t orig_pte)
{
	pgoff_t pgoff;

	flags |= FAULT_FLAG_NONLINEAR;
    1469:	41 83 c8 02          	or     $0x2,%r8d
 *
 * We enter with non-exclusive mmap_sem (to exclude vma changes,
 * but allow concurrent faults), and pte mapped but not yet locked.
 * We return with mmap_sem still held, but pte unmapped and unlocked.
 */
static int do_nonlinear_fault(struct mm_struct *mm, struct vm_area_struct *vma,
    146d:	48 89 d6             	mov    %rdx,%rsi
    1470:	48 89 e5             	mov    %rsp,%rbp
    1473:	48 83 ec 08          	sub    $0x8,%rsp
	flags |= FAULT_FLAG_NONLINEAR;

	if (!pte_unmap_same(mm, pmd, page_table, orig_pte))
	  return 0;

	if (unlikely(!(vma->vm_flags & VM_NONLINEAR))) {
    1477:	41 f6 42 52 80       	testb  $0x80,0x52(%r10)
    147c:	74 2c                	je     14aa <do_nonlinear_fault.isra.46+0x4a>
    147e:	49 89 fb             	mov    %rdi,%r11

	if (sizeof(pteval_t) > sizeof(long))
		ret = PVOP_CALLEE2(pteval_t, pv_mmu_ops.pte_val,
				   pte.pte, (u64)pte.pte >> 32);
	else
		ret = PVOP_CALLEE1(pteval_t, pv_mmu_ops.pte_val,
    1481:	4c 89 cf             	mov    %r9,%rdi
    1484:	ff 14 25 00 00 00 00 	callq  *0x0
		 */
		print_bad_pte(vma, address, orig_pte, NULL);
		return VM_FAULT_SIGBUS;
	}

	pgoff = pte_to_pgoff(orig_pte);
    148b:	48 c1 e0 12          	shl    $0x12,%rax
	return __do_fault(mm, vma, address, pmd, pgoff, flags, orig_pte);
    148f:	4c 89 0c 24          	mov    %r9,(%rsp)
    1493:	4c 89 d6             	mov    %r10,%rsi
    1496:	48 c1 e8 1e          	shr    $0x1e,%rax
    149a:	45 89 c1             	mov    %r8d,%r9d
    149d:	4c 89 df             	mov    %r11,%rdi
    14a0:	49 89 c0             	mov    %rax,%r8
    14a3:	e8 58 fa ff ff       	callq  f00 <__do_fault>
}
    14a8:	c9                   	leaveq 
    14a9:	c3                   	retq   

	if (unlikely(!(vma->vm_flags & VM_NONLINEAR))) {
		/*
		 * Page table corrupted: show pte and kill process.
		 */
		print_bad_pte(vma, address, orig_pte, NULL);
    14aa:	31 c9                	xor    %ecx,%ecx
    14ac:	4c 89 ca             	mov    %r9,%rdx
    14af:	4c 89 d7             	mov    %r10,%rdi
    14b2:	e8 b9 f2 ff ff       	callq  770 <print_bad_pte>
		return VM_FAULT_SIGBUS;
    14b7:	b8 02 00 00 00       	mov    $0x2,%eax
	}

	pgoff = pte_to_pgoff(orig_pte);
	return __do_fault(mm, vma, address, pmd, pgoff, flags, orig_pte);
}
    14bc:	c9                   	leaveq 
    14bd:	c3                   	retq   
    14be:	66 90                	xchg   %ax,%ax

00000000000014c0 <sync_mm_rss>:


#if defined(SPLIT_RSS_COUNTING)

void sync_mm_rss(struct mm_struct *mm)
{
    14c0:	e8 00 00 00 00       	callq  14c5 <sync_mm_rss+0x5>
    14c5:	55                   	push   %rbp
    14c6:	48 81 c7 c8 02 00 00 	add    $0x2c8,%rdi
	int i;

	for (i = 0; i < NR_MM_COUNTERS; i++) {
    14cd:	31 c0                	xor    %eax,%eax
    14cf:	65 48 8b 0c 25 00 00 	mov    %gs:0x0,%rcx
    14d6:	00 00 


#if defined(SPLIT_RSS_COUNTING)

void sync_mm_rss(struct mm_struct *mm)
{
    14d8:	48 89 e5             	mov    %rsp,%rbp
	int i;

	for (i = 0; i < NR_MM_COUNTERS; i++) {
		if (current->rss_stat.count[i]) {
    14db:	48 63 d0             	movslq %eax,%rdx
    14de:	48 81 c2 ac 00 00 00 	add    $0xac,%rdx
    14e5:	8b 74 91 0c          	mov    0xc(%rcx,%rdx,4),%esi
    14e9:	85 f6                	test   %esi,%esi
    14eb:	74 11                	je     14fe <sync_mm_rss+0x3e>
			add_mm_counter(mm, i, current->rss_stat.count[i]);
    14ed:	48 63 74 91 0c       	movslq 0xc(%rcx,%rdx,4),%rsi
    14f2:	f0 48 01 37          	lock add %rsi,(%rdi)
			current->rss_stat.count[i] = 0;
    14f6:	c7 44 91 0c 00 00 00 	movl   $0x0,0xc(%rcx,%rdx,4)
    14fd:	00 

void sync_mm_rss(struct mm_struct *mm)
{
	int i;

	for (i = 0; i < NR_MM_COUNTERS; i++) {
    14fe:	83 c0 01             	add    $0x1,%eax
    1501:	48 83 c7 08          	add    $0x8,%rdi
    1505:	83 f8 03             	cmp    $0x3,%eax
    1508:	75 d1                	jne    14db <sync_mm_rss+0x1b>
    150a:	65 48 8b 04 25 00 00 	mov    %gs:0x0,%rax
    1511:	00 00 
		if (current->rss_stat.count[i]) {
			add_mm_counter(mm, i, current->rss_stat.count[i]);
			current->rss_stat.count[i] = 0;
		}
	}
	current->rss_stat.events = 0;
    1513:	c7 80 b8 02 00 00 00 	movl   $0x0,0x2b8(%rax)
    151a:	00 00 00 
}
    151d:	5d                   	pop    %rbp
    151e:	c3                   	retq   
    151f:	90                   	nop

0000000000001520 <check_sync_rss_stat>:
#define dec_mm_counter_fast(mm, member) add_mm_counter_fast(mm, member, -1)

/* sync counter once per 64 page faults */
#define TASK_RSS_EVENTS_THRESH	(64)
static void check_sync_rss_stat(struct task_struct *task)
{
    1520:	e8 00 00 00 00       	callq  1525 <check_sync_rss_stat+0x5>
    1525:	65 48 8b 04 25 00 00 	mov    %gs:0x0,%rax
    152c:	00 00 
	if (unlikely(task != current))
    152e:	48 39 c7             	cmp    %rax,%rdi
    1531:	75 14                	jne    1547 <check_sync_rss_stat+0x27>
	  return;
	if (unlikely(task->rss_stat.events++ > TASK_RSS_EVENTS_THRESH))
    1533:	8b 87 b8 02 00 00    	mov    0x2b8(%rdi),%eax
    1539:	8d 50 01             	lea    0x1(%rax),%edx
    153c:	83 f8 40             	cmp    $0x40,%eax
    153f:	89 97 b8 02 00 00    	mov    %edx,0x2b8(%rdi)
    1545:	7f 01                	jg     1548 <check_sync_rss_stat+0x28>
	  sync_mm_rss(task->mm);
}
    1547:	c3                   	retq   
#define dec_mm_counter_fast(mm, member) add_mm_counter_fast(mm, member, -1)

/* sync counter once per 64 page faults */
#define TASK_RSS_EVENTS_THRESH	(64)
static void check_sync_rss_stat(struct task_struct *task)
{
    1548:	55                   	push   %rbp
	if (unlikely(task != current))
	  return;
	if (unlikely(task->rss_stat.events++ > TASK_RSS_EVENTS_THRESH))
	  sync_mm_rss(task->mm);
    1549:	48 8b bf a8 02 00 00 	mov    0x2a8(%rdi),%rdi
#define dec_mm_counter_fast(mm, member) add_mm_counter_fast(mm, member, -1)

/* sync counter once per 64 page faults */
#define TASK_RSS_EVENTS_THRESH	(64)
static void check_sync_rss_stat(struct task_struct *task)
{
    1550:	48 89 e5             	mov    %rsp,%rbp
	if (unlikely(task != current))
	  return;
	if (unlikely(task->rss_stat.events++ > TASK_RSS_EVENTS_THRESH))
	  sync_mm_rss(task->mm);
    1553:	e8 00 00 00 00       	callq  1558 <check_sync_rss_stat+0x38>
}
    1558:	5d                   	pop    %rbp
    1559:	c3                   	retq   
    155a:	66 0f 1f 44 00 00    	nopw   0x0(%rax,%rax,1)

0000000000001560 <tlb_gather_mmu>:
 *	Called to initialize an (on-stack) mmu_gather structure for page-table
 *	tear-down from @mm. The @fullmm argument is used when @mm is without
 *	users and we're going to destroy the full address space (exit/execve).
 */
void tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm, unsigned long start, unsigned long end)
{
    1560:	e8 00 00 00 00       	callq  1565 <tlb_gather_mmu+0x5>
	tlb->mm = mm;

	/* Is it from 0 to ~0? */
	tlb->fullmm     = !(start | (end+1));
    1565:	48 8d 41 01          	lea    0x1(%rcx),%rax
 *	Called to initialize an (on-stack) mmu_gather structure for page-table
 *	tear-down from @mm. The @fullmm argument is used when @mm is without
 *	users and we're going to destroy the full address space (exit/execve).
 */
void tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm, unsigned long start, unsigned long end)
{
    1569:	55                   	push   %rbp
	tlb->mm = mm;
    156a:	48 89 37             	mov    %rsi,(%rdi)

	/* Is it from 0 to ~0? */
	tlb->fullmm     = !(start | (end+1));
	tlb->need_flush_all = 0;
	tlb->start	= start;
    156d:	48 89 57 08          	mov    %rdx,0x8(%rdi)
	tlb->end	= end;
    1571:	48 89 4f 10          	mov    %rcx,0x10(%rdi)
void tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm, unsigned long start, unsigned long end)
{
	tlb->mm = mm;

	/* Is it from 0 to ~0? */
	tlb->fullmm     = !(start | (end+1));
    1575:	48 09 d0             	or     %rdx,%rax
    1578:	0f b6 47 18          	movzbl 0x18(%rdi),%eax
 *	Called to initialize an (on-stack) mmu_gather structure for page-table
 *	tear-down from @mm. The @fullmm argument is used when @mm is without
 *	users and we're going to destroy the full address space (exit/execve).
 */
void tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm, unsigned long start, unsigned long end)
{
    157c:	48 89 e5             	mov    %rsp,%rbp
	tlb->mm = mm;

	/* Is it from 0 to ~0? */
	tlb->fullmm     = !(start | (end+1));
    157f:	40 0f 94 c6          	sete   %sil
	tlb->need_flush_all = 0;
	tlb->start	= start;
	tlb->end	= end;
	tlb->need_flush = 0;
	tlb->local.next = NULL;
    1583:	48 c7 47 28 00 00 00 	movq   $0x0,0x28(%rdi)
    158a:	00 
	tlb->local.nr   = 0;
    158b:	c7 47 30 00 00 00 00 	movl   $0x0,0x30(%rdi)
void tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm, unsigned long start, unsigned long end)
{
	tlb->mm = mm;

	/* Is it from 0 to ~0? */
	tlb->fullmm     = !(start | (end+1));
    1592:	01 f6                	add    %esi,%esi
	tlb->start	= start;
	tlb->end	= end;
	tlb->need_flush = 0;
	tlb->local.next = NULL;
	tlb->local.nr   = 0;
	tlb->local.max  = ARRAY_SIZE(tlb->__pages);
    1594:	c7 47 34 08 00 00 00 	movl   $0x8,0x34(%rdi)
	tlb->active     = &tlb->local;
	tlb->batch_count = 0;
    159b:	c7 47 78 00 00 00 00 	movl   $0x0,0x78(%rdi)
void tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm, unsigned long start, unsigned long end)
{
	tlb->mm = mm;

	/* Is it from 0 to ~0? */
	tlb->fullmm     = !(start | (end+1));
    15a2:	83 e0 f8             	and    $0xfffffff8,%eax
	tlb->need_flush_all = 0;
	tlb->start	= start;
	tlb->end	= end;
	tlb->need_flush = 0;
    15a5:	09 f0                	or     %esi,%eax
    15a7:	88 47 18             	mov    %al,0x18(%rdi)
	tlb->local.next = NULL;
	tlb->local.nr   = 0;
	tlb->local.max  = ARRAY_SIZE(tlb->__pages);
	tlb->active     = &tlb->local;
    15aa:	48 8d 47 28          	lea    0x28(%rdi),%rax
    15ae:	48 89 47 20          	mov    %rax,0x20(%rdi)
	tlb->batch_count = 0;

#ifdef CONFIG_HAVE_RCU_TABLE_FREE
	tlb->batch = NULL;
#endif
}
    15b2:	5d                   	pop    %rbp
    15b3:	c3                   	retq   
    15b4:	66 66 66 2e 0f 1f 84 	data32 data32 nopw %cs:0x0(%rax,%rax,1)
    15bb:	00 00 00 00 00 

00000000000015c0 <tlb_flush_mmu>:

void tlb_flush_mmu(struct mmu_gather *tlb)
{
    15c0:	e8 00 00 00 00       	callq  15c5 <tlb_flush_mmu+0x5>
	struct mmu_gather_batch *batch;
	if (!tlb->need_flush)
    15c5:	f6 47 18 01          	testb  $0x1,0x18(%rdi)
    15c9:	74 0a                	je     15d5 <tlb_flush_mmu+0x15>
	tlb->batch = NULL;
#endif
}

void tlb_flush_mmu(struct mmu_gather *tlb)
{
    15cb:	55                   	push   %rbp
    15cc:	48 89 e5             	mov    %rsp,%rbp
    15cf:	e8 ec f0 ff ff       	callq  6c0 <tlb_flush_mmu.part.59>
	for (batch = &tlb->local; batch; batch = batch->next) {
		free_pages_and_swap_cache(batch->pages, batch->nr);
		batch->nr = 0;
	}
	tlb->active = &tlb->local;
}
    15d4:	5d                   	pop    %rbp
    15d5:	f3 c3                	repz retq 
    15d7:	66 0f 1f 84 00 00 00 	nopw   0x0(%rax,%rax,1)
    15de:	00 00 

00000000000015e0 <tlb_finish_mmu>:
/* tlb_finish_mmu
 *	Called at the end of the shootdown operation to free up any resources
 *	that were required.
 */
void tlb_finish_mmu(struct mmu_gather *tlb, unsigned long start, unsigned long end)
{
    15e0:	e8 00 00 00 00       	callq  15e5 <tlb_finish_mmu+0x5>
    15e5:	55                   	push   %rbp
    15e6:	48 89 e5             	mov    %rsp,%rbp
    15e9:	41 54                	push   %r12
    15eb:	49 89 fc             	mov    %rdi,%r12
    15ee:	53                   	push   %rbx
}

void tlb_flush_mmu(struct mmu_gather *tlb)
{
	struct mmu_gather_batch *batch;
	if (!tlb->need_flush)
    15ef:	f6 47 18 01          	testb  $0x1,0x18(%rdi)
    15f3:	75 3b                	jne    1630 <tlb_finish_mmu+0x50>
	tlb_flush_mmu(tlb);

	/* keep the page table cache within bounds */
	check_pgt_cache();

	for (batch = tlb->local.next; batch; batch = next) {
    15f5:	49 8b 7c 24 28       	mov    0x28(%r12),%rdi
    15fa:	48 85 ff             	test   %rdi,%rdi
    15fd:	75 0c                	jne    160b <tlb_finish_mmu+0x2b>
    15ff:	eb 19                	jmp    161a <tlb_finish_mmu+0x3a>
    1601:	0f 1f 80 00 00 00 00 	nopl   0x0(%rax)
    1608:	48 89 df             	mov    %rbx,%rdi
		next = batch->next;
    160b:	48 8b 1f             	mov    (%rdi),%rbx
		free_pages((unsigned long)batch, 0);
    160e:	31 f6                	xor    %esi,%esi
    1610:	e8 00 00 00 00       	callq  1615 <tlb_finish_mmu+0x35>
	tlb_flush_mmu(tlb);

	/* keep the page table cache within bounds */
	check_pgt_cache();

	for (batch = tlb->local.next; batch; batch = next) {
    1615:	48 85 db             	test   %rbx,%rbx
    1618:	75 ee                	jne    1608 <tlb_finish_mmu+0x28>
		next = batch->next;
		free_pages((unsigned long)batch, 0);
	}
	tlb->local.next = NULL;
    161a:	49 c7 44 24 28 00 00 	movq   $0x0,0x28(%r12)
    1621:	00 00 
}
    1623:	5b                   	pop    %rbx
    1624:	41 5c                	pop    %r12
    1626:	5d                   	pop    %rbp
    1627:	c3                   	retq   
    1628:	0f 1f 84 00 00 00 00 	nopl   0x0(%rax,%rax,1)
    162f:	00 
    1630:	e8 8b f0 ff ff       	callq  6c0 <tlb_flush_mmu.part.59>
    1635:	eb be                	jmp    15f5 <tlb_finish_mmu+0x15>
    1637:	66 0f 1f 84 00 00 00 	nopw   0x0(%rax,%rax,1)
    163e:	00 00 

0000000000001640 <__tlb_remove_page>:
 *	handling the additional races in SMP caused by other CPUs caching valid
 *	mappings in their TLBs. Returns the number of free page slots left.
 *	When out of page slots we must call tlb_flush_mmu().
 */
int __tlb_remove_page(struct mmu_gather *tlb, struct page *page)
{
    1640:	e8 00 00 00 00       	callq  1645 <__tlb_remove_page+0x5>
    1645:	55                   	push   %rbp
    1646:	48 89 e5             	mov    %rsp,%rbp
    1649:	53                   	push   %rbx
	struct mmu_gather_batch *batch;

	VM_BUG_ON(!tlb->need_flush);

	batch = tlb->active;
    164a:	48 8b 47 20          	mov    0x20(%rdi),%rax
 *	handling the additional races in SMP caused by other CPUs caching valid
 *	mappings in their TLBs. Returns the number of free page slots left.
 *	When out of page slots we must call tlb_flush_mmu().
 */
int __tlb_remove_page(struct mmu_gather *tlb, struct page *page)
{
    164e:	48 89 fb             	mov    %rdi,%rbx
	struct mmu_gather_batch *batch;

	VM_BUG_ON(!tlb->need_flush);

	batch = tlb->active;
	batch->pages[batch->nr++] = page;
    1651:	8b 48 08             	mov    0x8(%rax),%ecx
    1654:	8d 51 01             	lea    0x1(%rcx),%edx
    1657:	89 50 08             	mov    %edx,0x8(%rax)
    165a:	48 89 74 c8 10       	mov    %rsi,0x10(%rax,%rcx,8)
	if (batch->nr == batch->max) {
    165f:	8b 40 0c             	mov    0xc(%rax),%eax
    1662:	39 c2                	cmp    %eax,%edx
    1664:	74 0a                	je     1670 <__tlb_remove_page+0x30>
		  return 0;
		batch = tlb->active;
	}
	VM_BUG_ON(batch->nr > batch->max);

	return batch->max - batch->nr;
    1666:	29 d0                	sub    %edx,%eax
}
    1668:	5b                   	pop    %rbx
    1669:	5d                   	pop    %rbp
    166a:	c3                   	retq   
    166b:	0f 1f 44 00 00       	nopl   0x0(%rax,%rax,1)
static int tlb_next_batch(struct mmu_gather *tlb)
{
	struct mmu_gather_batch *batch;

	batch = tlb->active;
	if (batch->next) {
    1670:	48 8b 47 20          	mov    0x20(%rdi),%rax
    1674:	48 8b 10             	mov    (%rax),%rdx
    1677:	48 85 d2             	test   %rdx,%rdx
    167a:	74 14                	je     1690 <__tlb_remove_page+0x50>
		tlb->active = batch->next;
    167c:	48 89 57 20          	mov    %rdx,0x20(%rdi)
    1680:	8b 42 0c             	mov    0xc(%rdx),%eax
    1683:	8b 52 08             	mov    0x8(%rdx),%edx
    1686:	eb de                	jmp    1666 <__tlb_remove_page+0x26>
    1688:	0f 1f 84 00 00 00 00 	nopl   0x0(%rax,%rax,1)
    168f:	00 
		return 1;
	}

	if (tlb->batch_count == MAX_GATHER_BATCH_COUNT)
    1690:	83 7f 78 13          	cmpl   $0x13,0x78(%rdi)
    1694:	74 3a                	je     16d0 <__tlb_remove_page+0x90>
	  return 0;

	batch = (void *)__get_free_pages(GFP_NOWAIT | __GFP_NOWARN, 0);
    1696:	31 f6                	xor    %esi,%esi
    1698:	bf 00 02 00 00       	mov    $0x200,%edi
    169d:	e8 00 00 00 00       	callq  16a2 <__tlb_remove_page+0x62>
	if (!batch)
    16a2:	48 85 c0             	test   %rax,%rax
    16a5:	74 29                	je     16d0 <__tlb_remove_page+0x90>
	  return 0;

	tlb->batch_count++;
    16a7:	83 43 78 01          	addl   $0x1,0x78(%rbx)
	batch->next = NULL;
    16ab:	48 c7 00 00 00 00 00 	movq   $0x0,(%rax)
	batch->nr   = 0;
    16b2:	c7 40 08 00 00 00 00 	movl   $0x0,0x8(%rax)
	batch->max  = MAX_GATHER_BATCH;
    16b9:	c7 40 0c fe 01 00 00 	movl   $0x1fe,0xc(%rax)

	tlb->active->next = batch;
    16c0:	48 8b 53 20          	mov    0x20(%rbx),%rdx
    16c4:	48 89 02             	mov    %rax,(%rdx)
	tlb->active = batch;
    16c7:	48 89 43 20          	mov    %rax,0x20(%rbx)
    16cb:	48 89 c2             	mov    %rax,%rdx
    16ce:	eb b0                	jmp    1680 <__tlb_remove_page+0x40>

	batch = tlb->active;
	batch->pages[batch->nr++] = page;
	if (batch->nr == batch->max) {
		if (!tlb_next_batch(tlb))
		  return 0;
    16d0:	31 c0                	xor    %eax,%eax
    16d2:	eb 94                	jmp    1668 <__tlb_remove_page+0x28>
    16d4:	66 66 66 2e 0f 1f 84 	data32 data32 nopw %cs:0x0(%rax,%rax,1)
    16db:	00 00 00 00 00 

00000000000016e0 <free_pgd_range>:
 * This function frees user-level page tables of a process.
 */
void free_pgd_range(struct mmu_gather *tlb,
			unsigned long addr, unsigned long end,
			unsigned long floor, unsigned long ceiling)
{
    16e0:	e8 00 00 00 00       	callq  16e5 <free_pgd_range+0x5>
    16e5:	55                   	push   %rbp
    16e6:	48 89 e5             	mov    %rsp,%rbp
    16e9:	41 57                	push   %r15
    16eb:	49 89 ff             	mov    %rdi,%r15
    16ee:	41 56                	push   %r14
    16f0:	41 55                	push   %r13
    16f2:	41 54                	push   %r12
    16f4:	53                   	push   %rbx
    16f5:	48 81 ec 88 00 00 00 	sub    $0x88,%rsp
    16fc:	48 89 75 a0          	mov    %rsi,-0x60(%rbp)
	 * masks at different levels, in order to test whether a table
	 * now has no other vmas using it, so can be freed, we don't
	 * bother to round floor or end up - the tests don't need that.
	 */

	addr &= PMD_MASK;
    1700:	48 81 65 a0 00 00 e0 	andq   $0xffffffffffe00000,-0x60(%rbp)
    1707:	ff 
	if (addr < floor) {
    1708:	48 39 4d a0          	cmp    %rcx,-0x60(%rbp)
 * This function frees user-level page tables of a process.
 */
void free_pgd_range(struct mmu_gather *tlb,
			unsigned long addr, unsigned long end,
			unsigned long floor, unsigned long ceiling)
{
    170c:	48 89 55 90          	mov    %rdx,-0x70(%rbp)
    1710:	48 89 4d 80          	mov    %rcx,-0x80(%rbp)
    1714:	4c 89 85 70 ff ff ff 	mov    %r8,-0x90(%rbp)
	 * now has no other vmas using it, so can be freed, we don't
	 * bother to round floor or end up - the tests don't need that.
	 */

	addr &= PMD_MASK;
	if (addr < floor) {
    171b:	73 0e                	jae    172b <free_pgd_range+0x4b>
		addr += PMD_SIZE;
		if (!addr)
    171d:	48 81 45 a0 00 00 20 	addq   $0x200000,-0x60(%rbp)
    1724:	00 
    1725:	0f 84 fd 02 00 00    	je     1a28 <free_pgd_range+0x348>
		  return;
	}
	if (ceiling) {
    172b:	48 83 bd 70 ff ff ff 	cmpq   $0x0,-0x90(%rbp)
    1732:	00 
    1733:	0f 85 01 03 00 00    	jne    1a3a <free_pgd_range+0x35a>
    1739:	48 8b 45 90          	mov    -0x70(%rbp),%rax
    173d:	48 83 e8 01          	sub    $0x1,%rax
    1741:	48 89 85 78 ff ff ff 	mov    %rax,-0x88(%rbp)
		if (!ceiling)
		  return;
	}
	if (end - 1 > ceiling - 1)
	  end -= PMD_SIZE;
	if (addr > end - 1)
    1748:	48 8b 85 78 ff ff ff 	mov    -0x88(%rbp),%rax
    174f:	48 8b 4d a0          	mov    -0x60(%rbp),%rcx
    1753:	48 39 c1             	cmp    %rax,%rcx
    1756:	0f 87 cc 02 00 00    	ja     1a28 <free_pgd_range+0x348>
	  return;

	pgd = pgd_offset(tlb->mm, addr);
    175c:	48 c1 e9 24          	shr    $0x24,%rcx
    1760:	49 8b 07             	mov    (%r15),%rax
    1763:	48 89 4d 88          	mov    %rcx,-0x78(%rbp)

	start &= PUD_MASK;
	if (start < floor)
	  return;
	if (ceiling) {
		ceiling &= PUD_MASK;
    1767:	48 8b 8d 70 ff ff ff 	mov    -0x90(%rbp),%rcx
	if (end - 1 > ceiling - 1)
	  end -= PMD_SIZE;
	if (addr > end - 1)
	  return;

	pgd = pgd_offset(tlb->mm, addr);
    176e:	48 81 65 88 f8 0f 00 	andq   $0xff8,-0x78(%rbp)
    1775:	00 
    1776:	48 8b 40 40          	mov    0x40(%rax),%rax
    177a:	48 01 45 88          	add    %rax,-0x78(%rbp)

	start &= PUD_MASK;
	if (start < floor)
	  return;
	if (ceiling) {
		ceiling &= PUD_MASK;
    177e:	48 89 c8             	mov    %rcx,%rax
    1781:	48 25 00 00 00 c0    	and    $0xffffffffc0000000,%rax
    1787:	48 89 85 68 ff ff ff 	mov    %rax,-0x98(%rbp)

	start &= PGDIR_MASK;
	if (start < floor)
	  return;
	if (ceiling) {
		ceiling &= PGDIR_MASK;
    178e:	48 b8 00 00 00 00 80 	movabs $0xffffff8000000000,%rax
    1795:	ff ff ff 
    1798:	48 21 c8             	and    %rcx,%rax
    179b:	48 89 85 58 ff ff ff 	mov    %rax,-0xa8(%rbp)
		if (!ceiling)
		  return;
	}
	if (end - 1 > ceiling - 1)
    17a2:	48 83 e8 01          	sub    $0x1,%rax
    17a6:	48 89 85 50 ff ff ff 	mov    %rax,-0xb0(%rbp)
    17ad:	0f 1f 00             	nopl   (%rax)
	if (addr > end - 1)
	  return;

	pgd = pgd_offset(tlb->mm, addr);
	do {
		next = pgd_addr_end(addr, end);
    17b0:	48 b8 00 00 00 00 80 	movabs $0x8000000000,%rax
    17b7:	00 00 00 
    17ba:	48 03 45 a0          	add    -0x60(%rbp),%rax
    17be:	48 89 45 c8          	mov    %rax,-0x38(%rbp)
    17c2:	48 b8 00 00 00 00 80 	movabs $0xffffff8000000000,%rax
    17c9:	ff ff ff 
    17cc:	48 21 45 c8          	and    %rax,-0x38(%rbp)
    17d0:	48 8b 4d c8          	mov    -0x38(%rbp),%rcx
    17d4:	48 89 c8             	mov    %rcx,%rax
    17d7:	48 83 e8 01          	sub    $0x1,%rax
    17db:	48 3b 85 78 ff ff ff 	cmp    -0x88(%rbp),%rax
    17e2:	48 89 c8             	mov    %rcx,%rax
    17e5:	48 0f 43 45 90       	cmovae -0x70(%rbp),%rax
    17ea:	48 89 45 c8          	mov    %rax,-0x38(%rbp)
    17ee:	48 8b 45 88          	mov    -0x78(%rbp),%rax
    17f2:	48 8b 38             	mov    (%rax),%rdi
void pud_clear_bad(pud_t *);
void pmd_clear_bad(pmd_t *);

static inline int pgd_none_or_clear_bad(pgd_t *pgd)
{
	if (pgd_none(*pgd))
    17f5:	48 85 ff             	test   %rdi,%rdi
    17f8:	0f 84 02 03 00 00    	je     1b00 <free_pgd_range+0x420>
	return (pud_t *)pgd_page_vaddr(*pgd) + pud_index(address);
}

static inline int pgd_bad(pgd_t pgd)
{
	return (pgd_flags(pgd) & ~_PAGE_USER) != _KERNPG_TABLE;
    17fe:	48 b8 fb 0f 00 00 00 	movabs $0xffffc00000000ffb,%rax
    1805:	c0 ff ff 
    1808:	48 21 f8             	and    %rdi,%rax
		return 1;
	if (unlikely(pgd_bad(*pgd))) {
    180b:	48 83 f8 63          	cmp    $0x63,%rax
    180f:	0f 85 70 03 00 00    	jne    1b85 <free_pgd_range+0x4a5>

	if (sizeof(pgdval_t) > sizeof(long))
		ret =  PVOP_CALLEE2(pgdval_t, pv_mmu_ops.pgd_val,
				    pgd.pgd, (u64)pgd.pgd >> 32);
	else
		ret =  PVOP_CALLEE1(pgdval_t, pv_mmu_ops.pgd_val,
    1815:	ff 14 25 00 00 00 00 	callq  *0x0
#define pgd_page(pgd)		pfn_to_page(pgd_val(pgd) >> PAGE_SHIFT)

/* to find an entry in a page-table-directory. */
static inline unsigned long pud_index(unsigned long address)
{
	return (address >> PUD_SHIFT) & (PTRS_PER_PUD - 1);
    181c:	48 8b 5d a0          	mov    -0x60(%rbp),%rbx
	return pgd_flags(pgd) & _PAGE_PRESENT;
}

static inline unsigned long pgd_page_vaddr(pgd_t pgd)
{
	return (unsigned long)__va((unsigned long)pgd_val(pgd) & PTE_PFN_MASK);
    1820:	48 b9 00 00 00 00 00 	movabs $0xffff880000000000,%rcx
    1827:	88 ff ff 
    182a:	48 be 00 f0 ff ff ff 	movabs $0x3ffffffff000,%rsi
    1831:	3f 00 00 
    1834:	48 21 f0             	and    %rsi,%rax
	return pud_flags(pud) & _PAGE_PRESENT;
}

static inline unsigned long pud_page_vaddr(pud_t pud)
{
	return (unsigned long)__va((unsigned long)pud_val(pud) & PTE_PFN_MASK);
    1837:	48 89 4d 98          	mov    %rcx,-0x68(%rbp)
#define pgd_page(pgd)		pfn_to_page(pgd_val(pgd) >> PAGE_SHIFT)

/* to find an entry in a page-table-directory. */
static inline unsigned long pud_index(unsigned long address)
{
	return (address >> PUD_SHIFT) & (PTRS_PER_PUD - 1);
    183b:	48 89 da             	mov    %rbx,%rdx
}

static inline pud_t *pud_offset(pgd_t *pgd, unsigned long address)
{
	return (pud_t *)pgd_page_vaddr(*pgd) + pud_index(address);
    183e:	48 89 5d c0          	mov    %rbx,-0x40(%rbp)
#define pgd_page(pgd)		pfn_to_page(pgd_val(pgd) >> PAGE_SHIFT)

/* to find an entry in a page-table-directory. */
static inline unsigned long pud_index(unsigned long address)
{
	return (address >> PUD_SHIFT) & (PTRS_PER_PUD - 1);
    1842:	48 c1 ea 1b          	shr    $0x1b,%rdx
}

static inline pud_t *pud_offset(pgd_t *pgd, unsigned long address)
{
	return (pud_t *)pgd_page_vaddr(*pgd) + pud_index(address);
    1846:	81 e2 f8 0f 00 00    	and    $0xff8,%edx
	return pgd_flags(pgd) & _PAGE_PRESENT;
}

static inline unsigned long pgd_page_vaddr(pgd_t pgd)
{
	return (unsigned long)__va((unsigned long)pgd_val(pgd) & PTE_PFN_MASK);
    184c:	48 01 ca             	add    %rcx,%rdx
	return (address >> PUD_SHIFT) & (PTRS_PER_PUD - 1);
}

static inline pud_t *pud_offset(pgd_t *pgd, unsigned long address)
{
	return (pud_t *)pgd_page_vaddr(*pgd) + pud_index(address);
    184f:	48 01 d0             	add    %rdx,%rax
    1852:	48 89 45 b0          	mov    %rax,-0x50(%rbp)
    1856:	48 8b 45 c8          	mov    -0x38(%rbp),%rax
    185a:	48 83 e8 01          	sub    $0x1,%rax
    185e:	48 89 45 a8          	mov    %rax,-0x58(%rbp)
	if (ceiling) {
		ceiling &= PUD_MASK;
		if (!ceiling)
		  return;
	}
	if (end - 1 > ceiling - 1)
    1862:	48 8b 85 68 ff ff ff 	mov    -0x98(%rbp),%rax
    1869:	48 83 e8 01          	sub    $0x1,%rax
    186d:	48 89 85 60 ff ff ff 	mov    %rax,-0xa0(%rbp)
    1874:	0f 1f 40 00          	nopl   0x0(%rax)
	unsigned long start;

	start = addr;
	pud = pud_offset(pgd, addr);
	do {
		next = pud_addr_end(addr, end);
    1878:	48 8b 45 c0          	mov    -0x40(%rbp),%rax
    187c:	4c 8d b0 00 00 00 40 	lea    0x40000000(%rax),%r14
    1883:	49 81 e6 00 00 00 c0 	and    $0xffffffffc0000000,%r14
    188a:	49 8d 46 ff          	lea    -0x1(%r14),%rax
    188e:	48 3b 45 a8          	cmp    -0x58(%rbp),%rax
    1892:	48 8b 45 b0          	mov    -0x50(%rbp),%rax
    1896:	4c 0f 43 75 c8       	cmovae -0x38(%rbp),%r14
    189b:	48 8b 38             	mov    (%rax),%rdi
	return 0;
}

static inline int pud_none_or_clear_bad(pud_t *pud)
{
	if (pud_none(*pud))
    189e:	48 85 ff             	test   %rdi,%rdi
    18a1:	0f 84 69 01 00 00    	je     1a10 <free_pgd_range+0x330>
		return 1;
	if (unlikely(pud_bad(*pud))) {
    18a7:	48 b8 98 0f 00 00 00 	movabs $0xffffc00000000f98,%rax
    18ae:	c0 ff ff 
    18b1:	48 85 c7             	test   %rax,%rdi
    18b4:	0f 85 bb 02 00 00    	jne    1b75 <free_pgd_range+0x495>

	if (sizeof(pudval_t) > sizeof(long))
		ret =  PVOP_CALLEE2(pudval_t, pv_mmu_ops.pud_val,
				    pud.pud, (u64)pud.pud >> 32);
	else
		ret =  PVOP_CALLEE1(pudval_t, pv_mmu_ops.pud_val,
    18ba:	ff 14 25 00 00 00 00 	callq  *0x0
 * this macro returns the index of the entry in the pmd page which would
 * control the given virtual address
 */
static inline unsigned long pmd_index(unsigned long address)
{
	return (address >> PMD_SHIFT) & (PTRS_PER_PMD - 1);
    18c1:	48 8b 5d c0          	mov    -0x40(%rbp),%rbx
	return pud_flags(pud) & _PAGE_PRESENT;
}

static inline unsigned long pud_page_vaddr(pud_t pud)
{
	return (unsigned long)__va((unsigned long)pud_val(pud) & PTE_PFN_MASK);
    18c5:	48 ba 00 f0 ff ff ff 	movabs $0x3ffffffff000,%rdx
    18cc:	3f 00 00 
    18cf:	48 21 d0             	and    %rdx,%rax
 * has been handled earlier when unmapping all the memory regions.
 */
static void free_pte_range(struct mmu_gather *tlb, pmd_t *pmd,
			unsigned long addr)
{
	pgtable_t token = pmd_pgtable(*pmd);
    18d2:	48 89 55 b8          	mov    %rdx,-0x48(%rbp)
 * this macro returns the index of the entry in the pmd page which would
 * control the given virtual address
 */
static inline unsigned long pmd_index(unsigned long address)
{
	return (address >> PMD_SHIFT) & (PTRS_PER_PMD - 1);
    18d6:	49 89 dc             	mov    %rbx,%r12
    18d9:	49 c1 ec 12          	shr    $0x12,%r12
#define pud_page(pud)		pfn_to_page(pud_val(pud) >> PAGE_SHIFT)

/* Find an entry in the second-level page table.. */
static inline pmd_t *pmd_offset(pud_t *pud, unsigned long address)
{
	return (pmd_t *)pud_page_vaddr(*pud) + pmd_index(address);
    18dd:	41 81 e4 f8 0f 00 00 	and    $0xff8,%r12d
	return pud_flags(pud) & _PAGE_PRESENT;
}

static inline unsigned long pud_page_vaddr(pud_t pud)
{
	return (unsigned long)__va((unsigned long)pud_val(pud) & PTE_PFN_MASK);
    18e4:	4c 03 65 98          	add    -0x68(%rbp),%r12
#define pud_page(pud)		pfn_to_page(pud_val(pud) >> PAGE_SHIFT)

/* Find an entry in the second-level page table.. */
static inline pmd_t *pmd_offset(pud_t *pud, unsigned long address)
{
	return (pmd_t *)pud_page_vaddr(*pud) + pmd_index(address);
    18e8:	49 01 c4             	add    %rax,%r12
    18eb:	49 8d 46 ff          	lea    -0x1(%r14),%rax
    18ef:	4d 89 e5             	mov    %r12,%r13
    18f2:	48 89 45 d0          	mov    %rax,-0x30(%rbp)
    18f6:	66 2e 0f 1f 84 00 00 	nopw   %cs:0x0(%rax,%rax,1)
    18fd:	00 00 00 
	unsigned long start;

	start = addr;
	pmd = pmd_offset(pud, addr);
	do {
		next = pmd_addr_end(addr, end);
    1900:	48 81 c3 00 00 20 00 	add    $0x200000,%rbx
    1907:	49 8b 7d 00          	mov    0x0(%r13),%rdi
    190b:	48 81 e3 00 00 e0 ff 	and    $0xffffffffffe00000,%rbx
    1912:	48 8d 43 ff          	lea    -0x1(%rbx),%rax
    1916:	48 3b 45 d0          	cmp    -0x30(%rbp),%rax
    191a:	49 0f 43 de          	cmovae %r14,%rbx
	return 0;
}

static inline int pmd_none_or_clear_bad(pmd_t *pmd)
{
	if (pmd_none(*pmd))
    191e:	48 85 ff             	test   %rdi,%rdi
    1921:	74 72                	je     1995 <free_pgd_range+0x2b5>

static inline int pmd_bad(pmd_t pmd)
{
#ifdef CONFIG_NUMA_BALANCING
	/* pmd_numa check */
	if ((pmd_flags(pmd) & (_PAGE_NUMA|_PAGE_PRESENT)) == _PAGE_NUMA)
    1923:	48 89 f8             	mov    %rdi,%rax
    1926:	25 01 01 00 00       	and    $0x101,%eax
    192b:	48 3d 00 01 00 00    	cmp    $0x100,%rax
    1931:	74 17                	je     194a <free_pgd_range+0x26a>
		return 0;
#endif
	return (pmd_flags(pmd) & ~_PAGE_USER) != _KERNPG_TABLE;
    1933:	48 b8 fb 0f 00 00 00 	movabs $0xffffc00000000ffb,%rax
    193a:	c0 ff ff 
    193d:	48 21 f8             	and    %rdi,%rax
		return 1;
	if (unlikely(pmd_bad(*pmd))) {
    1940:	48 83 f8 63          	cmp    $0x63,%rax
    1944:	0f 85 1e 02 00 00    	jne    1b68 <free_pgd_range+0x488>

	if (sizeof(pmdval_t) > sizeof(long))
		ret =  PVOP_CALLEE2(pmdval_t, pv_mmu_ops.pmd_val,
				    pmd.pmd, (u64)pmd.pmd >> 32);
	else
		ret =  PVOP_CALLEE1(pmdval_t, pv_mmu_ops.pmd_val,
    194a:	ff 14 25 00 00 00 00 	callq  *0x0

	if (sizeof(pmdval_t) > sizeof(long))
		ret = PVOP_CALLEE2(pmdval_t, pv_mmu_ops.make_pmd,
				   val, (u64)val >> 32);
	else
		ret = PVOP_CALLEE1(pmdval_t, pv_mmu_ops.make_pmd,
    1951:	31 ff                	xor    %edi,%edi

	if (sizeof(pmdval_t) > sizeof(long))
		ret =  PVOP_CALLEE2(pmdval_t, pv_mmu_ops.pmd_val,
				    pmd.pmd, (u64)pmd.pmd >> 32);
	else
		ret =  PVOP_CALLEE1(pmdval_t, pv_mmu_ops.pmd_val,
    1953:	49 89 c4             	mov    %rax,%r12

	if (sizeof(pmdval_t) > sizeof(long))
		ret = PVOP_CALLEE2(pmdval_t, pv_mmu_ops.make_pmd,
				   val, (u64)val >> 32);
	else
		ret = PVOP_CALLEE1(pmdval_t, pv_mmu_ops.make_pmd,
    1956:	ff 14 25 00 00 00 00 	callq  *0x0
    195d:	48 89 c6             	mov    %rax,%rsi
	pmdval_t val = native_pmd_val(pmd);

	if (sizeof(pmdval_t) > sizeof(long))
		PVOP_VCALL3(pv_mmu_ops.set_pmd, pmdp, val, (u64)val >> 32);
	else
		PVOP_VCALL2(pv_mmu_ops.set_pmd, pmdp, val);
    1960:	4c 89 ef             	mov    %r13,%rdi
    1963:	ff 14 25 00 00 00 00 	callq  *0x0
 * has been handled earlier when unmapping all the memory regions.
 */
static void free_pte_range(struct mmu_gather *tlb, pmd_t *pmd,
			unsigned long addr)
{
	pgtable_t token = pmd_pgtable(*pmd);
    196a:	4c 23 65 b8          	and    -0x48(%rbp),%r12
	pmd_clear(pmd);
	pte_free_tlb(tlb, token, addr);
    196e:	41 80 4f 18 01       	orb    $0x1,0x18(%r15)
 * has been handled earlier when unmapping all the memory regions.
 */
static void free_pte_range(struct mmu_gather *tlb, pmd_t *pmd,
			unsigned long addr)
{
	pgtable_t token = pmd_pgtable(*pmd);
    1973:	48 b8 00 00 00 00 00 	movabs $0xffffea0000000000,%rax
    197a:	ea ff ff 
extern void ___pte_free_tlb(struct mmu_gather *tlb, struct page *pte);

static inline void __pte_free_tlb(struct mmu_gather *tlb, struct page *pte,
				  unsigned long address)
{
	___pte_free_tlb(tlb, pte);
    197d:	4c 89 ff             	mov    %r15,%rdi
    1980:	49 c1 ec 06          	shr    $0x6,%r12
    1984:	49 8d 34 04          	lea    (%r12,%rax,1),%rsi
    1988:	e8 00 00 00 00       	callq  198d <free_pgd_range+0x2ad>
	pmd_clear(pmd);
	pte_free_tlb(tlb, token, addr);
	atomic_long_dec(&tlb->mm->nr_ptes);
    198d:	49 8b 07             	mov    (%r15),%rax
 *
 * Atomically decrements @v by 1.
 */
static inline void atomic64_dec(atomic64_t *v)
{
	asm volatile(LOCK_PREFIX "decq %0"
    1990:	f0 48 ff 48 50       	lock decq 0x50(%rax)
	do {
		next = pmd_addr_end(addr, end);
		if (pmd_none_or_clear_bad(pmd))
		  continue;
		free_pte_range(tlb, pmd, addr);
	} while (pmd++, addr = next, addr != end);
    1995:	49 83 c5 08          	add    $0x8,%r13
    1999:	49 39 de             	cmp    %rbx,%r14
    199c:	0f 85 5e ff ff ff    	jne    1900 <free_pgd_range+0x220>

	start &= PUD_MASK;
    19a2:	48 8b 45 c0          	mov    -0x40(%rbp),%rax
    19a6:	48 25 00 00 00 c0    	and    $0xffffffffc0000000,%rax
	if (start < floor)
    19ac:	48 39 45 80          	cmp    %rax,-0x80(%rbp)
    19b0:	77 5e                	ja     1a10 <free_pgd_range+0x330>
	  return;
	if (ceiling) {
    19b2:	48 83 bd 70 ff ff ff 	cmpq   $0x0,-0x90(%rbp)
    19b9:	00 
    19ba:	0f 85 60 01 00 00    	jne    1b20 <free_pgd_range+0x440>

	if (sizeof(pudval_t) > sizeof(long))
		ret =  PVOP_CALLEE2(pudval_t, pv_mmu_ops.pud_val,
				    pud.pud, (u64)pud.pud >> 32);
	else
		ret =  PVOP_CALLEE1(pudval_t, pv_mmu_ops.pud_val,
    19c0:	48 8b 4d b0          	mov    -0x50(%rbp),%rcx
    19c4:	48 8b 39             	mov    (%rcx),%rdi
    19c7:	ff 14 25 00 00 00 00 	callq  *0x0

	if (sizeof(pudval_t) > sizeof(long))
		ret = PVOP_CALLEE2(pudval_t, pv_mmu_ops.make_pud,
				   val, (u64)val >> 32);
	else
		ret = PVOP_CALLEE1(pudval_t, pv_mmu_ops.make_pud,
    19ce:	31 ff                	xor    %edi,%edi

	if (sizeof(pudval_t) > sizeof(long))
		ret =  PVOP_CALLEE2(pudval_t, pv_mmu_ops.pud_val,
				    pud.pud, (u64)pud.pud >> 32);
	else
		ret =  PVOP_CALLEE1(pudval_t, pv_mmu_ops.pud_val,
    19d0:	48 89 c3             	mov    %rax,%rbx

	if (sizeof(pudval_t) > sizeof(long))
		ret = PVOP_CALLEE2(pudval_t, pv_mmu_ops.make_pud,
				   val, (u64)val >> 32);
	else
		ret = PVOP_CALLEE1(pudval_t, pv_mmu_ops.make_pud,
    19d3:	ff 14 25 00 00 00 00 	callq  *0x0
    19da:	48 89 c6             	mov    %rax,%rsi

	if (sizeof(pudval_t) > sizeof(long))
		PVOP_VCALL3(pv_mmu_ops.set_pud, pudp,
			    val, (u64)val >> 32);
	else
		PVOP_VCALL2(pv_mmu_ops.set_pud, pudp,
    19dd:	48 89 cf             	mov    %rcx,%rdi
    19e0:	ff 14 25 00 00 00 00 	callq  *0x0
	return pud_flags(pud) & _PAGE_PRESENT;
}

static inline unsigned long pud_page_vaddr(pud_t pud)
{
	return (unsigned long)__va((unsigned long)pud_val(pud) & PTE_PFN_MASK);
    19e7:	48 b8 00 f0 ff ff ff 	movabs $0x3ffffffff000,%rax
    19ee:	3f 00 00 
	if (end - 1 > ceiling - 1)
	  return;

	pmd = pmd_offset(pud, start);
	pud_clear(pud);
	pmd_free_tlb(tlb, pmd, start);
    19f1:	41 80 4f 18 01       	orb    $0x1,0x18(%r15)
extern void ___pmd_free_tlb(struct mmu_gather *tlb, pmd_t *pmd);

static inline void __pmd_free_tlb(struct mmu_gather *tlb, pmd_t *pmd,
				  unsigned long address)
{
	___pmd_free_tlb(tlb, pmd);
    19f6:	4c 89 ff             	mov    %r15,%rdi
    19f9:	48 21 c3             	and    %rax,%rbx
    19fc:	48 8b 45 98          	mov    -0x68(%rbp),%rax
    1a00:	48 8d 34 03          	lea    (%rbx,%rax,1),%rsi
    1a04:	e8 00 00 00 00       	callq  1a09 <free_pgd_range+0x329>
    1a09:	0f 1f 80 00 00 00 00 	nopl   0x0(%rax)
	do {
		next = pud_addr_end(addr, end);
		if (pud_none_or_clear_bad(pud))
		  continue;
		free_pmd_range(tlb, pud, addr, next, floor, ceiling);
	} while (pud++, addr = next, addr != end);
    1a10:	48 83 45 b0 08       	addq   $0x8,-0x50(%rbp)
    1a15:	4c 39 75 c8          	cmp    %r14,-0x38(%rbp)
    1a19:	74 75                	je     1a90 <free_pgd_range+0x3b0>
    1a1b:	4c 89 75 c0          	mov    %r14,-0x40(%rbp)
    1a1f:	e9 54 fe ff ff       	jmpq   1878 <free_pgd_range+0x198>
    1a24:	0f 1f 40 00          	nopl   0x0(%rax)
		next = pgd_addr_end(addr, end);
		if (pgd_none_or_clear_bad(pgd))
		  continue;
		free_pud_range(tlb, pgd, addr, next, floor, ceiling);
	} while (pgd++, addr = next, addr != end);
}
    1a28:	48 81 c4 88 00 00 00 	add    $0x88,%rsp
    1a2f:	5b                   	pop    %rbx
    1a30:	41 5c                	pop    %r12
    1a32:	41 5d                	pop    %r13
    1a34:	41 5e                	pop    %r14
    1a36:	41 5f                	pop    %r15
    1a38:	5d                   	pop    %rbp
    1a39:	c3                   	retq   
		if (!addr)
		  return;
	}
	if (ceiling) {
		ceiling &= PMD_MASK;
		if (!ceiling)
    1a3a:	48 81 a5 70 ff ff ff 	andq   $0xffffffffffe00000,-0x90(%rbp)
    1a41:	00 00 e0 ff 
    1a45:	74 e1                	je     1a28 <free_pgd_range+0x348>
		  return;
	}
	if (end - 1 > ceiling - 1)
    1a47:	48 8b 4d 90          	mov    -0x70(%rbp),%rcx
    1a4b:	48 89 c8             	mov    %rcx,%rax
    1a4e:	48 83 e8 01          	sub    $0x1,%rax
    1a52:	48 89 c2             	mov    %rax,%rdx
    1a55:	48 89 85 78 ff ff ff 	mov    %rax,-0x88(%rbp)
    1a5c:	48 8b 85 70 ff ff ff 	mov    -0x90(%rbp),%rax
    1a63:	48 83 e8 01          	sub    $0x1,%rax
    1a67:	48 39 c2             	cmp    %rax,%rdx
    1a6a:	0f 86 d8 fc ff ff    	jbe    1748 <free_pgd_range+0x68>
    1a70:	48 89 c8             	mov    %rcx,%rax
    1a73:	48 8d 89 ff ff df ff 	lea    -0x200001(%rcx),%rcx
	  end -= PMD_SIZE;
    1a7a:	48 2d 00 00 20 00    	sub    $0x200000,%rax
    1a80:	48 89 8d 78 ff ff ff 	mov    %rcx,-0x88(%rbp)
    1a87:	48 89 45 90          	mov    %rax,-0x70(%rbp)
    1a8b:	e9 b8 fc ff ff       	jmpq   1748 <free_pgd_range+0x68>
		if (pud_none_or_clear_bad(pud))
		  continue;
		free_pmd_range(tlb, pud, addr, next, floor, ceiling);
	} while (pud++, addr = next, addr != end);

	start &= PGDIR_MASK;
    1a90:	48 b8 00 00 00 00 80 	movabs $0xffffff8000000000,%rax
    1a97:	ff ff ff 
    1a9a:	48 23 45 a0          	and    -0x60(%rbp),%rax
	if (start < floor)
    1a9e:	48 39 45 80          	cmp    %rax,-0x80(%rbp)
    1aa2:	77 5c                	ja     1b00 <free_pgd_range+0x420>
	  return;
	if (ceiling) {
    1aa4:	48 83 bd 70 ff ff ff 	cmpq   $0x0,-0x90(%rbp)
    1aab:	00 
    1aac:	0f 85 96 00 00 00    	jne    1b48 <free_pgd_range+0x468>

	if (sizeof(pgdval_t) > sizeof(long))
		ret =  PVOP_CALLEE2(pgdval_t, pv_mmu_ops.pgd_val,
				    pgd.pgd, (u64)pgd.pgd >> 32);
	else
		ret =  PVOP_CALLEE1(pgdval_t, pv_mmu_ops.pgd_val,
    1ab2:	48 8b 4d 88          	mov    -0x78(%rbp),%rcx
    1ab6:	48 8b 39             	mov    (%rcx),%rdi
    1ab9:	ff 14 25 00 00 00 00 	callq  *0x0

	if (sizeof(pgdval_t) > sizeof(long))
		ret = PVOP_CALLEE2(pgdval_t, pv_mmu_ops.make_pgd,
				   val, (u64)val >> 32);
	else
		ret = PVOP_CALLEE1(pgdval_t, pv_mmu_ops.make_pgd,
    1ac0:	31 ff                	xor    %edi,%edi

	if (sizeof(pgdval_t) > sizeof(long))
		ret =  PVOP_CALLEE2(pgdval_t, pv_mmu_ops.pgd_val,
				    pgd.pgd, (u64)pgd.pgd >> 32);
	else
		ret =  PVOP_CALLEE1(pgdval_t, pv_mmu_ops.pgd_val,
    1ac2:	48 89 c3             	mov    %rax,%rbx

	if (sizeof(pgdval_t) > sizeof(long))
		ret = PVOP_CALLEE2(pgdval_t, pv_mmu_ops.make_pgd,
				   val, (u64)val >> 32);
	else
		ret = PVOP_CALLEE1(pgdval_t, pv_mmu_ops.make_pgd,
    1ac5:	ff 14 25 00 00 00 00 	callq  *0x0
    1acc:	48 89 c6             	mov    %rax,%rsi

	if (sizeof(pgdval_t) > sizeof(long))
		PVOP_VCALL3(pv_mmu_ops.set_pgd, pgdp,
			    val, (u64)val >> 32);
	else
		PVOP_VCALL2(pv_mmu_ops.set_pgd, pgdp,
    1acf:	48 89 cf             	mov    %rcx,%rdi
    1ad2:	ff 14 25 00 00 00 00 	callq  *0x0
	if (end - 1 > ceiling - 1)
	  return;

	pud = pud_offset(pgd, start);
	pgd_clear(pgd);
	pud_free_tlb(tlb, pud, start);
    1ad9:	41 80 4f 18 01       	orb    $0x1,0x18(%r15)
	return pgd_flags(pgd) & _PAGE_PRESENT;
}

static inline unsigned long pgd_page_vaddr(pgd_t pgd)
{
	return (unsigned long)__va((unsigned long)pgd_val(pgd) & PTE_PFN_MASK);
    1ade:	48 b8 00 f0 ff ff ff 	movabs $0x3ffffffff000,%rax
    1ae5:	3f 00 00 
    1ae8:	48 be 00 00 00 00 00 	movabs $0xffff880000000000,%rsi
    1aef:	88 ff ff 
    1af2:	48 21 c3             	and    %rax,%rbx
extern void ___pud_free_tlb(struct mmu_gather *tlb, pud_t *pud);

static inline void __pud_free_tlb(struct mmu_gather *tlb, pud_t *pud,
				  unsigned long address)
{
	___pud_free_tlb(tlb, pud);
    1af5:	4c 89 ff             	mov    %r15,%rdi
    1af8:	48 01 de             	add    %rbx,%rsi
    1afb:	e8 00 00 00 00       	callq  1b00 <free_pgd_range+0x420>
	do {
		next = pgd_addr_end(addr, end);
		if (pgd_none_or_clear_bad(pgd))
		  continue;
		free_pud_range(tlb, pgd, addr, next, floor, ceiling);
	} while (pgd++, addr = next, addr != end);
    1b00:	48 83 45 88 08       	addq   $0x8,-0x78(%rbp)
    1b05:	48 8b 45 c8          	mov    -0x38(%rbp),%rax
    1b09:	48 39 45 90          	cmp    %rax,-0x70(%rbp)
    1b0d:	0f 84 15 ff ff ff    	je     1a28 <free_pgd_range+0x348>
    1b13:	48 89 45 a0          	mov    %rax,-0x60(%rbp)
    1b17:	e9 94 fc ff ff       	jmpq   17b0 <free_pgd_range+0xd0>
    1b1c:	0f 1f 40 00          	nopl   0x0(%rax)
	start &= PUD_MASK;
	if (start < floor)
	  return;
	if (ceiling) {
		ceiling &= PUD_MASK;
		if (!ceiling)
    1b20:	48 83 bd 68 ff ff ff 	cmpq   $0x0,-0x98(%rbp)
    1b27:	00 
    1b28:	0f 84 e2 fe ff ff    	je     1a10 <free_pgd_range+0x330>
		  return;
	}
	if (end - 1 > ceiling - 1)
    1b2e:	48 8b 45 d0          	mov    -0x30(%rbp),%rax
    1b32:	48 39 85 60 ff ff ff 	cmp    %rax,-0xa0(%rbp)
    1b39:	0f 82 d1 fe ff ff    	jb     1a10 <free_pgd_range+0x330>
    1b3f:	e9 7c fe ff ff       	jmpq   19c0 <free_pgd_range+0x2e0>
    1b44:	0f 1f 40 00          	nopl   0x0(%rax)
	start &= PGDIR_MASK;
	if (start < floor)
	  return;
	if (ceiling) {
		ceiling &= PGDIR_MASK;
		if (!ceiling)
    1b48:	48 83 bd 58 ff ff ff 	cmpq   $0x0,-0xa8(%rbp)
    1b4f:	00 
    1b50:	74 ae                	je     1b00 <free_pgd_range+0x420>
		  return;
	}
	if (end - 1 > ceiling - 1)
    1b52:	48 8b 45 a8          	mov    -0x58(%rbp),%rax
    1b56:	48 39 85 50 ff ff ff 	cmp    %rax,-0xb0(%rbp)
    1b5d:	72 a1                	jb     1b00 <free_pgd_range+0x420>
    1b5f:	e9 4e ff ff ff       	jmpq   1ab2 <free_pgd_range+0x3d2>
    1b64:	0f 1f 40 00          	nopl   0x0(%rax)
		pmd_clear_bad(pmd);
    1b68:	4c 89 ef             	mov    %r13,%rdi
    1b6b:	e8 00 00 00 00       	callq  1b70 <free_pgd_range+0x490>
    1b70:	e9 20 fe ff ff       	jmpq   1995 <free_pgd_range+0x2b5>
static inline int pud_none_or_clear_bad(pud_t *pud)
{
	if (pud_none(*pud))
		return 1;
	if (unlikely(pud_bad(*pud))) {
		pud_clear_bad(pud);
    1b75:	48 8b 7d b0          	mov    -0x50(%rbp),%rdi
    1b79:	e8 00 00 00 00       	callq  1b7e <free_pgd_range+0x49e>
    1b7e:	66 90                	xchg   %ax,%ax
    1b80:	e9 8b fe ff ff       	jmpq   1a10 <free_pgd_range+0x330>
static inline int pgd_none_or_clear_bad(pgd_t *pgd)
{
	if (pgd_none(*pgd))
		return 1;
	if (unlikely(pgd_bad(*pgd))) {
		pgd_clear_bad(pgd);
    1b85:	48 8b 7d 88          	mov    -0x78(%rbp),%rdi
    1b89:	e8 00 00 00 00       	callq  1b8e <free_pgd_range+0x4ae>
    1b8e:	66 90                	xchg   %ax,%ax
    1b90:	e9 6b ff ff ff       	jmpq   1b00 <free_pgd_range+0x420>
    1b95:	66 66 2e 0f 1f 84 00 	data32 nopw %cs:0x0(%rax,%rax,1)
    1b9c:	00 00 00 00 

0000000000001ba0 <free_pgtables>:
	} while (pgd++, addr = next, addr != end);
}

void free_pgtables(struct mmu_gather *tlb, struct vm_area_struct *vma,
			unsigned long floor, unsigned long ceiling)
{
    1ba0:	e8 00 00 00 00       	callq  1ba5 <free_pgtables+0x5>
    1ba5:	55                   	push   %rbp
    1ba6:	48 89 e5             	mov    %rsp,%rbp
    1ba9:	41 57                	push   %r15
    1bab:	49 89 f7             	mov    %rsi,%r15
    1bae:	41 56                	push   %r14
    1bb0:	49 89 d6             	mov    %rdx,%r14
    1bb3:	41 55                	push   %r13
    1bb5:	49 89 fd             	mov    %rdi,%r13
    1bb8:	41 54                	push   %r12
    1bba:	53                   	push   %rbx
    1bbb:	48 83 ec 08          	sub    $0x8,%rsp
	while (vma) {
    1bbf:	48 85 f6             	test   %rsi,%rsi
	} while (pgd++, addr = next, addr != end);
}

void free_pgtables(struct mmu_gather *tlb, struct vm_area_struct *vma,
			unsigned long floor, unsigned long ceiling)
{
    1bc2:	48 89 4d d0          	mov    %rcx,-0x30(%rbp)
	while (vma) {
    1bc6:	0f 84 81 00 00 00    	je     1c4d <free_pgtables+0xad>
    1bcc:	0f 1f 40 00          	nopl   0x0(%rax)
		struct vm_area_struct *next = vma->vm_next;
    1bd0:	49 8b 5f 10          	mov    0x10(%r15),%rbx

		/*
		 * Hide vma from rmap and truncate_pagecache before freeing
		 * pgtables
		 */
		unlink_anon_vmas(vma);
    1bd4:	4c 89 ff             	mov    %r15,%rdi
void free_pgtables(struct mmu_gather *tlb, struct vm_area_struct *vma,
			unsigned long floor, unsigned long ceiling)
{
	while (vma) {
		struct vm_area_struct *next = vma->vm_next;
		unsigned long addr = vma->vm_start;
    1bd7:	4d 8b 27             	mov    (%r15),%r12

		/*
		 * Hide vma from rmap and truncate_pagecache before freeing
		 * pgtables
		 */
		unlink_anon_vmas(vma);
    1bda:	e8 00 00 00 00       	callq  1bdf <free_pgtables+0x3f>
		unlink_file_vma(vma);
    1bdf:	4c 89 ff             	mov    %r15,%rdi
    1be2:	e8 00 00 00 00       	callq  1be7 <free_pgtables+0x47>
						floor, next? next->vm_start: ceiling);
		} else {
			/*
			 * Optimization: gather nearby vmas into one call down
			 */
			while (next && next->vm_start <= vma->vm_end + PMD_SIZE
    1be7:	48 85 db             	test   %rbx,%rbx
    1bea:	0f 84 83 00 00 00    	je     1c73 <free_pgtables+0xd3>
    1bf0:	49 8b 57 08          	mov    0x8(%r15),%rdx
    1bf4:	4c 8b 03             	mov    (%rbx),%r8
    1bf7:	48 8d 82 00 00 20 00 	lea    0x200000(%rdx),%rax
    1bfe:	49 39 c0             	cmp    %rax,%r8
    1c01:	76 1b                	jbe    1c1e <free_pgtables+0x7e>
    1c03:	eb 57                	jmp    1c5c <free_pgtables+0xbc>
    1c05:	0f 1f 00             	nopl   (%rax)
    1c08:	48 8b 53 08          	mov    0x8(%rbx),%rdx
    1c0c:	4d 8b 07             	mov    (%r15),%r8
    1c0f:	48 8d 8a 00 00 20 00 	lea    0x200000(%rdx),%rcx
    1c16:	49 39 c8             	cmp    %rcx,%r8
    1c19:	77 45                	ja     1c60 <free_pgtables+0xc0>
    1c1b:	4c 89 fb             	mov    %r15,%rbx
						&& !is_vm_hugetlb_page(next)) {
				vma = next;
				next = vma->vm_next;
    1c1e:	4c 8b 7b 10          	mov    0x10(%rbx),%r15
				unlink_anon_vmas(vma);
    1c22:	48 89 df             	mov    %rbx,%rdi
    1c25:	e8 00 00 00 00       	callq  1c2a <free_pgtables+0x8a>
				unlink_file_vma(vma);
    1c2a:	48 89 df             	mov    %rbx,%rdi
    1c2d:	e8 00 00 00 00       	callq  1c32 <free_pgtables+0x92>
						floor, next? next->vm_start: ceiling);
		} else {
			/*
			 * Optimization: gather nearby vmas into one call down
			 */
			while (next && next->vm_start <= vma->vm_end + PMD_SIZE
    1c32:	4d 85 ff             	test   %r15,%r15
    1c35:	75 d1                	jne    1c08 <free_pgtables+0x68>
				vma = next;
				next = vma->vm_next;
				unlink_anon_vmas(vma);
				unlink_file_vma(vma);
			}
			free_pgd_range(tlb, addr, vma->vm_end,
    1c37:	48 8b 53 08          	mov    0x8(%rbx),%rdx
    1c3b:	4c 8b 45 d0          	mov    -0x30(%rbp),%r8
    1c3f:	4c 89 f1             	mov    %r14,%rcx
    1c42:	4c 89 e6             	mov    %r12,%rsi
    1c45:	4c 89 ef             	mov    %r13,%rdi
    1c48:	e8 00 00 00 00       	callq  1c4d <free_pgtables+0xad>
						floor, next? next->vm_start: ceiling);
		}
		vma = next;
	}
}
    1c4d:	48 83 c4 08          	add    $0x8,%rsp
    1c51:	5b                   	pop    %rbx
    1c52:	41 5c                	pop    %r12
    1c54:	41 5d                	pop    %r13
    1c56:	41 5e                	pop    %r14
    1c58:	41 5f                	pop    %r15
    1c5a:	5d                   	pop    %rbp
    1c5b:	c3                   	retq   
						floor, next? next->vm_start: ceiling);
		} else {
			/*
			 * Optimization: gather nearby vmas into one call down
			 */
			while (next && next->vm_start <= vma->vm_end + PMD_SIZE
    1c5c:	49 89 df             	mov    %rbx,%r15
    1c5f:	90                   	nop
				vma = next;
				next = vma->vm_next;
				unlink_anon_vmas(vma);
				unlink_file_vma(vma);
			}
			free_pgd_range(tlb, addr, vma->vm_end,
    1c60:	4c 89 f1             	mov    %r14,%rcx
    1c63:	4c 89 e6             	mov    %r12,%rsi
    1c66:	4c 89 ef             	mov    %r13,%rdi
    1c69:	e8 00 00 00 00       	callq  1c6e <free_pgtables+0xce>
    1c6e:	e9 5d ff ff ff       	jmpq   1bd0 <free_pgtables+0x30>
						floor, next? next->vm_start: ceiling);
		} else {
			/*
			 * Optimization: gather nearby vmas into one call down
			 */
			while (next && next->vm_start <= vma->vm_end + PMD_SIZE
    1c73:	4c 89 fb             	mov    %r15,%rbx
    1c76:	eb bf                	jmp    1c37 <free_pgtables+0x97>
    1c78:	0f 1f 84 00 00 00 00 	nopl   0x0(%rax,%rax,1)
    1c7f:	00 

0000000000001c80 <__pte_alloc>:
	}
}

int __pte_alloc(struct mm_struct *mm, struct vm_area_struct *vma,
			pmd_t *pmd, unsigned long address)
{
    1c80:	e8 00 00 00 00       	callq  1c85 <__pte_alloc+0x5>
    1c85:	55                   	push   %rbp
	spinlock_t *ptl;
	pgtable_t new = pte_alloc_one(mm, address);
    1c86:	48 89 ce             	mov    %rcx,%rsi
	}
}

int __pte_alloc(struct mm_struct *mm, struct vm_area_struct *vma,
			pmd_t *pmd, unsigned long address)
{
    1c89:	48 89 e5             	mov    %rsp,%rbp
    1c8c:	41 56                	push   %r14
    1c8e:	49 89 fe             	mov    %rdi,%r14
    1c91:	41 55                	push   %r13
    1c93:	41 54                	push   %r12
    1c95:	53                   	push   %rbx
    1c96:	48 89 d3             	mov    %rdx,%rbx
	spinlock_t *ptl;
	pgtable_t new = pte_alloc_one(mm, address);
    1c99:	e8 00 00 00 00       	callq  1c9e <__pte_alloc+0x1e>
	int wait_split_huge_page;
	if (!new)
    1c9e:	48 85 c0             	test   %rax,%rax

int __pte_alloc(struct mm_struct *mm, struct vm_area_struct *vma,
			pmd_t *pmd, unsigned long address)
{
	spinlock_t *ptl;
	pgtable_t new = pte_alloc_one(mm, address);
    1ca1:	49 89 c4             	mov    %rax,%r12
	int wait_split_huge_page;
	if (!new)
    1ca4:	0f 84 97 00 00 00    	je     1d41 <__pte_alloc+0xc1>
extern unsigned long max_pfn;
extern unsigned long phys_base;

static inline unsigned long __phys_addr_nodebug(unsigned long x)
{
	unsigned long y = x - __START_KERNEL_map;
    1caa:	ba 00 00 00 80       	mov    $0x80000000,%edx

	/* use the carry flag to determine if x was < __START_KERNEL_map */
	x = y + ((x > y) ? phys_base : (__START_KERNEL_map - PAGE_OFFSET));
    1caf:	48 b9 00 00 00 80 ff 	movabs $0x77ff80000000,%rcx
    1cb6:	77 00 00 
	raw_spin_lock_init(&(_lock)->rlock);		\
} while (0)

static inline void spin_lock(spinlock_t *lock)
{
	raw_spin_lock(&lock->rlock);
    1cb9:	48 b8 00 00 00 00 00 	movabs $0xffffea0000000000,%rax
    1cc0:	ea ff ff 
    1cc3:	48 01 da             	add    %rbx,%rdx
    1cc6:	48 0f 42 0d 00 00 00 	cmovb  0x0(%rip),%rcx        # 1cce <__pte_alloc+0x4e>
    1ccd:	00 
    1cce:	48 01 ca             	add    %rcx,%rdx

#if USE_SPLIT_PMD_PTLOCKS

static inline spinlock_t *pmd_lockptr(struct mm_struct *mm, pmd_t *pmd)
{
	return ptlock_ptr(virt_to_page(pmd));
    1cd1:	48 c1 ea 0c          	shr    $0xc,%rdx
    1cd5:	48 c1 e2 06          	shl    $0x6,%rdx
    1cd9:	4c 8b 6c 02 30       	mov    0x30(%rdx,%rax,1),%r13
    1cde:	4c 89 ef             	mov    %r13,%rdi
    1ce1:	e8 00 00 00 00       	callq  1ce6 <__pte_alloc+0x66>
	 */
	smp_wmb(); /* Could be smp_wmb__xxx(before|after)_spin_lock */

	ptl = pmd_lock(mm, pmd);
	wait_split_huge_page = 0;
	if (likely(pmd_none(*pmd))) {	/* Has another populated it ? */
    1ce6:	48 83 3b 00          	cmpq   $0x0,(%rbx)
    1cea:	75 5c                	jne    1d48 <__pte_alloc+0xc8>
 *
 * Atomically increments @v by 1.
 */
static inline void atomic64_inc(atomic64_t *v)
{
	asm volatile(LOCK_PREFIX "incq %0"
    1cec:	f0 49 ff 46 50       	lock incq 0x50(%r14)
}

static inline void pmd_populate(struct mm_struct *mm, pmd_t *pmd,
				struct page *pte)
{
	unsigned long pfn = page_to_pfn(pte);
    1cf1:	48 b8 00 00 00 00 00 	movabs $0x160000000000,%rax
    1cf8:	16 00 00 
	PVOP_VCALL2(pv_mmu_ops.pgd_free, mm, pgd);
}

static inline void paravirt_alloc_pte(struct mm_struct *mm, unsigned long pfn)
{
	PVOP_VCALL2(pv_mmu_ops.alloc_pte, mm, pfn);
    1cfb:	4c 89 f7             	mov    %r14,%rdi
    1cfe:	49 01 c4             	add    %rax,%r12
    1d01:	49 c1 fc 06          	sar    $0x6,%r12
    1d05:	4c 89 e6             	mov    %r12,%rsi
    1d08:	ff 14 25 00 00 00 00 	callq  *0x0

	paravirt_alloc_pte(mm, pfn);
	set_pmd(pmd, __pmd(((pteval_t)pfn << PAGE_SHIFT) | _PAGE_TABLE));
    1d0f:	4c 89 e7             	mov    %r12,%rdi
    1d12:	48 c1 e7 0c          	shl    $0xc,%rdi
    1d16:	48 83 cf 67          	or     $0x67,%rdi

	if (sizeof(pmdval_t) > sizeof(long))
		ret = PVOP_CALLEE2(pmdval_t, pv_mmu_ops.make_pmd,
				   val, (u64)val >> 32);
	else
		ret = PVOP_CALLEE1(pmdval_t, pv_mmu_ops.make_pmd,
    1d1a:	ff 14 25 00 00 00 00 	callq  *0x0
    1d21:	48 89 c6             	mov    %rax,%rsi
	pmdval_t val = native_pmd_val(pmd);

	if (sizeof(pmdval_t) > sizeof(long))
		PVOP_VCALL3(pv_mmu_ops.set_pmd, pmdp, val, (u64)val >> 32);
	else
		PVOP_VCALL2(pv_mmu_ops.set_pmd, pmdp, val);
    1d24:	48 89 df             	mov    %rbx,%rdi
    1d27:	ff 14 25 00 00 00 00 	callq  *0x0
	raw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \
} while (0)

static inline void spin_unlock(spinlock_t *lock)
{
	raw_spin_unlock(&lock->rlock);
    1d2e:	4c 89 ef             	mov    %r13,%rdi
    1d31:	e8 00 00 00 00       	callq  1d36 <__pte_alloc+0xb6>
	spin_unlock(ptl);
	if (new)
	  pte_free(mm, new);
	if (wait_split_huge_page)
	  wait_split_huge_page(vma->anon_vma, pmd);
	return 0;
    1d36:	31 c0                	xor    %eax,%eax
}
    1d38:	5b                   	pop    %rbx
    1d39:	41 5c                	pop    %r12
    1d3b:	41 5d                	pop    %r13
    1d3d:	41 5e                	pop    %r14
    1d3f:	5d                   	pop    %rbp
    1d40:	c3                   	retq   
{
	spinlock_t *ptl;
	pgtable_t new = pte_alloc_one(mm, address);
	int wait_split_huge_page;
	if (!new)
	  return -ENOMEM;
    1d41:	b8 f4 ff ff ff       	mov    $0xfffffff4,%eax
    1d46:	eb f0                	jmp    1d38 <__pte_alloc+0xb8>
    1d48:	4c 89 ef             	mov    %r13,%rdi
    1d4b:	e8 00 00 00 00       	callq  1d50 <__pte_alloc+0xd0>
{
	inc_zone_page_state(page, NR_PAGETABLE);
	return ptlock_init(page);
}

static inline void pgtable_page_dtor(struct page *page)
    1d50:	49 8b 7c 24 30       	mov    0x30(%r12),%rdi
}

/* Reset page->mapping so free_pages_check won't complain. */
static inline void pte_lock_deinit(struct page *page)
{
	page->mapping = NULL;
    1d55:	49 c7 44 24 08 00 00 	movq   $0x0,0x8(%r12)
    1d5c:	00 00 
	return true;
}

void ptlock_free(struct page *page)
{
	kfree(page->ptl);
    1d5e:	e8 00 00 00 00       	callq  1d63 <__pte_alloc+0xe3>
}

static inline void pgtable_page_dtor(struct page *page)
{
	pte_lock_deinit(page);
	dec_zone_page_state(page, NR_PAGETABLE);
    1d63:	be 0f 00 00 00       	mov    $0xf,%esi
    1d68:	4c 89 e7             	mov    %r12,%rdi
    1d6b:	e8 00 00 00 00       	callq  1d70 <__pte_alloc+0xf0>
}

static inline void pte_free(struct mm_struct *mm, struct page *pte)
{
	pgtable_page_dtor(pte);
	__free_page(pte);
    1d70:	31 f6                	xor    %esi,%esi
    1d72:	4c 89 e7             	mov    %r12,%rdi
    1d75:	e8 00 00 00 00       	callq  1d7a <__pte_alloc+0xfa>
	spin_unlock(ptl);
	if (new)
	  pte_free(mm, new);
	if (wait_split_huge_page)
	  wait_split_huge_page(vma->anon_vma, pmd);
	return 0;
    1d7a:	31 c0                	xor    %eax,%eax
    1d7c:	eb ba                	jmp    1d38 <__pte_alloc+0xb8>
    1d7e:	66 90                	xchg   %ax,%ax

0000000000001d80 <__pte_alloc_kernel>:
}

int __pte_alloc_kernel(pmd_t *pmd, unsigned long address)
{
    1d80:	e8 00 00 00 00       	callq  1d85 <__pte_alloc_kernel+0x5>
    1d85:	55                   	push   %rbp
    1d86:	48 89 e5             	mov    %rsp,%rbp
    1d89:	41 56                	push   %r14
    1d8b:	49 89 fe             	mov    %rdi,%r14
	pte_t *new = pte_alloc_one_kernel(&init_mm, address);
    1d8e:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
	  wait_split_huge_page(vma->anon_vma, pmd);
	return 0;
}

int __pte_alloc_kernel(pmd_t *pmd, unsigned long address)
{
    1d95:	41 55                	push   %r13
    1d97:	41 54                	push   %r12
    1d99:	53                   	push   %rbx
	pte_t *new = pte_alloc_one_kernel(&init_mm, address);
    1d9a:	e8 00 00 00 00       	callq  1d9f <__pte_alloc_kernel+0x1f>
	if (!new)
    1d9f:	48 85 c0             	test   %rax,%rax
	return 0;
}

int __pte_alloc_kernel(pmd_t *pmd, unsigned long address)
{
	pte_t *new = pte_alloc_one_kernel(&init_mm, address);
    1da2:	48 89 c3             	mov    %rax,%rbx
	if (!new)
    1da5:	0f 84 85 00 00 00    	je     1e30 <__pte_alloc_kernel+0xb0>
	raw_spin_lock_init(&(_lock)->rlock);		\
} while (0)

static inline void spin_lock(spinlock_t *lock)
{
	raw_spin_lock(&lock->rlock);
    1dab:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
    1db2:	e8 00 00 00 00       	callq  1db7 <__pte_alloc_kernel+0x37>
	  return -ENOMEM;

	smp_wmb(); /* See comment in __pte_alloc */

	spin_lock(&init_mm.page_table_lock);
	if (likely(pmd_none(*pmd))) {	/* Has another populated it ? */
    1db7:	49 83 3e 00          	cmpq   $0x0,(%r14)
    1dbb:	75 7a                	jne    1e37 <__pte_alloc_kernel+0xb7>
    1dbd:	49 bd 00 00 00 80 ff 	movabs $0x77ff80000000,%r13
    1dc4:	77 00 00 
extern unsigned long max_pfn;
extern unsigned long phys_base;

static inline unsigned long __phys_addr_nodebug(unsigned long x)
{
	unsigned long y = x - __START_KERNEL_map;
    1dc7:	41 bc 00 00 00 80    	mov    $0x80000000,%r12d
	PVOP_VCALL2(pv_mmu_ops.pgd_free, mm, pgd);
}

static inline void paravirt_alloc_pte(struct mm_struct *mm, unsigned long pfn)
{
	PVOP_VCALL2(pv_mmu_ops.alloc_pte, mm, pfn);
    1dcd:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi

	/* use the carry flag to determine if x was < __START_KERNEL_map */
	x = y + ((x > y) ? phys_base : (__START_KERNEL_map - PAGE_OFFSET));
    1dd4:	49 01 dc             	add    %rbx,%r12
    1dd7:	4c 89 ee             	mov    %r13,%rsi
    1dda:	48 0f 42 35 00 00 00 	cmovb  0x0(%rip),%rsi        # 1de2 <__pte_alloc_kernel+0x62>
    1de1:	00 
    1de2:	4c 01 e6             	add    %r12,%rsi
}

static inline void pmd_populate_kernel(struct mm_struct *mm,
				       pmd_t *pmd, pte_t *pte)
{
	paravirt_alloc_pte(mm, __pa(pte) >> PAGE_SHIFT);
    1de5:	48 c1 ee 0c          	shr    $0xc,%rsi
    1de9:	ff 14 25 00 00 00 00 	callq  *0x0
    1df0:	4c 39 e3             	cmp    %r12,%rbx
    1df3:	4c 0f 47 2d 00 00 00 	cmova  0x0(%rip),%r13        # 1dfb <__pte_alloc_kernel+0x7b>
    1dfa:	00 
    1dfb:	4b 8d 7c 25 00       	lea    0x0(%r13,%r12,1),%rdi
	set_pmd(pmd, __pmd(__pa(pte) | _PAGE_TABLE));
    1e00:	48 83 cf 67          	or     $0x67,%rdi

	if (sizeof(pmdval_t) > sizeof(long))
		ret = PVOP_CALLEE2(pmdval_t, pv_mmu_ops.make_pmd,
				   val, (u64)val >> 32);
	else
		ret = PVOP_CALLEE1(pmdval_t, pv_mmu_ops.make_pmd,
    1e04:	ff 14 25 00 00 00 00 	callq  *0x0
    1e0b:	48 89 c6             	mov    %rax,%rsi
	pmdval_t val = native_pmd_val(pmd);

	if (sizeof(pmdval_t) > sizeof(long))
		PVOP_VCALL3(pv_mmu_ops.set_pmd, pmdp, val, (u64)val >> 32);
	else
		PVOP_VCALL2(pv_mmu_ops.set_pmd, pmdp, val);
    1e0e:	4c 89 f7             	mov    %r14,%rdi
    1e11:	ff 14 25 00 00 00 00 	callq  *0x0
	raw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \
} while (0)

static inline void spin_unlock(spinlock_t *lock)
{
	raw_spin_unlock(&lock->rlock);
    1e18:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
    1e1f:	e8 00 00 00 00       	callq  1e24 <__pte_alloc_kernel+0xa4>
	} else
	  VM_BUG_ON(pmd_trans_splitting(*pmd));
	spin_unlock(&init_mm.page_table_lock);
	if (new)
	  pte_free_kernel(&init_mm, new);
	return 0;
    1e24:	31 c0                	xor    %eax,%eax
}
    1e26:	5b                   	pop    %rbx
    1e27:	41 5c                	pop    %r12
    1e29:	41 5d                	pop    %r13
    1e2b:	41 5e                	pop    %r14
    1e2d:	5d                   	pop    %rbp
    1e2e:	c3                   	retq   
    1e2f:	90                   	nop

int __pte_alloc_kernel(pmd_t *pmd, unsigned long address)
{
	pte_t *new = pte_alloc_one_kernel(&init_mm, address);
	if (!new)
	  return -ENOMEM;
    1e30:	b8 f4 ff ff ff       	mov    $0xfffffff4,%eax
    1e35:	eb ef                	jmp    1e26 <__pte_alloc_kernel+0xa6>
    1e37:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
    1e3e:	e8 00 00 00 00       	callq  1e43 <__pte_alloc_kernel+0xc3>
/* Should really implement gc for free page table pages. This could be
   done with a reference count in struct page. */

static inline void pte_free_kernel(struct mm_struct *mm, pte_t *pte)
{
	BUG_ON((unsigned long)pte & (PAGE_SIZE-1));
    1e43:	f7 c3 ff 0f 00 00    	test   $0xfff,%ebx
    1e49:	75 15                	jne    1e60 <__pte_alloc_kernel+0xe0>
	free_page((unsigned long)pte);
    1e4b:	48 89 df             	mov    %rbx,%rdi
    1e4e:	31 f6                	xor    %esi,%esi
    1e50:	e8 00 00 00 00       	callq  1e55 <__pte_alloc_kernel+0xd5>
	  VM_BUG_ON(pmd_trans_splitting(*pmd));
	spin_unlock(&init_mm.page_table_lock);
	if (new)
	  pte_free_kernel(&init_mm, new);
	return 0;
}
    1e55:	5b                   	pop    %rbx
    1e56:	41 5c                	pop    %r12
    1e58:	41 5d                	pop    %r13
    1e5a:	41 5e                	pop    %r14
	} else
	  VM_BUG_ON(pmd_trans_splitting(*pmd));
	spin_unlock(&init_mm.page_table_lock);
	if (new)
	  pte_free_kernel(&init_mm, new);
	return 0;
    1e5c:	31 c0                	xor    %eax,%eax
}
    1e5e:	5d                   	pop    %rbp
    1e5f:	c3                   	retq   
/* Should really implement gc for free page table pages. This could be
   done with a reference count in struct page. */

static inline void pte_free_kernel(struct mm_struct *mm, pte_t *pte)
{
	BUG_ON((unsigned long)pte & (PAGE_SIZE-1));
    1e60:	0f 0b                	ud2    
    1e62:	66 66 66 66 66 2e 0f 	data32 data32 data32 data32 nopw %cs:0x0(%rax,%rax,1)
    1e69:	1f 84 00 00 00 00 00 

0000000000001e70 <vm_normal_pfn_to_page>:
# define HAVE_PTE_SPECIAL 1
#else
# define HAVE_PTE_SPECIAL 0
#endif
struct page* vm_normal_pfn_to_page(struct vm_area_struct *vma, unsigned long addr,
			pte_t pte,unsigned long pfn){
    1e70:	e8 00 00 00 00       	callq  1e75 <vm_normal_pfn_to_page+0x5>
    1e75:	55                   	push   %rbp
	if (HAVE_PTE_SPECIAL) {
		if (likely(!pte_special(pte)))
    1e76:	f6 c6 02             	test   $0x2,%dh
# define HAVE_PTE_SPECIAL 1
#else
# define HAVE_PTE_SPECIAL 0
#endif
struct page* vm_normal_pfn_to_page(struct vm_area_struct *vma, unsigned long addr,
			pte_t pte,unsigned long pfn){
    1e79:	48 89 e5             	mov    %rsp,%rbp
	if (HAVE_PTE_SPECIAL) {
		if (likely(!pte_special(pte)))
    1e7c:	75 1c                	jne    1e9a <vm_normal_pfn_to_page+0x2a>
	if (is_zero_pfn(pfn)){
		//	print_bad_pte(vma, addr, pte, NULL);//ziqiao
		return NULL;
	}
check_pfn:
	if (unlikely(pfn > highest_memmap_pfn)) {
    1e7e:	48 3b 0d 00 00 00 00 	cmp    0x0(%rip),%rcx        # 1e85 <vm_normal_pfn_to_page+0x15>
    1e85:	77 28                	ja     1eaf <vm_normal_pfn_to_page+0x3f>
	/*
	 * NOTE! We still have PageReserved() pages in the page tables.
	 * eg. VDSO mappings can cause them to exist.
	 */
out:
	return pfn_to_page(pfn);
    1e87:	48 c1 e1 06          	shl    $0x6,%rcx
    1e8b:	48 b8 00 00 00 00 00 	movabs $0xffffea0000000000,%rax
    1e92:	ea ff ff 
    1e95:	48 01 c8             	add    %rcx,%rax
}
    1e98:	5d                   	pop    %rbp
    1e99:	c3                   	retq   
	if (HAVE_PTE_SPECIAL) {
		if (likely(!pte_special(pte)))
		  goto check_pfn;
		if (vma->vm_flags & (VM_PFNMAP | VM_MIXEDMAP)){
			//	print_bad_pte(vma, addr, pte, NULL);//ziqiao
			return NULL;
    1e9a:	31 c0                	xor    %eax,%eax
struct page* vm_normal_pfn_to_page(struct vm_area_struct *vma, unsigned long addr,
			pte_t pte,unsigned long pfn){
	if (HAVE_PTE_SPECIAL) {
		if (likely(!pte_special(pte)))
		  goto check_pfn;
		if (vma->vm_flags & (VM_PFNMAP | VM_MIXEDMAP)){
    1e9c:	48 f7 47 50 00 04 00 	testq  $0x10000400,0x50(%rdi)
    1ea3:	10 
    1ea4:	75 f2                	jne    1e98 <vm_normal_pfn_to_page+0x28>
			//	print_bad_pte(vma, addr, pte, NULL);//ziqiao
			return NULL;
		}
		if (!is_zero_pfn(pfn))
    1ea6:	48 3b 0d 00 00 00 00 	cmp    0x0(%rip),%rcx        # 1ead <vm_normal_pfn_to_page+0x3d>
    1ead:	74 e9                	je     1e98 <vm_normal_pfn_to_page+0x28>
		  print_bad_pte(vma, addr, pte, NULL);
    1eaf:	31 c9                	xor    %ecx,%ecx
    1eb1:	e8 ba e8 ff ff       	callq  770 <print_bad_pte>
		//	print_bad_pte(vma, addr, pte, NULL);//ziqiao
		return NULL;
    1eb6:	31 c0                	xor    %eax,%eax
	 * NOTE! We still have PageReserved() pages in the page tables.
	 * eg. VDSO mappings can cause them to exist.
	 */
out:
	return pfn_to_page(pfn);
}
    1eb8:	5d                   	pop    %rbp
    1eb9:	c3                   	retq   
    1eba:	66 0f 1f 44 00 00    	nopw   0x0(%rax,%rax,1)

0000000000001ec0 <vm_normal_page>:
struct page *vm_normal_page(struct vm_area_struct *vma, unsigned long addr,
			pte_t pte)
{
    1ec0:	e8 00 00 00 00       	callq  1ec5 <vm_normal_page+0x5>
    1ec5:	55                   	push   %rbp
    1ec6:	49 89 f8             	mov    %rdi,%r8

	if (sizeof(pteval_t) > sizeof(long))
		ret = PVOP_CALLEE2(pteval_t, pv_mmu_ops.pte_val,
				   pte.pte, (u64)pte.pte >> 32);
	else
		ret = PVOP_CALLEE1(pteval_t, pv_mmu_ops.pte_val,
    1ec9:	48 89 d7             	mov    %rdx,%rdi
    1ecc:	48 89 e5             	mov    %rsp,%rbp
    1ecf:	48 83 ec 08          	sub    $0x8,%rsp
    1ed3:	ff 14 25 00 00 00 00 	callq  *0x0
    1eda:	48 89 c1             	mov    %rax,%rcx
	return pte_flags(pte) & _PAGE_SPECIAL;
}

static inline unsigned long pte_pfn(pte_t pte)
{
	return (pte_val(pte) & PTE_PFN_MASK) >> PAGE_SHIFT;
    1edd:	48 c1 e1 12          	shl    $0x12,%rcx
    1ee1:	48 c1 e9 1e          	shr    $0x1e,%rcx
	unsigned long pfn = pte_pfn(pte);

	if (HAVE_PTE_SPECIAL) {
		if (likely(!pte_special(pte)))
    1ee5:	f6 c6 02             	test   $0x2,%dh
    1ee8:	75 1c                	jne    1f06 <vm_normal_page+0x46>
	if (is_zero_pfn(pfn)){
		//	print_bad_pte(vma, addr, pte, NULL);//ziqiao
		return NULL;
	}
check_pfn:
	if (unlikely(pfn > highest_memmap_pfn)) {
    1eea:	48 39 0d 00 00 00 00 	cmp    %rcx,0x0(%rip)        # 1ef1 <vm_normal_page+0x31>
    1ef1:	72 3c                	jb     1f2f <vm_normal_page+0x6f>
	/*
	 * NOTE! We still have PageReserved() pages in the page tables.
	 * eg. VDSO mappings can cause them to exist.
	 */
out:
	return pfn_to_page(pfn);
    1ef3:	48 c1 e1 06          	shl    $0x6,%rcx
    1ef7:	48 b8 00 00 00 00 00 	movabs $0xffffea0000000000,%rax
    1efe:	ea ff ff 
    1f01:	48 01 c8             	add    %rcx,%rax
}
    1f04:	c9                   	leaveq 
    1f05:	c3                   	retq   
	if (HAVE_PTE_SPECIAL) {
		if (likely(!pte_special(pte)))
		  goto check_pfn;
		if (vma->vm_flags & (VM_PFNMAP | VM_MIXEDMAP)){
			//	print_bad_pte(vma, addr, pte, NULL);//ziqiao
			return NULL;
    1f06:	31 c0                	xor    %eax,%eax
	unsigned long pfn = pte_pfn(pte);

	if (HAVE_PTE_SPECIAL) {
		if (likely(!pte_special(pte)))
		  goto check_pfn;
		if (vma->vm_flags & (VM_PFNMAP | VM_MIXEDMAP)){
    1f08:	49 f7 40 50 00 04 00 	testq  $0x10000400,0x50(%r8)
    1f0f:	10 
    1f10:	75 f2                	jne    1f04 <vm_normal_page+0x44>
			//	print_bad_pte(vma, addr, pte, NULL);//ziqiao
			return NULL;
		}
		if (!is_zero_pfn(pfn))
    1f12:	48 3b 0d 00 00 00 00 	cmp    0x0(%rip),%rcx        # 1f19 <vm_normal_page+0x59>
    1f19:	74 e9                	je     1f04 <vm_normal_page+0x44>
		  print_bad_pte(vma, addr, pte, NULL);
    1f1b:	31 c9                	xor    %ecx,%ecx
    1f1d:	4c 89 c7             	mov    %r8,%rdi
    1f20:	48 89 45 f8          	mov    %rax,-0x8(%rbp)
    1f24:	e8 47 e8 ff ff       	callq  770 <print_bad_pte>
    1f29:	48 8b 45 f8          	mov    -0x8(%rbp),%rax
	 * NOTE! We still have PageReserved() pages in the page tables.
	 * eg. VDSO mappings can cause them to exist.
	 */
out:
	return pfn_to_page(pfn);
}
    1f2d:	c9                   	leaveq 
    1f2e:	c3                   	retq   
		//	print_bad_pte(vma, addr, pte, NULL);//ziqiao
		return NULL;
	}
check_pfn:
	if (unlikely(pfn > highest_memmap_pfn)) {
		print_bad_pte(vma, addr, pte, NULL);
    1f2f:	31 c9                	xor    %ecx,%ecx
    1f31:	4c 89 c7             	mov    %r8,%rdi
    1f34:	e8 37 e8 ff ff       	callq  770 <print_bad_pte>
		return NULL;
    1f39:	31 c0                	xor    %eax,%eax
	 * NOTE! We still have PageReserved() pages in the page tables.
	 * eg. VDSO mappings can cause them to exist.
	 */
out:
	return pfn_to_page(pfn);
}
    1f3b:	c9                   	leaveq 
    1f3c:	c3                   	retq   
    1f3d:	0f 1f 00             	nopl   (%rax)

0000000000001f40 <unmap_single_vma>:

static void unmap_single_vma(struct mmu_gather *tlb,
			struct vm_area_struct *vma, unsigned long start_addr,
			unsigned long end_addr,
			struct zap_details *details)
{
    1f40:	e8 00 00 00 00       	callq  1f45 <unmap_single_vma+0x5>
    1f45:	55                   	push   %rbp
    1f46:	48 89 e5             	mov    %rsp,%rbp
    1f49:	41 57                	push   %r15
    1f4b:	41 56                	push   %r14
    1f4d:	41 55                	push   %r13
    1f4f:	41 54                	push   %r12
    1f51:	53                   	push   %rbx
    1f52:	48 81 ec a0 00 00 00 	sub    $0xa0,%rsp
	unsigned long start = max(vma->vm_start, start_addr);
    1f59:	48 8b 06             	mov    (%rsi),%rax

static void unmap_single_vma(struct mmu_gather *tlb,
			struct vm_area_struct *vma, unsigned long start_addr,
			unsigned long end_addr,
			struct zap_details *details)
{
    1f5c:	48 89 75 b8          	mov    %rsi,-0x48(%rbp)
    1f60:	4c 89 45 b0          	mov    %r8,-0x50(%rbp)
	unsigned long start = max(vma->vm_start, start_addr);
    1f64:	48 39 c2             	cmp    %rax,%rdx
    1f67:	48 0f 42 d0          	cmovb  %rax,%rdx
    1f6b:	49 89 d6             	mov    %rdx,%r14
	unsigned long end;

	if (start >= vma->vm_end)
    1f6e:	48 8b 56 08          	mov    0x8(%rsi),%rdx
    1f72:	49 39 d6             	cmp    %rdx,%r14
    1f75:	0f 83 09 05 00 00    	jae    2484 <unmap_single_vma+0x544>
	  return;
	end = min(vma->vm_end, end_addr);
    1f7b:	48 39 d1             	cmp    %rdx,%rcx
    1f7e:	48 0f 46 d1          	cmovbe %rcx,%rdx
	if (end <= vma->vm_start)
    1f82:	48 39 d0             	cmp    %rdx,%rax
	unsigned long start = max(vma->vm_start, start_addr);
	unsigned long end;

	if (start >= vma->vm_end)
	  return;
	end = min(vma->vm_end, end_addr);
    1f85:	48 89 95 48 ff ff ff 	mov    %rdx,-0xb8(%rbp)
	if (end <= vma->vm_start)
    1f8c:	0f 83 f2 04 00 00    	jae    2484 <unmap_single_vma+0x544>
	  return;

	if (vma->vm_file)
    1f92:	48 83 be a8 00 00 00 	cmpq   $0x0,0xa8(%rsi)
    1f99:	00 
    1f9a:	49 89 ff             	mov    %rdi,%r15
    1f9d:	48 89 f0             	mov    %rsi,%rax
    1fa0:	74 0b                	je     1fad <unmap_single_vma+0x6d>
	  uprobe_munmap(vma, start, end);
    1fa2:	4c 89 f6             	mov    %r14,%rsi
    1fa5:	48 89 c7             	mov    %rax,%rdi
    1fa8:	e8 00 00 00 00       	callq  1fad <unmap_single_vma+0x6d>

	if (unlikely(vma->vm_flags & VM_PFNMAP))
    1fad:	48 8b 45 b8          	mov    -0x48(%rbp),%rax
    1fb1:	f6 40 51 04          	testb  $0x4,0x51(%rax)
    1fb5:	0f 85 6d 07 00 00    	jne    2728 <unmap_single_vma+0x7e8>
	  untrack_pfn(vma, 0, 0);

	if (start != end) {
    1fbb:	4c 3b b5 48 ff ff ff 	cmp    -0xb8(%rbp),%r14
    1fc2:	0f 84 bc 04 00 00    	je     2484 <unmap_single_vma+0x544>
			struct zap_details *details)
{
	pgd_t *pgd;
	unsigned long next;

	if (details && !details->check_mapping && !details->nonlinear_vma)
    1fc8:	48 8b 45 b0          	mov    -0x50(%rbp),%rax
    1fcc:	48 85 c0             	test   %rax,%rax
    1fcf:	74 0b                	je     1fdc <unmap_single_vma+0x9c>
    1fd1:	48 83 78 08 00       	cmpq   $0x0,0x8(%rax)
    1fd6:	0f 84 33 07 00 00    	je     270f <unmap_single_vma+0x7cf>
	  details = NULL;

	BUG_ON(addr >= end);
    1fdc:	4c 3b b5 48 ff ff ff 	cmp    -0xb8(%rbp),%r14
    1fe3:	0f 83 24 07 00 00    	jae    270d <unmap_single_vma+0x7cd>
	mem_cgroup_uncharge_start();
    1fe9:	e8 00 00 00 00       	callq  1fee <unmap_single_vma+0xae>
	tlb_start_vma(tlb, vma);
	pgd = pgd_offset(vma->vm_mm, addr);
    1fee:	48 8b 45 b8          	mov    -0x48(%rbp),%rax
    1ff2:	4c 89 f1             	mov    %r14,%rcx
    1ff5:	48 c1 e9 24          	shr    $0x24,%rcx
    1ff9:	48 89 8d 40 ff ff ff 	mov    %rcx,-0xc0(%rbp)
    2000:	48 81 a5 40 ff ff ff 	andq   $0xff8,-0xc0(%rbp)
    2007:	f8 0f 00 00 
    200b:	48 8b 40 40          	mov    0x40(%rax),%rax
    200f:	48 8b 40 40          	mov    0x40(%rax),%rax
    2013:	48 01 85 40 ff ff ff 	add    %rax,-0xc0(%rbp)
    201a:	48 8b 85 48 ff ff ff 	mov    -0xb8(%rbp),%rax
    2021:	48 83 e8 01          	sub    $0x1,%rax
    2025:	48 89 85 38 ff ff ff 	mov    %rax,-0xc8(%rbp)
    202c:	65 48 8b 04 25 00 00 	mov    %gs:0x0,%rax
    2033:	00 00 
    2035:	48 89 85 70 ff ff ff 	mov    %rax,-0x90(%rbp)
	do {
		next = pgd_addr_end(addr, end);
    203c:	48 b8 00 00 00 00 80 	movabs $0x8000000000,%rax
    2043:	00 00 00 
    2046:	4c 01 f0             	add    %r14,%rax
    2049:	48 89 85 60 ff ff ff 	mov    %rax,-0xa0(%rbp)
    2050:	48 b8 00 00 00 00 80 	movabs $0xffffff8000000000,%rax
    2057:	ff ff ff 
    205a:	48 21 85 60 ff ff ff 	and    %rax,-0xa0(%rbp)
    2061:	48 8b 8d 60 ff ff ff 	mov    -0xa0(%rbp),%rcx
    2068:	48 89 c8             	mov    %rcx,%rax
    206b:	48 83 e8 01          	sub    $0x1,%rax
    206f:	48 3b 85 38 ff ff ff 	cmp    -0xc8(%rbp),%rax
    2076:	48 89 c8             	mov    %rcx,%rax
    2079:	48 0f 43 85 48 ff ff 	cmovae -0xb8(%rbp),%rax
    2080:	ff 
    2081:	48 89 85 60 ff ff ff 	mov    %rax,-0xa0(%rbp)
    2088:	48 8b 85 40 ff ff ff 	mov    -0xc0(%rbp),%rax
    208f:	48 8b 38             	mov    (%rax),%rdi
void pud_clear_bad(pud_t *);
void pmd_clear_bad(pmd_t *);

static inline int pgd_none_or_clear_bad(pgd_t *pgd)
{
	if (pgd_none(*pgd))
    2092:	48 85 ff             	test   %rdi,%rdi
    2095:	0f 84 9e 05 00 00    	je     2639 <unmap_single_vma+0x6f9>
	return (pud_t *)pgd_page_vaddr(*pgd) + pud_index(address);
}

static inline int pgd_bad(pgd_t pgd)
{
	return (pgd_flags(pgd) & ~_PAGE_USER) != _KERNPG_TABLE;
    209b:	48 b8 fb 0f 00 00 00 	movabs $0xffffc00000000ffb,%rax
    20a2:	c0 ff ff 
    20a5:	48 21 f8             	and    %rdi,%rax
		return 1;
	if (unlikely(pgd_bad(*pgd))) {
    20a8:	48 83 f8 63          	cmp    $0x63,%rax
    20ac:	0f 85 7b 05 00 00    	jne    262d <unmap_single_vma+0x6ed>

	if (sizeof(pgdval_t) > sizeof(long))
		ret =  PVOP_CALLEE2(pgdval_t, pv_mmu_ops.pgd_val,
				    pgd.pgd, (u64)pgd.pgd >> 32);
	else
		ret =  PVOP_CALLEE1(pgdval_t, pv_mmu_ops.pgd_val,
    20b2:	ff 14 25 00 00 00 00 	callq  *0x0
#define pgd_page(pgd)		pfn_to_page(pgd_val(pgd) >> PAGE_SHIFT)

/* to find an entry in a page-table-directory. */
static inline unsigned long pud_index(unsigned long address)
{
	return (address >> PUD_SHIFT) & (PTRS_PER_PUD - 1);
    20b9:	4c 89 f2             	mov    %r14,%rdx
	return pgd_flags(pgd) & _PAGE_PRESENT;
}

static inline unsigned long pgd_page_vaddr(pgd_t pgd)
{
	return (unsigned long)__va((unsigned long)pgd_val(pgd) & PTE_PFN_MASK);
    20bc:	48 b9 00 00 00 00 00 	movabs $0xffff880000000000,%rcx
    20c3:	88 ff ff 
    20c6:	48 be 00 f0 ff ff ff 	movabs $0x3ffffffff000,%rsi
    20cd:	3f 00 00 
#define pgd_page(pgd)		pfn_to_page(pgd_val(pgd) >> PAGE_SHIFT)

/* to find an entry in a page-table-directory. */
static inline unsigned long pud_index(unsigned long address)
{
	return (address >> PUD_SHIFT) & (PTRS_PER_PUD - 1);
    20d0:	48 c1 ea 1b          	shr    $0x1b,%rdx
	return pgd_flags(pgd) & _PAGE_PRESENT;
}

static inline unsigned long pgd_page_vaddr(pgd_t pgd)
{
	return (unsigned long)__va((unsigned long)pgd_val(pgd) & PTE_PFN_MASK);
    20d4:	48 21 f0             	and    %rsi,%rax
	return pud_flags(pud) & _PAGE_PRESENT;
}

static inline unsigned long pud_page_vaddr(pud_t pud)
{
	return (unsigned long)__va((unsigned long)pud_val(pud) & PTE_PFN_MASK);
    20d7:	48 89 4d 88          	mov    %rcx,-0x78(%rbp)
	return (address >> PUD_SHIFT) & (PTRS_PER_PUD - 1);
}

static inline pud_t *pud_offset(pgd_t *pgd, unsigned long address)
{
	return (pud_t *)pgd_page_vaddr(*pgd) + pud_index(address);
    20db:	81 e2 f8 0f 00 00    	and    $0xff8,%edx
    20e1:	4d 89 f4             	mov    %r14,%r12
	return pgd_flags(pgd) & _PAGE_PRESENT;
}

static inline unsigned long pgd_page_vaddr(pgd_t pgd)
{
	return (unsigned long)__va((unsigned long)pgd_val(pgd) & PTE_PFN_MASK);
    20e4:	48 01 ca             	add    %rcx,%rdx
	return (address >> PUD_SHIFT) & (PTRS_PER_PUD - 1);
}

static inline pud_t *pud_offset(pgd_t *pgd, unsigned long address)
{
	return (pud_t *)pgd_page_vaddr(*pgd) + pud_index(address);
    20e7:	48 01 d0             	add    %rdx,%rax
    20ea:	48 89 85 58 ff ff ff 	mov    %rax,-0xa8(%rbp)
    20f1:	48 8b 85 60 ff ff ff 	mov    -0xa0(%rbp),%rax
    20f8:	48 83 e8 01          	sub    $0x1,%rax
    20fc:	48 89 85 50 ff ff ff 	mov    %rax,-0xb0(%rbp)
    2103:	0f 1f 44 00 00       	nopl   0x0(%rax,%rax,1)
	pud_t *pud;
	unsigned long next;

	pud = pud_offset(pgd, addr);
	do {
		next = pud_addr_end(addr, end);
    2108:	49 8d 84 24 00 00 00 	lea    0x40000000(%r12),%rax
    210f:	40 
    2110:	48 89 85 78 ff ff ff 	mov    %rax,-0x88(%rbp)
    2117:	48 81 a5 78 ff ff ff 	andq   $0xffffffffc0000000,-0x88(%rbp)
    211e:	00 00 00 c0 
    2122:	48 8b 8d 78 ff ff ff 	mov    -0x88(%rbp),%rcx
    2129:	48 89 c8             	mov    %rcx,%rax
    212c:	48 83 e8 01          	sub    $0x1,%rax
    2130:	48 3b 85 50 ff ff ff 	cmp    -0xb0(%rbp),%rax
    2137:	48 89 c8             	mov    %rcx,%rax
    213a:	48 0f 43 85 60 ff ff 	cmovae -0xa0(%rbp),%rax
    2141:	ff 
    2142:	48 89 85 78 ff ff ff 	mov    %rax,-0x88(%rbp)
    2149:	48 8b 85 58 ff ff ff 	mov    -0xa8(%rbp),%rax
    2150:	48 8b 38             	mov    (%rax),%rdi
	return 0;
}

static inline int pud_none_or_clear_bad(pud_t *pud)
{
	if (pud_none(*pud))
    2153:	48 85 ff             	test   %rdi,%rdi
    2156:	0f 84 c5 04 00 00    	je     2621 <unmap_single_vma+0x6e1>
		return 1;
	if (unlikely(pud_bad(*pud))) {
    215c:	48 b8 98 0f 00 00 00 	movabs $0xffffc00000000f98,%rax
    2163:	c0 ff ff 
    2166:	48 85 c7             	test   %rax,%rdi
    2169:	0f 85 a6 04 00 00    	jne    2615 <unmap_single_vma+0x6d5>

	if (sizeof(pudval_t) > sizeof(long))
		ret =  PVOP_CALLEE2(pudval_t, pv_mmu_ops.pud_val,
				    pud.pud, (u64)pud.pud >> 32);
	else
		ret =  PVOP_CALLEE1(pudval_t, pv_mmu_ops.pud_val,
    216f:	ff 14 25 00 00 00 00 	callq  *0x0
 * this macro returns the index of the entry in the pmd page which would
 * control the given virtual address
 */
static inline unsigned long pmd_index(unsigned long address)
{
	return (address >> PMD_SHIFT) & (PTRS_PER_PMD - 1);
    2176:	4c 89 e2             	mov    %r12,%rdx
	return pud_flags(pud) & _PAGE_PRESENT;
}

static inline unsigned long pud_page_vaddr(pud_t pud)
{
	return (unsigned long)__va((unsigned long)pud_val(pud) & PTE_PFN_MASK);
    2179:	48 b9 00 f0 ff ff ff 	movabs $0x3ffffffff000,%rcx
    2180:	3f 00 00 
    2183:	4d 89 fe             	mov    %r15,%r14
 * this macro returns the index of the entry in the pmd page which would
 * control the given virtual address
 */
static inline unsigned long pmd_index(unsigned long address)
{
	return (address >> PMD_SHIFT) & (PTRS_PER_PMD - 1);
    2186:	48 c1 ea 12          	shr    $0x12,%rdx
	return pud_flags(pud) & _PAGE_PRESENT;
}

static inline unsigned long pud_page_vaddr(pud_t pud)
{
	return (unsigned long)__va((unsigned long)pud_val(pud) & PTE_PFN_MASK);
    218a:	48 21 c8             	and    %rcx,%rax
}
#endif /* ALLOC_SPLIT_PTLOCKS */

static inline spinlock_t *pte_lockptr(struct mm_struct *mm, pmd_t *pmd)
{
	return ptlock_ptr(pmd_page(*pmd));
    218d:	48 89 4d 80          	mov    %rcx,-0x80(%rbp)
#define pud_page(pud)		pfn_to_page(pud_val(pud) >> PAGE_SHIFT)

/* Find an entry in the second-level page table.. */
static inline pmd_t *pmd_offset(pud_t *pud, unsigned long address)
{
	return (pmd_t *)pud_page_vaddr(*pud) + pmd_index(address);
    2191:	81 e2 f8 0f 00 00    	and    $0xff8,%edx
	return pud_flags(pud) & _PAGE_PRESENT;
}

static inline unsigned long pud_page_vaddr(pud_t pud)
{
	return (unsigned long)__va((unsigned long)pud_val(pud) & PTE_PFN_MASK);
    2197:	48 03 55 88          	add    -0x78(%rbp),%rdx
    219b:	4d 89 e7             	mov    %r12,%r15
#define pud_page(pud)		pfn_to_page(pud_val(pud) >> PAGE_SHIFT)

/* Find an entry in the second-level page table.. */
static inline pmd_t *pmd_offset(pud_t *pud, unsigned long address)
{
	return (pmd_t *)pud_page_vaddr(*pud) + pmd_index(address);
    219e:	48 01 d0             	add    %rdx,%rax
    21a1:	48 89 45 90          	mov    %rax,-0x70(%rbp)
    21a5:	48 8b 85 78 ff ff ff 	mov    -0x88(%rbp),%rax
    21ac:	48 83 e8 01          	sub    $0x1,%rax
    21b0:	48 89 85 68 ff ff ff 	mov    %rax,-0x98(%rbp)
    21b7:	66 0f 1f 84 00 00 00 	nopw   0x0(%rax,%rax,1)
    21be:	00 00 
	pmd_t *pmd;
	unsigned long next;

	pmd = pmd_offset(pud, addr);
	do {
		next = pmd_addr_end(addr, end);
    21c0:	4d 8d af 00 00 20 00 	lea    0x200000(%r15),%r13
    21c7:	49 81 e5 00 00 e0 ff 	and    $0xffffffffffe00000,%r13
    21ce:	49 8d 45 ff          	lea    -0x1(%r13),%rax
    21d2:	48 39 85 68 ff ff ff 	cmp    %rax,-0x98(%rbp)
	/*
	 * Depend on compiler for an atomic pmd read. NOTE: this is
	 * only going to work, if the pmdval_t isn't larger than
	 * an unsigned long.
	 */
	return *pmdp;
    21d9:	48 8b 45 90          	mov    -0x70(%rbp),%rax
    21dd:	4c 0f 46 ad 78 ff ff 	cmovbe -0x88(%rbp),%r13
    21e4:	ff 
    21e5:	48 8b 00             	mov    (%rax),%rax
	 * confused.
	 */
#ifdef CONFIG_TRANSPARENT_HUGEPAGE
	barrier();
#endif
	if (pmd_none(pmdval) || pmd_trans_huge(pmdval))
    21e8:	48 85 c0             	test   %rax,%rax
    21eb:	0f 84 0f 04 00 00    	je     2600 <unmap_single_vma+0x6c0>

static inline int pmd_bad(pmd_t pmd)
{
#ifdef CONFIG_NUMA_BALANCING
	/* pmd_numa check */
	if ((pmd_flags(pmd) & (_PAGE_NUMA|_PAGE_PRESENT)) == _PAGE_NUMA)
    21f1:	48 89 c2             	mov    %rax,%rdx
    21f4:	81 e2 01 01 00 00    	and    $0x101,%edx
    21fa:	48 81 fa 00 01 00 00 	cmp    $0x100,%rdx
    2201:	74 17                	je     221a <unmap_single_vma+0x2da>
		return 0;
#endif
	return (pmd_flags(pmd) & ~_PAGE_USER) != _KERNPG_TABLE;
    2203:	48 ba fb 0f 00 00 00 	movabs $0xffffc00000000ffb,%rdx
    220a:	c0 ff ff 
    220d:	48 21 d0             	and    %rdx,%rax
		return 1;
	if (unlikely(pmd_bad(pmdval))) {
    2210:	48 83 f8 63          	cmp    $0x63,%rax
    2214:	0f 85 db 03 00 00    	jne    25f5 <unmap_single_vma+0x6b5>
static unsigned long zap_pte_range(struct mmu_gather *tlb,
			struct vm_area_struct *vma, pmd_t *pmd,
			unsigned long addr, unsigned long end,
			struct zap_details *details)
{
	struct mm_struct *mm = tlb->mm;
    221a:	49 8b 06             	mov    (%r14),%rax
    221d:	48 89 45 c0          	mov    %rax,-0x40(%rbp)
    2221:	0f 1f 80 00 00 00 00 	nopl   0x0(%rax)

	if (sizeof(pmdval_t) > sizeof(long))
		ret =  PVOP_CALLEE2(pmdval_t, pv_mmu_ops.pmd_val,
				    pmd.pmd, (u64)pmd.pmd >> 32);
	else
		ret =  PVOP_CALLEE1(pmdval_t, pv_mmu_ops.pmd_val,
    2228:	48 8b 75 90          	mov    -0x70(%rbp),%rsi
	return 0;
}

static inline void init_rss_vec(int *rss)
{
	memset(rss, 0, sizeof(int) * NR_MM_COUNTERS);
    222c:	48 c7 45 cc 00 00 00 	movq   $0x0,-0x34(%rbp)
    2233:	00 
    2234:	c7 45 d4 00 00 00 00 	movl   $0x0,-0x2c(%rbp)
    223b:	48 8b 3e             	mov    (%rsi),%rdi
    223e:	ff 14 25 00 00 00 00 	callq  *0x0
    2245:	48 8b 5d 80          	mov    -0x80(%rbp),%rbx
    2249:	48 b9 00 00 00 00 00 	movabs $0xffffea0000000000,%rcx
    2250:	ea ff ff 
    2253:	48 8b 3e             	mov    (%rsi),%rdi
    2256:	48 21 d8             	and    %rbx,%rax
    2259:	48 c1 e8 06          	shr    $0x6,%rax
    225d:	48 8b 44 08 30       	mov    0x30(%rax,%rcx,1),%rax
    2262:	48 89 c1             	mov    %rax,%rcx
    2265:	48 89 45 a0          	mov    %rax,-0x60(%rbp)
    2269:	ff 14 25 00 00 00 00 	callq  *0x0
	return (unsigned long)native_pmd_val(pmd) == 0;
}

static inline unsigned long pmd_page_vaddr(pmd_t pmd)
{
	return (unsigned long)__va(pmd_val(pmd) & PTE_PFN_MASK);
    2270:	48 8b 75 88          	mov    -0x78(%rbp),%rsi
    2274:	48 21 d8             	and    %rbx,%rax
 * this function returns the index of the entry in the pte page which would
 * control the given virtual address
 */
static inline unsigned long pte_index(unsigned long address)
{
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
    2277:	4c 89 fa             	mov    %r15,%rdx
    227a:	48 c1 ea 09          	shr    $0x9,%rdx
	raw_spin_lock_init(&(_lock)->rlock);		\
} while (0)

static inline void spin_lock(spinlock_t *lock)
{
	raw_spin_lock(&lock->rlock);
    227e:	48 89 cf             	mov    %rcx,%rdi
}

static inline pte_t *pte_offset_kernel(pmd_t *pmd, unsigned long address)
{
	return (pte_t *)pmd_page_vaddr(*pmd) + pte_index(address);
    2281:	81 e2 f8 0f 00 00    	and    $0xff8,%edx
	return (unsigned long)native_pmd_val(pmd) == 0;
}

static inline unsigned long pmd_page_vaddr(pmd_t pmd)
{
	return (unsigned long)__va(pmd_val(pmd) & PTE_PFN_MASK);
    2287:	48 8d 1c 30          	lea    (%rax,%rsi,1),%rbx
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
}

static inline pte_t *pte_offset_kernel(pmd_t *pmd, unsigned long address)
{
	return (pte_t *)pmd_page_vaddr(*pmd) + pte_index(address);
    228b:	48 01 d3             	add    %rdx,%rbx
    228e:	e8 00 00 00 00       	callq  2293 <unmap_single_vma+0x353>
}

#define  __HAVE_ARCH_ENTER_LAZY_MMU_MODE
static inline void arch_enter_lazy_mmu_mode(void)
{
	PVOP_VCALL0(pv_mmu_ops.lazy_mode.enter);
    2293:	ff 14 25 00 00 00 00 	callq  *0x0
    229a:	eb 53                	jmp    22ef <unmap_single_vma+0x3af>
    229c:	0f 1f 40 00          	nopl   0x0(%rax)
		}
		/*
		 * If details->check_mapping, we leave swap entries;
		 * if details->nonlinear_vma, we leave file entries.
		 */
		if (unlikely(details))
    22a0:	48 83 7d b0 00       	cmpq   $0x0,-0x50(%rbp)
    22a5:	75 34                	jne    22db <unmap_single_vma+0x39b>
		  continue;
		if (pte_file(ptent)) {
    22a7:	f6 c2 40             	test   $0x40,%dl
    22aa:	0f 84 f0 01 00 00    	je     24a0 <unmap_single_vma+0x560>
			if (unlikely(!(vma->vm_flags & VM_NONLINEAR)))
    22b0:	48 8b 45 b8          	mov    -0x48(%rbp),%rax
    22b4:	f6 40 52 80          	testb  $0x80,0x52(%rax)
    22b8:	0f 84 87 03 00 00    	je     2645 <unmap_single_vma+0x705>
	if (sizeof(pteval_t) > sizeof(long))
		ret = PVOP_CALLEE2(pteval_t,
				   pv_mmu_ops.make_pte,
				   val, (u64)val >> 32);
	else
		ret = PVOP_CALLEE1(pteval_t,
    22be:	31 ff                	xor    %edi,%edi
    22c0:	ff 14 25 00 00 00 00 	callq  *0x0
    22c7:	48 89 c1             	mov    %rax,%rcx
{
	if (sizeof(pteval_t) > sizeof(long))
		/* 5 arg words */
		pv_mmu_ops.set_pte_at(mm, addr, ptep, pte);
	else
		PVOP_VCALL4(pv_mmu_ops.set_pte_at, mm, addr, ptep, pte.pte);
    22ca:	48 8b 7d c0          	mov    -0x40(%rbp),%rdi
    22ce:	4c 89 fe             	mov    %r15,%rsi
    22d1:	48 89 da             	mov    %rbx,%rdx
    22d4:	ff 14 25 00 00 00 00 	callq  *0x0
			}
			if (unlikely(!free_swap_and_cache(entry)))
			  print_bad_pte(vma, addr, ptent, NULL);
		}
		pte_clear_not_present_full(mm, addr, pte, tlb->fullmm);
	} while (pte++, addr += PAGE_SIZE, addr != end);
    22db:	49 81 c7 00 10 00 00 	add    $0x1000,%r15
    22e2:	48 83 c3 08          	add    $0x8,%rbx
    22e6:	4d 39 fd             	cmp    %r15,%r13
    22e9:	0f 84 d9 02 00 00    	je     25c8 <unmap_single_vma+0x688>
	init_rss_vec(rss);
	start_pte = pte_offset_map_lock(mm, pmd, addr, &ptl);
	pte = start_pte;
	arch_enter_lazy_mmu_mode();
	do {
		pte_t ptent = *pte;
    22ef:	4c 8b 23             	mov    (%rbx),%r12
		if (pte_none(ptent)) {
    22f2:	4d 85 e4             	test   %r12,%r12
    22f5:	74 e4                	je     22db <unmap_single_vma+0x39b>
    22f7:	48 ba ff 0f 00 00 00 	movabs $0xffffc00000000fff,%rdx
    22fe:	c0 ff ff 
    2301:	4c 21 e2             	and    %r12,%rdx
			continue;
		}

		if (pte_present(ptent)) {
    2304:	f7 c2 01 01 00 00    	test   $0x101,%edx
    230a:	74 94                	je     22a0 <unmap_single_vma+0x360>
			struct page *page;

			page = vm_normal_page(vma, addr, ptent);
    230c:	48 8b 7d b8          	mov    -0x48(%rbp),%rdi
    2310:	4c 89 e2             	mov    %r12,%rdx
    2313:	4c 89 fe             	mov    %r15,%rsi
    2316:	e8 00 00 00 00       	callq  231b <unmap_single_vma+0x3db>
    231b:	49 89 c4             	mov    %rax,%r12
			if (unlikely(details) && page) {
    231e:	31 c0                	xor    %eax,%eax
    2320:	48 83 7d b0 00       	cmpq   $0x0,-0x50(%rbp)
    2325:	0f 95 c0             	setne  %al
    2328:	48 89 45 a8          	mov    %rax,-0x58(%rbp)
    232c:	0f 85 8e 03 00 00    	jne    26c0 <unmap_single_vma+0x780>
static inline pte_t ptep_get_and_clear_full(struct mm_struct *mm,
					    unsigned long addr, pte_t *ptep,
					    int full)
{
	pte_t pte;
	if (full) {
    2332:	41 f6 46 18 02       	testb  $0x2,0x18(%r14)
    2337:	0f 85 23 02 00 00    	jne    2560 <unmap_single_vma+0x620>
}

static inline pte_t native_ptep_get_and_clear(pte_t *xp)
{
#ifdef CONFIG_SMP
	return native_make_pte(xchg(&xp->pte, 0));
    233d:	31 c0                	xor    %eax,%eax
    233f:	48 87 03             	xchg   %rax,(%rbx)
    2342:	48 89 45 98          	mov    %rax,-0x68(%rbp)
}

static inline void pte_update(struct mm_struct *mm, unsigned long addr,
			      pte_t *ptep)
{
	PVOP_VCALL3(pv_mmu_ops.pte_update, mm, addr, ptep);
    2346:	48 8b 7d c0          	mov    -0x40(%rbp),%rdi
    234a:	4c 89 fe             	mov    %r15,%rsi
    234d:	48 89 da             	mov    %rbx,%rdx
    2350:	ff 14 25 00 00 00 00 	callq  *0x0
							 page->index > details->last_index))
				  continue;
			}
			ptent = ptep_get_and_clear_full(mm, addr, pte,
						tlb->fullmm);
			tlb_remove_tlb_entry(tlb, pte, addr);
    2357:	41 80 4e 18 01       	orb    $0x1,0x18(%r14)
			if (unlikely(!page))
    235c:	4d 85 e4             	test   %r12,%r12
    235f:	0f 84 76 ff ff ff    	je     22db <unmap_single_vma+0x39b>
			  continue;
			if (unlikely(details) && details->nonlinear_vma
    2365:	48 83 7d a8 00       	cmpq   $0x0,-0x58(%rbp)
    236a:	0f 85 02 03 00 00    	jne    2672 <unmap_single_vma+0x732>
				pte_t ptfile = pgoff_to_pte(page->index);
				if (pte_soft_dirty(ptent))
				  pte_file_mksoft_dirty(ptfile);
				set_pte_at(mm, addr, pte, ptfile);
			}
			if (PageAnon(page))
    2370:	41 f6 44 24 08 01    	testb  $0x1,0x8(%r12)
    2376:	0f 84 a4 01 00 00    	je     2520 <unmap_single_vma+0x5e0>
			  rss[MM_ANONPAGES]--;
    237c:	83 6d d0 01          	subl   $0x1,-0x30(%rbp)
				if (pte_young(ptent) &&
							likely(!(vma->vm_flags & VM_SEQ_READ)))
				  mark_page_accessed(page);
				rss[MM_FILEPAGES]--;
			}
			page_remove_rmap(page);
    2380:	4c 89 e7             	mov    %r12,%rdi
    2383:	e8 00 00 00 00       	callq  2388 <unmap_single_vma+0x448>
			dec_page_counter_in_ns(page,vma);
    2388:	48 8b 75 b8          	mov    -0x48(%rbp),%rsi
    238c:	4c 89 e7             	mov    %r12,%rdi
    238f:	e8 00 00 00 00       	callq  2394 <unmap_single_vma+0x454>
 *
 * Atomically reads the value of @v.
 */
static inline int atomic_read(const atomic_t *v)
{
	return (*(volatile int *)&(v)->counter);
    2394:	41 8b 44 24 18       	mov    0x18(%r12),%eax
			if (unlikely(page_mapcount(page) < 0))
    2399:	83 c0 01             	add    $0x1,%eax
    239c:	0f 88 b8 02 00 00    	js     265a <unmap_single_vma+0x71a>
			  print_bad_pte(vma, addr, ptent, page);
			force_flush = !__tlb_remove_page(tlb, page);
    23a2:	4c 89 e6             	mov    %r12,%rsi
    23a5:	4c 89 f7             	mov    %r14,%rdi
    23a8:	e8 00 00 00 00       	callq  23ad <unmap_single_vma+0x46d>
			if (force_flush)
    23ad:	85 c0                	test   %eax,%eax
    23af:	0f 85 26 ff ff ff    	jne    22db <unmap_single_vma+0x39b>
			}
			page_remove_rmap(page);
			dec_page_counter_in_ns(page,vma);
			if (unlikely(page_mapcount(page) < 0))
			  print_bad_pte(vma, addr, ptent, page);
			force_flush = !__tlb_remove_page(tlb, page);
    23b5:	bb 01 00 00 00       	mov    $0x1,%ebx

static inline void add_mm_rss_vec(struct mm_struct *mm, int *rss)
{
	int i;

	if (current->mm == mm)
    23ba:	48 8b 85 70 ff ff ff 	mov    -0x90(%rbp),%rax
    23c1:	48 8b 75 c0          	mov    -0x40(%rbp),%rsi
    23c5:	48 3b b0 a8 02 00 00 	cmp    0x2a8(%rax),%rsi
    23cc:	0f 84 36 02 00 00    	je     2608 <unmap_single_vma+0x6c8>
    23d2:	48 8b 4d c0          	mov    -0x40(%rbp),%rcx
    23d6:	48 8d 45 cc          	lea    -0x34(%rbp),%rax
    23da:	48 8d 75 d8          	lea    -0x28(%rbp),%rsi
    23de:	48 8d 91 c8 02 00 00 	lea    0x2c8(%rcx),%rdx
	  sync_mm_rss(mm);
	for (i = 0; i < NR_MM_COUNTERS; i++)
	  if (rss[i])
    23e5:	48 63 08             	movslq (%rax),%rcx
    23e8:	85 c9                	test   %ecx,%ecx
    23ea:	74 04                	je     23f0 <unmap_single_vma+0x4b0>
 *
 * Atomically adds @i to @v.
 */
static inline void atomic64_add(long i, atomic64_t *v)
{
	asm volatile(LOCK_PREFIX "addq %1,%0"
    23ec:	f0 48 01 0a          	lock add %rcx,(%rdx)
    23f0:	48 83 c0 04          	add    $0x4,%rax
    23f4:	48 83 c2 08          	add    $0x8,%rdx
{
	int i;

	if (current->mm == mm)
	  sync_mm_rss(mm);
	for (i = 0; i < NR_MM_COUNTERS; i++)
    23f8:	48 39 f0             	cmp    %rsi,%rax
    23fb:	75 e8                	jne    23e5 <unmap_single_vma+0x4a5>
	PVOP_VCALL0(pv_mmu_ops.lazy_mode.enter);
}

static inline void arch_leave_lazy_mmu_mode(void)
{
	PVOP_VCALL0(pv_mmu_ops.lazy_mode.leave);
    23fd:	ff 14 25 00 00 00 00 	callq  *0x0
	raw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \
} while (0)

static inline void spin_unlock(spinlock_t *lock)
{
	raw_spin_unlock(&lock->rlock);
    2404:	48 8b 7d a0          	mov    -0x60(%rbp),%rdi
    2408:	e8 00 00 00 00       	callq  240d <unmap_single_vma+0x4cd>
	/*
	 * mmu_gather ran out of room to batch pages, we break out of
	 * the PTE lock to avoid doing the potential expensive TLB invalidate
	 * and page-free while holding it.
	 */
	if (force_flush) {
    240d:	85 db                	test   %ebx,%ebx
    240f:	74 24                	je     2435 <unmap_single_vma+0x4f5>
}

void tlb_flush_mmu(struct mmu_gather *tlb)
{
	struct mmu_gather_batch *batch;
	if (!tlb->need_flush)
    2411:	41 f6 46 18 01       	testb  $0x1,0x18(%r14)
		/*
		 * Flush the TLB just for the previous segment,
		 * then update the range to be the remaining
		 * TLB range.
		 */
		old_end = tlb->end;
    2416:	49 8b 5e 10          	mov    0x10(%r14),%rbx
		tlb->end = addr;
    241a:	4d 89 7e 10          	mov    %r15,0x10(%r14)
}

void tlb_flush_mmu(struct mmu_gather *tlb)
{
	struct mmu_gather_batch *batch;
	if (!tlb->need_flush)
    241e:	0f 85 ac 01 00 00    	jne    25d0 <unmap_single_vma+0x690>
		tlb_flush_mmu(tlb);

		tlb->start = addr;
		tlb->end = old_end;

		if (addr != end)
    2424:	4d 39 fd             	cmp    %r15,%r13
		old_end = tlb->end;
		tlb->end = addr;

		tlb_flush_mmu(tlb);

		tlb->start = addr;
    2427:	4d 89 7e 08          	mov    %r15,0x8(%r14)
		tlb->end = old_end;
    242b:	49 89 5e 10          	mov    %rbx,0x10(%r14)

		if (addr != end)
    242f:	0f 85 f3 fd ff ff    	jne    2228 <unmap_single_vma+0x2e8>
		 */
		if (pmd_none_or_trans_huge_or_clear_bad(pmd))
		  goto next;
		next = zap_pte_range(tlb, vma, pmd, addr, next, details);
next:
		cond_resched();
    2435:	e8 00 00 00 00       	callq  243a <unmap_single_vma+0x4fa>
	} while (pmd++, addr = next, addr != end);
    243a:	48 83 45 90 08       	addq   $0x8,-0x70(%rbp)
    243f:	4c 39 bd 78 ff ff ff 	cmp    %r15,-0x88(%rbp)
    2446:	0f 85 74 fd ff ff    	jne    21c0 <unmap_single_vma+0x280>
    244c:	4d 89 fc             	mov    %r15,%r12
    244f:	4d 89 f7             	mov    %r14,%r15
	do {
		next = pud_addr_end(addr, end);
		if (pud_none_or_clear_bad(pud))
		  continue;
		next = zap_pmd_range(tlb, vma, pud, addr, next, details);
	} while (pud++, addr = next, addr != end);
    2452:	48 83 85 58 ff ff ff 	addq   $0x8,-0xa8(%rbp)
    2459:	08 
    245a:	4c 39 a5 60 ff ff ff 	cmp    %r12,-0xa0(%rbp)
    2461:	0f 85 a1 fc ff ff    	jne    2108 <unmap_single_vma+0x1c8>
    2467:	4d 89 e6             	mov    %r12,%r14
	do {
		next = pgd_addr_end(addr, end);
		if (pgd_none_or_clear_bad(pgd))
		  continue;
		next = zap_pud_range(tlb, vma, pgd, addr, next, details);
	} while (pgd++, addr = next, addr != end);
    246a:	48 83 85 40 ff ff ff 	addq   $0x8,-0xc0(%rbp)
    2471:	08 
    2472:	4c 39 b5 48 ff ff ff 	cmp    %r14,-0xb8(%rbp)
    2479:	0f 85 bd fb ff ff    	jne    203c <unmap_single_vma+0xfc>
	tlb_end_vma(tlb, vma);
	mem_cgroup_uncharge_end();
    247f:	e8 00 00 00 00       	callq  2484 <unmap_single_vma+0x544>
				i_mmap_unlock_write(vma->vm_file->f_mapping);
			}
		} else
		  unmap_page_range(tlb, vma, start, end, details);
	}
}
    2484:	48 81 c4 a0 00 00 00 	add    $0xa0,%rsp
    248b:	5b                   	pop    %rbx
    248c:	41 5c                	pop    %r12
    248e:	41 5d                	pop    %r13
    2490:	41 5e                	pop    %r14
    2492:	41 5f                	pop    %r15
    2494:	5d                   	pop    %rbp
    2495:	c3                   	retq   
    2496:	66 2e 0f 1f 84 00 00 	nopw   %cs:0x0(%rax,%rax,1)
    249d:	00 00 00 

static inline pte_t pte_clear_flags(pte_t pte, pteval_t clear)
{
	pteval_t v = native_pte_val(pte);

	return native_make_pte(v & ~clear);
    24a0:	4c 89 e7             	mov    %r12,%rdi
    24a3:	48 be 00 00 00 00 00 	movabs $0x4000000000000,%rsi
    24aa:	00 04 00 
    24ad:	40 80 e7 7f          	and    $0x7f,%dil
    24b1:	81 e2 80 00 00 00    	and    $0x80,%edx
    24b7:	49 0f 44 fc          	cmove  %r12,%rdi
    24bb:	48 89 f8             	mov    %rdi,%rax
    24be:	48 83 e0 ef          	and    $0xffffffffffffffef,%rax
    24c2:	48 85 f7             	test   %rsi,%rdi
    24c5:	48 0f 45 f8          	cmovne %rax,%rdi

	if (sizeof(pteval_t) > sizeof(long))
		ret = PVOP_CALLEE2(pteval_t, pv_mmu_ops.pte_val,
				   pte.pte, (u64)pte.pte >> 32);
	else
		ret = PVOP_CALLEE1(pteval_t, pv_mmu_ops.pte_val,
    24c9:	ff 14 25 00 00 00 00 	callq  *0x0
	if (pte_swp_soft_dirty(pte))	
	  pte = pte_swp_clear_soft_dirty(pte);
	if(	pte.pte & _PAGE_NCACHE)
	  pte= pte_clear_flags(pte, _PAGE_CACHE_UC_MINUS);
	arch_entry = __pte_to_swp_entry(pte);
	return swp_entry(__swp_type(arch_entry), __swp_offset(arch_entry));
    24d0:	48 89 c7             	mov    %rax,%rdi
    24d3:	48 c1 e8 09          	shr    $0x9,%rax
    24d7:	48 d1 ef             	shr    %rdi
    24da:	83 e7 1f             	and    $0x1f,%edi
 */
static inline swp_entry_t swp_entry(unsigned long type, pgoff_t offset)
{
	swp_entry_t ret;

	ret.val = (type << SWP_TYPE_SHIFT(ret)) |
    24dd:	48 c1 e7 39          	shl    $0x39,%rdi
    24e1:	48 09 c7             	or     %rax,%rdi
 * Extract the `type' field from a swp_entry_t.  The swp_entry_t is in
 * arch-independent format
 */
static inline unsigned swp_type(swp_entry_t entry)
{
	return (entry.val  >> SWP_TYPE_SHIFT(entry));
    24e4:	48 89 f8             	mov    %rdi,%rax
    24e7:	48 c1 e8 39          	shr    $0x39,%rax
			if (unlikely(!(vma->vm_flags & VM_NONLINEAR)))
			  print_bad_pte(vma, addr, ptent, NULL);
		} else {
			swp_entry_t entry = pte_to_swp_entry(ptent);

			if (!non_swap_entry(entry))
    24eb:	83 f8 1c             	cmp    $0x1c,%eax
    24ee:	0f 87 84 00 00 00    	ja     2578 <unmap_single_vma+0x638>
			  rss[MM_SWAPENTS]--;
    24f4:	83 6d d4 01          	subl   $0x1,-0x2c(%rbp)
				if (PageAnon(page))
				  rss[MM_ANONPAGES]--;
				else
				  rss[MM_FILEPAGES]--;
			}
			if (unlikely(!free_swap_and_cache(entry)))
    24f8:	e8 00 00 00 00       	callq  24fd <unmap_single_vma+0x5bd>
    24fd:	85 c0                	test   %eax,%eax
    24ff:	0f 85 b9 fd ff ff    	jne    22be <unmap_single_vma+0x37e>
			  print_bad_pte(vma, addr, ptent, NULL);
    2505:	48 8b 7d b8          	mov    -0x48(%rbp),%rdi
    2509:	31 c9                	xor    %ecx,%ecx
    250b:	4c 89 e2             	mov    %r12,%rdx
    250e:	4c 89 fe             	mov    %r15,%rsi
    2511:	e8 5a e2 ff ff       	callq  770 <print_bad_pte>
    2516:	e9 a3 fd ff ff       	jmpq   22be <unmap_single_vma+0x37e>
    251b:	0f 1f 44 00 00       	nopl   0x0(%rax,%rax,1)
    2520:	48 ba ff 0f 00 00 00 	movabs $0xffffc00000000fff,%rdx
    2527:	c0 ff ff 
    252a:	48 23 55 98          	and    -0x68(%rbp),%rdx
				set_pte_at(mm, addr, pte, ptfile);
			}
			if (PageAnon(page))
			  rss[MM_ANONPAGES]--;
			else {
				if (pte_dirty(ptent))
    252e:	f6 c2 40             	test   $0x40,%dl
    2531:	0f 85 a9 00 00 00    	jne    25e0 <unmap_single_vma+0x6a0>
				  set_page_dirty(page);
				if (pte_young(ptent) &&
    2537:	83 e2 20             	and    $0x20,%edx
    253a:	74 12                	je     254e <unmap_single_vma+0x60e>
    253c:	48 8b 45 b8          	mov    -0x48(%rbp),%rax
    2540:	f6 40 51 80          	testb  $0x80,0x51(%rax)
    2544:	75 08                	jne    254e <unmap_single_vma+0x60e>
							likely(!(vma->vm_flags & VM_SEQ_READ)))
				  mark_page_accessed(page);
    2546:	4c 89 e7             	mov    %r12,%rdi
    2549:	e8 00 00 00 00       	callq  254e <unmap_single_vma+0x60e>
				rss[MM_FILEPAGES]--;
    254e:	83 6d cc 01          	subl   $0x1,-0x34(%rbp)
    2552:	e9 29 fe ff ff       	jmpq   2380 <unmap_single_vma+0x440>
    2557:	66 0f 1f 84 00 00 00 	nopw   0x0(%rax,%rax,1)
    255e:	00 00 
void early_alloc_pgt_buf(void);

/* local pte updates need not use xchg for locking */
static inline pte_t native_local_ptep_get_and_clear(pte_t *ptep)
{
	pte_t res = *ptep;
    2560:	48 8b 03             	mov    (%rbx),%rax


static inline void native_pte_clear(struct mm_struct *mm, unsigned long addr,
				    pte_t *ptep)
{
	*ptep = native_make_pte(0);
    2563:	48 c7 03 00 00 00 00 	movq   $0x0,(%rbx)
    256a:	48 89 45 98          	mov    %rax,-0x68(%rbp)
    256e:	e9 e4 fd ff ff       	jmpq   2357 <unmap_single_vma+0x417>
    2573:	0f 1f 44 00 00       	nopl   0x0(%rax,%rax,1)
			page_to_pfn(page));
}

static inline int is_migration_entry(swp_entry_t entry)
{
	return unlikely(swp_type(entry) == SWP_MIGRATION_READ ||
    2578:	83 e8 1e             	sub    $0x1e,%eax
    257b:	83 f8 01             	cmp    $0x1,%eax
    257e:	0f 87 74 ff ff ff    	ja     24f8 <unmap_single_vma+0x5b8>
 * Extract the `offset' field from a swp_entry_t.  The swp_entry_t is in
 * arch-independent format
 */
static inline pgoff_t swp_offset(swp_entry_t entry)
{
	return entry.val & SWP_OFFSET_MASK(entry);
    2584:	48 b8 ff ff ff ff ff 	movabs $0x1ffffffffffffff,%rax
    258b:	ff ff 01 
	return unlikely(swp_type(entry) == SWP_MIGRATION_WRITE);
}

static inline struct page *migration_entry_to_page(swp_entry_t entry)
{
	struct page *p = pfn_to_page(swp_offset(entry));
    258e:	48 b9 00 00 00 00 00 	movabs $0xffffea0000000000,%rcx
    2595:	ea ff ff 
 * Extract the `offset' field from a swp_entry_t.  The swp_entry_t is in
 * arch-independent format
 */
static inline pgoff_t swp_offset(swp_entry_t entry)
{
	return entry.val & SWP_OFFSET_MASK(entry);
    2598:	48 21 f8             	and    %rdi,%rax
	return unlikely(swp_type(entry) == SWP_MIGRATION_WRITE);
}

static inline struct page *migration_entry_to_page(swp_entry_t entry)
{
	struct page *p = pfn_to_page(swp_offset(entry));
    259b:	48 c1 e0 06          	shl    $0x6,%rax
    259f:	48 01 c8             	add    %rcx,%rax
    25a2:	48 8b 10             	mov    (%rax),%rdx
	/*
	 * Any use of migration entries may only occur while the
	 * corresponding page is locked
	 */
	BUG_ON(!PageLocked(p));
    25a5:	83 e2 01             	and    $0x1,%edx
    25a8:	0f 84 9b 01 00 00    	je     2749 <unmap_single_vma+0x809>
			else if (is_migration_entry(entry)) {
				struct page *page;

				page = migration_entry_to_page(entry);

				if (PageAnon(page))
    25ae:	f6 40 08 01          	testb  $0x1,0x8(%rax)
    25b2:	0f 84 88 01 00 00    	je     2740 <unmap_single_vma+0x800>
				  rss[MM_ANONPAGES]--;
    25b8:	83 6d d0 01          	subl   $0x1,-0x30(%rbp)
    25bc:	e9 37 ff ff ff       	jmpq   24f8 <unmap_single_vma+0x5b8>
    25c1:	0f 1f 80 00 00 00 00 	nopl   0x0(%rax)
			}
			if (unlikely(!free_swap_and_cache(entry)))
			  print_bad_pte(vma, addr, ptent, NULL);
		}
		pte_clear_not_present_full(mm, addr, pte, tlb->fullmm);
	} while (pte++, addr += PAGE_SIZE, addr != end);
    25c8:	31 db                	xor    %ebx,%ebx
    25ca:	e9 eb fd ff ff       	jmpq   23ba <unmap_single_vma+0x47a>
    25cf:	90                   	nop
    25d0:	4c 89 f7             	mov    %r14,%rdi
    25d3:	e8 e8 e0 ff ff       	callq  6c0 <tlb_flush_mmu.part.59>
    25d8:	e9 47 fe ff ff       	jmpq   2424 <unmap_single_vma+0x4e4>
    25dd:	0f 1f 00             	nopl   (%rax)
			}
			if (PageAnon(page))
			  rss[MM_ANONPAGES]--;
			else {
				if (pte_dirty(ptent))
				  set_page_dirty(page);
    25e0:	4c 89 e7             	mov    %r12,%rdi
    25e3:	48 89 55 a8          	mov    %rdx,-0x58(%rbp)
    25e7:	e8 00 00 00 00       	callq  25ec <unmap_single_vma+0x6ac>
    25ec:	48 8b 55 a8          	mov    -0x58(%rbp),%rdx
    25f0:	e9 42 ff ff ff       	jmpq   2537 <unmap_single_vma+0x5f7>
		pmd_clear_bad(pmd);
    25f5:	48 8b 7d 90          	mov    -0x70(%rbp),%rdi
    25f9:	e8 00 00 00 00       	callq  25fe <unmap_single_vma+0x6be>
    25fe:	66 90                	xchg   %ax,%ax
			}
			if (unlikely(!free_swap_and_cache(entry)))
			  print_bad_pte(vma, addr, ptent, NULL);
		}
		pte_clear_not_present_full(mm, addr, pte, tlb->fullmm);
	} while (pte++, addr += PAGE_SIZE, addr != end);
    2600:	4d 89 ef             	mov    %r13,%r15
    2603:	e9 2d fe ff ff       	jmpq   2435 <unmap_single_vma+0x4f5>
static inline void add_mm_rss_vec(struct mm_struct *mm, int *rss)
{
	int i;

	if (current->mm == mm)
	  sync_mm_rss(mm);
    2608:	48 89 f7             	mov    %rsi,%rdi
    260b:	e8 00 00 00 00       	callq  2610 <unmap_single_vma+0x6d0>
    2610:	e9 bd fd ff ff       	jmpq   23d2 <unmap_single_vma+0x492>
static inline int pud_none_or_clear_bad(pud_t *pud)
{
	if (pud_none(*pud))
		return 1;
	if (unlikely(pud_bad(*pud))) {
		pud_clear_bad(pud);
    2615:	48 8b bd 58 ff ff ff 	mov    -0xa8(%rbp),%rdi
    261c:	e8 00 00 00 00       	callq  2621 <unmap_single_vma+0x6e1>
			}
			if (unlikely(!free_swap_and_cache(entry)))
			  print_bad_pte(vma, addr, ptent, NULL);
		}
		pte_clear_not_present_full(mm, addr, pte, tlb->fullmm);
	} while (pte++, addr += PAGE_SIZE, addr != end);
    2621:	4c 8b a5 78 ff ff ff 	mov    -0x88(%rbp),%r12
    2628:	e9 25 fe ff ff       	jmpq   2452 <unmap_single_vma+0x512>
static inline int pgd_none_or_clear_bad(pgd_t *pgd)
{
	if (pgd_none(*pgd))
		return 1;
	if (unlikely(pgd_bad(*pgd))) {
		pgd_clear_bad(pgd);
    262d:	48 8b bd 40 ff ff ff 	mov    -0xc0(%rbp),%rdi
    2634:	e8 00 00 00 00       	callq  2639 <unmap_single_vma+0x6f9>
    2639:	4c 8b b5 60 ff ff ff 	mov    -0xa0(%rbp),%r14
    2640:	e9 25 fe ff ff       	jmpq   246a <unmap_single_vma+0x52a>
		 */
		if (unlikely(details))
		  continue;
		if (pte_file(ptent)) {
			if (unlikely(!(vma->vm_flags & VM_NONLINEAR)))
			  print_bad_pte(vma, addr, ptent, NULL);
    2645:	31 c9                	xor    %ecx,%ecx
    2647:	4c 89 e2             	mov    %r12,%rdx
    264a:	4c 89 fe             	mov    %r15,%rsi
    264d:	48 89 c7             	mov    %rax,%rdi
    2650:	e8 1b e1 ff ff       	callq  770 <print_bad_pte>
    2655:	e9 64 fc ff ff       	jmpq   22be <unmap_single_vma+0x37e>
				rss[MM_FILEPAGES]--;
			}
			page_remove_rmap(page);
			dec_page_counter_in_ns(page,vma);
			if (unlikely(page_mapcount(page) < 0))
			  print_bad_pte(vma, addr, ptent, page);
    265a:	48 8b 55 98          	mov    -0x68(%rbp),%rdx
    265e:	48 8b 7d b8          	mov    -0x48(%rbp),%rdi
    2662:	4c 89 e1             	mov    %r12,%rcx
    2665:	4c 89 fe             	mov    %r15,%rsi
    2668:	e8 03 e1 ff ff       	callq  770 <print_bad_pte>
    266d:	e9 30 fd ff ff       	jmpq   23a2 <unmap_single_vma+0x462>
			ptent = ptep_get_and_clear_full(mm, addr, pte,
						tlb->fullmm);
			tlb_remove_tlb_entry(tlb, pte, addr);
			if (unlikely(!page))
			  continue;
			if (unlikely(details) && details->nonlinear_vma
    2672:	48 8b 45 b0          	mov    -0x50(%rbp),%rax
    2676:	48 8b 10             	mov    (%rax),%rdx
    2679:	48 85 d2             	test   %rdx,%rdx
    267c:	0f 84 ee fc ff ff    	je     2370 <unmap_single_vma+0x430>
					unsigned long address)
{
	pgoff_t pgoff;
	if (unlikely(is_vm_hugetlb_page(vma)))
		return linear_hugepage_index(vma, address);
	pgoff = (address - vma->vm_start) >> PAGE_SHIFT;
    2682:	4c 89 f8             	mov    %r15,%rax
    2685:	48 2b 02             	sub    (%rdx),%rax
						&& linear_page_index(details->nonlinear_vma,
							addr) != page->index) {
    2688:	49 8b 4c 24 10       	mov    0x10(%r12),%rcx
    268d:	48 c1 e8 0c          	shr    $0xc,%rax
	pgoff += vma->vm_pgoff;
    2691:	48 03 82 a0 00 00 00 	add    0xa0(%rdx),%rax
						tlb->fullmm);
			tlb_remove_tlb_entry(tlb, pte, addr);
			if (unlikely(!page))
			  continue;
			if (unlikely(details) && details->nonlinear_vma
						&& linear_page_index(details->nonlinear_vma,
    2698:	48 39 c8             	cmp    %rcx,%rax
    269b:	0f 84 cf fc ff ff    	je     2370 <unmap_single_vma+0x430>
							addr) != page->index) {
				pte_t ptfile = pgoff_to_pte(page->index);
    26a1:	48 c1 e1 0c          	shl    $0xc,%rcx
{
	if (sizeof(pteval_t) > sizeof(long))
		/* 5 arg words */
		pv_mmu_ops.set_pte_at(mm, addr, ptep, pte);
	else
		PVOP_VCALL4(pv_mmu_ops.set_pte_at, mm, addr, ptep, pte.pte);
    26a5:	48 8b 7d c0          	mov    -0x40(%rbp),%rdi
    26a9:	4c 89 fe             	mov    %r15,%rsi
    26ac:	48 83 c9 40          	or     $0x40,%rcx
    26b0:	48 89 da             	mov    %rbx,%rdx
    26b3:	ff 14 25 00 00 00 00 	callq  *0x0
    26ba:	e9 b1 fc ff ff       	jmpq   2370 <unmap_single_vma+0x430>
    26bf:	90                   	nop

		if (pte_present(ptent)) {
			struct page *page;

			page = vm_normal_page(vma, addr, ptent);
			if (unlikely(details) && page) {
    26c0:	4d 85 e4             	test   %r12,%r12
    26c3:	0f 84 69 fc ff ff    	je     2332 <unmap_single_vma+0x3f2>
				/*
				 * unmap_shared_mapping_pages() wants to
				 * invalidate cache without truncating:
				 * unmap shared but keep private pages.
				 */
				if (details->check_mapping &&
    26c9:	48 8b 45 b0          	mov    -0x50(%rbp),%rax
    26cd:	48 8b 40 08          	mov    0x8(%rax),%rax
    26d1:	48 85 c0             	test   %rax,%rax
    26d4:	74 0b                	je     26e1 <unmap_single_vma+0x7a1>
    26d6:	49 3b 44 24 08       	cmp    0x8(%r12),%rax
    26db:	0f 85 fa fb ff ff    	jne    22db <unmap_single_vma+0x39b>
				  continue;
				/*
				 * Each page->index must be checked when
				 * invalidating or truncating nonlinear.
				 */
				if (details->nonlinear_vma &&
    26e1:	48 8b 4d b0          	mov    -0x50(%rbp),%rcx
    26e5:	48 83 39 00          	cmpq   $0x0,(%rcx)
    26e9:	0f 84 43 fc ff ff    	je     2332 <unmap_single_vma+0x3f2>
							(page->index < details->first_index ||
    26ef:	49 8b 44 24 10       	mov    0x10(%r12),%rax
				  continue;
				/*
				 * Each page->index must be checked when
				 * invalidating or truncating nonlinear.
				 */
				if (details->nonlinear_vma &&
    26f4:	48 3b 41 10          	cmp    0x10(%rcx),%rax
    26f8:	0f 82 dd fb ff ff    	jb     22db <unmap_single_vma+0x39b>
							(page->index < details->first_index ||
    26fe:	48 3b 41 18          	cmp    0x18(%rcx),%rax
    2702:	0f 87 d3 fb ff ff    	ja     22db <unmap_single_vma+0x39b>
    2708:	e9 25 fc ff ff       	jmpq   2332 <unmap_single_vma+0x3f2>
	unsigned long next;

	if (details && !details->check_mapping && !details->nonlinear_vma)
	  details = NULL;

	BUG_ON(addr >= end);
    270d:	0f 0b                	ud2    
{
	pgd_t *pgd;
	unsigned long next;

	if (details && !details->check_mapping && !details->nonlinear_vma)
	  details = NULL;
    270f:	48 83 38 00          	cmpq   $0x0,(%rax)
    2713:	48 89 c1             	mov    %rax,%rcx
    2716:	b8 00 00 00 00       	mov    $0x0,%eax
    271b:	48 0f 45 c1          	cmovne %rcx,%rax
    271f:	48 89 45 b0          	mov    %rax,-0x50(%rbp)
    2723:	e9 b4 f8 ff ff       	jmpq   1fdc <unmap_single_vma+0x9c>

	if (vma->vm_file)
	  uprobe_munmap(vma, start, end);

	if (unlikely(vma->vm_flags & VM_PFNMAP))
	  untrack_pfn(vma, 0, 0);
    2728:	31 d2                	xor    %edx,%edx
    272a:	31 f6                	xor    %esi,%esi
    272c:	48 89 c7             	mov    %rax,%rdi
    272f:	e8 00 00 00 00       	callq  2734 <unmap_single_vma+0x7f4>
    2734:	e9 82 f8 ff ff       	jmpq   1fbb <unmap_single_vma+0x7b>
    2739:	0f 1f 80 00 00 00 00 	nopl   0x0(%rax)
				page = migration_entry_to_page(entry);

				if (PageAnon(page))
				  rss[MM_ANONPAGES]--;
				else
				  rss[MM_FILEPAGES]--;
    2740:	83 6d cc 01          	subl   $0x1,-0x34(%rbp)
    2744:	e9 af fd ff ff       	jmpq   24f8 <unmap_single_vma+0x5b8>
    2749:	0f 1f 80 00 00 00 00 	nopl   0x0(%rax)
    2750:	e8 00 00 00 00       	callq  2755 <unmap_single_vma+0x815>
    2755:	66 66 2e 0f 1f 84 00 	data32 nopw %cs:0x0(%rax,%rax,1)
    275c:	00 00 00 00 

0000000000002760 <zap_page_range_single>:
 *
 * The range must fit into one VMA.
 */
static void zap_page_range_single(struct vm_area_struct *vma, unsigned long address,
			unsigned long size, struct zap_details *details)
{
    2760:	e8 00 00 00 00       	callq  2765 <zap_page_range_single+0x5>
    2765:	55                   	push   %rbp
    2766:	48 89 e5             	mov    %rsp,%rbp
    2769:	41 57                	push   %r15
    276b:	49 89 cf             	mov    %rcx,%r15
    276e:	41 56                	push   %r14
    2770:	49 89 fe             	mov    %rdi,%r14
    2773:	41 55                	push   %r13
	struct mm_struct *mm = vma->vm_mm;
	struct mmu_gather tlb;
	unsigned long end = address + size;
    2775:	4c 8d 2c 16          	lea    (%rsi,%rdx,1),%r13
 *
 * The range must fit into one VMA.
 */
static void zap_page_range_single(struct vm_area_struct *vma, unsigned long address,
			unsigned long size, struct zap_details *details)
{
    2779:	41 54                	push   %r12
    277b:	49 89 f4             	mov    %rsi,%r12
    277e:	53                   	push   %rbx
    277f:	48 83 c4 80          	add    $0xffffffffffffff80,%rsp
	struct mm_struct *mm = vma->vm_mm;
    2783:	48 8b 5f 40          	mov    0x40(%rdi),%rbx
	struct mmu_gather tlb;
	unsigned long end = address + size;

	lru_add_drain();
    2787:	e8 00 00 00 00       	callq  278c <zap_page_range_single+0x2c>
void tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm, unsigned long start, unsigned long end)
{
	tlb->mm = mm;

	/* Is it from 0 to ~0? */
	tlb->fullmm     = !(start | (end+1));
    278c:	49 8d 45 01          	lea    0x1(%r13),%rax
	tlb->need_flush_all = 0;
	tlb->start	= start;
    2790:	4c 89 a5 60 ff ff ff 	mov    %r12,-0xa0(%rbp)
	tlb->end	= end;
    2797:	4c 89 ad 68 ff ff ff 	mov    %r13,-0x98(%rbp)
 *	tear-down from @mm. The @fullmm argument is used when @mm is without
 *	users and we're going to destroy the full address space (exit/execve).
 */
void tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm, unsigned long start, unsigned long end)
{
	tlb->mm = mm;
    279e:	48 89 9d 58 ff ff ff 	mov    %rbx,-0xa8(%rbp)
	tlb->fullmm     = !(start | (end+1));
	tlb->need_flush_all = 0;
	tlb->start	= start;
	tlb->end	= end;
	tlb->need_flush = 0;
	tlb->local.next = NULL;
    27a5:	48 c7 45 80 00 00 00 	movq   $0x0,-0x80(%rbp)
    27ac:	00 
void tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm, unsigned long start, unsigned long end)
{
	tlb->mm = mm;

	/* Is it from 0 to ~0? */
	tlb->fullmm     = !(start | (end+1));
    27ad:	4c 09 e0             	or     %r12,%rax
    27b0:	0f b6 85 70 ff ff ff 	movzbl -0x90(%rbp),%eax
	tlb->need_flush_all = 0;
	tlb->start	= start;
	tlb->end	= end;
	tlb->need_flush = 0;
	tlb->local.next = NULL;
	tlb->local.nr   = 0;
    27b7:	c7 45 88 00 00 00 00 	movl   $0x0,-0x78(%rbp)
void tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm, unsigned long start, unsigned long end)
{
	tlb->mm = mm;

	/* Is it from 0 to ~0? */
	tlb->fullmm     = !(start | (end+1));
    27be:	0f 94 c2             	sete   %dl
	tlb->start	= start;
	tlb->end	= end;
	tlb->need_flush = 0;
	tlb->local.next = NULL;
	tlb->local.nr   = 0;
	tlb->local.max  = ARRAY_SIZE(tlb->__pages);
    27c1:	c7 45 8c 08 00 00 00 	movl   $0x8,-0x74(%rbp)
	tlb->active     = &tlb->local;
	tlb->batch_count = 0;
    27c8:	c7 45 d0 00 00 00 00 	movl   $0x0,-0x30(%rbp)
void tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm, unsigned long start, unsigned long end)
{
	tlb->mm = mm;

	/* Is it from 0 to ~0? */
	tlb->fullmm     = !(start | (end+1));
    27cf:	01 d2                	add    %edx,%edx
    27d1:	83 e0 f8             	and    $0xfffffff8,%eax
	tlb->need_flush_all = 0;
	tlb->start	= start;
	tlb->end	= end;
	tlb->need_flush = 0;
    27d4:	09 d0                	or     %edx,%eax
 * Atomically reads the value of @v.
 * Doesn't imply a read memory barrier.
 */
static inline long atomic64_read(const atomic64_t *v)
{
	return (*(volatile long *)&(v)->counter);
    27d6:	48 8b 93 c8 02 00 00 	mov    0x2c8(%rbx),%rdx
    27dd:	48 8b 8b d0 02 00 00 	mov    0x2d0(%rbx),%rcx
    27e4:	88 85 70 ff ff ff    	mov    %al,-0x90(%rbp)
	tlb->local.next = NULL;
	tlb->local.nr   = 0;
	tlb->local.max  = ARRAY_SIZE(tlb->__pages);
	tlb->active     = &tlb->local;
    27ea:	48 8d 85 58 ff ff ff 	lea    -0xa8(%rbp),%rax
    27f1:	48 83 c0 28          	add    $0x28,%rax
    27f5:	48 89 85 78 ff ff ff 	mov    %rax,-0x88(%rbp)
    27fc:	31 c0                	xor    %eax,%eax
    27fe:	48 85 c9             	test   %rcx,%rcx
    2801:	48 0f 48 c8          	cmovs  %rax,%rcx
    2805:	48 85 d2             	test   %rdx,%rdx
    2808:	48 0f 49 c2          	cmovns %rdx,%rax
	atomic_long_dec(&mm->rss_stat.count[member]);
}

static inline unsigned long get_mm_rss(struct mm_struct *mm)
{
	return get_mm_counter(mm, MM_FILEPAGES) +
    280c:	48 01 c8             	add    %rcx,%rax

static inline void update_hiwater_rss(struct mm_struct *mm)
{
	unsigned long _rss = get_mm_rss(mm);

	if ((mm)->hiwater_rss < _rss)
    280f:	48 3b 83 b8 00 00 00 	cmp    0xb8(%rbx),%rax
    2816:	76 07                	jbe    281f <zap_page_range_single+0xbf>
		(mm)->hiwater_rss = _rss;
    2818:	48 89 83 b8 00 00 00 	mov    %rax,0xb8(%rbx)
}

static inline void mmu_notifier_invalidate_range_start(struct mm_struct *mm,
				  unsigned long start, unsigned long end)
{
	if (mm_has_notifiers(mm))
    281f:	48 83 bb a0 03 00 00 	cmpq   $0x0,0x3a0(%rbx)
    2826:	00 
    2827:	75 43                	jne    286c <zap_page_range_single+0x10c>

	lru_add_drain();
	tlb_gather_mmu(&tlb, mm, address, end);
	update_hiwater_rss(mm);
	mmu_notifier_invalidate_range_start(mm, address, end);
	unmap_single_vma(&tlb, vma, address, end, details);
    2829:	48 8d bd 58 ff ff ff 	lea    -0xa8(%rbp),%rdi
    2830:	4d 89 f8             	mov    %r15,%r8
    2833:	4c 89 e9             	mov    %r13,%rcx
    2836:	4c 89 e2             	mov    %r12,%rdx
    2839:	4c 89 f6             	mov    %r14,%rsi
    283c:	e8 ff f6 ff ff       	callq  1f40 <unmap_single_vma>
}

static inline void mmu_notifier_invalidate_range_end(struct mm_struct *mm,
				  unsigned long start, unsigned long end)
{
	if (mm_has_notifiers(mm))
    2841:	48 83 bb a0 03 00 00 	cmpq   $0x0,0x3a0(%rbx)
    2848:	00 
    2849:	75 31                	jne    287c <zap_page_range_single+0x11c>
	mmu_notifier_invalidate_range_end(mm, address, end);
	tlb_finish_mmu(&tlb, address, end);
    284b:	48 8d bd 58 ff ff ff 	lea    -0xa8(%rbp),%rdi
    2852:	4c 89 ea             	mov    %r13,%rdx
    2855:	4c 89 e6             	mov    %r12,%rsi
    2858:	e8 00 00 00 00       	callq  285d <zap_page_range_single+0xfd>
}
    285d:	48 83 ec 80          	sub    $0xffffffffffffff80,%rsp
    2861:	5b                   	pop    %rbx
    2862:	41 5c                	pop    %r12
    2864:	41 5d                	pop    %r13
    2866:	41 5e                	pop    %r14
    2868:	41 5f                	pop    %r15
    286a:	5d                   	pop    %rbp
    286b:	c3                   	retq   

static inline void mmu_notifier_invalidate_range_start(struct mm_struct *mm,
				  unsigned long start, unsigned long end)
{
	if (mm_has_notifiers(mm))
		__mmu_notifier_invalidate_range_start(mm, start, end);
    286c:	4c 89 ea             	mov    %r13,%rdx
    286f:	4c 89 e6             	mov    %r12,%rsi
    2872:	48 89 df             	mov    %rbx,%rdi
    2875:	e8 00 00 00 00       	callq  287a <zap_page_range_single+0x11a>
    287a:	eb ad                	jmp    2829 <zap_page_range_single+0xc9>

static inline void mmu_notifier_invalidate_range_end(struct mm_struct *mm,
				  unsigned long start, unsigned long end)
{
	if (mm_has_notifiers(mm))
		__mmu_notifier_invalidate_range_end(mm, start, end);
    287c:	4c 89 ea             	mov    %r13,%rdx
    287f:	4c 89 e6             	mov    %r12,%rsi
    2882:	48 89 df             	mov    %rbx,%rdi
    2885:	e8 00 00 00 00       	callq  288a <zap_page_range_single+0x12a>
    288a:	eb bf                	jmp    284b <zap_page_range_single+0xeb>
    288c:	0f 1f 40 00          	nopl   0x0(%rax)

0000000000002890 <zap_vma_ptes>:
 *
 * Returns 0 if successful.
 */
int zap_vma_ptes(struct vm_area_struct *vma, unsigned long address,
			unsigned long size)
{
    2890:	e8 00 00 00 00       	callq  2895 <zap_vma_ptes+0x5>
	if (address < vma->vm_start || address + size > vma->vm_end ||
    2895:	48 39 37             	cmp    %rsi,(%rdi)
    2898:	77 26                	ja     28c0 <zap_vma_ptes+0x30>
    289a:	48 8d 04 16          	lea    (%rsi,%rdx,1),%rax
    289e:	48 3b 47 08          	cmp    0x8(%rdi),%rax
				!(vma->vm_flags & VM_PFNMAP))
	  return -1;
    28a2:	b8 ff ff ff ff       	mov    $0xffffffff,%eax
 * Returns 0 if successful.
 */
int zap_vma_ptes(struct vm_area_struct *vma, unsigned long address,
			unsigned long size)
{
	if (address < vma->vm_start || address + size > vma->vm_end ||
    28a7:	77 27                	ja     28d0 <zap_vma_ptes+0x40>
    28a9:	f6 47 51 04          	testb  $0x4,0x51(%rdi)
    28ad:	74 0e                	je     28bd <zap_vma_ptes+0x2d>
 *
 * Returns 0 if successful.
 */
int zap_vma_ptes(struct vm_area_struct *vma, unsigned long address,
			unsigned long size)
{
    28af:	55                   	push   %rbp
	if (address < vma->vm_start || address + size > vma->vm_end ||
				!(vma->vm_flags & VM_PFNMAP))
	  return -1;
	zap_page_range_single(vma, address, size, NULL);
    28b0:	31 c9                	xor    %ecx,%ecx
 *
 * Returns 0 if successful.
 */
int zap_vma_ptes(struct vm_area_struct *vma, unsigned long address,
			unsigned long size)
{
    28b2:	48 89 e5             	mov    %rsp,%rbp
	if (address < vma->vm_start || address + size > vma->vm_end ||
				!(vma->vm_flags & VM_PFNMAP))
	  return -1;
	zap_page_range_single(vma, address, size, NULL);
    28b5:	e8 a6 fe ff ff       	callq  2760 <zap_page_range_single>
	return 0;
}
    28ba:	5d                   	pop    %rbp
{
	if (address < vma->vm_start || address + size > vma->vm_end ||
				!(vma->vm_flags & VM_PFNMAP))
	  return -1;
	zap_page_range_single(vma, address, size, NULL);
	return 0;
    28bb:	31 c0                	xor    %eax,%eax
}
    28bd:	f3 c3                	repz retq 
    28bf:	90                   	nop
int zap_vma_ptes(struct vm_area_struct *vma, unsigned long address,
			unsigned long size)
{
	if (address < vma->vm_start || address + size > vma->vm_end ||
				!(vma->vm_flags & VM_PFNMAP))
	  return -1;
    28c0:	b8 ff ff ff ff       	mov    $0xffffffff,%eax
    28c5:	c3                   	retq   
    28c6:	66 2e 0f 1f 84 00 00 	nopw   %cs:0x0(%rax,%rax,1)
    28cd:	00 00 00 
    28d0:	f3 c3                	repz retq 
    28d2:	66 66 66 66 66 2e 0f 	data32 data32 data32 data32 nopw %cs:0x0(%rax,%rax,1)
    28d9:	1f 84 00 00 00 00 00 

00000000000028e0 <unmap_mapping_range>:
 * @even_cows: 1 when truncating a file, unmap even private COWed pages;
 * but 0 when invalidating pagecache, don't throw away private data.
 */
void unmap_mapping_range(struct address_space *mapping,
			loff_t const holebegin, loff_t const holelen, int even_cows)
{
    28e0:	e8 00 00 00 00       	callq  28e5 <unmap_mapping_range+0x5>
    28e5:	55                   	push   %rbp
	struct zap_details details;
	pgoff_t hba = holebegin >> PAGE_SHIFT;
	pgoff_t hlen = (holelen + PAGE_SIZE - 1) >> PAGE_SHIFT;
    28e6:	48 81 c2 ff 0f 00 00 	add    $0xfff,%rdx
 */
void unmap_mapping_range(struct address_space *mapping,
			loff_t const holebegin, loff_t const holelen, int even_cows)
{
	struct zap_details details;
	pgoff_t hba = holebegin >> PAGE_SHIFT;
    28ed:	48 c1 fe 0c          	sar    $0xc,%rsi
	pgoff_t hlen = (holelen + PAGE_SIZE - 1) >> PAGE_SHIFT;
    28f1:	48 c1 ea 0c          	shr    $0xc,%rdx
			(holebegin + holelen + PAGE_SIZE - 1) >> PAGE_SHIFT;
		if (holeend & ~(long long)ULONG_MAX)
		  hlen = ULONG_MAX - hba + 1;
	}

	details.check_mapping = even_cows? NULL: mapping;
    28f5:	31 c0                	xor    %eax,%eax
 * @even_cows: 1 when truncating a file, unmap even private COWed pages;
 * but 0 when invalidating pagecache, don't throw away private data.
 */
void unmap_mapping_range(struct address_space *mapping,
			loff_t const holebegin, loff_t const holelen, int even_cows)
{
    28f7:	48 89 e5             	mov    %rsp,%rbp
    28fa:	41 57                	push   %r15
	}

	details.check_mapping = even_cows? NULL: mapping;
	details.nonlinear_vma = NULL;
	details.first_index = hba;
	details.last_index = hba + hlen - 1;
    28fc:	48 8d 54 16 ff       	lea    -0x1(%rsi,%rdx,1),%rdx
 * @even_cows: 1 when truncating a file, unmap even private COWed pages;
 * but 0 when invalidating pagecache, don't throw away private data.
 */
void unmap_mapping_range(struct address_space *mapping,
			loff_t const holebegin, loff_t const holelen, int even_cows)
{
    2901:	41 56                	push   %r14
    2903:	41 55                	push   %r13
    2905:	41 54                	push   %r12
#define PAGECACHE_TAG_WRITEBACK	1
#define PAGECACHE_TAG_TOWRITE	2

int mapping_tagged(struct address_space *mapping, int tag);
static inline void i_mmap_lock_write(struct address_space *mapping){
 down_write(&mapping->i_mmap_rwsem);
    2907:	4c 8d 67 50          	lea    0x50(%rdi),%r12
    290b:	53                   	push   %rbx
    290c:	48 89 fb             	mov    %rdi,%rbx
    290f:	48 83 ec 20          	sub    $0x20,%rsp
			(holebegin + holelen + PAGE_SIZE - 1) >> PAGE_SHIFT;
		if (holeend & ~(long long)ULONG_MAX)
		  hlen = ULONG_MAX - hba + 1;
	}

	details.check_mapping = even_cows? NULL: mapping;
    2913:	85 c9                	test   %ecx,%ecx
	details.nonlinear_vma = NULL;
    2915:	48 c7 45 b8 00 00 00 	movq   $0x0,-0x48(%rbp)
    291c:	00 
			(holebegin + holelen + PAGE_SIZE - 1) >> PAGE_SHIFT;
		if (holeend & ~(long long)ULONG_MAX)
		  hlen = ULONG_MAX - hba + 1;
	}

	details.check_mapping = even_cows? NULL: mapping;
    291d:	48 0f 44 c7          	cmove  %rdi,%rax
	details.nonlinear_vma = NULL;
	details.first_index = hba;
	details.last_index = hba + hlen - 1;
    2921:	48 39 d6             	cmp    %rdx,%rsi
    2924:	4c 89 e7             	mov    %r12,%rdi
			(holebegin + holelen + PAGE_SIZE - 1) >> PAGE_SHIFT;
		if (holeend & ~(long long)ULONG_MAX)
		  hlen = ULONG_MAX - hba + 1;
	}

	details.check_mapping = even_cows? NULL: mapping;
    2927:	48 89 45 c0          	mov    %rax,-0x40(%rbp)
	details.nonlinear_vma = NULL;
	details.first_index = hba;
	details.last_index = hba + hlen - 1;
    292b:	48 c7 c0 ff ff ff ff 	mov    $0xffffffffffffffff,%rax
		  hlen = ULONG_MAX - hba + 1;
	}

	details.check_mapping = even_cows? NULL: mapping;
	details.nonlinear_vma = NULL;
	details.first_index = hba;
    2932:	48 89 75 c8          	mov    %rsi,-0x38(%rbp)
	details.last_index = hba + hlen - 1;
    2936:	48 0f 46 c2          	cmovbe %rdx,%rax
    293a:	48 89 45 d0          	mov    %rax,-0x30(%rbp)
    293e:	e8 00 00 00 00       	callq  2943 <unmap_mapping_range+0x63>
	if (details.last_index < details.first_index)
	  details.last_index = ULONG_MAX;

	i_mmap_lock_write(mapping);
	if (unlikely(!RB_EMPTY_ROOT(&mapping->i_mmap)))
    2943:	48 83 7b 38 00       	cmpq   $0x0,0x38(%rbx)
    2948:	75 28                	jne    2972 <unmap_mapping_range+0x92>
 * list_empty - tests whether a list is empty
 * @head: the list to test.
 */
static inline int list_empty(const struct list_head *head)
{
	return head->next == head;
    294a:	48 8b 43 40          	mov    0x40(%rbx),%rax
	  unmap_mapping_range_tree(&mapping->i_mmap, &details);
	if (unlikely(!list_empty(&mapping->i_mmap_nonlinear)))
    294e:	4c 8d 6b 40          	lea    0x40(%rbx),%r13
    2952:	49 39 c5             	cmp    %rax,%r13
    2955:	0f 85 b1 00 00 00    	jne    2a0c <unmap_mapping_range+0x12c>
}
static inline void i_mmap_unlock_write(struct address_space *mapping){
 up_write(&mapping->i_mmap_rwsem);
    295b:	4c 89 e7             	mov    %r12,%rdi
    295e:	e8 00 00 00 00       	callq  2963 <unmap_mapping_range+0x83>
	  unmap_mapping_range_list(&mapping->i_mmap_nonlinear, &details);

	i_mmap_unlock_write(mapping);
}
    2963:	48 83 c4 20          	add    $0x20,%rsp
    2967:	5b                   	pop    %rbx
    2968:	41 5c                	pop    %r12
    296a:	41 5d                	pop    %r13
    296c:	41 5e                	pop    %r14
    296e:	41 5f                	pop    %r15
    2970:	5d                   	pop    %rbp
    2971:	c3                   	retq   
			struct zap_details *details)
{
	struct vm_area_struct *vma;
	pgoff_t vba, vea, zba, zea;

	vma_interval_tree_foreach(vma, root,
    2972:	48 8b 55 d0          	mov    -0x30(%rbp),%rdx
    2976:	48 8b 75 c8          	mov    -0x38(%rbp),%rsi
	if (details.last_index < details.first_index)
	  details.last_index = ULONG_MAX;

	i_mmap_lock_write(mapping);
	if (unlikely(!RB_EMPTY_ROOT(&mapping->i_mmap)))
	  unmap_mapping_range_tree(&mapping->i_mmap, &details);
    297a:	48 8d 7b 38          	lea    0x38(%rbx),%rdi
			struct zap_details *details)
{
	struct vm_area_struct *vma;
	pgoff_t vba, vea, zba, zea;

	vma_interval_tree_foreach(vma, root,
    297e:	e8 00 00 00 00       	callq  2983 <unmap_mapping_range+0xa3>
    2983:	48 85 c0             	test   %rax,%rax
    2986:	49 89 c5             	mov    %rax,%r13
    2989:	74 bf                	je     294a <unmap_mapping_range+0x6a>
    298b:	4c 8d 75 b8          	lea    -0x48(%rbp),%r14
		if (zea > vea)
		  zea = vea;

		unmap_mapping_range_vma(vma,
					((zba - vba) << PAGE_SHIFT) + vma->vm_start,
					((zea - vba + 1) << PAGE_SHIFT) + vma->vm_start,
    298f:	41 bf 01 00 00 00    	mov    $0x1,%r15d
	zap_page_range_single(vma, start_addr, end_addr - start_addr, details);
}
static inline void unmap_mapping_range_tree(struct rb_root *root,
			struct zap_details *details)
{
	struct vm_area_struct *vma;
    2995:	49 8b 4d 00          	mov    0x0(%r13),%rcx
	return vma;
}

static inline unsigned long vma_pages(struct vm_area_struct *vma)
{
	return (vma->vm_end - vma->vm_start) >> PAGE_SHIFT;
    2999:	49 8b 45 08          	mov    0x8(%r13),%rax
		if (zea > vea)
		  zea = vea;

		unmap_mapping_range_vma(vma,
					((zba - vba) << PAGE_SHIFT) + vma->vm_start,
					((zea - vba + 1) << PAGE_SHIFT) + vma->vm_start,
    299d:	4d 89 f8             	mov    %r15,%r8
	pgoff_t vba, vea, zba, zea;

	vma_interval_tree_foreach(vma, root,
				details->first_index, details->last_index) {

		vba = vma->vm_pgoff;
    29a0:	49 8b 95 a0 00 00 00 	mov    0xa0(%r13),%rdx
		vea = vba + vma_pages(vma) - 1;
		/* Assume for now that PAGE_CACHE_SHIFT == PAGE_SHIFT */
		zba = details->first_index;
		if (zba < vba)
		  zba = vba;
		zea = details->last_index;
    29a7:	48 8b 75 d0          	mov    -0x30(%rbp),%rsi
    29ab:	48 29 c8             	sub    %rcx,%rax
    29ae:	48 c1 e8 0c          	shr    $0xc,%rax

	vma_interval_tree_foreach(vma, root,
				details->first_index, details->last_index) {

		vba = vma->vm_pgoff;
		vea = vba + vma_pages(vma) - 1;
    29b2:	48 8d 7c 02 ff       	lea    -0x1(%rdx,%rax,1),%rdi
		/* Assume for now that PAGE_CACHE_SHIFT == PAGE_SHIFT */
		zba = details->first_index;
    29b7:	48 8b 45 c8          	mov    -0x38(%rbp),%rax
    29bb:	48 39 d0             	cmp    %rdx,%rax
    29be:	48 0f 42 c2          	cmovb  %rdx,%rax
		if (zea > vea)
		  zea = vea;

		unmap_mapping_range_vma(vma,
					((zba - vba) << PAGE_SHIFT) + vma->vm_start,
					((zea - vba + 1) << PAGE_SHIFT) + vma->vm_start,
    29c2:	49 29 d0             	sub    %rdx,%r8
		zea = details->last_index;
		if (zea > vea)
		  zea = vea;

		unmap_mapping_range_vma(vma,
					((zba - vba) << PAGE_SHIFT) + vma->vm_start,
    29c5:	48 29 d0             	sub    %rdx,%rax
					((zea - vba + 1) << PAGE_SHIFT) + vma->vm_start,
    29c8:	4c 89 c2             	mov    %r8,%rdx
		zea = details->last_index;
		if (zea > vea)
		  zea = vea;

		unmap_mapping_range_vma(vma,
					((zba - vba) << PAGE_SHIFT) + vma->vm_start,
    29cb:	48 c1 e0 0c          	shl    $0xc,%rax
    29cf:	48 39 f7             	cmp    %rsi,%rdi
    29d2:	48 0f 46 f7          	cmovbe %rdi,%rsi
}
static void unmap_mapping_range_vma(struct vm_area_struct *vma,
			unsigned long start_addr, unsigned long end_addr,
			struct zap_details *details)
{
	zap_page_range_single(vma, start_addr, end_addr - start_addr, details);
    29d6:	4c 89 ef             	mov    %r13,%rdi
		if (zea > vea)
		  zea = vea;

		unmap_mapping_range_vma(vma,
					((zba - vba) << PAGE_SHIFT) + vma->vm_start,
					((zea - vba + 1) << PAGE_SHIFT) + vma->vm_start,
    29d9:	48 01 f2             	add    %rsi,%rdx
		  zba = vba;
		zea = details->last_index;
		if (zea > vea)
		  zea = vea;

		unmap_mapping_range_vma(vma,
    29dc:	48 8d 34 01          	lea    (%rcx,%rax,1),%rsi
}
static void unmap_mapping_range_vma(struct vm_area_struct *vma,
			unsigned long start_addr, unsigned long end_addr,
			struct zap_details *details)
{
	zap_page_range_single(vma, start_addr, end_addr - start_addr, details);
    29e0:	4c 89 f1             	mov    %r14,%rcx
		if (zea > vea)
		  zea = vea;

		unmap_mapping_range_vma(vma,
					((zba - vba) << PAGE_SHIFT) + vma->vm_start,
					((zea - vba + 1) << PAGE_SHIFT) + vma->vm_start,
    29e3:	48 c1 e2 0c          	shl    $0xc,%rdx
}
static void unmap_mapping_range_vma(struct vm_area_struct *vma,
			unsigned long start_addr, unsigned long end_addr,
			struct zap_details *details)
{
	zap_page_range_single(vma, start_addr, end_addr - start_addr, details);
    29e7:	48 29 c2             	sub    %rax,%rdx
    29ea:	e8 71 fd ff ff       	callq  2760 <zap_page_range_single>
			struct zap_details *details)
{
	struct vm_area_struct *vma;
	pgoff_t vba, vea, zba, zea;

	vma_interval_tree_foreach(vma, root,
    29ef:	48 8b 55 d0          	mov    -0x30(%rbp),%rdx
    29f3:	48 8b 75 c8          	mov    -0x38(%rbp),%rsi
    29f7:	4c 89 ef             	mov    %r13,%rdi
    29fa:	e8 00 00 00 00       	callq  29ff <unmap_mapping_range+0x11f>
    29ff:	48 85 c0             	test   %rax,%rax
    2a02:	49 89 c5             	mov    %rax,%r13
    2a05:	75 8e                	jne    2995 <unmap_mapping_range+0xb5>
    2a07:	e9 3e ff ff ff       	jmpq   294a <unmap_mapping_range+0x6a>
	 * In nonlinear VMAs there is no correspondence between virtual address
	 * offset and file offset.  So we must perform an exhaustive search
	 * across *all* the pages in each nonlinear VMA, not just the pages
	 * whose virtual address lies outside the file truncation point.
	 */
	list_for_each_entry(vma, head, shared.nonlinear) {
    2a0c:	48 8d 58 a8          	lea    -0x58(%rax),%rbx
    2a10:	4c 8d 75 b8          	lea    -0x48(%rbp),%r14
		details->nonlinear_vma = vma;
    2a14:	48 89 5d b8          	mov    %rbx,-0x48(%rbp)
}
static void unmap_mapping_range_vma(struct vm_area_struct *vma,
			unsigned long start_addr, unsigned long end_addr,
			struct zap_details *details)
{
	zap_page_range_single(vma, start_addr, end_addr - start_addr, details);
    2a18:	48 8b 53 08          	mov    0x8(%rbx),%rdx
    2a1c:	48 89 df             	mov    %rbx,%rdi
	 * across *all* the pages in each nonlinear VMA, not just the pages
	 * whose virtual address lies outside the file truncation point.
	 */
	list_for_each_entry(vma, head, shared.nonlinear) {
		details->nonlinear_vma = vma;
		unmap_mapping_range_vma(vma, vma->vm_start, vma->vm_end, details);
    2a1f:	48 8b 33             	mov    (%rbx),%rsi
}
static void unmap_mapping_range_vma(struct vm_area_struct *vma,
			unsigned long start_addr, unsigned long end_addr,
			struct zap_details *details)
{
	zap_page_range_single(vma, start_addr, end_addr - start_addr, details);
    2a22:	4c 89 f1             	mov    %r14,%rcx
    2a25:	48 29 f2             	sub    %rsi,%rdx
    2a28:	e8 33 fd ff ff       	callq  2760 <zap_page_range_single>
	 * In nonlinear VMAs there is no correspondence between virtual address
	 * offset and file offset.  So we must perform an exhaustive search
	 * across *all* the pages in each nonlinear VMA, not just the pages
	 * whose virtual address lies outside the file truncation point.
	 */
	list_for_each_entry(vma, head, shared.nonlinear) {
    2a2d:	48 8b 43 58          	mov    0x58(%rbx),%rax
    2a31:	49 39 c5             	cmp    %rax,%r13
    2a34:	48 8d 58 a8          	lea    -0x58(%rax),%rbx
    2a38:	75 da                	jne    2a14 <unmap_mapping_range+0x134>
    2a3a:	e9 1c ff ff ff       	jmpq   295b <unmap_mapping_range+0x7b>
    2a3f:	90                   	nop

0000000000002a40 <copy_pte_range>:
}

int copy_pte_range(struct mm_struct *dst_mm, struct mm_struct *src_mm,
			pmd_t *dst_pmd, pmd_t *src_pmd, struct vm_area_struct *vma,
			unsigned long addr, unsigned long end)
{
    2a40:	e8 00 00 00 00       	callq  2a45 <copy_pte_range+0x5>
    2a45:	55                   	push   %rbp
    2a46:	48 89 f8             	mov    %rdi,%rax
			{
				printk("from copy one pte, pte=%lx",pte.pte);
				return entry.val;
			}
			/* make sure dst_mm is on swapoff's mmlist. */
			if (unlikely(list_empty(&dst_mm->mmlist))) {
    2a49:	48 05 a8 00 00 00    	add    $0xa8,%rax
}

int copy_pte_range(struct mm_struct *dst_mm, struct mm_struct *src_mm,
			pmd_t *dst_pmd, pmd_t *src_pmd, struct vm_area_struct *vma,
			unsigned long addr, unsigned long end)
{
    2a4f:	48 89 e5             	mov    %rsp,%rbp
    2a52:	41 57                	push   %r15
	pte_t *orig_src_pte, *orig_dst_pte;
	pte_t *src_pte, *dst_pte;
	spinlock_t *src_ptl, *dst_ptl;
	int progress = 0;
    2a54:	45 31 ff             	xor    %r15d,%r15d
}

int copy_pte_range(struct mm_struct *dst_mm, struct mm_struct *src_mm,
			pmd_t *dst_pmd, pmd_t *src_pmd, struct vm_area_struct *vma,
			unsigned long addr, unsigned long end)
{
    2a57:	41 56                	push   %r14
    2a59:	4d 89 ce             	mov    %r9,%r14
    2a5c:	41 55                	push   %r13
    2a5e:	41 54                	push   %r12
    2a60:	53                   	push   %rbx
    2a61:	48 81 ec 88 00 00 00 	sub    $0x88,%rsp
			{
				printk("from copy one pte, pte=%lx",pte.pte);
				return entry.val;
			}
			/* make sure dst_mm is on swapoff's mmlist. */
			if (unlikely(list_empty(&dst_mm->mmlist))) {
    2a68:	48 89 85 58 ff ff ff 	mov    %rax,-0xa8(%rbp)
				spin_lock(&mmlist_lock);
				if (list_empty(&dst_mm->mmlist))
				  list_add(&dst_mm->mmlist,
    2a6f:	48 89 f0             	mov    %rsi,%rax
    2a72:	48 05 a8 00 00 00    	add    $0xa8,%rax
}

int copy_pte_range(struct mm_struct *dst_mm, struct mm_struct *src_mm,
			pmd_t *dst_pmd, pmd_t *src_pmd, struct vm_area_struct *vma,
			unsigned long addr, unsigned long end)
{
    2a78:	48 89 4d 90          	mov    %rcx,-0x70(%rbp)
    2a7c:	48 89 7d a0          	mov    %rdi,-0x60(%rbp)
DECLARE_PER_CPU(unsigned long, kernel_stack);

static inline struct thread_info *current_thread_info(void)
{
	struct thread_info *ti;
	ti = (void *)(this_cpu_read_stable(kernel_stack) +
    2a80:	65 48 8b 0c 25 00 00 	mov    %gs:0x0,%rcx
    2a87:	00 00 
			}
			/* make sure dst_mm is on swapoff's mmlist. */
			if (unlikely(list_empty(&dst_mm->mmlist))) {
				spin_lock(&mmlist_lock);
				if (list_empty(&dst_mm->mmlist))
				  list_add(&dst_mm->mmlist,
    2a89:	48 89 85 50 ff ff ff 	mov    %rax,-0xb0(%rbp)
}

int copy_pte_range(struct mm_struct *dst_mm, struct mm_struct *src_mm,
			pmd_t *dst_pmd, pmd_t *src_pmd, struct vm_area_struct *vma,
			unsigned long addr, unsigned long end)
{
    2a90:	48 89 b5 68 ff ff ff 	mov    %rsi,-0x98(%rbp)
    2a97:	65 48 8b 04 25 00 00 	mov    %gs:0x0,%rax
    2a9e:	00 00 
    2aa0:	48 89 55 98          	mov    %rdx,-0x68(%rbp)
    2aa4:	4c 89 45 b8          	mov    %r8,-0x48(%rbp)
	pte_t *orig_src_pte, *orig_dst_pte;
	pte_t *src_pte, *dst_pte;
	spinlock_t *src_ptl, *dst_ptl;
	int progress = 0;
	int rss[NR_MM_COUNTERS];
	swp_entry_t entry = (swp_entry_t){0};
    2aa8:	48 c7 45 c0 00 00 00 	movq   $0x0,-0x40(%rbp)
    2aaf:	00 
    2ab0:	48 89 4d 80          	mov    %rcx,-0x80(%rbp)
    2ab4:	48 89 85 60 ff ff ff 	mov    %rax,-0xa0(%rbp)
    2abb:	0f 1f 44 00 00       	nopl   0x0(%rax,%rax,1)
    2ac0:	48 8b 45 98          	mov    -0x68(%rbp),%rax
	return 0;
}

static inline void init_rss_vec(int *rss)
{
	memset(rss, 0, sizeof(int) * NR_MM_COUNTERS);
    2ac4:	48 c7 45 cc 00 00 00 	movq   $0x0,-0x34(%rbp)
    2acb:	00 
    2acc:	c7 45 d4 00 00 00 00 	movl   $0x0,-0x2c(%rbp)
    2ad3:	48 8b 00             	mov    (%rax),%rax
	swp_entry_t entry = (swp_entry_t){0};

again:
	init_rss_vec(rss);

	dst_pte = pte_alloc_map_lock(dst_mm, dst_pmd, addr, &dst_ptl);
    2ad6:	48 85 c0             	test   %rax,%rax
    2ad9:	0f 84 d4 03 00 00    	je     2eb3 <copy_pte_range+0x473>
    2adf:	48 89 c7             	mov    %rax,%rdi

	if (sizeof(pmdval_t) > sizeof(long))
		ret =  PVOP_CALLEE2(pmdval_t, pv_mmu_ops.pmd_val,
				    pmd.pmd, (u64)pmd.pmd >> 32);
	else
		ret =  PVOP_CALLEE1(pmdval_t, pv_mmu_ops.pmd_val,
    2ae2:	ff 14 25 00 00 00 00 	callq  *0x0
}
#endif /* ALLOC_SPLIT_PTLOCKS */

static inline spinlock_t *pte_lockptr(struct mm_struct *mm, pmd_t *pmd)
{
	return ptlock_ptr(pmd_page(*pmd));
    2ae9:	48 bb 00 f0 ff ff ff 	movabs $0x3ffffffff000,%rbx
    2af0:	3f 00 00 
    2af3:	49 bd 00 00 00 00 00 	movabs $0xffffea0000000000,%r13
    2afa:	ea ff ff 
    2afd:	48 21 d8             	and    %rbx,%rax
    2b00:	48 c1 e8 06          	shr    $0x6,%rax
    2b04:	4a 8b 44 28 30       	mov    0x30(%rax,%r13,1),%rax
    2b09:	48 89 c1             	mov    %rax,%rcx
    2b0c:	48 89 85 78 ff ff ff 	mov    %rax,-0x88(%rbp)
    2b13:	48 8b 45 98          	mov    -0x68(%rbp),%rax
    2b17:	48 8b 38             	mov    (%rax),%rdi
    2b1a:	ff 14 25 00 00 00 00 	callq  *0x0
 * this function returns the index of the entry in the pte page which would
 * control the given virtual address
 */
static inline unsigned long pte_index(unsigned long address)
{
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
    2b21:	4c 89 f2             	mov    %r14,%rdx
	return (unsigned long)native_pmd_val(pmd) == 0;
}

static inline unsigned long pmd_page_vaddr(pmd_t pmd)
{
	return (unsigned long)__va(pmd_val(pmd) & PTE_PFN_MASK);
    2b24:	49 bc 00 00 00 00 00 	movabs $0xffff880000000000,%r12
    2b2b:	88 ff ff 
    2b2e:	48 21 d8             	and    %rbx,%rax
 * this function returns the index of the entry in the pte page which would
 * control the given virtual address
 */
static inline unsigned long pte_index(unsigned long address)
{
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
    2b31:	48 c1 ea 09          	shr    $0x9,%rdx
	raw_spin_lock_init(&(_lock)->rlock);		\
} while (0)

static inline void spin_lock(spinlock_t *lock)
{
	raw_spin_lock(&lock->rlock);
    2b35:	48 89 cf             	mov    %rcx,%rdi
}

static inline pte_t *pte_offset_kernel(pmd_t *pmd, unsigned long address)
{
	return (pte_t *)pmd_page_vaddr(*pmd) + pte_index(address);
    2b38:	81 e2 f8 0f 00 00    	and    $0xff8,%edx
	return (unsigned long)native_pmd_val(pmd) == 0;
}

static inline unsigned long pmd_page_vaddr(pmd_t pmd)
{
	return (unsigned long)__va(pmd_val(pmd) & PTE_PFN_MASK);
    2b3e:	49 01 d4             	add    %rdx,%r12
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
}

static inline pte_t *pte_offset_kernel(pmd_t *pmd, unsigned long address)
{
	return (pte_t *)pmd_page_vaddr(*pmd) + pte_index(address);
    2b41:	4c 01 e0             	add    %r12,%rax
    2b44:	48 89 45 b0          	mov    %rax,-0x50(%rbp)
    2b48:	e8 00 00 00 00       	callq  2b4d <copy_pte_range+0x10d>
	if (!dst_pte)
    2b4d:	48 83 7d b0 00       	cmpq   $0x0,-0x50(%rbp)
    2b52:	0f 84 c1 02 00 00    	je     2e19 <copy_pte_range+0x3d9>
    2b58:	48 8b 4d 90          	mov    -0x70(%rbp),%rcx
    2b5c:	48 8b 39             	mov    (%rcx),%rdi
    2b5f:	ff 14 25 00 00 00 00 	callq  *0x0
	return (unsigned long)native_pmd_val(pmd) == 0;
}

static inline unsigned long pmd_page_vaddr(pmd_t pmd)
{
	return (unsigned long)__va(pmd_val(pmd) & PTE_PFN_MASK);
    2b66:	48 21 d8             	and    %rbx,%rax
    2b69:	48 8b 39             	mov    (%rcx),%rdi
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
}

static inline pte_t *pte_offset_kernel(pmd_t *pmd, unsigned long address)
{
	return (pte_t *)pmd_page_vaddr(*pmd) + pte_index(address);
    2b6c:	49 01 c4             	add    %rax,%r12
    2b6f:	ff 14 25 00 00 00 00 	callq  *0x0
    2b76:	48 21 d8             	and    %rbx,%rax
    2b79:	48 c1 e8 06          	shr    $0x6,%rax
	  return -ENOMEM;
	src_pte = pte_offset_map(src_pmd, addr);
	src_ptl = pte_lockptr(src_mm, src_pmd);
	spin_lock_nested(src_ptl, SINGLE_DEPTH_NESTING);
    2b7d:	49 8b 4c 05 30       	mov    0x30(%r13,%rax,1),%rcx
    2b82:	48 89 cf             	mov    %rcx,%rdi
    2b85:	48 89 8d 70 ff ff ff 	mov    %rcx,-0x90(%rbp)
    2b8c:	e8 00 00 00 00       	callq  2b91 <copy_pte_range+0x151>
}

#define  __HAVE_ARCH_ENTER_LAZY_MMU_MODE
static inline void arch_enter_lazy_mmu_mode(void)
{
	PVOP_VCALL0(pv_mmu_ops.lazy_mode.enter);
    2b91:	ff 14 25 00 00 00 00 	callq  *0x0
    2b98:	48 89 5d 88          	mov    %rbx,-0x78(%rbp)
    2b9c:	4c 8b 6d b0          	mov    -0x50(%rbp),%r13
    2ba0:	48 8b 4d c0          	mov    -0x40(%rbp),%rcx
    2ba4:	eb 30                	jmp    2bd6 <copy_pte_range+0x196>
    2ba6:	66 2e 0f 1f 84 00 00 	nopw   %cs:0x0(%rax,%rax,1)
    2bad:	00 00 00 
    2bb0:	49 8b 1c 24          	mov    (%r12),%rbx
			progress = 0;
			if (need_resched() ||
						spin_needbreak(src_ptl) || spin_needbreak(dst_ptl))
			  break;
		}
		if (pte_none(*src_pte)) {
    2bb4:	48 85 db             	test   %rbx,%rbx
    2bb7:	75 43                	jne    2bfc <copy_pte_range+0x1bc>
			progress++;
    2bb9:	41 83 c7 01          	add    $0x1,%r15d
		entry.val = copy_one_pte(dst_mm, src_mm, dst_pte, src_pte,dst_pmd,src_pmd,
					vma, addr, rss);
		if (entry.val)
		  break;
		progress += 8;
	} while (dst_pte++, src_pte++, addr += PAGE_SIZE, addr != end);
    2bbd:	49 83 c5 08          	add    $0x8,%r13
    2bc1:	49 83 c4 08          	add    $0x8,%r12
    2bc5:	49 81 c6 00 10 00 00 	add    $0x1000,%r14
    2bcc:	4c 3b 75 10          	cmp    0x10(%rbp),%r14
    2bd0:	0f 84 a2 01 00 00    	je     2d78 <copy_pte_range+0x338>
	do {
		/*
		 * We are holding two locks at this point - either of them
		 * could generate latencies in another task on another CPU.
		 */
		if (progress >= 32) {
    2bd6:	41 83 ff 1f          	cmp    $0x1f,%r15d
    2bda:	7e d4                	jle    2bb0 <copy_pte_range+0x170>
    2bdc:	48 8b 45 80          	mov    -0x80(%rbp),%rax
    2be0:	48 8b 90 38 e0 ff ff 	mov    -0x1fc8(%rax),%rdx
			progress = 0;
			if (need_resched() ||
    2be7:	83 e2 08             	and    $0x8,%edx
    2bea:	0f 85 60 02 00 00    	jne    2e50 <copy_pte_range+0x410>
    2bf0:	49 8b 1c 24          	mov    (%r12),%rbx
		/*
		 * We are holding two locks at this point - either of them
		 * could generate latencies in another task on another CPU.
		 */
		if (progress >= 32) {
			progress = 0;
    2bf4:	45 31 ff             	xor    %r15d,%r15d
			if (need_resched() ||
						spin_needbreak(src_ptl) || spin_needbreak(dst_ptl))
			  break;
		}
		if (pte_none(*src_pte)) {
    2bf7:	48 85 db             	test   %rbx,%rbx
    2bfa:	74 bd                	je     2bb9 <copy_pte_range+0x179>
static inline unsigned long
copy_one_pte(struct mm_struct *dst_mm, struct mm_struct *src_mm,
			pte_t *dst_pte, pte_t *src_pte, pmd_t *dst_pmd, pmd_t *src_pmd,struct vm_area_struct *vma,
			unsigned long addr, int *rss)
{
	unsigned long vm_flags = vma->vm_flags;
    2bfc:	48 8b 45 b8          	mov    -0x48(%rbp),%rax
    2c00:	48 8b 40 50          	mov    0x50(%rax),%rax
    2c04:	48 89 45 c0          	mov    %rax,-0x40(%rbp)

	if (sizeof(pmdval_t) > sizeof(long))
		ret =  PVOP_CALLEE2(pmdval_t, pv_mmu_ops.pmd_val,
				    pmd.pmd, (u64)pmd.pmd >> 32);
	else
		ret =  PVOP_CALLEE1(pmdval_t, pv_mmu_ops.pmd_val,
    2c08:	48 8b 45 90          	mov    -0x70(%rbp),%rax
    2c0c:	48 8b 38             	mov    (%rax),%rdi
    2c0f:	ff 14 25 00 00 00 00 	callq  *0x0
    2c16:	48 8b 75 88          	mov    -0x78(%rbp),%rsi
    2c1a:	48 ba 00 00 00 00 00 	movabs $0xffffea0000000000,%rdx
    2c21:	ea ff ff 
    2c24:	48 21 f0             	and    %rsi,%rax
    2c27:	48 c1 e8 06          	shr    $0x6,%rax
    2c2b:	48 8b 44 10 30       	mov    0x30(%rax,%rdx,1),%rax
    2c30:	48 89 45 b0          	mov    %rax,-0x50(%rbp)
    2c34:	48 8b 45 98          	mov    -0x68(%rbp),%rax
    2c38:	48 8b 38             	mov    (%rax),%rdi
    2c3b:	ff 14 25 00 00 00 00 	callq  *0x0
    2c42:	48 21 f0             	and    %rsi,%rax
    2c45:	48 c1 e8 06          	shr    $0x6,%rax
    2c49:	48 8b 44 02 30       	mov    0x30(%rdx,%rax,1),%rax
    2c4e:	48 89 45 a8          	mov    %rax,-0x58(%rbp)
    2c52:	48 b8 ff 0f 00 00 00 	movabs $0xffffc00000000fff,%rax
    2c59:	c0 ff ff 
    2c5c:	48 21 d8             	and    %rbx,%rax
	spinlock_t *src_ptl, *dst_ptl;
	src_ptl = pte_lockptr(src_mm, src_pmd);
	dst_ptl = pte_lockptr(dst_mm, dst_pmd);

	/* pte contains position in swap or file, so copy. */
	if (unlikely(!pte_present(pte))) {
    2c5f:	a9 01 01 00 00       	test   $0x101,%eax
    2c64:	0f 84 f2 01 00 00    	je     2e5c <copy_pte_range+0x41c>
	add_taint(TAINT_BAD_PAGE, LOCKDEP_NOW_UNRELIABLE);
}

static inline bool is_cow_mapping(vm_flags_t flags)
{
	return (flags & (VM_SHARED | VM_MAYWRITE)) == VM_MAYWRITE;
    2c6a:	48 8b 45 c0          	mov    -0x40(%rbp),%rax
    2c6e:	83 e0 28             	and    $0x28,%eax

	/*
	 * If it's a COW mapping, write protect it both
	 * in the parent and the child
	 */
	if (is_cow_mapping(vm_flags)) {
    2c71:	48 83 f8 20          	cmp    $0x20,%rax
    2c75:	0f 84 b5 00 00 00    	je     2d30 <copy_pte_range+0x2f0>

static inline pte_t pte_clear_flags(pte_t pte, pteval_t clear)
{
	pteval_t v = native_pte_val(pte);

	return native_make_pte(v & ~clear);
    2c7b:	48 89 d8             	mov    %rbx,%rax
	 */
	if (vm_flags & VM_SHARED)
	  pte = pte_mkclean(pte);
	pte = pte_mkold(pte);

	page = vm_normal_page(vma, addr, pte);
    2c7e:	48 8b 7d b8          	mov    -0x48(%rbp),%rdi
    2c82:	4c 89 f6             	mov    %r14,%rsi
    2c85:	48 83 e0 bf          	and    $0xffffffffffffffbf,%rax
    2c89:	f6 45 c0 08          	testb  $0x8,-0x40(%rbp)
    2c8d:	48 0f 45 d8          	cmovne %rax,%rbx
    2c91:	48 83 e3 df          	and    $0xffffffffffffffdf,%rbx
    2c95:	48 89 da             	mov    %rbx,%rdx
    2c98:	e8 00 00 00 00       	callq  2c9d <copy_pte_range+0x25d>
	if (page) {
    2c9d:	48 85 c0             	test   %rax,%rax
	 */
	if (vm_flags & VM_SHARED)
	  pte = pte_mkclean(pte);
	pte = pte_mkold(pte);

	page = vm_normal_page(vma, addr, pte);
    2ca0:	48 89 c2             	mov    %rax,%rdx
	if (page) {
    2ca3:	74 5a                	je     2cff <copy_pte_range+0x2bf>
    2ca5:	48 8b 00             	mov    (%rax),%rax
void get_huge_page_tail(struct page *page);/*ziqiao*/
extern bool __get_page_tail(struct page *page);

static inline void get_page(struct page *page)
{
	if (unlikely(PageTail(page)))
    2ca8:	f6 c4 80             	test   $0x80,%ah
    2cab:	0f 85 7f 01 00 00    	jne    2e30 <copy_pte_range+0x3f0>
 *
 * Atomically increments @v by 1.
 */
static inline void atomic_inc(atomic_t *v)
{
	asm volatile(LOCK_PREFIX "incl %0"
    2cb1:	f0 ff 42 1c          	lock incl 0x1c(%rdx)
    2cb5:	f0 ff 42 18          	lock incl 0x18(%rdx)
	raw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \
} while (0)

static inline void spin_unlock(spinlock_t *lock)
{
	raw_spin_unlock(&lock->rlock);
    2cb9:	48 8b 7d b0          	mov    -0x50(%rbp),%rdi
    2cbd:	48 89 55 c0          	mov    %rdx,-0x40(%rbp)
    2cc1:	e8 00 00 00 00       	callq  2cc6 <copy_pte_range+0x286>
    2cc6:	48 8b 7d a8          	mov    -0x58(%rbp),%rdi
    2cca:	e8 00 00 00 00       	callq  2ccf <copy_pte_range+0x28f>
		get_page(page);
		page_dup_rmap(page);
		spin_unlock(src_ptl);
		spin_unlock(dst_ptl);
		inc_page_counter_in_ns(page,vma);
    2ccf:	48 8b 55 c0          	mov    -0x40(%rbp),%rdx
    2cd3:	48 8b 75 b8          	mov    -0x48(%rbp),%rsi
    2cd7:	48 89 d7             	mov    %rdx,%rdi
    2cda:	e8 00 00 00 00       	callq  2cdf <copy_pte_range+0x29f>
	raw_spin_lock_init(&(_lock)->rlock);		\
} while (0)

static inline void spin_lock(spinlock_t *lock)
{
	raw_spin_lock(&lock->rlock);
    2cdf:	48 8b 7d a8          	mov    -0x58(%rbp),%rdi
    2ce3:	e8 00 00 00 00       	callq  2ce8 <copy_pte_range+0x2a8>
		spin_lock(dst_ptl);
		spin_lock_nested(src_ptl, SINGLE_DEPTH_NESTING);
    2ce8:	48 8b 7d b0          	mov    -0x50(%rbp),%rdi
    2cec:	e8 00 00 00 00       	callq  2cf1 <copy_pte_range+0x2b1>

		if (PageAnon(page))
    2cf1:	48 8b 55 c0          	mov    -0x40(%rbp),%rdx
    2cf5:	f6 42 08 01          	testb  $0x1,0x8(%rdx)
    2cf9:	74 25                	je     2d20 <copy_pte_range+0x2e0>
		  rss[MM_ANONPAGES]++;
    2cfb:	83 45 d0 01          	addl   $0x1,-0x30(%rbp)
{
	if (sizeof(pteval_t) > sizeof(long))
		/* 5 arg words */
		pv_mmu_ops.set_pte_at(mm, addr, ptep, pte);
	else
		PVOP_VCALL4(pv_mmu_ops.set_pte_at, mm, addr, ptep, pte.pte);
    2cff:	48 8b 7d a0          	mov    -0x60(%rbp),%rdi
    2d03:	4c 89 f6             	mov    %r14,%rsi
    2d06:	4c 89 ea             	mov    %r13,%rdx
    2d09:	48 89 d9             	mov    %rbx,%rcx
    2d0c:	ff 14 25 00 00 00 00 	callq  *0x0
		}
		entry.val = copy_one_pte(dst_mm, src_mm, dst_pte, src_pte,dst_pmd,src_pmd,
					vma, addr, rss);
		if (entry.val)
		  break;
		progress += 8;
    2d13:	41 83 c7 08          	add    $0x8,%r15d
    2d17:	31 c9                	xor    %ecx,%ecx
    2d19:	e9 9f fe ff ff       	jmpq   2bbd <copy_pte_range+0x17d>
    2d1e:	66 90                	xchg   %ax,%ax
		spin_lock_nested(src_ptl, SINGLE_DEPTH_NESTING);

		if (PageAnon(page))
		  rss[MM_ANONPAGES]++;
		else
		  rss[MM_FILEPAGES]++;
    2d20:	83 45 cc 01          	addl   $0x1,-0x34(%rbp)
    2d24:	eb d9                	jmp    2cff <copy_pte_range+0x2bf>
    2d26:	66 2e 0f 1f 84 00 00 	nopw   %cs:0x0(%rax,%rax,1)
    2d2d:	00 00 00 
 */
static __always_inline void
clear_bit(long nr, volatile unsigned long *addr)
{
	if (IS_IMMEDIATE(nr)) {
		asm volatile(LOCK_PREFIX "andb %1,%0"
    2d30:	f0 41 80 24 24 fd    	lock andb $0xfd,(%r12)
}

static inline void pte_update(struct mm_struct *mm, unsigned long addr,
			      pte_t *ptep)
{
	PVOP_VCALL3(pv_mmu_ops.pte_update, mm, addr, ptep);
    2d36:	48 8b bd 68 ff ff ff 	mov    -0x98(%rbp),%rdi
    2d3d:	4c 89 f6             	mov    %r14,%rsi
    2d40:	4c 89 e2             	mov    %r12,%rdx
    2d43:	ff 14 25 00 00 00 00 	callq  *0x0
    2d4a:	48 83 e3 fd          	and    $0xfffffffffffffffd,%rbx
    2d4e:	e9 28 ff ff ff       	jmpq   2c7b <copy_pte_range+0x23b>
		if (!pte_file(pte)) {
			swp_entry_t entry = pte_to_swp_entry(pte);

			if (swap_duplicate(entry) < 0)
			{
				printk("from copy one pte, pte=%lx",pte.pte);
    2d53:	31 c0                	xor    %eax,%eax
    2d55:	48 89 de             	mov    %rbx,%rsi
    2d58:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
    2d5f:	48 89 4d c0          	mov    %rcx,-0x40(%rbp)
    2d63:	e8 00 00 00 00       	callq  2d68 <copy_pte_range+0x328>
			progress++;
			continue;
		}
		entry.val = copy_one_pte(dst_mm, src_mm, dst_pte, src_pte,dst_pmd,src_pmd,
					vma, addr, rss);
		if (entry.val)
    2d68:	48 8b 4d c0          	mov    -0x40(%rbp),%rcx
    2d6c:	48 85 c9             	test   %rcx,%rcx
    2d6f:	74 a2                	je     2d13 <copy_pte_range+0x2d3>
    2d71:	0f 1f 80 00 00 00 00 	nopl   0x0(%rax)
    2d78:	48 89 4d c0          	mov    %rcx,-0x40(%rbp)
	PVOP_VCALL0(pv_mmu_ops.lazy_mode.enter);
}

static inline void arch_leave_lazy_mmu_mode(void)
{
	PVOP_VCALL0(pv_mmu_ops.lazy_mode.leave);
    2d7c:	ff 14 25 00 00 00 00 	callq  *0x0
	raw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \
} while (0)

static inline void spin_unlock(spinlock_t *lock)
{
	raw_spin_unlock(&lock->rlock);
    2d83:	48 8b bd 70 ff ff ff 	mov    -0x90(%rbp),%rdi
    2d8a:	e8 00 00 00 00       	callq  2d8f <copy_pte_range+0x34f>

static inline void add_mm_rss_vec(struct mm_struct *mm, int *rss)
{
	int i;

	if (current->mm == mm)
    2d8f:	48 8b 85 60 ff ff ff 	mov    -0xa0(%rbp),%rax
    2d96:	48 8b 4d a0          	mov    -0x60(%rbp),%rcx
    2d9a:	48 3b 88 a8 02 00 00 	cmp    0x2a8(%rax),%rcx
    2da1:	74 6c                	je     2e0f <copy_pte_range+0x3cf>
    2da3:	48 8b 4d a0          	mov    -0x60(%rbp),%rcx
    2da7:	48 8d 45 cc          	lea    -0x34(%rbp),%rax
    2dab:	48 8d 75 d8          	lea    -0x28(%rbp),%rsi
    2daf:	48 8d 91 c8 02 00 00 	lea    0x2c8(%rcx),%rdx
	  sync_mm_rss(mm);
	for (i = 0; i < NR_MM_COUNTERS; i++)
	  if (rss[i])
    2db6:	48 63 08             	movslq (%rax),%rcx
    2db9:	85 c9                	test   %ecx,%ecx
    2dbb:	74 04                	je     2dc1 <copy_pte_range+0x381>
 *
 * Atomically adds @i to @v.
 */
static inline void atomic64_add(long i, atomic64_t *v)
{
	asm volatile(LOCK_PREFIX "addq %1,%0"
    2dbd:	f0 48 01 0a          	lock add %rcx,(%rdx)
    2dc1:	48 83 c0 04          	add    $0x4,%rax
    2dc5:	48 83 c2 08          	add    $0x8,%rdx
{
	int i;

	if (current->mm == mm)
	  sync_mm_rss(mm);
	for (i = 0; i < NR_MM_COUNTERS; i++)
    2dc9:	48 39 f0             	cmp    %rsi,%rax
    2dcc:	75 e8                	jne    2db6 <copy_pte_range+0x376>
    2dce:	48 8b bd 78 ff ff ff 	mov    -0x88(%rbp),%rdi
    2dd5:	e8 00 00 00 00       	callq  2dda <copy_pte_range+0x39a>
	arch_leave_lazy_mmu_mode();
	spin_unlock(src_ptl);
	pte_unmap(orig_src_pte);
	add_mm_rss_vec(dst_mm, rss);
	pte_unmap_unlock(orig_dst_pte, dst_ptl);
	cond_resched();
    2dda:	e8 00 00 00 00       	callq  2ddf <copy_pte_range+0x39f>
	if (entry.val) {
    2ddf:	48 83 7d c0 00       	cmpq   $0x0,-0x40(%rbp)
    2de4:	75 12                	jne    2df8 <copy_pte_range+0x3b8>
		if (add_swap_count_continuation(entry, GFP_KERNEL) < 0)
		  return -ENOMEM;
		progress = 0;
	}
	if (addr != end)
    2de6:	4c 3b 75 10          	cmp    0x10(%rbp),%r14
    2dea:	0f 85 d0 fc ff ff    	jne    2ac0 <copy_pte_range+0x80>
	  goto again;
	return 0;
    2df0:	31 c0                	xor    %eax,%eax
    2df2:	eb 2a                	jmp    2e1e <copy_pte_range+0x3de>
    2df4:	0f 1f 40 00          	nopl   0x0(%rax)
	pte_unmap(orig_src_pte);
	add_mm_rss_vec(dst_mm, rss);
	pte_unmap_unlock(orig_dst_pte, dst_ptl);
	cond_resched();
	if (entry.val) {
		if (add_swap_count_continuation(entry, GFP_KERNEL) < 0)
    2df8:	48 8b 7d c0          	mov    -0x40(%rbp),%rdi
    2dfc:	be d0 00 00 00       	mov    $0xd0,%esi
    2e01:	e8 00 00 00 00       	callq  2e06 <copy_pte_range+0x3c6>
    2e06:	85 c0                	test   %eax,%eax
    2e08:	78 0f                	js     2e19 <copy_pte_range+0x3d9>
		  return -ENOMEM;
		progress = 0;
    2e0a:	45 31 ff             	xor    %r15d,%r15d
    2e0d:	eb d7                	jmp    2de6 <copy_pte_range+0x3a6>
static inline void add_mm_rss_vec(struct mm_struct *mm, int *rss)
{
	int i;

	if (current->mm == mm)
	  sync_mm_rss(mm);
    2e0f:	48 89 cf             	mov    %rcx,%rdi
    2e12:	e8 00 00 00 00       	callq  2e17 <copy_pte_range+0x3d7>
    2e17:	eb 8a                	jmp    2da3 <copy_pte_range+0x363>
again:
	init_rss_vec(rss);

	dst_pte = pte_alloc_map_lock(dst_mm, dst_pmd, addr, &dst_ptl);
	if (!dst_pte)
	  return -ENOMEM;
    2e19:	b8 f4 ff ff ff       	mov    $0xfffffff4,%eax
		progress = 0;
	}
	if (addr != end)
	  goto again;
	return 0;
}
    2e1e:	48 81 c4 88 00 00 00 	add    $0x88,%rsp
    2e25:	5b                   	pop    %rbx
    2e26:	41 5c                	pop    %r12
    2e28:	41 5d                	pop    %r13
    2e2a:	41 5e                	pop    %r14
    2e2c:	41 5f                	pop    %r15
    2e2e:	5d                   	pop    %rbp
    2e2f:	c3                   	retq   
		if (likely(__get_page_tail(page)))
    2e30:	48 89 d7             	mov    %rdx,%rdi
    2e33:	48 89 55 c0          	mov    %rdx,-0x40(%rbp)
    2e37:	e8 00 00 00 00       	callq  2e3c <copy_pte_range+0x3fc>
    2e3c:	84 c0                	test   %al,%al
    2e3e:	48 8b 55 c0          	mov    -0x40(%rbp),%rdx
    2e42:	0f 85 6d fe ff ff    	jne    2cb5 <copy_pte_range+0x275>
    2e48:	e9 64 fe ff ff       	jmpq   2cb1 <copy_pte_range+0x271>
    2e4d:	0f 1f 00             	nopl   (%rax)
    2e50:	48 89 4d c0          	mov    %rcx,-0x40(%rbp)
		/*
		 * We are holding two locks at this point - either of them
		 * could generate latencies in another task on another CPU.
		 */
		if (progress >= 32) {
			progress = 0;
    2e54:	45 31 ff             	xor    %r15d,%r15d
    2e57:	e9 20 ff ff ff       	jmpq   2d7c <copy_pte_range+0x33c>
	src_ptl = pte_lockptr(src_mm, src_pmd);
	dst_ptl = pte_lockptr(dst_mm, dst_pmd);

	/* pte contains position in swap or file, so copy. */
	if (unlikely(!pte_present(pte))) {
		if (!pte_file(pte)) {
    2e5c:	a8 40                	test   $0x40,%al
    2e5e:	66 90                	xchg   %ax,%ax
    2e60:	0f 85 99 fe ff ff    	jne    2cff <copy_pte_range+0x2bf>
			swp_entry_t entry = pte_to_swp_entry(pte);
    2e66:	48 89 df             	mov    %rbx,%rdi
    2e69:	e8 32 d3 ff ff       	callq  1a0 <pte_to_swp_entry>

			if (swap_duplicate(entry) < 0)
    2e6e:	48 89 c7             	mov    %rax,%rdi
    2e71:	48 89 45 b0          	mov    %rax,-0x50(%rbp)
    2e75:	e8 00 00 00 00       	callq  2e7a <copy_pte_range+0x43a>
    2e7a:	85 c0                	test   %eax,%eax
    2e7c:	48 8b 4d b0          	mov    -0x50(%rbp),%rcx
    2e80:	0f 88 cd fe ff ff    	js     2d53 <copy_pte_range+0x313>
			{
				printk("from copy one pte, pte=%lx",pte.pte);
				return entry.val;
			}
			/* make sure dst_mm is on swapoff's mmlist. */
			if (unlikely(list_empty(&dst_mm->mmlist))) {
    2e86:	48 8b 45 a0          	mov    -0x60(%rbp),%rax
    2e8a:	48 8b b5 58 ff ff ff 	mov    -0xa8(%rbp),%rsi
    2e91:	48 3b b0 a8 00 00 00 	cmp    0xa8(%rax),%rsi
    2e98:	0f 84 ce 00 00 00    	je     2f6c <copy_pte_range+0x52c>
 * Extract the `type' field from a swp_entry_t.  The swp_entry_t is in
 * arch-independent format
 */
static inline unsigned swp_type(swp_entry_t entry)
{
	return (entry.val  >> SWP_TYPE_SHIFT(entry));
    2e9e:	48 89 ca             	mov    %rcx,%rdx
    2ea1:	48 c1 ea 39          	shr    $0x39,%rdx
				if (list_empty(&dst_mm->mmlist))
				  list_add(&dst_mm->mmlist,
							  &src_mm->mmlist);
				spin_unlock(&mmlist_lock);
			}
			if (likely(!non_swap_entry(entry)))
    2ea5:	83 fa 1c             	cmp    $0x1c,%edx
    2ea8:	77 2f                	ja     2ed9 <copy_pte_range+0x499>
			  rss[MM_SWAPENTS]++;
    2eaa:	83 45 d4 01          	addl   $0x1,-0x2c(%rbp)
    2eae:	e9 4c fe ff ff       	jmpq   2cff <copy_pte_range+0x2bf>
	swp_entry_t entry = (swp_entry_t){0};

again:
	init_rss_vec(rss);

	dst_pte = pte_alloc_map_lock(dst_mm, dst_pmd, addr, &dst_ptl);
    2eb3:	48 8b 55 98          	mov    -0x68(%rbp),%rdx
    2eb7:	48 8b 7d a0          	mov    -0x60(%rbp),%rdi
    2ebb:	31 f6                	xor    %esi,%esi
    2ebd:	4c 89 f1             	mov    %r14,%rcx
    2ec0:	e8 00 00 00 00       	callq  2ec5 <copy_pte_range+0x485>
    2ec5:	85 c0                	test   %eax,%eax
    2ec7:	0f 85 4c ff ff ff    	jne    2e19 <copy_pte_range+0x3d9>
    2ecd:	48 8b 45 98          	mov    -0x68(%rbp),%rax
    2ed1:	48 8b 38             	mov    (%rax),%rdi
    2ed4:	e9 09 fc ff ff       	jmpq   2ae2 <copy_pte_range+0xa2>
			page_to_pfn(page));
}

static inline int is_migration_entry(swp_entry_t entry)
{
	return unlikely(swp_type(entry) == SWP_MIGRATION_READ ||
    2ed9:	8d 42 e2             	lea    -0x1e(%rdx),%eax
    2edc:	83 f8 01             	cmp    $0x1,%eax
    2edf:	0f 87 1a fe ff ff    	ja     2cff <copy_pte_range+0x2bf>
 * Extract the `offset' field from a swp_entry_t.  The swp_entry_t is in
 * arch-independent format
 */
static inline pgoff_t swp_offset(swp_entry_t entry)
{
	return entry.val & SWP_OFFSET_MASK(entry);
    2ee5:	48 b8 ff ff ff ff ff 	movabs $0x1ffffffffffffff,%rax
    2eec:	ff ff 01 
    2eef:	48 21 c8             	and    %rcx,%rax
	return unlikely(swp_type(entry) == SWP_MIGRATION_WRITE);
}

static inline struct page *migration_entry_to_page(swp_entry_t entry)
{
	struct page *p = pfn_to_page(swp_offset(entry));
    2ef2:	48 b9 00 00 00 00 00 	movabs $0xffffea0000000000,%rcx
    2ef9:	ea ff ff 
    2efc:	48 89 c6             	mov    %rax,%rsi
    2eff:	48 c1 e6 06          	shl    $0x6,%rsi
    2f03:	48 01 f1             	add    %rsi,%rcx
}

static __always_inline int constant_test_bit(long nr, const volatile unsigned long *addr)
{
	return ((1UL << (nr & (BITS_PER_LONG-1))) &
		(addr[nr >> _BITOPS_LONG_SHIFT])) != 0;
    2f06:	48 8b 31             	mov    (%rcx),%rsi
	/*
	 * Any use of migration entries may only occur while the
	 * corresponding page is locked
	 */
	BUG_ON(!PageLocked(p));
    2f09:	83 e6 01             	and    $0x1,%esi
    2f0c:	0f 84 dd 00 00 00    	je     2fef <copy_pte_range+0x5af>
			if (likely(!non_swap_entry(entry)))
			  rss[MM_SWAPENTS]++;
			else if (is_migration_entry(entry)) {
				page = migration_entry_to_page(entry);

				if (PageAnon(page))
    2f12:	f6 41 08 01          	testb  $0x1,0x8(%rcx)
    2f16:	0f 84 ca 00 00 00    	je     2fe6 <copy_pte_range+0x5a6>
				  rss[MM_ANONPAGES]++;
    2f1c:	83 45 d0 01          	addl   $0x1,-0x30(%rbp)
				else
				  rss[MM_FILEPAGES]++;

				if (is_write_migration_entry(entry) &&
    2f20:	83 fa 1f             	cmp    $0x1f,%edx
    2f23:	0f 85 d6 fd ff ff    	jne    2cff <copy_pte_range+0x2bf>
	add_taint(TAINT_BAD_PAGE, LOCKDEP_NOW_UNRELIABLE);
}

static inline bool is_cow_mapping(vm_flags_t flags)
{
	return (flags & (VM_SHARED | VM_MAYWRITE)) == VM_MAYWRITE;
    2f29:	48 8b 55 c0          	mov    -0x40(%rbp),%rdx
    2f2d:	83 e2 28             	and    $0x28,%edx
				if (PageAnon(page))
				  rss[MM_ANONPAGES]++;
				else
				  rss[MM_FILEPAGES]++;

				if (is_write_migration_entry(entry) &&
    2f30:	48 83 fa 20          	cmp    $0x20,%rdx
    2f34:	0f 85 c5 fd ff ff    	jne    2cff <copy_pte_range+0x2bf>
 */
static inline pte_t swp_entry_to_pte(swp_entry_t entry)
{
	swp_entry_t arch_entry;

	arch_entry = __swp_entry(swp_type(entry), swp_offset(entry));
    2f3a:	48 c1 e0 09          	shl    $0x9,%rax
{
	if (sizeof(pteval_t) > sizeof(long))
		/* 5 arg words */
		pv_mmu_ops.set_pte_at(mm, addr, ptep, pte);
	else
		PVOP_VCALL4(pv_mmu_ops.set_pte_at, mm, addr, ptep, pte.pte);
    2f3e:	48 8b bd 68 ff ff ff 	mov    -0x98(%rbp),%rdi
    2f45:	4c 89 f6             	mov    %r14,%rsi
    2f48:	48 89 c3             	mov    %rax,%rbx

static inline pte_t pte_set_flags(pte_t pte, pteval_t set)
{
	pteval_t v = native_pte_val(pte);

	return native_make_pte(v | set);
    2f4b:	0c bc                	or     $0xbc,%al
    2f4d:	4c 89 e2             	mov    %r12,%rdx
    2f50:	48 83 cb 3c          	or     $0x3c,%rbx
    2f54:	41 f6 04 24 80       	testb  $0x80,(%r12)
    2f59:	48 0f 45 d8          	cmovne %rax,%rbx
    2f5d:	48 89 d9             	mov    %rbx,%rcx
    2f60:	ff 14 25 00 00 00 00 	callq  *0x0
    2f67:	e9 93 fd ff ff       	jmpq   2cff <copy_pte_range+0x2bf>
	raw_spin_lock_init(&(_lock)->rlock);		\
} while (0)

static inline void spin_lock(spinlock_t *lock)
{
	raw_spin_lock(&lock->rlock);
    2f6c:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
    2f73:	48 89 4d b0          	mov    %rcx,-0x50(%rbp)
    2f77:	e8 00 00 00 00       	callq  2f7c <copy_pte_range+0x53c>
				return entry.val;
			}
			/* make sure dst_mm is on swapoff's mmlist. */
			if (unlikely(list_empty(&dst_mm->mmlist))) {
				spin_lock(&mmlist_lock);
				if (list_empty(&dst_mm->mmlist))
    2f7c:	48 8b 45 a0          	mov    -0x60(%rbp),%rax
    2f80:	48 8b 8d 58 ff ff ff 	mov    -0xa8(%rbp),%rcx
    2f87:	48 3b 88 a8 00 00 00 	cmp    0xa8(%rax),%rcx
    2f8e:	48 8b 4d b0          	mov    -0x50(%rbp),%rcx
    2f92:	75 39                	jne    2fcd <copy_pte_range+0x58d>
 * Insert a new entry after the specified head.
 * This is good for implementing stacks.
 */
static inline void list_add(struct list_head *new, struct list_head *head)
{
	__list_add(new, head, head->next);
    2f94:	48 8b b5 68 ff ff ff 	mov    -0x98(%rbp),%rsi
#ifndef CONFIG_DEBUG_LIST
static inline void __list_add(struct list_head *new,
			      struct list_head *prev,
			      struct list_head *next)
{
	next->prev = new;
    2f9b:	48 8b bd 58 ff ff ff 	mov    -0xa8(%rbp),%rdi
	new->next = next;
    2fa2:	48 8b 55 a0          	mov    -0x60(%rbp),%rdx
 * Insert a new entry after the specified head.
 * This is good for implementing stacks.
 */
static inline void list_add(struct list_head *new, struct list_head *head)
{
	__list_add(new, head, head->next);
    2fa6:	48 8b 86 a8 00 00 00 	mov    0xa8(%rsi),%rax
#ifndef CONFIG_DEBUG_LIST
static inline void __list_add(struct list_head *new,
			      struct list_head *prev,
			      struct list_head *next)
{
	next->prev = new;
    2fad:	48 89 78 08          	mov    %rdi,0x8(%rax)
	new->next = next;
    2fb1:	48 89 82 a8 00 00 00 	mov    %rax,0xa8(%rdx)
				  list_add(&dst_mm->mmlist,
    2fb8:	48 8b 85 50 ff ff ff 	mov    -0xb0(%rbp),%rax
    2fbf:	48 89 82 b0 00 00 00 	mov    %rax,0xb0(%rdx)
	new->prev = prev;
	prev->next = new;
    2fc6:	48 89 be a8 00 00 00 	mov    %rdi,0xa8(%rsi)
	raw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \
} while (0)

static inline void spin_unlock(spinlock_t *lock)
{
	raw_spin_unlock(&lock->rlock);
    2fcd:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
    2fd4:	48 89 4d b0          	mov    %rcx,-0x50(%rbp)
    2fd8:	e8 00 00 00 00       	callq  2fdd <copy_pte_range+0x59d>
    2fdd:	48 8b 4d b0          	mov    -0x50(%rbp),%rcx
    2fe1:	e9 b8 fe ff ff       	jmpq   2e9e <copy_pte_range+0x45e>
				page = migration_entry_to_page(entry);

				if (PageAnon(page))
				  rss[MM_ANONPAGES]++;
				else
				  rss[MM_FILEPAGES]++;
    2fe6:	83 45 cc 01          	addl   $0x1,-0x34(%rbp)
    2fea:	e9 31 ff ff ff       	jmpq   2f20 <copy_pte_range+0x4e0>
    2fef:	e8 00 00 00 00       	callq  2ff4 <copy_pte_range+0x5b4>
    2ff4:	66 66 66 2e 0f 1f 84 	data32 data32 nopw %cs:0x0(%rax,%rax,1)
    2ffb:	00 00 00 00 00 

0000000000003000 <unmap_vmas>:
 * drops the lock and schedules.
 */
void unmap_vmas(struct mmu_gather *tlb,
			struct vm_area_struct *vma, unsigned long start_addr,
			unsigned long end_addr)
{
    3000:	e8 00 00 00 00       	callq  3005 <unmap_vmas+0x5>
    3005:	55                   	push   %rbp
    3006:	48 89 e5             	mov    %rsp,%rbp
    3009:	41 57                	push   %r15
    300b:	41 56                	push   %r14
    300d:	49 89 fe             	mov    %rdi,%r14
    3010:	41 55                	push   %r13
    3012:	49 89 d5             	mov    %rdx,%r13
    3015:	41 54                	push   %r12
    3017:	49 89 cc             	mov    %rcx,%r12
    301a:	53                   	push   %rbx
	struct mm_struct *mm = vma->vm_mm;
    301b:	4c 8b 7e 40          	mov    0x40(%rsi),%r15
 * drops the lock and schedules.
 */
void unmap_vmas(struct mmu_gather *tlb,
			struct vm_area_struct *vma, unsigned long start_addr,
			unsigned long end_addr)
{
    301f:	48 89 f3             	mov    %rsi,%rbx
}

static inline void mmu_notifier_invalidate_range_start(struct mm_struct *mm,
				  unsigned long start, unsigned long end)
{
	if (mm_has_notifiers(mm))
    3022:	49 83 bf a0 03 00 00 	cmpq   $0x0,0x3a0(%r15)
    3029:	00 
    302a:	74 21                	je     304d <unmap_vmas+0x4d>
    302c:	eb 39                	jmp    3067 <unmap_vmas+0x67>
    302e:	66 90                	xchg   %ax,%ax
	struct mm_struct *mm = vma->vm_mm;

	mmu_notifier_invalidate_range_start(mm, start_addr, end_addr);
	for ( ; vma && vma->vm_start < end_addr; vma = vma->vm_next)
    3030:	4c 3b 23             	cmp    (%rbx),%r12
    3033:	76 1d                	jbe    3052 <unmap_vmas+0x52>
	  unmap_single_vma(tlb, vma, start_addr, end_addr, NULL);
    3035:	48 89 de             	mov    %rbx,%rsi
    3038:	45 31 c0             	xor    %r8d,%r8d
    303b:	4c 89 e1             	mov    %r12,%rcx
    303e:	4c 89 ea             	mov    %r13,%rdx
    3041:	4c 89 f7             	mov    %r14,%rdi
    3044:	e8 f7 ee ff ff       	callq  1f40 <unmap_single_vma>
			unsigned long end_addr)
{
	struct mm_struct *mm = vma->vm_mm;

	mmu_notifier_invalidate_range_start(mm, start_addr, end_addr);
	for ( ; vma && vma->vm_start < end_addr; vma = vma->vm_next)
    3049:	48 8b 5b 10          	mov    0x10(%rbx),%rbx
    304d:	48 85 db             	test   %rbx,%rbx
    3050:	75 de                	jne    3030 <unmap_vmas+0x30>
}

static inline void mmu_notifier_invalidate_range_end(struct mm_struct *mm,
				  unsigned long start, unsigned long end)
{
	if (mm_has_notifiers(mm))
    3052:	49 83 bf a0 03 00 00 	cmpq   $0x0,0x3a0(%r15)
    3059:	00 
    305a:	75 1b                	jne    3077 <unmap_vmas+0x77>
	  unmap_single_vma(tlb, vma, start_addr, end_addr, NULL);
	mmu_notifier_invalidate_range_end(mm, start_addr, end_addr);
}
    305c:	5b                   	pop    %rbx
    305d:	41 5c                	pop    %r12
    305f:	41 5d                	pop    %r13
    3061:	41 5e                	pop    %r14
    3063:	41 5f                	pop    %r15
    3065:	5d                   	pop    %rbp
    3066:	c3                   	retq   

static inline void mmu_notifier_invalidate_range_start(struct mm_struct *mm,
				  unsigned long start, unsigned long end)
{
	if (mm_has_notifiers(mm))
		__mmu_notifier_invalidate_range_start(mm, start, end);
    3067:	48 89 ca             	mov    %rcx,%rdx
    306a:	4c 89 ee             	mov    %r13,%rsi
    306d:	4c 89 ff             	mov    %r15,%rdi
    3070:	e8 00 00 00 00       	callq  3075 <unmap_vmas+0x75>
    3075:	eb d6                	jmp    304d <unmap_vmas+0x4d>

static inline void mmu_notifier_invalidate_range_end(struct mm_struct *mm,
				  unsigned long start, unsigned long end)
{
	if (mm_has_notifiers(mm))
		__mmu_notifier_invalidate_range_end(mm, start, end);
    3077:	4c 89 e2             	mov    %r12,%rdx
    307a:	4c 89 ee             	mov    %r13,%rsi
    307d:	4c 89 ff             	mov    %r15,%rdi
    3080:	e8 00 00 00 00       	callq  3085 <unmap_vmas+0x85>
    3085:	eb d5                	jmp    305c <unmap_vmas+0x5c>
    3087:	66 0f 1f 84 00 00 00 	nopw   0x0(%rax,%rax,1)
    308e:	00 00 

0000000000003090 <zap_page_range>:
 *
 * Caller must protect the VMA list
 */
void zap_page_range(struct vm_area_struct *vma, unsigned long start,
			unsigned long size, struct zap_details *details)
{
    3090:	e8 00 00 00 00       	callq  3095 <zap_page_range+0x5>
    3095:	55                   	push   %rbp
    3096:	48 89 e5             	mov    %rsp,%rbp
    3099:	41 57                	push   %r15
    309b:	41 56                	push   %r14
    309d:	49 89 ce             	mov    %rcx,%r14
    30a0:	41 55                	push   %r13
    30a2:	49 89 f5             	mov    %rsi,%r13
    30a5:	41 54                	push   %r12
	struct mm_struct *mm = vma->vm_mm;
	struct mmu_gather tlb;
	unsigned long end = start + size;
    30a7:	4c 8d 24 16          	lea    (%rsi,%rdx,1),%r12
 *
 * Caller must protect the VMA list
 */
void zap_page_range(struct vm_area_struct *vma, unsigned long start,
			unsigned long size, struct zap_details *details)
{
    30ab:	53                   	push   %rbx
    30ac:	48 89 fb             	mov    %rdi,%rbx
    30af:	48 83 c4 80          	add    $0xffffffffffffff80,%rsp
	struct mm_struct *mm = vma->vm_mm;
    30b3:	4c 8b 7f 40          	mov    0x40(%rdi),%r15
	struct mmu_gather tlb;
	unsigned long end = start + size;

	lru_add_drain();
    30b7:	e8 00 00 00 00       	callq  30bc <zap_page_range+0x2c>
void tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm, unsigned long start, unsigned long end)
{
	tlb->mm = mm;

	/* Is it from 0 to ~0? */
	tlb->fullmm     = !(start | (end+1));
    30bc:	49 8d 44 24 01       	lea    0x1(%r12),%rax
	tlb->need_flush_all = 0;
	tlb->start	= start;
    30c1:	4c 89 ad 60 ff ff ff 	mov    %r13,-0xa0(%rbp)
	tlb->end	= end;
    30c8:	4c 89 a5 68 ff ff ff 	mov    %r12,-0x98(%rbp)
 *	tear-down from @mm. The @fullmm argument is used when @mm is without
 *	users and we're going to destroy the full address space (exit/execve).
 */
void tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm, unsigned long start, unsigned long end)
{
	tlb->mm = mm;
    30cf:	4c 89 bd 58 ff ff ff 	mov    %r15,-0xa8(%rbp)
	tlb->fullmm     = !(start | (end+1));
	tlb->need_flush_all = 0;
	tlb->start	= start;
	tlb->end	= end;
	tlb->need_flush = 0;
	tlb->local.next = NULL;
    30d6:	48 c7 45 80 00 00 00 	movq   $0x0,-0x80(%rbp)
    30dd:	00 
void tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm, unsigned long start, unsigned long end)
{
	tlb->mm = mm;

	/* Is it from 0 to ~0? */
	tlb->fullmm     = !(start | (end+1));
    30de:	4c 09 e8             	or     %r13,%rax
    30e1:	0f b6 85 70 ff ff ff 	movzbl -0x90(%rbp),%eax
	tlb->need_flush_all = 0;
	tlb->start	= start;
	tlb->end	= end;
	tlb->need_flush = 0;
	tlb->local.next = NULL;
	tlb->local.nr   = 0;
    30e8:	c7 45 88 00 00 00 00 	movl   $0x0,-0x78(%rbp)
void tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm, unsigned long start, unsigned long end)
{
	tlb->mm = mm;

	/* Is it from 0 to ~0? */
	tlb->fullmm     = !(start | (end+1));
    30ef:	0f 94 c2             	sete   %dl
	tlb->start	= start;
	tlb->end	= end;
	tlb->need_flush = 0;
	tlb->local.next = NULL;
	tlb->local.nr   = 0;
	tlb->local.max  = ARRAY_SIZE(tlb->__pages);
    30f2:	c7 45 8c 08 00 00 00 	movl   $0x8,-0x74(%rbp)
	tlb->active     = &tlb->local;
	tlb->batch_count = 0;
    30f9:	c7 45 d0 00 00 00 00 	movl   $0x0,-0x30(%rbp)
void tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm, unsigned long start, unsigned long end)
{
	tlb->mm = mm;

	/* Is it from 0 to ~0? */
	tlb->fullmm     = !(start | (end+1));
    3100:	01 d2                	add    %edx,%edx
    3102:	83 e0 f8             	and    $0xfffffff8,%eax
	tlb->need_flush_all = 0;
	tlb->start	= start;
	tlb->end	= end;
	tlb->need_flush = 0;
    3105:	09 d0                	or     %edx,%eax
 * Atomically reads the value of @v.
 * Doesn't imply a read memory barrier.
 */
static inline long atomic64_read(const atomic64_t *v)
{
	return (*(volatile long *)&(v)->counter);
    3107:	49 8b 97 c8 02 00 00 	mov    0x2c8(%r15),%rdx
    310e:	49 8b 8f d0 02 00 00 	mov    0x2d0(%r15),%rcx
    3115:	88 85 70 ff ff ff    	mov    %al,-0x90(%rbp)
	tlb->local.next = NULL;
	tlb->local.nr   = 0;
	tlb->local.max  = ARRAY_SIZE(tlb->__pages);
	tlb->active     = &tlb->local;
    311b:	48 8d 85 58 ff ff ff 	lea    -0xa8(%rbp),%rax
    3122:	48 83 c0 28          	add    $0x28,%rax
    3126:	48 89 85 78 ff ff ff 	mov    %rax,-0x88(%rbp)
    312d:	31 c0                	xor    %eax,%eax
    312f:	48 85 c9             	test   %rcx,%rcx
    3132:	48 0f 48 c8          	cmovs  %rax,%rcx
    3136:	48 85 d2             	test   %rdx,%rdx
    3139:	48 0f 49 c2          	cmovns %rdx,%rax
	atomic_long_dec(&mm->rss_stat.count[member]);
}

static inline unsigned long get_mm_rss(struct mm_struct *mm)
{
	return get_mm_counter(mm, MM_FILEPAGES) +
    313d:	48 01 c8             	add    %rcx,%rax

static inline void update_hiwater_rss(struct mm_struct *mm)
{
	unsigned long _rss = get_mm_rss(mm);

	if ((mm)->hiwater_rss < _rss)
    3140:	49 3b 87 b8 00 00 00 	cmp    0xb8(%r15),%rax
    3147:	76 07                	jbe    3150 <zap_page_range+0xc0>
		(mm)->hiwater_rss = _rss;
    3149:	49 89 87 b8 00 00 00 	mov    %rax,0xb8(%r15)
}

static inline void mmu_notifier_invalidate_range_start(struct mm_struct *mm,
				  unsigned long start, unsigned long end)
{
	if (mm_has_notifiers(mm))
    3150:	49 83 bf a0 03 00 00 	cmpq   $0x0,0x3a0(%r15)
    3157:	00 
    3158:	74 27                	je     3181 <zap_page_range+0xf1>
    315a:	eb 55                	jmp    31b1 <zap_page_range+0x121>
    315c:	0f 1f 40 00          	nopl   0x0(%rax)

	lru_add_drain();
	tlb_gather_mmu(&tlb, mm, start, end);
	update_hiwater_rss(mm);
	mmu_notifier_invalidate_range_start(mm, start, end);
	for ( ; vma && vma->vm_start < end; vma = vma->vm_next)
    3160:	4c 3b 23             	cmp    (%rbx),%r12
    3163:	76 21                	jbe    3186 <zap_page_range+0xf6>
	  unmap_single_vma(&tlb, vma, start, end, details);
    3165:	48 8d bd 58 ff ff ff 	lea    -0xa8(%rbp),%rdi
    316c:	48 89 de             	mov    %rbx,%rsi
    316f:	4d 89 f0             	mov    %r14,%r8
    3172:	4c 89 e1             	mov    %r12,%rcx
    3175:	4c 89 ea             	mov    %r13,%rdx
    3178:	e8 c3 ed ff ff       	callq  1f40 <unmap_single_vma>

	lru_add_drain();
	tlb_gather_mmu(&tlb, mm, start, end);
	update_hiwater_rss(mm);
	mmu_notifier_invalidate_range_start(mm, start, end);
	for ( ; vma && vma->vm_start < end; vma = vma->vm_next)
    317d:	48 8b 5b 10          	mov    0x10(%rbx),%rbx
    3181:	48 85 db             	test   %rbx,%rbx
    3184:	75 da                	jne    3160 <zap_page_range+0xd0>
}

static inline void mmu_notifier_invalidate_range_end(struct mm_struct *mm,
				  unsigned long start, unsigned long end)
{
	if (mm_has_notifiers(mm))
    3186:	49 83 bf a0 03 00 00 	cmpq   $0x0,0x3a0(%r15)
    318d:	00 
    318e:	75 31                	jne    31c1 <zap_page_range+0x131>
	  unmap_single_vma(&tlb, vma, start, end, details);
	mmu_notifier_invalidate_range_end(mm, start, end);
	tlb_finish_mmu(&tlb, start, end);
    3190:	48 8d bd 58 ff ff ff 	lea    -0xa8(%rbp),%rdi
    3197:	4c 89 e2             	mov    %r12,%rdx
    319a:	4c 89 ee             	mov    %r13,%rsi
    319d:	e8 00 00 00 00       	callq  31a2 <zap_page_range+0x112>
}
    31a2:	48 83 ec 80          	sub    $0xffffffffffffff80,%rsp
    31a6:	5b                   	pop    %rbx
    31a7:	41 5c                	pop    %r12
    31a9:	41 5d                	pop    %r13
    31ab:	41 5e                	pop    %r14
    31ad:	41 5f                	pop    %r15
    31af:	5d                   	pop    %rbp
    31b0:	c3                   	retq   

static inline void mmu_notifier_invalidate_range_start(struct mm_struct *mm,
				  unsigned long start, unsigned long end)
{
	if (mm_has_notifiers(mm))
		__mmu_notifier_invalidate_range_start(mm, start, end);
    31b1:	4c 89 e2             	mov    %r12,%rdx
    31b4:	4c 89 ee             	mov    %r13,%rsi
    31b7:	4c 89 ff             	mov    %r15,%rdi
    31ba:	e8 00 00 00 00       	callq  31bf <zap_page_range+0x12f>
    31bf:	eb c0                	jmp    3181 <zap_page_range+0xf1>

static inline void mmu_notifier_invalidate_range_end(struct mm_struct *mm,
				  unsigned long start, unsigned long end)
{
	if (mm_has_notifiers(mm))
		__mmu_notifier_invalidate_range_end(mm, start, end);
    31c1:	4c 89 e2             	mov    %r12,%rdx
    31c4:	4c 89 ee             	mov    %r13,%rsi
    31c7:	4c 89 ff             	mov    %r15,%rdi
    31ca:	e8 00 00 00 00       	callq  31cf <zap_page_range+0x13f>
    31cf:	eb bf                	jmp    3190 <zap_page_range+0x100>
    31d1:	66 66 66 66 66 66 2e 	data32 data32 data32 data32 data32 nopw %cs:0x0(%rax,%rax,1)
    31d8:	0f 1f 84 00 00 00 00 
    31df:	00 

00000000000031e0 <follow_page_mask>:
 * by a page descriptor (see also vm_normal_page()).
 */
struct page *follow_page_mask(struct vm_area_struct *vma,
			unsigned long address, unsigned int flags,
			unsigned int *page_mask)
{
    31e0:	e8 00 00 00 00       	callq  31e5 <follow_page_mask+0x5>
    31e5:	55                   	push   %rbp
		BUG_ON(flags & FOLL_GET);
		goto out;
	}

	page = NULL;
	pgd = pgd_offset(mm, address);
    31e6:	48 89 f0             	mov    %rsi,%rax
    31e9:	48 c1 e8 27          	shr    $0x27,%rax
 * by a page descriptor (see also vm_normal_page()).
 */
struct page *follow_page_mask(struct vm_area_struct *vma,
			unsigned long address, unsigned int flags,
			unsigned int *page_mask)
{
    31ed:	48 89 e5             	mov    %rsp,%rbp
    31f0:	41 57                	push   %r15
		BUG_ON(flags & FOLL_GET);
		goto out;
	}

	page = NULL;
	pgd = pgd_offset(mm, address);
    31f2:	25 ff 01 00 00       	and    $0x1ff,%eax
 * by a page descriptor (see also vm_normal_page()).
 */
struct page *follow_page_mask(struct vm_area_struct *vma,
			unsigned long address, unsigned int flags,
			unsigned int *page_mask)
{
    31f7:	41 56                	push   %r14
    31f9:	41 55                	push   %r13
    31fb:	41 89 d5             	mov    %edx,%r13d
    31fe:	41 54                	push   %r12
    3200:	49 89 f4             	mov    %rsi,%r12
    3203:	53                   	push   %rbx
    3204:	48 89 fb             	mov    %rdi,%rbx
    3207:	48 83 ec 28          	sub    $0x28,%rsp
	pud_t *pud;
	pmd_t *pmd;
	pte_t *ptep, pte;
	spinlock_t *ptl;
	struct page *page;
	struct mm_struct *mm = vma->vm_mm;
    320b:	4c 8b 4f 40          	mov    0x40(%rdi),%r9

	*page_mask = 0;
    320f:	c7 01 00 00 00 00    	movl   $0x0,(%rcx)
    3215:	49 8b 51 40          	mov    0x40(%r9),%rdx
    3219:	48 8b 3c c2          	mov    (%rdx,%rax,8),%rdi
		goto out;
	}

	page = NULL;
	pgd = pgd_offset(mm, address);
	if (pgd_none(*pgd) || unlikely(pgd_bad(*pgd)))
    321d:	48 85 ff             	test   %rdi,%rdi
    3220:	74 7e                	je     32a0 <follow_page_mask+0xc0>
	return (pud_t *)pgd_page_vaddr(*pgd) + pud_index(address);
}

static inline int pgd_bad(pgd_t pgd)
{
	return (pgd_flags(pgd) & ~_PAGE_USER) != _KERNPG_TABLE;
    3222:	48 b8 fb 0f 00 00 00 	movabs $0xffffc00000000ffb,%rax
    3229:	c0 ff ff 
    322c:	48 21 f8             	and    %rdi,%rax
    322f:	48 83 f8 63          	cmp    $0x63,%rax
    3233:	75 6b                	jne    32a0 <follow_page_mask+0xc0>

	if (sizeof(pgdval_t) > sizeof(long))
		ret =  PVOP_CALLEE2(pgdval_t, pv_mmu_ops.pgd_val,
				    pgd.pgd, (u64)pgd.pgd >> 32);
	else
		ret =  PVOP_CALLEE1(pgdval_t, pv_mmu_ops.pgd_val,
    3235:	ff 14 25 00 00 00 00 	callq  *0x0
#define pgd_page(pgd)		pfn_to_page(pgd_val(pgd) >> PAGE_SHIFT)

/* to find an entry in a page-table-directory. */
static inline unsigned long pud_index(unsigned long address)
{
	return (address >> PUD_SHIFT) & (PTRS_PER_PUD - 1);
    323c:	48 89 f7             	mov    %rsi,%rdi
	return pgd_flags(pgd) & _PAGE_PRESENT;
}

static inline unsigned long pgd_page_vaddr(pgd_t pgd)
{
	return (unsigned long)__va((unsigned long)pgd_val(pgd) & PTE_PFN_MASK);
    323f:	48 ba 00 f0 ff ff ff 	movabs $0x3ffffffff000,%rdx
    3246:	3f 00 00 
    3249:	48 be 00 00 00 00 00 	movabs $0xffff880000000000,%rsi
    3250:	88 ff ff 
#define pgd_page(pgd)		pfn_to_page(pgd_val(pgd) >> PAGE_SHIFT)

/* to find an entry in a page-table-directory. */
static inline unsigned long pud_index(unsigned long address)
{
	return (address >> PUD_SHIFT) & (PTRS_PER_PUD - 1);
    3253:	48 c1 ef 1b          	shr    $0x1b,%rdi
	return pgd_flags(pgd) & _PAGE_PRESENT;
}

static inline unsigned long pgd_page_vaddr(pgd_t pgd)
{
	return (unsigned long)__va((unsigned long)pgd_val(pgd) & PTE_PFN_MASK);
    3257:	48 21 d0             	and    %rdx,%rax
    325a:	81 e7 f8 0f 00 00    	and    $0xff8,%edi
    3260:	48 01 c7             	add    %rax,%rdi
    3263:	48 8b 3c 37          	mov    (%rdi,%rsi,1),%rdi
	  goto no_page_table;

	pud = pud_offset(pgd, address);
	if (pud_none(*pud))
    3267:	48 85 ff             	test   %rdi,%rdi
    326a:	74 34                	je     32a0 <follow_page_mask+0xc0>
		if (flags & FOLL_GET)
		  goto out;
		page = follow_huge_pud(mm, address, pud, flags & FOLL_WRITE);
		goto out;
	}
	if (unlikely(pud_bad(*pud)))
    326c:	48 b8 98 0f 00 00 00 	movabs $0xffffc00000000f98,%rax
    3273:	c0 ff ff 
    3276:	48 85 c7             	test   %rax,%rdi
    3279:	75 25                	jne    32a0 <follow_page_mask+0xc0>

	if (sizeof(pudval_t) > sizeof(long))
		ret =  PVOP_CALLEE2(pudval_t, pv_mmu_ops.pud_val,
				    pud.pud, (u64)pud.pud >> 32);
	else
		ret =  PVOP_CALLEE1(pudval_t, pv_mmu_ops.pud_val,
    327b:	ff 14 25 00 00 00 00 	callq  *0x0
 * this macro returns the index of the entry in the pmd page which would
 * control the given virtual address
 */
static inline unsigned long pmd_index(unsigned long address)
{
	return (address >> PMD_SHIFT) & (PTRS_PER_PMD - 1);
    3282:	4c 89 e1             	mov    %r12,%rcx
	return pud_flags(pud) & _PAGE_PRESENT;
}

static inline unsigned long pud_page_vaddr(pud_t pud)
{
	return (unsigned long)__va((unsigned long)pud_val(pud) & PTE_PFN_MASK);
    3285:	48 21 d0             	and    %rdx,%rax
 * this macro returns the index of the entry in the pmd page which would
 * control the given virtual address
 */
static inline unsigned long pmd_index(unsigned long address)
{
	return (address >> PMD_SHIFT) & (PTRS_PER_PMD - 1);
    3288:	48 c1 e9 12          	shr    $0x12,%rcx
#define pud_page(pud)		pfn_to_page(pud_val(pud) >> PAGE_SHIFT)

/* Find an entry in the second-level page table.. */
static inline pmd_t *pmd_offset(pud_t *pud, unsigned long address)
{
	return (pmd_t *)pud_page_vaddr(*pud) + pmd_index(address);
    328c:	81 e1 f8 0f 00 00    	and    $0xff8,%ecx
	return pud_flags(pud) & _PAGE_PRESENT;
}

static inline unsigned long pud_page_vaddr(pud_t pud)
{
	return (unsigned long)__va((unsigned long)pud_val(pud) & PTE_PFN_MASK);
    3292:	48 01 f1             	add    %rsi,%rcx
#define pud_page(pud)		pfn_to_page(pud_val(pud) >> PAGE_SHIFT)

/* Find an entry in the second-level page table.. */
static inline pmd_t *pmd_offset(pud_t *pud, unsigned long address)
{
	return (pmd_t *)pud_page_vaddr(*pud) + pmd_index(address);
    3295:	48 01 c1             	add    %rax,%rcx
    3298:	48 8b 39             	mov    (%rcx),%rdi
	  goto no_page_table;

	pmd = pmd_offset(pud, address);
	if (pmd_none(*pmd))
    329b:	48 85 ff             	test   %rdi,%rdi
    329e:	75 40                	jne    32e0 <follow_page_mask+0x100>
	 * page tables.  Return error instead of NULL to skip handle_mm_fault,
	 * then get_dump_page() will return NULL to leave a hole in the dump.
	 * But we can only make this optimization where a hole would surely
	 * be zero-filled if handle_mm_fault() actually did handle it.
	 */
	if ((flags & FOLL_DUMP) &&
    32a0:	41 83 e5 08          	and    $0x8,%r13d
    32a4:	0f 84 36 01 00 00    	je     33e0 <follow_page_mask+0x200>
				(!vma->vm_ops || !vma->vm_ops->fault))
    32aa:	48 8b 93 98 00 00 00 	mov    0x98(%rbx),%rdx
	  return ERR_PTR(-EFAULT);
    32b1:	48 c7 c0 f2 ff ff ff 	mov    $0xfffffffffffffff2,%rax
	 * page tables.  Return error instead of NULL to skip handle_mm_fault,
	 * then get_dump_page() will return NULL to leave a hole in the dump.
	 * But we can only make this optimization where a hole would surely
	 * be zero-filled if handle_mm_fault() actually did handle it.
	 */
	if ((flags & FOLL_DUMP) &&
    32b8:	48 85 d2             	test   %rdx,%rdx
    32bb:	74 0c                	je     32c9 <follow_page_mask+0xe9>
				(!vma->vm_ops || !vma->vm_ops->fault))
	  return ERR_PTR(-EFAULT);
	return page;
    32bd:	48 83 7a 10 01       	cmpq   $0x1,0x10(%rdx)
    32c2:	48 19 c0             	sbb    %rax,%rax
    32c5:	48 83 e0 f2          	and    $0xfffffffffffffff2,%rax
}
    32c9:	48 83 c4 28          	add    $0x28,%rsp
    32cd:	5b                   	pop    %rbx
    32ce:	41 5c                	pop    %r12
    32d0:	41 5d                	pop    %r13
    32d2:	41 5e                	pop    %r14
    32d4:	41 5f                	pop    %r15
    32d6:	5d                   	pop    %rbp
    32d7:	c3                   	retq   
    32d8:	0f 1f 84 00 00 00 00 	nopl   0x0(%rax,%rax,1)
    32df:	00 
				goto out;
			}
		}
		goto out;
	}
	if ((flags & FOLL_NUMA) && pmd_numa(*pmd))
    32e0:	44 89 e8             	mov    %r13d,%eax
    32e3:	25 00 02 00 00       	and    $0x200,%eax
    32e8:	89 45 c0             	mov    %eax,-0x40(%rbp)
    32eb:	74 10                	je     32fd <follow_page_mask+0x11d>
#endif

#ifndef pmd_numa
static inline int pmd_numa(pmd_t pmd)
{
	return (pmd_flags(pmd) &
    32ed:	48 89 f8             	mov    %rdi,%rax
    32f0:	25 01 01 00 00       	and    $0x101,%eax
    32f5:	48 3d 00 01 00 00    	cmp    $0x100,%rax
    32fb:	74 a3                	je     32a0 <follow_page_mask+0xc0>
 * this function returns the index of the entry in the pte page which would
 * control the given virtual address
 */
static inline unsigned long pte_index(unsigned long address)
{
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
    32fd:	4c 89 e0             	mov    %r12,%rax
	return (unsigned long)native_pmd_val(pmd) == 0;
}

static inline unsigned long pmd_page_vaddr(pmd_t pmd)
{
	return (unsigned long)__va(pmd_val(pmd) & PTE_PFN_MASK);
    3300:	49 be 00 00 00 00 00 	movabs $0xffff880000000000,%r14
    3307:	88 ff ff 
}
#endif /* ALLOC_SPLIT_PTLOCKS */

static inline spinlock_t *pte_lockptr(struct mm_struct *mm, pmd_t *pmd)
{
	return ptlock_ptr(pmd_page(*pmd));
    330a:	49 bb 00 f0 ff ff ff 	movabs $0x3ffffffff000,%r11
    3311:	3f 00 00 
 * this function returns the index of the entry in the pte page which would
 * control the given virtual address
 */
static inline unsigned long pte_index(unsigned long address)
{
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
    3314:	48 c1 e8 09          	shr    $0x9,%rax
    3318:	4c 89 4d b0          	mov    %r9,-0x50(%rbp)
    331c:	4c 89 65 b8          	mov    %r12,-0x48(%rbp)
}

static inline pte_t *pte_offset_kernel(pmd_t *pmd, unsigned long address)
{
	return (pte_t *)pmd_page_vaddr(*pmd) + pte_index(address);
    3320:	25 f8 0f 00 00       	and    $0xff8,%eax
	return (unsigned long)native_pmd_val(pmd) == 0;
}

static inline unsigned long pmd_page_vaddr(pmd_t pmd)
{
	return (unsigned long)__va(pmd_val(pmd) & PTE_PFN_MASK);
    3325:	4c 01 f0             	add    %r14,%rax
    3328:	49 89 ce             	mov    %rcx,%r14
    332b:	48 89 45 d0          	mov    %rax,-0x30(%rbp)
		/*
		 * KSM's break_ksm() relies upon recognizing a ksm page
		 * even while it is being migrated, so for that case we
		 * need migration_entry_wait().
		 */
		if (likely(!(flags & FOLL_MIGRATION)))
    332f:	44 89 e8             	mov    %r13d,%eax
    3332:	25 00 04 00 00       	and    $0x400,%eax
    3337:	89 45 c4             	mov    %eax,-0x3c(%rbp)

static inline int pmd_bad(pmd_t pmd)
{
#ifdef CONFIG_NUMA_BALANCING
	/* pmd_numa check */
	if ((pmd_flags(pmd) & (_PAGE_NUMA|_PAGE_PRESENT)) == _PAGE_NUMA)
    333a:	48 89 f8             	mov    %rdi,%rax
    333d:	25 01 01 00 00       	and    $0x101,%eax
    3342:	48 3d 00 01 00 00    	cmp    $0x100,%rax
    3348:	74 17                	je     3361 <follow_page_mask+0x181>
		return 0;
#endif
	return (pmd_flags(pmd) & ~_PAGE_USER) != _KERNPG_TABLE;
    334a:	48 b8 fb 0f 00 00 00 	movabs $0xffffc00000000ffb,%rax
    3351:	c0 ff ff 
    3354:	48 21 f8             	and    %rdi,%rax
		} else
		  spin_unlock(ptl);
		/* fall through */
	}
split_fallthrough:
	if (unlikely(pmd_bad(*pmd)))
    3357:	48 83 f8 63          	cmp    $0x63,%rax
    335b:	0f 85 3f ff ff ff    	jne    32a0 <follow_page_mask+0xc0>

	if (sizeof(pmdval_t) > sizeof(long))
		ret =  PVOP_CALLEE2(pmdval_t, pv_mmu_ops.pmd_val,
				    pmd.pmd, (u64)pmd.pmd >> 32);
	else
		ret =  PVOP_CALLEE1(pmdval_t, pv_mmu_ops.pmd_val,
    3361:	ff 14 25 00 00 00 00 	callq  *0x0
    3368:	4c 21 d8             	and    %r11,%rax
    336b:	48 be 00 00 00 00 00 	movabs $0xffffea0000000000,%rsi
    3372:	ea ff ff 
    3375:	4c 89 5d c8          	mov    %r11,-0x38(%rbp)
    3379:	48 c1 e8 06          	shr    $0x6,%rax
    337d:	49 8b 3e             	mov    (%r14),%rdi
    3380:	4c 8b 64 30 30       	mov    0x30(%rax,%rsi,1),%r12
    3385:	ff 14 25 00 00 00 00 	callq  *0x0
	raw_spin_lock_init(&(_lock)->rlock);		\
} while (0)

static inline void spin_lock(spinlock_t *lock)
{
	raw_spin_lock(&lock->rlock);
    338c:	4c 89 e7             	mov    %r12,%rdi
    338f:	49 89 c7             	mov    %rax,%r15
    3392:	e8 00 00 00 00       	callq  3397 <follow_page_mask+0x1b7>
	return (unsigned long)native_pmd_val(pmd) == 0;
}

static inline unsigned long pmd_page_vaddr(pmd_t pmd)
{
	return (unsigned long)__va(pmd_val(pmd) & PTE_PFN_MASK);
    3397:	4c 8b 5d c8          	mov    -0x38(%rbp),%r11
	  goto no_page_table;

	ptep = pte_offset_map_lock(mm, pmd, address, &ptl);

	pte = *ptep;
    339b:	48 8b 45 d0          	mov    -0x30(%rbp),%rax
    339f:	4d 21 df             	and    %r11,%r15
    33a2:	49 8b 3c 07          	mov    (%r15,%rax,1),%rdi
    33a6:	48 b8 ff 0f 00 00 00 	movabs $0xffffc00000000fff,%rax
    33ad:	c0 ff ff 
    33b0:	48 21 f8             	and    %rdi,%rax
	if (!pte_present(pte)) {
    33b3:	a9 01 01 00 00       	test   $0x101,%eax
	return a.pte == b.pte;
}

static inline int pte_present(pte_t a)
{
	return pte_flags(a) & (_PAGE_PRESENT | _PAGE_PROTNONE |
    33b8:	41 89 c0             	mov    %eax,%r8d
    33bb:	75 34                	jne    33f1 <follow_page_mask+0x211>
		/*
		 * KSM's break_ksm() relies upon recognizing a ksm page
		 * even while it is being migrated, so for that case we
		 * need migration_entry_wait().
		 */
		if (likely(!(flags & FOLL_MIGRATION)))
    33bd:	8b 55 c4             	mov    -0x3c(%rbp),%edx
    33c0:	85 d2                	test   %edx,%edx
    33c2:	0f 85 10 01 00 00    	jne    34d8 <follow_page_mask+0x2f8>
    33c8:	49 89 ff             	mov    %rdi,%r15
	raw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \
} while (0)

static inline void spin_unlock(spinlock_t *lock)
{
	raw_spin_unlock(&lock->rlock);
    33cb:	4c 89 e7             	mov    %r12,%rdi
    33ce:	e8 00 00 00 00       	callq  33d3 <follow_page_mask+0x1f3>
	pte_unmap_unlock(ptep, ptl);
	return ERR_PTR(-EFAULT);

no_page:
	pte_unmap_unlock(ptep, ptl);
	if (!pte_none(pte))
    33d3:	4d 85 ff             	test   %r15,%r15
    33d6:	0f 84 c4 fe ff ff    	je     32a0 <follow_page_mask+0xc0>
    33dc:	0f 1f 40 00          	nopl   0x0(%rax)
	  return page;
    33e0:	31 c0                	xor    %eax,%eax
	 */
	if ((flags & FOLL_DUMP) &&
				(!vma->vm_ops || !vma->vm_ops->fault))
	  return ERR_PTR(-EFAULT);
	return page;
}
    33e2:	48 83 c4 28          	add    $0x28,%rsp
    33e6:	5b                   	pop    %rbx
    33e7:	41 5c                	pop    %r12
    33e9:	41 5d                	pop    %r13
    33eb:	41 5e                	pop    %r14
    33ed:	41 5f                	pop    %r15
    33ef:	5d                   	pop    %rbp
    33f0:	c3                   	retq   
		  goto no_page;
		pte_unmap_unlock(ptep, ptl);
		migration_entry_wait(mm, pmd, address);
		goto split_fallthrough;
	}
	if ((flags & FOLL_NUMA) && pte_numa(pte))
    33f1:	8b 45 c0             	mov    -0x40(%rbp),%eax
    33f4:	4d 89 e6             	mov    %r12,%r14
    33f7:	49 89 ff             	mov    %rdi,%r15
    33fa:	4c 8b 65 b8          	mov    -0x48(%rbp),%r12
    33fe:	85 c0                	test   %eax,%eax
    3400:	74 14                	je     3416 <follow_page_mask+0x236>
 * (because _PAGE_PRESENT is not set).
 */
#ifndef pte_numa
static inline int pte_numa(pte_t pte)
{
	return (pte_flags(pte) &
    3402:	48 89 f8             	mov    %rdi,%rax
    3405:	25 01 01 00 00       	and    $0x101,%eax
    340a:	48 3d 00 01 00 00    	cmp    $0x100,%rax
    3410:	0f 84 eb 00 00 00    	je     3501 <follow_page_mask+0x321>
	  goto no_page;
	if ((flags & FOLL_WRITE) && !pte_write(pte))
    3416:	44 89 e8             	mov    %r13d,%eax
    3419:	83 e0 01             	and    $0x1,%eax
    341c:	89 45 d0             	mov    %eax,-0x30(%rbp)
    341f:	74 0a                	je     342b <follow_page_mask+0x24b>
    3421:	41 f6 c0 02          	test   $0x2,%r8b
    3425:	0f 84 90 00 00 00    	je     34bb <follow_page_mask+0x2db>
	  goto unlock;

	page = vm_normal_page(vma, address, pte);
    342b:	4c 89 e6             	mov    %r12,%rsi
    342e:	4c 89 fa             	mov    %r15,%rdx
    3431:	48 89 df             	mov    %rbx,%rdi
    3434:	44 89 45 c8          	mov    %r8d,-0x38(%rbp)
    3438:	e8 00 00 00 00       	callq  343d <follow_page_mask+0x25d>
	if (unlikely(!page)) {
    343d:	48 85 c0             	test   %rax,%rax
	if ((flags & FOLL_NUMA) && pte_numa(pte))
	  goto no_page;
	if ((flags & FOLL_WRITE) && !pte_write(pte))
	  goto unlock;

	page = vm_normal_page(vma, address, pte);
    3440:	49 89 c4             	mov    %rax,%r12
	if (unlikely(!page)) {
    3443:	44 8b 45 c8          	mov    -0x38(%rbp),%r8d
    3447:	0f 84 d0 00 00 00    	je     351d <follow_page_mask+0x33d>
					!is_zero_pfn(pte_pfn(pte)))
		  goto bad_page;
		page = pte_page(pte);
	}

	if (flags & FOLL_GET)
    344d:	41 f6 c5 04          	test   $0x4,%r13b
    3451:	75 6d                	jne    34c0 <follow_page_mask+0x2e0>
	  get_page_foll(page);
	if (flags & FOLL_TOUCH) {
    3453:	41 f6 c5 02          	test   $0x2,%r13b
    3457:	74 20                	je     3479 <follow_page_mask+0x299>
		if ((flags & FOLL_WRITE) &&
    3459:	83 7d d0 00          	cmpl   $0x0,-0x30(%rbp)
    345d:	74 12                	je     3471 <follow_page_mask+0x291>
    345f:	41 80 e0 40          	and    $0x40,%r8b
    3463:	75 0c                	jne    3471 <follow_page_mask+0x291>
    3465:	49 8b 04 24          	mov    (%r12),%rax
					!pte_dirty(pte) && !PageDirty(page))
    3469:	a8 10                	test   $0x10,%al
    346b:	0f 84 9f 00 00 00    	je     3510 <follow_page_mask+0x330>
		/*
		 * pte_mkyoung() would be more correct here, but atomic care
		 * is needed to avoid losing the dirty bit: it is easier to use
		 * mark_page_accessed().
		 */
		mark_page_accessed(page);
    3471:	4c 89 e7             	mov    %r12,%rdi
    3474:	e8 00 00 00 00       	callq  3479 <follow_page_mask+0x299>
	}
	if ((flags & FOLL_MLOCK) && (vma->vm_flags & VM_LOCKED)) {
    3479:	41 83 e5 40          	and    $0x40,%r13d
    347d:	74 2c                	je     34ab <follow_page_mask+0x2cb>
    347f:	f6 43 51 20          	testb  $0x20,0x51(%rbx)
    3483:	74 26                	je     34ab <follow_page_mask+0x2cb>
		 *
		 * If the page is already locked, we don't need to
		 * handle it now - vmscan will handle it later if and
		 * when it attempts to reclaim the page.
		 */
		if (page->mapping && trylock_page(page)) {
    3485:	49 83 7c 24 08 00    	cmpq   $0x0,0x8(%r12)
    348b:	74 1e                	je     34ab <follow_page_mask+0x2cb>
 * This operation is atomic and cannot be reordered.
 * It also implies a memory barrier.
 */
static inline int test_and_set_bit(long nr, volatile unsigned long *addr)
{
	GEN_BINARY_RMWcc(LOCK_PREFIX "bts", *addr, "Ir", nr, "%0", "c");
    348d:	f0 41 0f ba 2c 24 00 	lock btsl $0x0,(%r12)
    3494:	72 15                	jb     34ab <follow_page_mask+0x2cb>
			lru_add_drain();  /* push cached pages to LRU */
    3496:	e8 00 00 00 00       	callq  349b <follow_page_mask+0x2bb>
			 * Because we lock page here, and migration is
			 * blocked by the pte's page reference, and we
			 * know the page is still mapped, we don't even
			 * need to check for file-cache page truncation.
			 */
			mlock_vma_page(page);
    349b:	4c 89 e7             	mov    %r12,%rdi
    349e:	e8 00 00 00 00       	callq  34a3 <follow_page_mask+0x2c3>
			unlock_page(page);
    34a3:	4c 89 e7             	mov    %r12,%rdi
    34a6:	e8 00 00 00 00       	callq  34ab <follow_page_mask+0x2cb>
    34ab:	4c 89 f7             	mov    %r14,%rdi
    34ae:	e8 00 00 00 00       	callq  34b3 <follow_page_mask+0x2d3>
		}
	}
unlock:
	pte_unmap_unlock(ptep, ptl);
out:
	return page;
    34b3:	4c 89 e0             	mov    %r12,%rax
    34b6:	e9 0e fe ff ff       	jmpq   32c9 <follow_page_mask+0xe9>
	if (!IS_ERR(page)) {
		BUG_ON(flags & FOLL_GET);
		goto out;
	}

	page = NULL;
    34bb:	45 31 e4             	xor    %r12d,%r12d
    34be:	eb eb                	jmp    34ab <follow_page_mask+0x2cb>
}

static __always_inline int constant_test_bit(long nr, const volatile unsigned long *addr)
{
	return ((1UL << (nr & (BITS_PER_LONG-1))) &
		(addr[nr >> _BITOPS_LONG_SHIFT])) != 0;
    34c0:	49 8b 04 24          	mov    (%r12),%rax
 * follow_page() and it must be called while holding the proper PT
 * lock while the pte (or pmd_trans_huge) is still mapping the page.
 */
static inline void get_page_foll(struct page *page)
{
	if (unlikely(PageTail(page)))
    34c4:	f6 c4 80             	test   $0x80,%ah
    34c7:	0f 85 bb 00 00 00    	jne    3588 <follow_page_mask+0x3a8>
    34cd:	f0 41 ff 44 24 1c    	lock incl 0x1c(%r12)
    34d3:	e9 7b ff ff ff       	jmpq   3453 <follow_page_mask+0x273>
		 * even while it is being migrated, so for that case we
		 * need migration_entry_wait().
		 */
		if (likely(!(flags & FOLL_MIGRATION)))
		  goto no_page;
		if (pte_none(pte) || pte_file(pte))
    34d8:	48 85 ff             	test   %rdi,%rdi
    34db:	0f 84 9a 00 00 00    	je     357b <follow_page_mask+0x39b>
    34e1:	a8 40                	test   $0x40,%al
    34e3:	75 19                	jne    34fe <follow_page_mask+0x31e>
		  goto no_page;
		entry = pte_to_swp_entry(pte);
    34e5:	e8 b6 cc ff ff       	callq  1a0 <pte_to_swp_entry>
 * Extract the `type' field from a swp_entry_t.  The swp_entry_t is in
 * arch-independent format
 */
static inline unsigned swp_type(swp_entry_t entry)
{
	return (entry.val  >> SWP_TYPE_SHIFT(entry));
    34ea:	48 c1 e8 39          	shr    $0x39,%rax
			page_to_pfn(page));
}

static inline int is_migration_entry(swp_entry_t entry)
{
	return unlikely(swp_type(entry) == SWP_MIGRATION_READ ||
    34ee:	4c 8b 5d c8          	mov    -0x38(%rbp),%r11
    34f2:	83 e8 1e             	sub    $0x1e,%eax
    34f5:	83 f8 01             	cmp    $0x1,%eax
    34f8:	0f 86 a4 00 00 00    	jbe    35a2 <follow_page_mask+0x3c2>
    34fe:	4d 89 e6             	mov    %r12,%r14
    3501:	4c 89 f7             	mov    %r14,%rdi
    3504:	e8 00 00 00 00       	callq  3509 <follow_page_mask+0x329>
	return ERR_PTR(-EFAULT);

no_page:
	pte_unmap_unlock(ptep, ptl);
	if (!pte_none(pte))
	  return page;
    3509:	31 c0                	xor    %eax,%eax
    350b:	e9 d2 fe ff ff       	jmpq   33e2 <follow_page_mask+0x202>
	if (flags & FOLL_GET)
	  get_page_foll(page);
	if (flags & FOLL_TOUCH) {
		if ((flags & FOLL_WRITE) &&
					!pte_dirty(pte) && !PageDirty(page))
		  set_page_dirty(page);
    3510:	4c 89 e7             	mov    %r12,%rdi
    3513:	e8 00 00 00 00       	callq  3518 <follow_page_mask+0x338>
    3518:	e9 54 ff ff ff       	jmpq   3471 <follow_page_mask+0x291>
	if ((flags & FOLL_WRITE) && !pte_write(pte))
	  goto unlock;

	page = vm_normal_page(vma, address, pte);
	if (unlikely(!page)) {
		if ((flags & FOLL_DUMP) ||
    351d:	41 f6 c5 08          	test   $0x8,%r13b
    3521:	75 24                	jne    3547 <follow_page_mask+0x367>

	if (sizeof(pteval_t) > sizeof(long))
		ret = PVOP_CALLEE2(pteval_t, pv_mmu_ops.pte_val,
				   pte.pte, (u64)pte.pte >> 32);
	else
		ret = PVOP_CALLEE1(pteval_t, pv_mmu_ops.pte_val,
    3523:	4c 89 ff             	mov    %r15,%rdi
    3526:	ff 14 25 00 00 00 00 	callq  *0x0
	return pte_flags(pte) & _PAGE_SPECIAL;
}

static inline unsigned long pte_pfn(pte_t pte)
{
	return (pte_val(pte) & PTE_PFN_MASK) >> PAGE_SHIFT;
    352d:	48 ba 00 f0 ff ff ff 	movabs $0x3ffffffff000,%rdx
    3534:	3f 00 00 
    3537:	48 21 d0             	and    %rdx,%rax
    353a:	48 c1 e8 0c          	shr    $0xc,%rax
    353e:	48 3b 05 00 00 00 00 	cmp    0x0(%rip),%rax        # 3545 <follow_page_mask+0x365>
    3545:	74 14                	je     355b <follow_page_mask+0x37b>
    3547:	4c 89 f7             	mov    %r14,%rdi
    354a:	e8 00 00 00 00       	callq  354f <follow_page_mask+0x36f>
out:
	return page;

bad_page:
	pte_unmap_unlock(ptep, ptl);
	return ERR_PTR(-EFAULT);
    354f:	48 c7 c0 f2 ff ff ff 	mov    $0xfffffffffffffff2,%rax
    3556:	e9 6e fd ff ff       	jmpq   32c9 <follow_page_mask+0xe9>
    355b:	ff 14 25 00 00 00 00 	callq  *0x0
    3562:	48 21 d0             	and    %rdx,%rax
	page = vm_normal_page(vma, address, pte);
	if (unlikely(!page)) {
		if ((flags & FOLL_DUMP) ||
					!is_zero_pfn(pte_pfn(pte)))
		  goto bad_page;
		page = pte_page(pte);
    3565:	49 bc 00 00 00 00 00 	movabs $0xffffea0000000000,%r12
    356c:	ea ff ff 
    356f:	48 c1 e8 06          	shr    $0x6,%rax
    3573:	49 01 c4             	add    %rax,%r12
    3576:	e9 d2 fe ff ff       	jmpq   344d <follow_page_mask+0x26d>
    357b:	4c 89 e7             	mov    %r12,%rdi
    357e:	e8 00 00 00 00       	callq  3583 <follow_page_mask+0x3a3>
    3583:	e9 18 fd ff ff       	jmpq   32a0 <follow_page_mask+0xc0>
		/*
		 * This is safe only because
		 * __split_huge_page_refcount() can't run under
		 * get_page_foll() because we hold the proper PT lock.
		 */
		__get_page_tail_foll(page, true);
    3588:	be 01 00 00 00       	mov    $0x1,%esi
    358d:	4c 89 e7             	mov    %r12,%rdi
    3590:	44 89 45 c8          	mov    %r8d,-0x38(%rbp)
    3594:	e8 00 00 00 00       	callq  3599 <follow_page_mask+0x3b9>
    3599:	44 8b 45 c8          	mov    -0x38(%rbp),%r8d
    359d:	e9 b1 fe ff ff       	jmpq   3453 <follow_page_mask+0x273>
    35a2:	4c 89 e7             	mov    %r12,%rdi
    35a5:	4c 89 5d c8          	mov    %r11,-0x38(%rbp)
    35a9:	e8 00 00 00 00       	callq  35ae <follow_page_mask+0x3ce>
		  goto no_page;
		entry = pte_to_swp_entry(pte);
		if (!is_migration_entry(entry))
		  goto no_page;
		pte_unmap_unlock(ptep, ptl);
		migration_entry_wait(mm, pmd, address);
    35ae:	48 8b 7d b0          	mov    -0x50(%rbp),%rdi
    35b2:	48 8b 55 b8          	mov    -0x48(%rbp),%rdx
    35b6:	4c 89 f6             	mov    %r14,%rsi
    35b9:	e8 00 00 00 00       	callq  35be <follow_page_mask+0x3de>
    35be:	49 8b 3e             	mov    (%r14),%rdi
    35c1:	4c 8b 5d c8          	mov    -0x38(%rbp),%r11
    35c5:	e9 70 fd ff ff       	jmpq   333a <follow_page_mask+0x15a>
    35ca:	66 0f 1f 44 00 00    	nopw   0x0(%rax,%rax,1)

00000000000035d0 <del_page_from_coa>:
 * but allow concurrent faults), with pte both mapped and locked.
 * We return with mmap_sem still held, but pte unmapped and unlocked.
 */

struct sclock_coa_parent *get_original(struct sclock_coa_parent *coa_parent);
void del_page_from_coa(struct page* old_page){
    35d0:	e8 00 00 00 00       	callq  35d5 <del_page_from_coa+0x5>
    35d5:	55                   	push   %rbp
	if(!old_page)
    35d6:	48 85 ff             	test   %rdi,%rdi
 * but allow concurrent faults), with pte both mapped and locked.
 * We return with mmap_sem still held, but pte unmapped and unlocked.
 */

struct sclock_coa_parent *get_original(struct sclock_coa_parent *coa_parent);
void del_page_from_coa(struct page* old_page){
    35d9:	48 89 e5             	mov    %rsp,%rbp
    35dc:	41 54                	push   %r12
    35de:	53                   	push   %rbx
    35df:	48 89 fb             	mov    %rdi,%rbx
	if(!old_page)
    35e2:	74 09                	je     35ed <del_page_from_coa+0x1d>
	  return;
	//retry:
	if(old_page->coa_head){
    35e4:	4c 8b 67 38          	mov    0x38(%rdi),%r12
    35e8:	4d 85 e4             	test   %r12,%r12
    35eb:	75 05                	jne    35f2 <del_page_from_coa+0x22>
		}else{
			del_coa_parent_slow(coa_entry);
		}
		coa_list_unlock(get_original(coa_entry));
	}
}
    35ed:	5b                   	pop    %rbx
    35ee:	41 5c                	pop    %r12
    35f0:	5d                   	pop    %rbp
    35f1:	c3                   	retq   
	return !memcmp_pages(page1, page2);
}


struct sclock_coa_parent *get_original(struct sclock_coa_parent *coa_parent){
	if(coa_parent->parent_head==NULL){
    35f2:	49 8b 44 24 18       	mov    0x18(%r12),%rax
    35f7:	48 85 c0             	test   %rax,%rax
    35fa:	49 0f 44 c4          	cmove  %r12,%rax
	atomic_t copy_counter;
	struct list_head node;
	struct mutex lock;
};
static inline void coa_list_lock(struct sclock_coa_parent* coa_parent){
	mutex_lock(&coa_parent->lock);
    35fe:	48 8d 78 40          	lea    0x40(%rax),%rdi
    3602:	e8 00 00 00 00       	callq  3607 <del_page_from_coa+0x37>
	if(old_page->coa_head){
		unsigned long flags;
		struct sclock_coa_parent* coa_entry,*coa_parent;
		coa_entry=list_entry(old_page->coa_head,struct sclock_coa_parent,head);
		coa_list_lock(get_original(coa_entry));
		printk("del page from copy on write\n");
    3607:	31 c0                	xor    %eax,%eax
    3609:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
    3610:	e8 00 00 00 00       	callq  3615 <del_page_from_coa+0x45>
		if(coa_entry->pfn!=page_to_pfn(old_page)){
    3615:	48 b8 00 00 00 00 00 	movabs $0x160000000000,%rax
    361c:	16 00 00 
    361f:	48 01 c3             	add    %rax,%rbx
    3622:	48 c1 fb 06          	sar    $0x6,%rbx
    3626:	49 39 5c 24 10       	cmp    %rbx,0x10(%r12)
    362b:	75 28                	jne    3655 <del_page_from_coa+0x85>
			coa_list_unlock(get_original(coa_entry));
			printk("different, give up....\n");
			return;
		}
		if(coa_entry->parent_head){
    362d:	49 83 7c 24 18 00    	cmpq   $0x0,0x18(%r12)
    3633:	74 48                	je     367d <del_page_from_coa+0xad>
			//	struct sclock_coa_parent *coa_parent=list_entry(coa_entry->parent_head,struct sclock_coa_parent,head);
			coa_entry->pfn=COA_DEL;
    3635:	49 c7 44 24 10 ff ff 	movq   $0xffffffffffffffff,0x10(%r12)
    363c:	ff ff 
	return !memcmp_pages(page1, page2);
}


struct sclock_coa_parent *get_original(struct sclock_coa_parent *coa_parent){
	if(coa_parent->parent_head==NULL){
    363e:	49 8b 44 24 18       	mov    0x18(%r12),%rax
    3643:	48 85 c0             	test   %rax,%rax
    3646:	49 0f 44 c4          	cmove  %r12,%rax
static inline int coa_list_trylock(struct sclock_coa_parent* coa_parent){
	return mutex_trylock(&coa_parent->lock);
}
long RedoRequestProtectOthers(struct task_struct *p);
static inline void coa_list_unlock(struct sclock_coa_parent* coa_parent){
	mutex_unlock(&coa_parent->lock);
    364a:	48 8d 78 40          	lea    0x40(%rax),%rdi
    364e:	e8 00 00 00 00       	callq  3653 <del_page_from_coa+0x83>
    3653:	eb 98                	jmp    35ed <del_page_from_coa+0x1d>
    3655:	49 8b 44 24 18       	mov    0x18(%r12),%rax
    365a:	48 85 c0             	test   %rax,%rax
    365d:	49 0f 44 c4          	cmove  %r12,%rax
    3661:	48 8d 78 40          	lea    0x40(%rax),%rdi
    3665:	e8 00 00 00 00       	callq  366a <del_page_from_coa+0x9a>
		coa_entry=list_entry(old_page->coa_head,struct sclock_coa_parent,head);
		coa_list_lock(get_original(coa_entry));
		printk("del page from copy on write\n");
		if(coa_entry->pfn!=page_to_pfn(old_page)){
			coa_list_unlock(get_original(coa_entry));
			printk("different, give up....\n");
    366a:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
    3671:	31 c0                	xor    %eax,%eax
    3673:	e8 00 00 00 00       	callq  3678 <del_page_from_coa+0xa8>
			return;
    3678:	e9 70 ff ff ff       	jmpq   35ed <del_page_from_coa+0x1d>
		if(coa_entry->parent_head){
			//	struct sclock_coa_parent *coa_parent=list_entry(coa_entry->parent_head,struct sclock_coa_parent,head);
			coa_entry->pfn=COA_DEL;
			//goto reuse;
		}else{
			del_coa_parent_slow(coa_entry);
    367d:	4c 89 e7             	mov    %r12,%rdi
    3680:	e8 00 00 00 00       	callq  3685 <del_page_from_coa+0xb5>
    3685:	eb b7                	jmp    363e <del_page_from_coa+0x6e>
    3687:	66 0f 1f 84 00 00 00 	nopw   0x0(%rax,%rax,1)
    368e:	00 00 

0000000000003690 <do_wp_page>:

static int do_wp_page(struct mm_struct *mm, struct vm_area_struct *vma,
			unsigned long address, pte_t *page_table, pmd_t *pmd,
			spinlock_t *ptl, pte_t orig_pte)
__releases(ptl)
{
    3690:	e8 00 00 00 00       	callq  3695 <do_wp_page+0x5>
    3695:	55                   	push   %rbp
    3696:	48 89 e5             	mov    %rsp,%rbp
    3699:	41 57                	push   %r15
    369b:	4d 89 cf             	mov    %r9,%r15
    369e:	41 56                	push   %r14
    36a0:	4d 89 c6             	mov    %r8,%r14
    36a3:	41 55                	push   %r13
    36a5:	49 89 d5             	mov    %rdx,%r13
    36a8:	41 54                	push   %r12
    36aa:	49 89 f4             	mov    %rsi,%r12
	int ret = 0;
	int page_mkwrite = 0;
	struct page *dirty_page = NULL;
	unsigned long mmun_start = 0;	/* For mmu_notifiers */
	unsigned long mmun_end = 0;	/* For mmu_notifiers */
	old_page = vm_normal_page(vma, address, orig_pte);
    36ad:	4c 89 ee             	mov    %r13,%rsi

static int do_wp_page(struct mm_struct *mm, struct vm_area_struct *vma,
			unsigned long address, pte_t *page_table, pmd_t *pmd,
			spinlock_t *ptl, pte_t orig_pte)
__releases(ptl)
{
    36b0:	53                   	push   %rbx
    36b1:	48 83 ec 50          	sub    $0x50,%rsp
	int ret = 0;
	int page_mkwrite = 0;
	struct page *dirty_page = NULL;
	unsigned long mmun_start = 0;	/* For mmu_notifiers */
	unsigned long mmun_end = 0;	/* For mmu_notifiers */
	old_page = vm_normal_page(vma, address, orig_pte);
    36b5:	48 8b 55 10          	mov    0x10(%rbp),%rdx

static int do_wp_page(struct mm_struct *mm, struct vm_area_struct *vma,
			unsigned long address, pte_t *page_table, pmd_t *pmd,
			spinlock_t *ptl, pte_t orig_pte)
__releases(ptl)
{
    36b9:	48 89 7d b0          	mov    %rdi,-0x50(%rbp)
	int ret = 0;
	int page_mkwrite = 0;
	struct page *dirty_page = NULL;
	unsigned long mmun_start = 0;	/* For mmu_notifiers */
	unsigned long mmun_end = 0;	/* For mmu_notifiers */
	old_page = vm_normal_page(vma, address, orig_pte);
    36bd:	4c 89 e7             	mov    %r12,%rdi

static int do_wp_page(struct mm_struct *mm, struct vm_area_struct *vma,
			unsigned long address, pte_t *page_table, pmd_t *pmd,
			spinlock_t *ptl, pte_t orig_pte)
__releases(ptl)
{
    36c0:	48 89 4d a8          	mov    %rcx,-0x58(%rbp)
	int ret = 0;
	int page_mkwrite = 0;
	struct page *dirty_page = NULL;
	unsigned long mmun_start = 0;	/* For mmu_notifiers */
	unsigned long mmun_end = 0;	/* For mmu_notifiers */
	old_page = vm_normal_page(vma, address, orig_pte);
    36c4:	e8 00 00 00 00       	callq  36c9 <do_wp_page+0x39>
	unsigned long flags;
	if (!old_page) {
    36c9:	48 85 c0             	test   %rax,%rax
	int ret = 0;
	int page_mkwrite = 0;
	struct page *dirty_page = NULL;
	unsigned long mmun_start = 0;	/* For mmu_notifiers */
	unsigned long mmun_end = 0;	/* For mmu_notifiers */
	old_page = vm_normal_page(vma, address, orig_pte);
    36cc:	48 89 c3             	mov    %rax,%rbx
	unsigned long flags;
	if (!old_page) {
    36cf:	0f 84 5b 05 00 00    	je     3c30 <do_wp_page+0x5a0>
	return page->mapping;
}

static inline int PageAnon(struct page *page)
{
	return ((unsigned long)page->mapping & PAGE_MAPPING_ANON) != 0;
    36d5:	48 8b 40 08          	mov    0x8(%rax),%rax
	}
	/*
	 * Take out anonymous pages first, anonymous shared vmas are
	 * not dirty accountable.
	 */
	if (PageAnon(old_page) && !PageKsm(old_page)) {
    36d9:	a8 01                	test   $0x1,%al
    36db:	0f 85 4f 03 00 00    	jne    3a30 <do_wp_page+0x3a0>
			page_move_anon_rmap(old_page, vma, address);
			unlock_page(old_page);
			goto reuse;
		}
		unlock_page(old_page);
	} else if (unlikely((vma->vm_flags & (VM_WRITE|VM_SHARED)) ==
    36e1:	49 8b 44 24 50       	mov    0x50(%r12),%rax
    36e6:	83 e0 0a             	and    $0xa,%eax
    36e9:	48 83 f8 0a          	cmp    $0xa,%rax
    36ed:	0f 84 37 06 00 00    	je     3d2a <do_wp_page+0x69a>
    36f3:	48 8b 03             	mov    (%rbx),%rax
void get_huge_page_tail(struct page *page);/*ziqiao*/
extern bool __get_page_tail(struct page *page);

static inline void get_page(struct page *page)
{
	if (unlikely(PageTail(page)))
    36f6:	f6 c4 80             	test   $0x80,%ah
    36f9:	0f 85 ed 06 00 00    	jne    3dec <do_wp_page+0x75c>
    36ff:	f0 ff 43 1c          	lock incl 0x1c(%rbx)
    3703:	4c 89 ff             	mov    %r15,%rdi
    3706:	e8 00 00 00 00       	callq  370b <do_wp_page+0x7b>
	 */
	page_cache_get(old_page);
gotten:
	pte_unmap_unlock(page_table, ptl);

	if (unlikely(anon_vma_prepare(vma)))
    370b:	4c 89 e7             	mov    %r12,%rdi
    370e:	e8 00 00 00 00       	callq  3713 <do_wp_page+0x83>
    3713:	85 c0                	test   %eax,%eax
    3715:	0f 85 1d 04 00 00    	jne    3b38 <do_wp_page+0x4a8>
    371b:	48 8b 7d 10          	mov    0x10(%rbp),%rdi
    371f:	ff 14 25 00 00 00 00 	callq  *0x0
    3726:	48 c1 e0 12          	shl    $0x12,%rax
    372a:	31 f6                	xor    %esi,%esi
    372c:	4c 89 e9             	mov    %r13,%rcx
    372f:	48 c1 e8 1e          	shr    $0x1e,%rax
    3733:	4c 89 e2             	mov    %r12,%rdx
    3736:	65 44 8b 04 25 00 00 	mov    %gs:0x0,%r8d
    373d:	00 00 
	  goto oom;

	if (is_zero_pfn(pte_pfn(orig_pte))) {
    373f:	48 3b 05 00 00 00 00 	cmp    0x0(%rip),%rax        # 3746 <do_wp_page+0xb6>
    3746:	0f 84 14 04 00 00    	je     3b60 <do_wp_page+0x4d0>
		new_page = alloc_zeroed_user_highpage_movable(vma, address);
		if (!new_page)
		  goto oom;
	} else {
		new_page = alloc_page_vma(GFP_HIGHUSER_MOVABLE, vma, address);
    374c:	bf da 00 02 00       	mov    $0x200da,%edi
    3751:	e8 00 00 00 00       	callq  3756 <do_wp_page+0xc6>
		if (!new_page)
    3756:	48 85 c0             	test   %rax,%rax
	if (is_zero_pfn(pte_pfn(orig_pte))) {
		new_page = alloc_zeroed_user_highpage_movable(vma, address);
		if (!new_page)
		  goto oom;
	} else {
		new_page = alloc_page_vma(GFP_HIGHUSER_MOVABLE, vma, address);
    3759:	49 89 c7             	mov    %rax,%r15
		if (!new_page)
    375c:	0f 84 d6 03 00 00    	je     3b38 <do_wp_page+0x4a8>
	 * If the source page was a PFN mapping, we don't have
	 * a "struct page" for it. We do a best-effort copy by
	 * just copying from the original user address. If that
	 * fails, we just zero-fill it. Live with it.
	 */
	if (unlikely(!src)) {
    3762:	48 85 db             	test   %rbx,%rbx
    3765:	0f 84 96 06 00 00    	je     3e01 <do_wp_page+0x771>
 * The various preempt_count add/sub methods
 */

static __always_inline void __preempt_count_add(int val)
{
	__this_cpu_add_4(__preempt_count, val);
    376b:	65 ff 04 25 00 00 00 	incl   %gs:0x0
    3772:	00 
    3773:	65 ff 04 25 00 00 00 	incl   %gs:0x0
    377a:	00 
 */
#include <linux/vmstat.h>

static __always_inline void *lowmem_page_address(const struct page *page)
{
	return __va(PFN_PHYS(page_to_pfn(page)));
    377b:	48 b8 00 00 00 00 00 	movabs $0x160000000000,%rax
    3782:	16 00 00 
    3785:	48 bf 00 00 00 00 00 	movabs $0xffff880000000000,%rdi
    378c:	88 ff ff 
    378f:	48 8d 34 03          	lea    (%rbx,%rax,1),%rsi
    3793:	4c 01 f8             	add    %r15,%rax
    3796:	48 c1 f8 06          	sar    $0x6,%rax
    379a:	48 c1 fe 06          	sar    $0x6,%rsi
    379e:	48 c1 e0 0c          	shl    $0xc,%rax
    37a2:	48 c1 e6 0c          	shl    $0xc,%rsi
    37a6:	48 01 fe             	add    %rdi,%rsi
    37a9:	48 01 c7             	add    %rax,%rdi
    37ac:	e8 00 00 00 00       	callq  37b1 <do_wp_page+0x121>
}

static __always_inline void __preempt_count_sub(int val)
{
	__this_cpu_add_4(__preempt_count, -val);
    37b1:	65 ff 0c 25 00 00 00 	decl   %gs:0x0
    37b8:	00 
    37b9:	65 ff 0c 25 00 00 00 	decl   %gs:0x0
    37c0:	00 
 * If it's called on the same region of memory simultaneously, the effect
 * may be that only one operation succeeds.
 */
static inline void __set_bit(long nr, volatile unsigned long *addr)
{
	asm volatile("bts %1,%0" : ADDR : "Ir" (nr) : "memory");
    37c1:	41 0f ba 2f 03       	btsl   $0x3,(%r15)
		if (!new_page)
		  goto oom;
		cow_user_page(new_page, old_page, address, vma);
	}
	__SetPageUptodate(new_page);
	if (mem_cgroup_newpage_charge(new_page, mm, GFP_KERNEL))
    37c6:	48 8b 75 b0          	mov    -0x50(%rbp),%rsi
    37ca:	ba d0 00 00 00       	mov    $0xd0,%edx
    37cf:	4c 89 ff             	mov    %r15,%rdi
    37d2:	e8 00 00 00 00       	callq  37d7 <do_wp_page+0x147>
    37d7:	85 c0                	test   %eax,%eax
    37d9:	0f 85 51 03 00 00    	jne    3b30 <do_wp_page+0x4a0>
	  goto oom_free_new;
	mmun_start  = address & PAGE_MASK;
    37df:	4c 89 e8             	mov    %r13,%rax
    37e2:	48 25 00 f0 ff ff    	and    $0xfffffffffffff000,%rax
	mmun_end    = mmun_start + PAGE_SIZE;
    37e8:	48 8d 88 00 10 00 00 	lea    0x1000(%rax),%rcx
		cow_user_page(new_page, old_page, address, vma);
	}
	__SetPageUptodate(new_page);
	if (mem_cgroup_newpage_charge(new_page, mm, GFP_KERNEL))
	  goto oom_free_new;
	mmun_start  = address & PAGE_MASK;
    37ef:	48 89 45 98          	mov    %rax,-0x68(%rbp)
}

static inline void mmu_notifier_invalidate_range_start(struct mm_struct *mm,
				  unsigned long start, unsigned long end)
{
	if (mm_has_notifiers(mm))
    37f3:	48 8b 45 b0          	mov    -0x50(%rbp),%rax
	mmun_end    = mmun_start + PAGE_SIZE;
    37f7:	48 89 4d 90          	mov    %rcx,-0x70(%rbp)
    37fb:	48 83 b8 a0 03 00 00 	cmpq   $0x0,0x3a0(%rax)
    3802:	00 
    3803:	0f 85 3b 06 00 00    	jne    3e44 <do_wp_page+0x7b4>

	if (sizeof(pmdval_t) > sizeof(long))
		ret =  PVOP_CALLEE2(pmdval_t, pv_mmu_ops.pmd_val,
				    pmd.pmd, (u64)pmd.pmd >> 32);
	else
		ret =  PVOP_CALLEE1(pmdval_t, pv_mmu_ops.pmd_val,
    3809:	49 8b 3e             	mov    (%r14),%rdi
    380c:	ff 14 25 00 00 00 00 	callq  *0x0
}
#endif /* ALLOC_SPLIT_PTLOCKS */

static inline spinlock_t *pte_lockptr(struct mm_struct *mm, pmd_t *pmd)
{
	return ptlock_ptr(pmd_page(*pmd));
    3813:	48 be 00 f0 ff ff ff 	movabs $0x3ffffffff000,%rsi
    381a:	3f 00 00 
    381d:	48 ba 00 00 00 00 00 	movabs $0xffffea0000000000,%rdx
    3824:	ea ff ff 
    3827:	49 8b 3e             	mov    (%r14),%rdi
    382a:	48 21 f0             	and    %rsi,%rax
    382d:	48 c1 e8 06          	shr    $0x6,%rax
    3831:	48 8b 44 10 30       	mov    0x30(%rax,%rdx,1),%rax
    3836:	49 89 c2             	mov    %rax,%r10
    3839:	48 89 45 88          	mov    %rax,-0x78(%rbp)
    383d:	ff 14 25 00 00 00 00 	callq  *0x0
 * this function returns the index of the entry in the pte page which would
 * control the given virtual address
 */
static inline unsigned long pte_index(unsigned long address)
{
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
    3844:	4c 89 ea             	mov    %r13,%rdx
	return (unsigned long)native_pmd_val(pmd) == 0;
}

static inline unsigned long pmd_page_vaddr(pmd_t pmd)
{
	return (unsigned long)__va(pmd_val(pmd) & PTE_PFN_MASK);
    3847:	48 b9 00 00 00 00 00 	movabs $0xffff880000000000,%rcx
    384e:	88 ff ff 
    3851:	48 21 f0             	and    %rsi,%rax
 * this function returns the index of the entry in the pte page which would
 * control the given virtual address
 */
static inline unsigned long pte_index(unsigned long address)
{
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
    3854:	48 c1 ea 09          	shr    $0x9,%rdx
	raw_spin_lock_init(&(_lock)->rlock);		\
} while (0)

static inline void spin_lock(spinlock_t *lock)
{
	raw_spin_lock(&lock->rlock);
    3858:	4c 89 d7             	mov    %r10,%rdi
    385b:	4c 89 55 a0          	mov    %r10,-0x60(%rbp)
}

static inline pte_t *pte_offset_kernel(pmd_t *pmd, unsigned long address)
{
	return (pte_t *)pmd_page_vaddr(*pmd) + pte_index(address);
    385f:	81 e2 f8 0f 00 00    	and    $0xff8,%edx
	return (unsigned long)native_pmd_val(pmd) == 0;
}

static inline unsigned long pmd_page_vaddr(pmd_t pmd)
{
	return (unsigned long)__va(pmd_val(pmd) & PTE_PFN_MASK);
    3865:	48 01 ca             	add    %rcx,%rdx
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
}

static inline pte_t *pte_offset_kernel(pmd_t *pmd, unsigned long address)
{
	return (pte_t *)pmd_page_vaddr(*pmd) + pte_index(address);
    3868:	48 01 d0             	add    %rdx,%rax
    386b:	48 89 45 a8          	mov    %rax,-0x58(%rbp)
    386f:	e8 00 00 00 00       	callq  3874 <do_wp_page+0x1e4>
	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);
	/*
	 * Re-check the pte - we dropped the lock
	 */
	page_table = pte_offset_map_lock(mm, pmd, address, &ptl);
	if (likely(pte_same(*page_table, orig_pte))) {
    3874:	48 8b 45 a8          	mov    -0x58(%rbp),%rax
    3878:	48 8b 4d 10          	mov    0x10(%rbp),%rcx
    387c:	48 39 08             	cmp    %rcx,(%rax)
    387f:	0f 85 8c 05 00 00    	jne    3e11 <do_wp_page+0x781>
		if (old_page) {
    3885:	48 85 db             	test   %rbx,%rbx
    3888:	0f 84 72 04 00 00    	je     3d00 <do_wp_page+0x670>
			if (!PageAnon(old_page)) {
    388e:	f6 43 08 01          	testb  $0x1,0x8(%rbx)
    3892:	0f 84 c0 03 00 00    	je     3c58 <do_wp_page+0x5c8>
				inc_mm_counter_fast(mm, MM_ANONPAGES);
			}
		} else
		  inc_mm_counter_fast(mm, MM_ANONPAGES);
		flush_cache_page(vma, address, pte_pfn(orig_pte));
		entry = mk_pte(new_page, vma->vm_page_prot);
    3898:	49 8b 54 24 48       	mov    0x48(%r12),%rdx
    389d:	48 b8 00 00 00 00 00 	movabs $0x4000000000000,%rax
    38a4:	00 04 00 
    38a7:	48 21 d0             	and    %rdx,%rax
    38aa:	48 89 c6             	mov    %rax,%rsi
    38ad:	0f 84 cd 02 00 00    	je     3b80 <do_wp_page+0x4f0>
}

static __always_inline int constant_test_bit(long nr, const volatile unsigned long *addr)
{
	return ((1UL << (nr & (BITS_PER_LONG-1))) &
		(addr[nr >> _BITOPS_LONG_SHIFT])) != 0;
    38b3:	49 8b 07             	mov    (%r15),%rax
    38b6:	48 be 00 00 00 00 00 	movabs $0x8000000000000,%rsi
    38bd:	00 08 00 
    38c0:	a9 00 00 00 02       	test   $0x2000000,%eax
    38c5:	b8 00 00 00 00       	mov    $0x0,%eax
    38ca:	48 0f 44 f0          	cmove  %rax,%rsi
    38ce:	48 89 d0             	mov    %rdx,%rax
    38d1:	48 83 c8 10          	or     $0x10,%rax
    38d5:	48 09 f0             	or     %rsi,%rax
static inline pgprotval_t massage_pgprot(pgprot_t pgprot)
{
	pgprotval_t protval = pgprot_val(pgprot);

	if (protval & _PAGE_PRESENT)
	  protval &= __supported_pte_mask;
    38d8:	48 89 c7             	mov    %rax,%rdi
    38db:	48 ba 00 00 00 00 00 	movabs $0x160000000000,%rdx
    38e2:	16 00 00 
    38e5:	48 23 3d 00 00 00 00 	and    0x0(%rip),%rdi        # 38ec <do_wp_page+0x25c>
    38ec:	4c 01 fa             	add    %r15,%rdx
    38ef:	48 c1 fa 06          	sar    $0x6,%rdx
	return protval;
}

static inline pte_t pfn_pte(unsigned long page_nr, pgprot_t pgprot)
{
	return __pte(((phys_addr_t)page_nr << PAGE_SHIFT) |
    38f3:	48 c1 e2 0c          	shl    $0xc,%rdx
static inline pgprotval_t massage_pgprot(pgprot_t pgprot)
{
	pgprotval_t protval = pgprot_val(pgprot);

	if (protval & _PAGE_PRESENT)
	  protval &= __supported_pte_mask;
    38f7:	a8 01                	test   $0x1,%al
    38f9:	48 0f 44 f8          	cmove  %rax,%rdi
	return protval;
}

static inline pte_t pfn_pte(unsigned long page_nr, pgprot_t pgprot)
{
	return __pte(((phys_addr_t)page_nr << PAGE_SHIFT) |
    38fd:	48 09 d7             	or     %rdx,%rdi
	if (sizeof(pteval_t) > sizeof(long))
		ret = PVOP_CALLEE2(pteval_t,
				   pv_mmu_ops.make_pte,
				   val, (u64)val >> 32);
	else
		ret = PVOP_CALLEE1(pteval_t,
    3900:	ff 14 25 00 00 00 00 	callq  *0x0
 * pte_mkwrite.  But get_user_pages can cause write faults for mappings
 * that do not have writing enabled, when used by access_process_vm.
 */
static inline pte_t maybe_mkwrite(pte_t pte, struct vm_area_struct *vma)
{
	if (likely(vma->vm_flags & VM_WRITE))
    3907:	41 f6 44 24 50 02    	testb  $0x2,0x50(%r12)
    390d:	0f 84 22 05 00 00    	je     3e35 <do_wp_page+0x7a5>

static inline pte_t pte_set_flags(pte_t pte, pteval_t set)
{
	pteval_t v = native_pte_val(pte);

	return native_make_pte(v | set);
    3913:	48 0d 42 08 00 00    	or     $0x842,%rax
    3919:	48 89 45 a0          	mov    %rax,-0x60(%rbp)
		 * Clear the pte entry and flush it first, before updating the
		 * pte with the new entry. This will avoid a race condition
		 * seen in the presence of one thread doing SMC and another
		 * thread doing COW.
		 */
		ptep_clear_flush(vma, address, page_table);
    391d:	48 8b 55 a8          	mov    -0x58(%rbp),%rdx
    3921:	4c 89 ee             	mov    %r13,%rsi
    3924:	4c 89 e7             	mov    %r12,%rdi
    3927:	e8 00 00 00 00       	callq  392c <do_wp_page+0x29c>
		/*
		 * We call the notify macro here because, when using secondary
		 * mmu page tables (such as kvm shadow page tables), we want the
		 * new page to be mapped directly into the secondary page table.
		 */
		page_add_new_anon_rmap(new_page, vma, address);
    392c:	4c 89 ea             	mov    %r13,%rdx
    392f:	4c 89 e6             	mov    %r12,%rsi
    3932:	4c 89 ff             	mov    %r15,%rdi
    3935:	e8 00 00 00 00       	callq  393a <do_wp_page+0x2aa>
}

static inline void mmu_notifier_change_pte(struct mm_struct *mm,
					   unsigned long address, pte_t pte)
{
	if (mm_has_notifiers(mm))
    393a:	48 8b 45 b0          	mov    -0x50(%rbp),%rax
    393e:	48 83 b8 a0 03 00 00 	cmpq   $0x0,0x3a0(%rax)
    3945:	00 
    3946:	0f 85 d5 04 00 00    	jne    3e21 <do_wp_page+0x791>
{
	if (sizeof(pteval_t) > sizeof(long))
		/* 5 arg words */
		pv_mmu_ops.set_pte_at(mm, addr, ptep, pte);
	else
		PVOP_VCALL4(pv_mmu_ops.set_pte_at, mm, addr, ptep, pte.pte);
    394c:	48 8b 7d b0          	mov    -0x50(%rbp),%rdi
    3950:	4c 89 ee             	mov    %r13,%rsi
    3953:	48 8b 55 a8          	mov    -0x58(%rbp),%rdx
    3957:	48 8b 4d a0          	mov    -0x60(%rbp),%rcx
    395b:	ff 14 25 00 00 00 00 	callq  *0x0
	raw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \
} while (0)

static inline void spin_unlock(spinlock_t *lock)
{
	raw_spin_unlock(&lock->rlock);
    3962:	48 8b 7d 88          	mov    -0x78(%rbp),%rdi
    3966:	e8 00 00 00 00       	callq  396b <do_wp_page+0x2db>
		set_pte_at_notify(mm, address, page_table, entry);
		update_mmu_cache(vma, address, page_table);
		pte_unmap_unlock(page_table, ptl);


		if (old_page) {
    396b:	48 85 db             	test   %rbx,%rbx
    396e:	74 13                	je     3983 <do_wp_page+0x2f3>
			 * decremented mapcount is visible. And the old page
			 * cannot be reused until after the decremented
			 * mapcount is visible. So transitively, TLBs to
			 * old page will be flushed before it can be reused.
			 */
			page_remove_rmap(old_page);
    3970:	48 89 df             	mov    %rbx,%rdi
    3973:	e8 00 00 00 00       	callq  3978 <do_wp_page+0x2e8>
			dec_page_counter_in_ns(old_page,vma);
    3978:	4c 89 e6             	mov    %r12,%rsi
    397b:	48 89 df             	mov    %rbx,%rdi
    397e:	e8 00 00 00 00       	callq  3983 <do_wp_page+0x2f3>

	if (sizeof(pmdval_t) > sizeof(long))
		ret =  PVOP_CALLEE2(pmdval_t, pv_mmu_ops.pmd_val,
				    pmd.pmd, (u64)pmd.pmd >> 32);
	else
		ret =  PVOP_CALLEE1(pmdval_t, pv_mmu_ops.pmd_val,
    3983:	49 8b 3e             	mov    (%r14),%rdi
    3986:	ff 14 25 00 00 00 00 	callq  *0x0
}
#endif /* ALLOC_SPLIT_PTLOCKS */

static inline spinlock_t *pte_lockptr(struct mm_struct *mm, pmd_t *pmd)
{
	return ptlock_ptr(pmd_page(*pmd));
    398d:	48 c1 e0 12          	shl    $0x12,%rax
    3991:	48 ba 00 00 00 00 00 	movabs $0xffffea0000000000,%rdx
    3998:	ea ff ff 
    399b:	49 8b 3e             	mov    (%r14),%rdi
    399e:	48 c1 e8 1e          	shr    $0x1e,%rax
    39a2:	48 c1 e0 06          	shl    $0x6,%rax
    39a6:	4c 8b 7c 10 30       	mov    0x30(%rax,%rdx,1),%r15
    39ab:	ff 14 25 00 00 00 00 	callq  *0x0
	raw_spin_lock_init(&(_lock)->rlock);		\
} while (0)

static inline void spin_lock(spinlock_t *lock)
{
	raw_spin_lock(&lock->rlock);
    39b2:	4c 89 ff             	mov    %r15,%rdi
    39b5:	4c 89 7d a0          	mov    %r15,-0x60(%rbp)
		}
		page_table = pte_offset_map_lock(mm, pmd, address, &ptl);

		/* Free the old page.. */
		new_page = old_page;
		ret |= VM_FAULT_WRITE;
    39b9:	41 bd 08 00 00 00    	mov    $0x8,%r13d
    39bf:	e8 00 00 00 00       	callq  39c4 <do_wp_page+0x334>
	} else
	  mem_cgroup_uncharge_page(new_page);

	if (new_page)
    39c4:	48 85 db             	test   %rbx,%rbx
    39c7:	49 89 df             	mov    %rbx,%r15
    39ca:	74 08                	je     39d4 <do_wp_page+0x344>
	  page_cache_release(new_page);
    39cc:	4c 89 ff             	mov    %r15,%rdi
    39cf:	e8 00 00 00 00       	callq  39d4 <do_wp_page+0x344>
	raw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \
} while (0)

static inline void spin_unlock(spinlock_t *lock)
{
	raw_spin_unlock(&lock->rlock);
    39d4:	48 8b 7d a0          	mov    -0x60(%rbp),%rdi
    39d8:	e8 00 00 00 00       	callq  39dd <do_wp_page+0x34d>
unlock:
	pte_unmap_unlock(page_table, ptl);
after_lock:
	if (mmun_end > mmun_start)
    39dd:	48 8b 45 90          	mov    -0x70(%rbp),%rax
    39e1:	48 39 45 98          	cmp    %rax,-0x68(%rbp)
    39e5:	73 12                	jae    39f9 <do_wp_page+0x369>
}

static inline void mmu_notifier_invalidate_range_end(struct mm_struct *mm,
				  unsigned long start, unsigned long end)
{
	if (mm_has_notifiers(mm))
    39e7:	48 8b 45 b0          	mov    -0x50(%rbp),%rax
    39eb:	48 83 b8 a0 03 00 00 	cmpq   $0x0,0x3a0(%rax)
    39f2:	00 
    39f3:	0f 85 6b 04 00 00    	jne    3e64 <do_wp_page+0x7d4>
	  mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);
	if (old_page) {
    39f9:	48 85 db             	test   %rbx,%rbx
    39fc:	0f 84 ee 02 00 00    	je     3cf0 <do_wp_page+0x660>
		/*
		 * Don't let another task, with possibly unlocked vma,
		 * keep the mlocked page.
		 */
		if ((ret & VM_FAULT_WRITE) && (vma->vm_flags & VM_LOCKED)) {
    3a02:	45 85 ed             	test   %r13d,%r13d
    3a05:	74 0c                	je     3a13 <do_wp_page+0x383>
    3a07:	41 f6 44 24 51 20    	testb  $0x20,0x51(%r12)
    3a0d:	0f 85 f5 01 00 00    	jne    3c08 <do_wp_page+0x578>
			lock_page(old_page);	/* LRU manipulation */
			munlock_vma_page(old_page);
			unlock_page(old_page);
		}
		page_cache_release(old_page);
    3a13:	48 89 df             	mov    %rbx,%rdi
    3a16:	e8 00 00 00 00       	callq  3a1b <do_wp_page+0x38b>
	return VM_FAULT_OOM;

unwritable_page:
	page_cache_release(old_page);
	return ret;
}
    3a1b:	48 83 c4 50          	add    $0x50,%rsp
		if ((ret & VM_FAULT_WRITE) && (vma->vm_flags & VM_LOCKED)) {
			lock_page(old_page);	/* LRU manipulation */
			munlock_vma_page(old_page);
			unlock_page(old_page);
		}
		page_cache_release(old_page);
    3a1f:	44 89 e8             	mov    %r13d,%eax
	return VM_FAULT_OOM;

unwritable_page:
	page_cache_release(old_page);
	return ret;
}
    3a22:	5b                   	pop    %rbx
    3a23:	41 5c                	pop    %r12
    3a25:	41 5d                	pop    %r13
    3a27:	41 5e                	pop    %r14
    3a29:	41 5f                	pop    %r15
    3a2b:	5d                   	pop    %rbp
    3a2c:	c3                   	retq   
    3a2d:	0f 1f 00             	nopl   (%rax)
 * is found in VM_MERGEABLE vmas.  It's a PageAnon page, pointing not to any
 * anon_vma, but to that page's node of the stable tree.
 */
static inline int PageKsm(struct page *page)
{
	return ((unsigned long)page->mapping & PAGE_MAPPING_FLAGS) ==
    3a30:	83 e0 03             	and    $0x3,%eax
	}
	/*
	 * Take out anonymous pages first, anonymous shared vmas are
	 * not dirty accountable.
	 */
	if (PageAnon(old_page) && !PageKsm(old_page)) {
    3a33:	48 83 f8 03          	cmp    $0x3,%rax
    3a37:	0f 84 a4 fc ff ff    	je     36e1 <do_wp_page+0x51>
 * This operation is atomic and cannot be reordered.
 * It also implies a memory barrier.
 */
static inline int test_and_set_bit(long nr, volatile unsigned long *addr)
{
	GEN_BINARY_RMWcc(LOCK_PREFIX "bts", *addr, "Ir", nr, "%0", "c");
    3a3d:	f0 0f ba 2b 00       	lock btsl $0x0,(%rbx)
    3a42:	0f 82 58 01 00 00    	jb     3ba0 <do_wp_page+0x510>
				goto unlock;
			}
			page_cache_release(old_page);
		}
		/*if write to a coa page, delete it to avoid merge*/
		if (reuse_swap_page(old_page)) {
    3a48:	48 89 df             	mov    %rbx,%rdi
    3a4b:	e8 00 00 00 00       	callq  3a50 <do_wp_page+0x3c0>
    3a50:	85 c0                	test   %eax,%eax
    3a52:	0f 84 28 02 00 00    	je     3c80 <do_wp_page+0x5f0>
			/*
			 * The page is all ours.  Move it to our anon_vma so
			 * the rmap code will not search our parent or siblings.
			 * Protected against the rmap code by the page lock.
			 */
			page_move_anon_rmap(old_page, vma, address);
    3a58:	4c 89 ea             	mov    %r13,%rdx
    3a5b:	4c 89 e6             	mov    %r12,%rsi
    3a5e:	48 89 df             	mov    %rbx,%rdi
    3a61:	e8 00 00 00 00       	callq  3a66 <do_wp_page+0x3d6>
			unlock_page(old_page);
    3a66:	48 89 df             	mov    %rbx,%rdi
{
	struct page *old_page, *new_page = NULL;
	pte_t entry;
	int ret = 0;
	int page_mkwrite = 0;
	struct page *dirty_page = NULL;
    3a69:	45 31 f6             	xor    %r14d,%r14d
			 * The page is all ours.  Move it to our anon_vma so
			 * the rmap code will not search our parent or siblings.
			 * Protected against the rmap code by the page lock.
			 */
			page_move_anon_rmap(old_page, vma, address);
			unlock_page(old_page);
    3a6c:	e8 00 00 00 00       	callq  3a71 <do_wp_page+0x3e1>
__releases(ptl)
{
	struct page *old_page, *new_page = NULL;
	pte_t entry;
	int ret = 0;
	int page_mkwrite = 0;
    3a71:	c7 45 b0 00 00 00 00 	movl   $0x0,-0x50(%rbp)
		 * Clear the pages cpupid information as the existing
		 * information potentially belongs to a now completely
		 * unrelated process.
		 */
		if (old_page)
		  page_cpupid_xchg_last(old_page, (1 << LAST_CPUPID_SHIFT) - 1);
    3a78:	be ff ff 00 00       	mov    $0xffff,%esi
    3a7d:	48 89 df             	mov    %rbx,%rdi
    3a80:	e8 00 00 00 00       	callq  3a85 <do_wp_page+0x3f5>
    3a85:	49 8b 44 24 50       	mov    0x50(%r12),%rax
 * pte_mkwrite.  But get_user_pages can cause write faults for mappings
 * that do not have writing enabled, when used by access_process_vm.
 */
static inline pte_t maybe_mkwrite(pte_t pte, struct vm_area_struct *vma)
{
	if (likely(vma->vm_flags & VM_WRITE))
    3a8a:	a8 02                	test   $0x2,%al
    3a8c:	48 8b 4d 10          	mov    0x10(%rbp),%rcx
    3a90:	0f 84 c2 03 00 00    	je     3e58 <do_wp_page+0x7c8>
    3a96:	48 81 c9 62 08 00 00 	or     $0x862,%rcx

		flush_cache_page(vma, address, pte_pfn(orig_pte));
		entry = pte_mkyoung(orig_pte);
		entry = maybe_mkwrite(pte_mkdirty(entry), vma);
		if (ptep_set_access_flags(vma, address, page_table, entry,1))
    3a9d:	48 8b 55 a8          	mov    -0x58(%rbp),%rdx
    3aa1:	41 b8 01 00 00 00    	mov    $0x1,%r8d
    3aa7:	4c 89 ee             	mov    %r13,%rsi
    3aaa:	4c 89 e7             	mov    %r12,%rdi
    3aad:	e8 00 00 00 00       	callq  3ab2 <do_wp_page+0x422>
    3ab2:	4c 89 ff             	mov    %r15,%rdi
    3ab5:	e8 00 00 00 00       	callq  3aba <do_wp_page+0x42a>
		  update_mmu_cache(vma, address, page_table);
		pte_unmap_unlock(page_table, ptl);
		ret |= VM_FAULT_WRITE;

		if (!dirty_page){
    3aba:	4d 85 f6             	test   %r14,%r14
    3abd:	0f 84 5d 02 00 00    	je     3d20 <do_wp_page+0x690>
		 * bit after it clear all dirty ptes, but before a racing
		 * do_wp_page installs a dirty pte.
		 *
		 * __do_fault is protected similarly.
		 */
		if (!page_mkwrite) {
    3ac3:	8b 45 b0             	mov    -0x50(%rbp),%eax
    3ac6:	85 c0                	test   %eax,%eax
    3ac8:	0f 85 c2 01 00 00    	jne    3c90 <do_wp_page+0x600>
}

static __always_inline int constant_test_bit(long nr, const volatile unsigned long *addr)
{
	return ((1UL << (nr & (BITS_PER_LONG-1))) &
		(addr[nr >> _BITOPS_LONG_SHIFT])) != 0;
    3ace:	49 8b 06             	mov    (%r14),%rax
 * ie with increased "page->count" so that the page won't
 * go away during the wait..
 */
static inline void wait_on_page_locked(struct page *page)
{
	if (PageLocked(page))
    3ad1:	a8 01                	test   $0x1,%al
    3ad3:	0f 85 07 02 00 00    	jne    3ce0 <do_wp_page+0x650>
			wait_on_page_locked(dirty_page);
			set_page_dirty_balance(dirty_page, page_mkwrite);
    3ad9:	31 f6                	xor    %esi,%esi
    3adb:	4c 89 f7             	mov    %r14,%rdi
    3ade:	e8 00 00 00 00       	callq  3ae3 <do_wp_page+0x453>
			/* file_update_time outside page_lock */
			if (vma->vm_file)
    3ae3:	49 8b bc 24 a8 00 00 	mov    0xa8(%r12),%rdi
    3aea:	00 
    3aeb:	48 85 ff             	test   %rdi,%rdi
    3aee:	74 1a                	je     3b0a <do_wp_page+0x47a>
		}
		coa_list_unlock(get_original(coa_entry));
	}
}

static int do_wp_page(struct mm_struct *mm, struct vm_area_struct *vma,
    3af0:	4d 8b a4 24 b0 00 00 	mov    0xb0(%r12),%r12
    3af7:	00 
static inline void vma_do_file_update_time(struct vm_area_struct *vma,
					   const char func[], int line)
{
	struct file *f = vma->vm_file, *pr = vma->vm_prfile;
	aufs_trace(f, pr, func, line, __func__);
	file_update_time(f);
    3af8:	e8 00 00 00 00       	callq  3afd <do_wp_page+0x46d>
	if (f && pr)
    3afd:	4d 85 e4             	test   %r12,%r12
    3b00:	74 08                	je     3b0a <do_wp_page+0x47a>
		file_update_time(pr);
    3b02:	4c 89 e7             	mov    %r12,%rdi
    3b05:	e8 00 00 00 00       	callq  3b0a <do_wp_page+0x47a>
			set_page_dirty_balance(dirty_page, page_mkwrite);
			/* file_update_time outside page_lock */
			if (vma->vm_file)
			  vma_file_update_time(vma);
		}
		put_page(dirty_page);
    3b0a:	4c 89 f7             	mov    %r14,%rdi
    3b0d:	e8 00 00 00 00       	callq  3b12 <do_wp_page+0x482>
				 * but still dirty their pages
				 */
				balance_dirty_pages_ratelimited(mapping);
			}
		}
		del_page_from_coa(old_page);
    3b12:	48 89 df             	mov    %rbx,%rdi
    3b15:	e8 00 00 00 00       	callq  3b1a <do_wp_page+0x48a>
	return VM_FAULT_OOM;

unwritable_page:
	page_cache_release(old_page);
	return ret;
}
    3b1a:	48 83 c4 50          	add    $0x50,%rsp
				 */
				balance_dirty_pages_ratelimited(mapping);
			}
		}
		del_page_from_coa(old_page);
		return ret;
    3b1e:	b8 08 00 00 00       	mov    $0x8,%eax
	return VM_FAULT_OOM;

unwritable_page:
	page_cache_release(old_page);
	return ret;
}
    3b23:	5b                   	pop    %rbx
    3b24:	41 5c                	pop    %r12
    3b26:	41 5d                	pop    %r13
    3b28:	41 5e                	pop    %r14
    3b2a:	41 5f                	pop    %r15
    3b2c:	5d                   	pop    %rbp
    3b2d:	c3                   	retq   
    3b2e:	66 90                	xchg   %ax,%ax
		}
		page_cache_release(old_page);
	}
	return ret;
oom_free_new:
	page_cache_release(new_page);
    3b30:	4c 89 ff             	mov    %r15,%rdi
    3b33:	e8 00 00 00 00       	callq  3b38 <do_wp_page+0x4a8>
oom:
	if (old_page)
    3b38:	48 85 db             	test   %rbx,%rbx
	  page_cache_release(old_page);
	return VM_FAULT_OOM;
    3b3b:	b8 01 00 00 00       	mov    $0x1,%eax
	}
	return ret;
oom_free_new:
	page_cache_release(new_page);
oom:
	if (old_page)
    3b40:	74 0e                	je     3b50 <do_wp_page+0x4c0>
	  page_cache_release(old_page);
    3b42:	48 89 df             	mov    %rbx,%rdi
    3b45:	89 45 b0             	mov    %eax,-0x50(%rbp)
    3b48:	e8 00 00 00 00       	callq  3b4d <do_wp_page+0x4bd>
    3b4d:	8b 45 b0             	mov    -0x50(%rbp),%eax
	return VM_FAULT_OOM;

unwritable_page:
	page_cache_release(old_page);
	return ret;
}
    3b50:	48 83 c4 50          	add    $0x50,%rsp
    3b54:	5b                   	pop    %rbx
    3b55:	41 5c                	pop    %r12
    3b57:	41 5d                	pop    %r13
    3b59:	41 5e                	pop    %r14
    3b5b:	41 5f                	pop    %r15
    3b5d:	5d                   	pop    %rbp
    3b5e:	c3                   	retq   
    3b5f:	90                   	nop
    3b60:	bf da 80 02 00       	mov    $0x280da,%edi
    3b65:	e8 00 00 00 00       	callq  3b6a <do_wp_page+0x4da>
	if (unlikely(anon_vma_prepare(vma)))
	  goto oom;

	if (is_zero_pfn(pte_pfn(orig_pte))) {
		new_page = alloc_zeroed_user_highpage_movable(vma, address);
		if (!new_page)
    3b6a:	48 85 c0             	test   %rax,%rax
    3b6d:	49 89 c7             	mov    %rax,%r15
    3b70:	0f 85 4b fc ff ff    	jne    37c1 <do_wp_page+0x131>
    3b76:	eb c0                	jmp    3b38 <do_wp_page+0x4a8>
    3b78:	0f 1f 84 00 00 00 00 	nopl   0x0(%rax,%rax,1)
    3b7f:	00 
    3b80:	49 8b 3f             	mov    (%r15),%rdi
				inc_mm_counter_fast(mm, MM_ANONPAGES);
			}
		} else
		  inc_mm_counter_fast(mm, MM_ANONPAGES);
		flush_cache_page(vma, address, pte_pfn(orig_pte));
		entry = mk_pte(new_page, vma->vm_page_prot);
    3b83:	48 b8 00 00 00 00 00 	movabs $0x8000000000000,%rax
    3b8a:	00 08 00 
    3b8d:	f7 c7 00 00 00 02    	test   $0x2000000,%edi
    3b93:	48 0f 44 c6          	cmove  %rsi,%rax
    3b97:	48 09 d0             	or     %rdx,%rax
    3b9a:	e9 39 fd ff ff       	jmpq   38d8 <do_wp_page+0x248>
    3b9f:	90                   	nop
	 * Take out anonymous pages first, anonymous shared vmas are
	 * not dirty accountable.
	 */
	if (PageAnon(old_page) && !PageKsm(old_page)) {
		if (!trylock_page(old_page)) {
			page_cache_get(old_page);
    3ba0:	48 89 df             	mov    %rbx,%rdi
    3ba3:	e8 58 c5 ff ff       	callq  100 <get_page>
    3ba8:	4c 89 ff             	mov    %r15,%rdi
    3bab:	e8 00 00 00 00       	callq  3bb0 <do_wp_page+0x520>
			pte_unmap_unlock(page_table, ptl);
			lock_page(old_page);
    3bb0:	48 89 df             	mov    %rbx,%rdi
    3bb3:	e8 98 cb ff ff       	callq  750 <lock_page>
			page_table = pte_offset_map_lock(mm, pmd, address,
    3bb8:	4c 89 f7             	mov    %r14,%rdi
    3bbb:	e8 70 c5 ff ff       	callq  130 <pte_lockptr.isra.16>
    3bc0:	4c 89 ee             	mov    %r13,%rsi
    3bc3:	49 89 c7             	mov    %rax,%r15
    3bc6:	4c 89 f7             	mov    %r14,%rdi
    3bc9:	e8 32 c4 ff ff       	callq  0 <pte_offset_kernel>
	raw_spin_lock_init(&(_lock)->rlock);		\
} while (0)

static inline void spin_lock(spinlock_t *lock)
{
	raw_spin_lock(&lock->rlock);
    3bce:	4c 89 ff             	mov    %r15,%rdi
    3bd1:	48 89 45 a8          	mov    %rax,-0x58(%rbp)
    3bd5:	e8 00 00 00 00       	callq  3bda <do_wp_page+0x54a>
						&ptl);
			if (!pte_same(*page_table, orig_pte)) {
    3bda:	48 8b 45 a8          	mov    -0x58(%rbp),%rax
    3bde:	48 8b 4d 10          	mov    0x10(%rbp),%rcx
    3be2:	48 39 08             	cmp    %rcx,(%rax)
    3be5:	0f 84 c8 02 00 00    	je     3eb3 <do_wp_page+0x823>
			 * MMU to tell us if they didn't also make it writable.
			 */
			page_table = pte_offset_map_lock(mm, pmd, address,
						&ptl);
			if (!pte_same(*page_table, orig_pte)) {
				unlock_page(old_page);
    3beb:	48 89 df             	mov    %rbx,%rdi
	raw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \
} while (0)

static inline void spin_unlock(spinlock_t *lock)
{
	raw_spin_unlock(&lock->rlock);
    3bee:	45 31 ed             	xor    %r13d,%r13d
    3bf1:	e8 00 00 00 00       	callq  3bf6 <do_wp_page+0x566>
    3bf6:	4c 89 ff             	mov    %r15,%rdi
    3bf9:	e8 00 00 00 00       	callq  3bfe <do_wp_page+0x56e>
    3bfe:	e9 10 fe ff ff       	jmpq   3a13 <do_wp_page+0x383>
    3c03:	0f 1f 44 00 00       	nopl   0x0(%rax,%rax,1)
 * This operation is atomic and cannot be reordered.
 * It also implies a memory barrier.
 */
static inline int test_and_set_bit(long nr, volatile unsigned long *addr)
{
	GEN_BINARY_RMWcc(LOCK_PREFIX "bts", *addr, "Ir", nr, "%0", "c");
    3c08:	f0 0f ba 2b 00       	lock btsl $0x0,(%rbx)
    3c0d:	0f 82 bd 00 00 00    	jb     3cd0 <do_wp_page+0x640>
		 * Don't let another task, with possibly unlocked vma,
		 * keep the mlocked page.
		 */
		if ((ret & VM_FAULT_WRITE) && (vma->vm_flags & VM_LOCKED)) {
			lock_page(old_page);	/* LRU manipulation */
			munlock_vma_page(old_page);
    3c13:	48 89 df             	mov    %rbx,%rdi
    3c16:	e8 00 00 00 00       	callq  3c1b <do_wp_page+0x58b>
			unlock_page(old_page);
    3c1b:	48 89 df             	mov    %rbx,%rdi
    3c1e:	e8 00 00 00 00       	callq  3c23 <do_wp_page+0x593>
    3c23:	e9 eb fd ff ff       	jmpq   3a13 <do_wp_page+0x383>
    3c28:	0f 1f 84 00 00 00 00 	nopl   0x0(%rax,%rax,1)
    3c2f:	00 
		 *
		 * We should not cow pages in a shared writeable mapping.
		 * Just mark the pages writable as we can't do any dirty
		 * accounting on raw pfn maps.
		 */
		if ((vma->vm_flags & (VM_WRITE|VM_SHARED)) ==
    3c30:	49 8b 44 24 50       	mov    0x50(%r12),%rax
    3c35:	48 89 c2             	mov    %rax,%rdx
    3c38:	83 e2 0a             	and    $0xa,%edx
    3c3b:	48 83 fa 0a          	cmp    $0xa,%rdx
    3c3f:	0f 85 be fa ff ff    	jne    3703 <do_wp_page+0x73>
{
	struct page *old_page, *new_page = NULL;
	pte_t entry;
	int ret = 0;
	int page_mkwrite = 0;
	struct page *dirty_page = NULL;
    3c45:	45 31 f6             	xor    %r14d,%r14d
__releases(ptl)
{
	struct page *old_page, *new_page = NULL;
	pte_t entry;
	int ret = 0;
	int page_mkwrite = 0;
    3c48:	c7 45 b0 00 00 00 00 	movl   $0x0,-0x50(%rbp)
    3c4f:	e9 36 fe ff ff       	jmpq   3a8a <do_wp_page+0x3fa>
    3c54:	0f 1f 40 00          	nopl   0x0(%rax)
	 */
	page_table = pte_offset_map_lock(mm, pmd, address, &ptl);
	if (likely(pte_same(*page_table, orig_pte))) {
		if (old_page) {
			if (!PageAnon(old_page)) {
				dec_mm_counter_fast(mm, MM_FILEPAGES);
    3c58:	48 8b 7d b0          	mov    -0x50(%rbp),%rdi
    3c5c:	31 f6                	xor    %esi,%esi
    3c5e:	ba ff ff ff ff       	mov    $0xffffffff,%edx
    3c63:	e8 f8 c4 ff ff       	callq  160 <add_mm_counter_fast>
				inc_mm_counter_fast(mm, MM_ANONPAGES);
    3c68:	48 8b 7d b0          	mov    -0x50(%rbp),%rdi
    3c6c:	ba 01 00 00 00       	mov    $0x1,%edx
    3c71:	be 01 00 00 00       	mov    $0x1,%esi
    3c76:	e8 e5 c4 ff ff       	callq  160 <add_mm_counter_fast>
    3c7b:	e9 18 fc ff ff       	jmpq   3898 <do_wp_page+0x208>
			 */
			page_move_anon_rmap(old_page, vma, address);
			unlock_page(old_page);
			goto reuse;
		}
		unlock_page(old_page);
    3c80:	48 89 df             	mov    %rbx,%rdi
    3c83:	e8 00 00 00 00       	callq  3c88 <do_wp_page+0x5f8>
    3c88:	e9 66 fa ff ff       	jmpq   36f3 <do_wp_page+0x63>
    3c8d:	0f 1f 00             	nopl   (%rax)
			set_page_dirty_balance(dirty_page, page_mkwrite);
			/* file_update_time outside page_lock */
			if (vma->vm_file)
			  vma_file_update_time(vma);
		}
		put_page(dirty_page);
    3c90:	4c 89 f7             	mov    %r14,%rdi
    3c93:	e8 00 00 00 00       	callq  3c98 <do_wp_page+0x608>
		if (page_mkwrite) {
			struct address_space *mapping = dirty_page->mapping;
    3c98:	4d 8b 66 08          	mov    0x8(%r14),%r12

			set_page_dirty(dirty_page);
    3c9c:	4c 89 f7             	mov    %r14,%rdi
    3c9f:	e8 00 00 00 00       	callq  3ca4 <do_wp_page+0x614>
			unlock_page(dirty_page);
    3ca4:	4c 89 f7             	mov    %r14,%rdi
    3ca7:	e8 00 00 00 00       	callq  3cac <do_wp_page+0x61c>
			page_cache_release(dirty_page);
    3cac:	4c 89 f7             	mov    %r14,%rdi
    3caf:	e8 00 00 00 00       	callq  3cb4 <do_wp_page+0x624>
			if (mapping)	{
    3cb4:	4d 85 e4             	test   %r12,%r12
    3cb7:	0f 84 55 fe ff ff    	je     3b12 <do_wp_page+0x482>
				/*
				 * Some device drivers do not set page.mapping
				 * but still dirty their pages
				 */
				balance_dirty_pages_ratelimited(mapping);
    3cbd:	4c 89 e7             	mov    %r12,%rdi
    3cc0:	e8 00 00 00 00       	callq  3cc5 <do_wp_page+0x635>
    3cc5:	e9 48 fe ff ff       	jmpq   3b12 <do_wp_page+0x482>
    3cca:	66 0f 1f 44 00 00    	nopw   0x0(%rax,%rax,1)
 */
static inline void lock_page(struct page *page)
{
	might_sleep();
	if (!trylock_page(page))
		__lock_page(page);
    3cd0:	48 89 df             	mov    %rbx,%rdi
    3cd3:	e8 00 00 00 00       	callq  3cd8 <do_wp_page+0x648>
    3cd8:	e9 36 ff ff ff       	jmpq   3c13 <do_wp_page+0x583>
    3cdd:	0f 1f 00             	nopl   (%rax)
 * go away during the wait..
 */
static inline void wait_on_page_locked(struct page *page)
{
	if (PageLocked(page))
		wait_on_page_bit(page, PG_locked);
    3ce0:	31 f6                	xor    %esi,%esi
    3ce2:	4c 89 f7             	mov    %r14,%rdi
    3ce5:	e8 00 00 00 00       	callq  3cea <do_wp_page+0x65a>
    3cea:	e9 ea fd ff ff       	jmpq   3ad9 <do_wp_page+0x449>
    3cef:	90                   	nop
    3cf0:	44 89 e8             	mov    %r13d,%eax
    3cf3:	e9 58 fe ff ff       	jmpq   3b50 <do_wp_page+0x4c0>
    3cf8:	0f 1f 84 00 00 00 00 	nopl   0x0(%rax,%rax,1)
    3cff:	00 
			if (!PageAnon(old_page)) {
				dec_mm_counter_fast(mm, MM_FILEPAGES);
				inc_mm_counter_fast(mm, MM_ANONPAGES);
			}
		} else
		  inc_mm_counter_fast(mm, MM_ANONPAGES);
    3d00:	48 8b 7d b0          	mov    -0x50(%rbp),%rdi
    3d04:	ba 01 00 00 00       	mov    $0x1,%edx
    3d09:	be 01 00 00 00       	mov    $0x1,%esi
    3d0e:	e8 4d c4 ff ff       	callq  160 <add_mm_counter_fast>
    3d13:	e9 80 fb ff ff       	jmpq   3898 <do_wp_page+0x208>
    3d18:	0f 1f 84 00 00 00 00 	nopl   0x0(%rax,%rax,1)
    3d1f:	00 
		  update_mmu_cache(vma, address, page_table);
		pte_unmap_unlock(page_table, ptl);
		ret |= VM_FAULT_WRITE;

		if (!dirty_page){
			return ret;
    3d20:	b8 08 00 00 00       	mov    $0x8,%eax
    3d25:	e9 26 fe ff ff       	jmpq   3b50 <do_wp_page+0x4c0>
		/*
		 * Only catch write-faults on shared writable pages,
		 * read-only shared pages can get COWed by
		 * get_user_pages(.write=1, .force=1).
		 */
		if (vma->vm_ops && vma->vm_ops->page_mkwrite) {
    3d2a:	49 8b 84 24 98 00 00 	mov    0x98(%r12),%rax
    3d31:	00 
    3d32:	48 85 c0             	test   %rax,%rax
    3d35:	0f 84 3e 01 00 00    	je     3e79 <do_wp_page+0x7e9>
    3d3b:	48 83 78 18 00       	cmpq   $0x0,0x18(%rax)
    3d40:	0f 84 33 01 00 00    	je     3e79 <do_wp_page+0x7e9>
			struct vm_fault vmf;
			int tmp;

			vmf.virtual_address = (void __user *)(address &
    3d46:	4c 89 e8             	mov    %r13,%rax
			 * for the page to get into an appropriate state.
			 *
			 * We do this without the lock held, so that it can
			 * sleep if it needs to.
			 */
			page_cache_get(old_page);
    3d49:	48 89 df             	mov    %rbx,%rdi
			int tmp;

			vmf.virtual_address = (void __user *)(address &
						PAGE_MASK);
			vmf.pgoff = old_page->index;
			vmf.flags = FAULT_FLAG_WRITE|FAULT_FLAG_MKWRITE;
    3d4c:	c7 45 b8 05 00 00 00 	movl   $0x5,-0x48(%rbp)
		 */
		if (vma->vm_ops && vma->vm_ops->page_mkwrite) {
			struct vm_fault vmf;
			int tmp;

			vmf.virtual_address = (void __user *)(address &
    3d53:	48 25 00 f0 ff ff    	and    $0xfffffffffffff000,%rax
    3d59:	48 89 45 c8          	mov    %rax,-0x38(%rbp)
						PAGE_MASK);
			vmf.pgoff = old_page->index;
    3d5d:	48 8b 43 10          	mov    0x10(%rbx),%rax
			vmf.flags = FAULT_FLAG_WRITE|FAULT_FLAG_MKWRITE;
			vmf.page = old_page;
    3d61:	48 89 5d d0          	mov    %rbx,-0x30(%rbp)
			struct vm_fault vmf;
			int tmp;

			vmf.virtual_address = (void __user *)(address &
						PAGE_MASK);
			vmf.pgoff = old_page->index;
    3d65:	48 89 45 c0          	mov    %rax,-0x40(%rbp)
			 * for the page to get into an appropriate state.
			 *
			 * We do this without the lock held, so that it can
			 * sleep if it needs to.
			 */
			page_cache_get(old_page);
    3d69:	e8 92 c3 ff ff       	callq  100 <get_page>
    3d6e:	4c 89 ff             	mov    %r15,%rdi
    3d71:	e8 00 00 00 00       	callq  3d76 <do_wp_page+0x6e6>
			pte_unmap_unlock(page_table, ptl);

			tmp = vma->vm_ops->page_mkwrite(vma, &vmf);
    3d76:	49 8b 84 24 98 00 00 	mov    0x98(%r12),%rax
    3d7d:	00 
    3d7e:	48 8d 75 b8          	lea    -0x48(%rbp),%rsi
    3d82:	4c 89 e7             	mov    %r12,%rdi
    3d85:	ff 50 18             	callq  *0x18(%rax)
			if (unlikely(tmp &
    3d88:	a9 33 09 00 00       	test   $0x933,%eax
			 * sleep if it needs to.
			 */
			page_cache_get(old_page);
			pte_unmap_unlock(page_table, ptl);

			tmp = vma->vm_ops->page_mkwrite(vma, &vmf);
    3d8d:	41 89 c7             	mov    %eax,%r15d
			if (unlikely(tmp &
    3d90:	0f 85 0d 01 00 00    	jne    3ea3 <do_wp_page+0x813>
							(VM_FAULT_ERROR | VM_FAULT_NOPAGE))) {
				ret = tmp;
				goto unwritable_page;
			}
			if (unlikely(!(tmp & VM_FAULT_LOCKED))) {
    3d96:	41 81 e7 00 02 00 00 	and    $0x200,%r15d
    3d9d:	0f 84 e2 00 00 00    	je     3e85 <do_wp_page+0x7f5>
			 * Since we dropped the lock we need to revalidate
			 * the PTE as someone else may have changed it.  If
			 * they did, we just return, as we can count on the
			 * MMU to tell us if they didn't also make it writable.
			 */
			page_table = pte_offset_map_lock(mm, pmd, address,
    3da3:	4c 89 f7             	mov    %r14,%rdi
    3da6:	e8 85 c3 ff ff       	callq  130 <pte_lockptr.isra.16>
    3dab:	4c 89 f7             	mov    %r14,%rdi
    3dae:	49 89 c7             	mov    %rax,%r15
    3db1:	4c 89 ee             	mov    %r13,%rsi
    3db4:	e8 47 c2 ff ff       	callq  0 <pte_offset_kernel>
	raw_spin_lock_init(&(_lock)->rlock);		\
} while (0)

static inline void spin_lock(spinlock_t *lock)
{
	raw_spin_lock(&lock->rlock);
    3db9:	4c 89 ff             	mov    %r15,%rdi
    3dbc:	49 89 c6             	mov    %rax,%r14
    3dbf:	48 89 45 a8          	mov    %rax,-0x58(%rbp)
    3dc3:	e8 00 00 00 00       	callq  3dc8 <do_wp_page+0x738>
						&ptl);
			if (!pte_same(*page_table, orig_pte)) {
    3dc8:	48 8b 4d 10          	mov    0x10(%rbp),%rcx
    3dcc:	49 39 0e             	cmp    %rcx,(%r14)
    3dcf:	0f 85 16 fe ff ff    	jne    3beb <do_wp_page+0x55b>
				unlock_page(old_page);
				goto unlock;
			}

			page_mkwrite = 1;
    3dd5:	c7 45 b0 01 00 00 00 	movl   $0x1,-0x50(%rbp)
		}
		dirty_page = old_page;
		get_page(dirty_page);
    3ddc:	48 89 df             	mov    %rbx,%rdi
    3ddf:	49 89 de             	mov    %rbx,%r14
    3de2:	e8 19 c3 ff ff       	callq  100 <get_page>
    3de7:	e9 8c fc ff ff       	jmpq   3a78 <do_wp_page+0x3e8>
extern bool __get_page_tail(struct page *page);

static inline void get_page(struct page *page)
{
	if (unlikely(PageTail(page)))
		if (likely(__get_page_tail(page)))
    3dec:	48 89 df             	mov    %rbx,%rdi
    3def:	e8 00 00 00 00       	callq  3df4 <do_wp_page+0x764>
    3df4:	84 c0                	test   %al,%al
    3df6:	0f 85 07 f9 ff ff    	jne    3703 <do_wp_page+0x73>
    3dfc:	e9 fe f8 ff ff       	jmpq   36ff <do_wp_page+0x6f>
    3e01:	4c 89 ee             	mov    %r13,%rsi
    3e04:	48 89 c7             	mov    %rax,%rdi
    3e07:	e8 00 00 00 00       	callq  3e0c <do_wp_page+0x77c>
    3e0c:	e9 b0 f9 ff ff       	jmpq   37c1 <do_wp_page+0x131>

		/* Free the old page.. */
		new_page = old_page;
		ret |= VM_FAULT_WRITE;
	} else
	  mem_cgroup_uncharge_page(new_page);
    3e11:	4c 89 ff             	mov    %r15,%rdi
			spinlock_t *ptl, pte_t orig_pte)
__releases(ptl)
{
	struct page *old_page, *new_page = NULL;
	pte_t entry;
	int ret = 0;
    3e14:	45 31 ed             	xor    %r13d,%r13d

		/* Free the old page.. */
		new_page = old_page;
		ret |= VM_FAULT_WRITE;
	} else
	  mem_cgroup_uncharge_page(new_page);
    3e17:	e8 00 00 00 00       	callq  3e1c <do_wp_page+0x78c>
    3e1c:	e9 ab fb ff ff       	jmpq   39cc <do_wp_page+0x33c>

static inline void mmu_notifier_change_pte(struct mm_struct *mm,
					   unsigned long address, pte_t pte)
{
	if (mm_has_notifiers(mm))
		__mmu_notifier_change_pte(mm, address, pte);
    3e21:	48 8b 55 a0          	mov    -0x60(%rbp),%rdx
    3e25:	4c 89 ee             	mov    %r13,%rsi
    3e28:	48 89 c7             	mov    %rax,%rdi
    3e2b:	e8 00 00 00 00       	callq  3e30 <do_wp_page+0x7a0>
    3e30:	e9 17 fb ff ff       	jmpq   394c <do_wp_page+0x2bc>
    3e35:	48 0d 40 08 00 00    	or     $0x840,%rax
    3e3b:	48 89 45 a0          	mov    %rax,-0x60(%rbp)
    3e3f:	e9 d9 fa ff ff       	jmpq   391d <do_wp_page+0x28d>

static inline void mmu_notifier_invalidate_range_start(struct mm_struct *mm,
				  unsigned long start, unsigned long end)
{
	if (mm_has_notifiers(mm))
		__mmu_notifier_invalidate_range_start(mm, start, end);
    3e44:	48 8b 75 98          	mov    -0x68(%rbp),%rsi
    3e48:	48 89 ca             	mov    %rcx,%rdx
    3e4b:	48 89 c7             	mov    %rax,%rdi
    3e4e:	e8 00 00 00 00       	callq  3e53 <do_wp_page+0x7c3>
    3e53:	e9 b1 f9 ff ff       	jmpq   3809 <do_wp_page+0x179>
    3e58:	48 81 c9 60 08 00 00 	or     $0x860,%rcx
    3e5f:	e9 39 fc ff ff       	jmpq   3a9d <do_wp_page+0x40d>

static inline void mmu_notifier_invalidate_range_end(struct mm_struct *mm,
				  unsigned long start, unsigned long end)
{
	if (mm_has_notifiers(mm))
		__mmu_notifier_invalidate_range_end(mm, start, end);
    3e64:	48 8b 55 90          	mov    -0x70(%rbp),%rdx
    3e68:	48 8b 75 98          	mov    -0x68(%rbp),%rsi
    3e6c:	48 89 c7             	mov    %rax,%rdi
    3e6f:	e8 00 00 00 00       	callq  3e74 <do_wp_page+0x7e4>
    3e74:	e9 80 fb ff ff       	jmpq   39f9 <do_wp_page+0x369>
__releases(ptl)
{
	struct page *old_page, *new_page = NULL;
	pte_t entry;
	int ret = 0;
	int page_mkwrite = 0;
    3e79:	c7 45 b0 00 00 00 00 	movl   $0x0,-0x50(%rbp)
    3e80:	e9 57 ff ff ff       	jmpq   3ddc <do_wp_page+0x74c>
							(VM_FAULT_ERROR | VM_FAULT_NOPAGE))) {
				ret = tmp;
				goto unwritable_page;
			}
			if (unlikely(!(tmp & VM_FAULT_LOCKED))) {
				lock_page(old_page);
    3e85:	48 89 df             	mov    %rbx,%rdi
    3e88:	e8 c3 c8 ff ff       	callq  750 <lock_page>
				if (!old_page->mapping) {
    3e8d:	48 83 7b 08 00       	cmpq   $0x0,0x8(%rbx)
    3e92:	0f 85 0b ff ff ff    	jne    3da3 <do_wp_page+0x713>
					ret = 0; /* retry the fault */
					unlock_page(old_page);
    3e98:	48 89 df             	mov    %rbx,%rdi
				goto unwritable_page;
			}
			if (unlikely(!(tmp & VM_FAULT_LOCKED))) {
				lock_page(old_page);
				if (!old_page->mapping) {
					ret = 0; /* retry the fault */
    3e9b:	45 31 ff             	xor    %r15d,%r15d
					unlock_page(old_page);
    3e9e:	e8 00 00 00 00       	callq  3ea3 <do_wp_page+0x813>
	if (old_page)
	  page_cache_release(old_page);
	return VM_FAULT_OOM;

unwritable_page:
	page_cache_release(old_page);
    3ea3:	48 89 df             	mov    %rbx,%rdi
    3ea6:	e8 00 00 00 00       	callq  3eab <do_wp_page+0x81b>
	return ret;
    3eab:	44 89 f8             	mov    %r15d,%eax
    3eae:	e9 9d fc ff ff       	jmpq   3b50 <do_wp_page+0x4c0>
						&ptl);
			if (!pte_same(*page_table, orig_pte)) {
				unlock_page(old_page);
				goto unlock;
			}
			page_cache_release(old_page);
    3eb3:	48 89 df             	mov    %rbx,%rdi
    3eb6:	e8 00 00 00 00       	callq  3ebb <do_wp_page+0x82b>
    3ebb:	e9 88 fb ff ff       	jmpq   3a48 <do_wp_page+0x3b8>

0000000000003ec0 <do_swap_page.isra.68>:
/*
 * We enter with non-exclusive mmap_sem (to exclude vma changes,
 * but allow concurrent faults), and pte mapped but not yet locked.
 * We return with mmap_sem still held, but pte unmapped and unlocked.
 */
static int do_swap_page(struct mm_struct *mm, struct vm_area_struct *vma,
    3ec0:	e8 00 00 00 00       	callq  3ec5 <do_swap_page.isra.68+0x5>
    3ec5:	55                   	push   %rbp
    3ec6:	48 b8 ff 0f 00 00 00 	movabs $0xffffc00000000fff,%rax
    3ecd:	c0 ff ff 
    3ed0:	4c 21 c8             	and    %r9,%rax
    3ed3:	48 89 e5             	mov    %rsp,%rbp
    3ed6:	41 57                	push   %r15
    3ed8:	41 56                	push   %r14
    3eda:	4d 89 ce             	mov    %r9,%r14
    3edd:	41 55                	push   %r13
    3edf:	41 54                	push   %r12
    3ee1:	53                   	push   %rbx
    3ee2:	48 83 ec 60          	sub    $0x60,%rsp
	return ret;
}
static inline swp_entry_t pte_to_swp_entry(pte_t pte)
{
	swp_entry_t arch_entry;
	BUG_ON(pte_file(pte));
    3ee6:	a8 40                	test   $0x40,%al
    3ee8:	48 89 7d b8          	mov    %rdi,-0x48(%rbp)
    3eec:	48 89 75 c0          	mov    %rsi,-0x40(%rbp)
    3ef0:	48 89 55 c8          	mov    %rdx,-0x38(%rbp)
    3ef4:	44 89 45 b4          	mov    %r8d,-0x4c(%rbp)
    3ef8:	0f 85 54 06 00 00    	jne    4552 <do_swap_page.isra.68+0x692>

static inline pte_t pte_clear_flags(pte_t pte, pteval_t clear)
{
	pteval_t v = native_pte_val(pte);

	return native_make_pte(v & ~clear);
    3efe:	4c 89 cf             	mov    %r9,%rdi
    3f01:	48 ba 00 00 00 00 00 	movabs $0x4000000000000,%rdx
    3f08:	00 04 00 
    3f0b:	49 89 cd             	mov    %rcx,%r13
    3f0e:	40 80 e7 7f          	and    $0x7f,%dil
    3f12:	a8 80                	test   $0x80,%al
    3f14:	49 0f 44 f9          	cmove  %r9,%rdi
    3f18:	48 89 f8             	mov    %rdi,%rax
    3f1b:	48 83 e0 ef          	and    $0xffffffffffffffef,%rax
    3f1f:	48 85 d7             	test   %rdx,%rdi
    3f22:	48 0f 45 f8          	cmovne %rax,%rdi

	if (sizeof(pteval_t) > sizeof(long))
		ret = PVOP_CALLEE2(pteval_t, pv_mmu_ops.pte_val,
				   pte.pte, (u64)pte.pte >> 32);
	else
		ret = PVOP_CALLEE1(pteval_t, pv_mmu_ops.pte_val,
    3f26:	ff 14 25 00 00 00 00 	callq  *0x0
	if (pte_swp_soft_dirty(pte))	
	  pte = pte_swp_clear_soft_dirty(pte);
	if(	pte.pte & _PAGE_NCACHE)
	  pte= pte_clear_flags(pte, _PAGE_CACHE_UC_MINUS);
	arch_entry = __pte_to_swp_entry(pte);
	return swp_entry(__swp_type(arch_entry), __swp_offset(arch_entry));
    3f2d:	49 89 c7             	mov    %rax,%r15
    3f30:	48 c1 e8 09          	shr    $0x9,%rax
    3f34:	49 d1 ef             	shr    %r15
    3f37:	41 83 e7 1f          	and    $0x1f,%r15d
 */
static inline swp_entry_t swp_entry(unsigned long type, pgoff_t offset)
{
	swp_entry_t ret;

	ret.val = (type << SWP_TYPE_SHIFT(ret)) |
    3f3b:	49 c1 e7 39          	shl    $0x39,%r15
    3f3f:	49 09 c7             	or     %rax,%r15
 * Extract the `type' field from a swp_entry_t.  The swp_entry_t is in
 * arch-independent format
 */
static inline unsigned swp_type(swp_entry_t entry)
{
	return (entry.val  >> SWP_TYPE_SHIFT(entry));
    3f42:	4c 89 f8             	mov    %r15,%rax
    3f45:	48 c1 e8 39          	shr    $0x39,%rax

	if (!pte_unmap_same(mm, pmd, page_table, orig_pte))
	  goto out;

	entry = pte_to_swp_entry(orig_pte);
	if (unlikely(non_swap_entry(entry))) {
    3f49:	83 f8 1c             	cmp    $0x1c,%eax
    3f4c:	0f 87 cc 05 00 00    	ja     451e <do_swap_page.isra.68+0x65e>
    3f52:	65 48 8b 04 25 00 00 	mov    %gs:0x0,%rax
    3f59:	00 00 
		return 0;
}

static inline void delayacct_set_flag(int flag)
{
	if (current->delays)
    3f5b:	48 83 b8 a0 08 00 00 	cmpq   $0x0,0x8a0(%rax)
    3f62:	00 
    3f63:	74 0b                	je     3f70 <do_swap_page.isra.68+0xb0>
		current->delays->flags |= flag;
    3f65:	48 8b 80 a0 08 00 00 	mov    0x8a0(%rax),%rax
    3f6c:	83 48 18 01          	orl    $0x1,0x18(%rax)
			ret = VM_FAULT_SIGBUS;
		}
		goto out;
	}
	delayacct_set_flag(DELAYACCT_PF_SWAPIN);
	page = lookup_swap_cache(entry);
    3f70:	4c 89 ff             	mov    %r15,%rdi
    3f73:	e8 00 00 00 00       	callq  3f78 <do_swap_page.isra.68+0xb8>
	if (!page) {
    3f78:	48 85 c0             	test   %rax,%rax
			ret = VM_FAULT_SIGBUS;
		}
		goto out;
	}
	delayacct_set_flag(DELAYACCT_PF_SWAPIN);
	page = lookup_swap_cache(entry);
    3f7b:	48 89 c3             	mov    %rax,%rbx
	if (!page) {
    3f7e:	0f 84 84 04 00 00    	je     4408 <do_swap_page.isra.68+0x548>
}

static __always_inline int constant_test_bit(long nr, const volatile unsigned long *addr)
{
	return ((1UL << (nr & (BITS_PER_LONG-1))) &
		(addr[nr >> _BITOPS_LONG_SHIFT])) != 0;
    3f84:	48 8b 00             	mov    (%rax),%rax

		/* Had to read the page from swap area: Major fault */
		ret = VM_FAULT_MAJOR;
		count_vm_event(PGMAJFAULT);
		mem_cgroup_count_vm_event(mm, PGMAJFAULT);
	} else if (PageHWPoison(page)) {
    3f87:	a9 00 00 80 00       	test   $0x800000,%eax
    3f8c:	75 5a                	jne    3fe8 <do_swap_page.isra.68+0x128>
	swp_entry_t entry;
	pte_t pte;
	int locked;
	struct mem_cgroup *ptr;
	int exclusive = 0;
	int ret = 0;
    3f8e:	45 31 e4             	xor    %r12d,%r12d
 * This operation is atomic and cannot be reordered.
 * It also implies a memory barrier.
 */
static inline int test_and_set_bit(long nr, volatile unsigned long *addr)
{
	GEN_BINARY_RMWcc(LOCK_PREFIX "bts", *addr, "Ir", nr, "%0", "c");
    3f91:	f0 0f ba 2b 00       	lock btsl $0x0,(%rbx)
    3f96:	0f 82 74 03 00 00    	jb     4310 <do_swap_page.isra.68+0x450>
 */
static inline int lock_page_or_retry(struct page *page, struct mm_struct *mm,
				     unsigned int flags)
{
	might_sleep();
	return trylock_page(page) || __lock_page_or_retry(page, mm, flags);
    3f9c:	ba 01 00 00 00       	mov    $0x1,%edx
    3fa1:	65 48 8b 04 25 00 00 	mov    %gs:0x0,%rax
    3fa8:	00 00 
}

static inline void delayacct_clear_flag(int flag)
{
	if (current->delays)
    3faa:	48 83 b8 a0 08 00 00 	cmpq   $0x0,0x8a0(%rax)
    3fb1:	00 
    3fb2:	74 0b                	je     3fbf <do_swap_page.isra.68+0xff>
		current->delays->flags &= ~flag;
    3fb4:	48 8b 80 a0 08 00 00 	mov    0x8a0(%rax),%rax
    3fbb:	83 60 18 fe          	andl   $0xfffffffe,0x18(%rax)

	swapcache = page;
	locked = lock_page_or_retry(page, mm, flags);

	delayacct_clear_flag(DELAYACCT_PF_SWAPIN);
	if (!locked) {
    3fbf:	85 d2                	test   %edx,%edx
    3fc1:	75 55                	jne    4018 <do_swap_page.isra.68+0x158>
		ret |= VM_FAULT_RETRY;
    3fc3:	41 81 cc 00 04 00 00 	or     $0x400,%r12d
	mem_cgroup_cancel_charge_swapin(ptr);
	pte_unmap_unlock(page_table, ptl);
out_page:
	unlock_page(page);
out_release:
	page_cache_release(page);
    3fca:	48 89 df             	mov    %rbx,%rdi
    3fcd:	e8 00 00 00 00       	callq  3fd2 <do_swap_page.isra.68+0x112>
	if (page != swapcache) {
		unlock_page(swapcache);
		page_cache_release(swapcache);
	}
	return ret;
}
    3fd2:	48 83 c4 60          	add    $0x60,%rsp
    3fd6:	44 89 e0             	mov    %r12d,%eax
    3fd9:	5b                   	pop    %rbx
    3fda:	41 5c                	pop    %r12
    3fdc:	41 5d                	pop    %r13
    3fde:	41 5e                	pop    %r14
    3fe0:	41 5f                	pop    %r15
    3fe2:	5d                   	pop    %rbp
    3fe3:	c3                   	retq   
    3fe4:	0f 1f 40 00          	nopl   0x0(%rax)
    3fe8:	65 48 8b 04 25 00 00 	mov    %gs:0x0,%rax
    3fef:	00 00 
		current->delays->flags |= flag;
}

static inline void delayacct_clear_flag(int flag)
{
	if (current->delays)
    3ff1:	48 83 b8 a0 08 00 00 	cmpq   $0x0,0x8a0(%rax)
    3ff8:	00 
    3ff9:	0f 84 f9 03 00 00    	je     43f8 <do_swap_page.isra.68+0x538>
		current->delays->flags &= ~flag;
    3fff:	48 8b 80 a0 08 00 00 	mov    0x8a0(%rax),%rax
	} else if (PageHWPoison(page)) {
		/*
		 * hwpoisoned dirty swapcache pages are kept for killing
		 * owner processes (which may be unknown at hwpoison time)
		 */
		ret = VM_FAULT_HWPOISON;
    4006:	41 bc 10 00 00 00    	mov    $0x10,%r12d
    400c:	83 60 18 fe          	andl   $0xfffffffe,0x18(%rax)
    4010:	eb b8                	jmp    3fca <do_swap_page.isra.68+0x10a>
    4012:	66 0f 1f 44 00 00    	nopw   0x0(%rax,%rax,1)
}

static __always_inline int constant_test_bit(long nr, const volatile unsigned long *addr)
{
	return ((1UL << (nr & (BITS_PER_LONG-1))) &
		(addr[nr >> _BITOPS_LONG_SHIFT])) != 0;
    4018:	48 8b 03             	mov    (%rbx),%rax
	 * Make sure try_to_free_swap or reuse_swap_page or swapoff did not
	 * release the swapcache from under us.  The page pin, and pte_same
	 * test below, are not enough to exclude that.  Even if it is still
	 * swapcache, we need to check that the page's swap has not changed.
	 */
	if (unlikely(!PageSwapCache(page) || page_private(page) != entry.val))
    401b:	a9 00 00 01 00       	test   $0x10000,%eax
    4020:	0f 84 db 02 00 00    	je     4301 <do_swap_page.isra.68+0x441>
    4026:	48 8b 43 30          	mov    0x30(%rbx),%rax
    402a:	4c 39 f8             	cmp    %r15,%rax
    402d:	48 89 45 a0          	mov    %rax,-0x60(%rbp)
    4031:	0f 85 ca 02 00 00    	jne    4301 <do_swap_page.isra.68+0x441>
	  goto out_page;

	page = ksm_might_need_to_copy(page, vma, address);
    4037:	48 8b 55 c8          	mov    -0x38(%rbp),%rdx
    403b:	48 8b 75 c0          	mov    -0x40(%rbp),%rsi
    403f:	48 89 df             	mov    %rbx,%rdi
    4042:	e8 00 00 00 00       	callq  4047 <do_swap_page.isra.68+0x187>
	if (unlikely(!page)) {
    4047:	48 85 c0             	test   %rax,%rax
	 * swapcache, we need to check that the page's swap has not changed.
	 */
	if (unlikely(!PageSwapCache(page) || page_private(page) != entry.val))
	  goto out_page;

	page = ksm_might_need_to_copy(page, vma, address);
    404a:	49 89 c7             	mov    %rax,%r15
	if (unlikely(!page)) {
    404d:	0f 84 04 05 00 00    	je     4557 <do_swap_page.isra.68+0x697>
		printk("swap fault due to !page\n");
		page = swapcache;
		goto out_page;
	}

	if (mem_cgroup_try_charge_swapin(mm, page, GFP_KERNEL, &ptr)) {
    4053:	48 8b 7d b8          	mov    -0x48(%rbp),%rdi
    4057:	48 8d 4d d0          	lea    -0x30(%rbp),%rcx
    405b:	ba d0 00 00 00       	mov    $0xd0,%edx
    4060:	48 89 c6             	mov    %rax,%rsi
    4063:	e8 00 00 00 00       	callq  4068 <do_swap_page.isra.68+0x1a8>
    4068:	85 c0                	test   %eax,%eax
    406a:	0f 85 00 05 00 00    	jne    4570 <do_swap_page.isra.68+0x6b0>

	if (sizeof(pmdval_t) > sizeof(long))
		ret =  PVOP_CALLEE2(pmdval_t, pv_mmu_ops.pmd_val,
				    pmd.pmd, (u64)pmd.pmd >> 32);
	else
		ret =  PVOP_CALLEE1(pmdval_t, pv_mmu_ops.pmd_val,
    4070:	49 8b 7d 00          	mov    0x0(%r13),%rdi
    4074:	ff 14 25 00 00 00 00 	callq  *0x0
}
#endif /* ALLOC_SPLIT_PTLOCKS */

static inline spinlock_t *pte_lockptr(struct mm_struct *mm, pmd_t *pmd)
{
	return ptlock_ptr(pmd_page(*pmd));
    407b:	48 b9 00 f0 ff ff ff 	movabs $0x3ffffffff000,%rcx
    4082:	3f 00 00 
    4085:	48 ba 00 00 00 00 00 	movabs $0xffffea0000000000,%rdx
    408c:	ea ff ff 
    408f:	49 8b 7d 00          	mov    0x0(%r13),%rdi
    4093:	48 21 c8             	and    %rcx,%rax
    4096:	48 c1 e8 06          	shr    $0x6,%rax
    409a:	48 8b 44 10 30       	mov    0x30(%rax,%rdx,1),%rax
    409f:	49 89 c2             	mov    %rax,%r10
    40a2:	48 89 45 98          	mov    %rax,-0x68(%rbp)
    40a6:	ff 14 25 00 00 00 00 	callq  *0x0
 * this function returns the index of the entry in the pte page which would
 * control the given virtual address
 */
static inline unsigned long pte_index(unsigned long address)
{
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
    40ad:	48 8b 55 c8          	mov    -0x38(%rbp),%rdx
	return (unsigned long)native_pmd_val(pmd) == 0;
}

static inline unsigned long pmd_page_vaddr(pmd_t pmd)
{
	return (unsigned long)__va(pmd_val(pmd) & PTE_PFN_MASK);
    40b1:	48 be 00 00 00 00 00 	movabs $0xffff880000000000,%rsi
    40b8:	88 ff ff 
    40bb:	48 21 c8             	and    %rcx,%rax
    40be:	4c 89 d7             	mov    %r10,%rdi
 * this function returns the index of the entry in the pte page which would
 * control the given virtual address
 */
static inline unsigned long pte_index(unsigned long address)
{
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
    40c1:	48 c1 ea 09          	shr    $0x9,%rdx
}

static inline pte_t *pte_offset_kernel(pmd_t *pmd, unsigned long address)
{
	return (pte_t *)pmd_page_vaddr(*pmd) + pte_index(address);
    40c5:	81 e2 f8 0f 00 00    	and    $0xff8,%edx
	return (unsigned long)native_pmd_val(pmd) == 0;
}

static inline unsigned long pmd_page_vaddr(pmd_t pmd)
{
	return (unsigned long)__va(pmd_val(pmd) & PTE_PFN_MASK);
    40cb:	48 01 d6             	add    %rdx,%rsi
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
}

static inline pte_t *pte_offset_kernel(pmd_t *pmd, unsigned long address)
{
	return (pte_t *)pmd_page_vaddr(*pmd) + pte_index(address);
    40ce:	48 01 f0             	add    %rsi,%rax
	return (unsigned long)native_pmd_val(pmd) == 0;
}

static inline unsigned long pmd_page_vaddr(pmd_t pmd)
{
	return (unsigned long)__va(pmd_val(pmd) & PTE_PFN_MASK);
    40d1:	48 89 75 88          	mov    %rsi,-0x78(%rbp)
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
}

static inline pte_t *pte_offset_kernel(pmd_t *pmd, unsigned long address)
{
	return (pte_t *)pmd_page_vaddr(*pmd) + pte_index(address);
    40d5:	48 89 45 a8          	mov    %rax,-0x58(%rbp)
    40d9:	e8 00 00 00 00       	callq  40de <do_swap_page.isra.68+0x21e>
    40de:	48 8b 45 a8          	mov    -0x58(%rbp),%rax
    40e2:	48 8b 08             	mov    (%rax),%rcx

	/*
	 * Back out if somebody else already faulted in this pte.
	 */
	page_table = pte_offset_map_lock(mm, pmd, address, &ptl);
	if (unlikely(!pte_same(*page_table, orig_pte)))
    40e5:	4c 39 f1             	cmp    %r14,%rcx
    40e8:	0f 85 e8 01 00 00    	jne    42d6 <do_swap_page.isra.68+0x416>
    40ee:	49 8b 07             	mov    (%r15),%rax
	 * We can skip the barrier if the page is not uptodate, because
	 * we wouldn't be reading anything from it.
	 *
	 * See SetPageUptodate() for the other side of the story.
	 */
	if (ret)
    40f1:	a8 08                	test   $0x8,%al
    40f3:	0f 84 d7 01 00 00    	je     42d0 <do_swap_page.isra.68+0x410>
    40f9:	48 89 4d 90          	mov    %rcx,-0x70(%rbp)
	 * mem_cgroup_commit_charge_swapin() may not be able to find swp_entry
	 * in page->private. In this case, a record in swap_cgroup  is silently
	 * discarded at swap_free().
	 */

	inc_mm_counter_fast(mm, MM_ANONPAGES);
    40fd:	4c 8b 75 b8          	mov    -0x48(%rbp),%r14
    4101:	ba 01 00 00 00       	mov    $0x1,%edx
    4106:	be 01 00 00 00       	mov    $0x1,%esi
    410b:	4c 89 f7             	mov    %r14,%rdi
    410e:	e8 4d c0 ff ff       	callq  160 <add_mm_counter_fast>
	dec_mm_counter_fast(mm, MM_SWAPENTS);
    4113:	ba ff ff ff ff       	mov    $0xffffffff,%edx
    4118:	be 02 00 00 00       	mov    $0x2,%esi
    411d:	4c 89 f7             	mov    %r14,%rdi
    4120:	e8 3b c0 ff ff       	callq  160 <add_mm_counter_fast>
	pte = mk_pte(page, vma->vm_page_prot);
    4125:	48 8b 45 c0          	mov    -0x40(%rbp),%rax
    4129:	48 8b 4d 90          	mov    -0x70(%rbp),%rcx
    412d:	48 8b 50 48          	mov    0x48(%rax),%rdx
    4131:	48 b8 00 00 00 00 00 	movabs $0x4000000000000,%rax
    4138:	00 04 00 
    413b:	48 21 d0             	and    %rdx,%rax
    413e:	48 89 c6             	mov    %rax,%rsi
    4141:	0f 84 f9 01 00 00    	je     4340 <do_swap_page.isra.68+0x480>
    4147:	49 8b 07             	mov    (%r15),%rax
    414a:	48 be 00 00 00 00 00 	movabs $0x8000000000000,%rsi
    4151:	00 08 00 
    4154:	a9 00 00 00 02       	test   $0x2000000,%eax
    4159:	b8 00 00 00 00       	mov    $0x0,%eax
    415e:	48 0f 44 f0          	cmove  %rax,%rsi
    4162:	48 89 d0             	mov    %rdx,%rax
    4165:	48 83 c8 10          	or     $0x10,%rax
    4169:	48 09 f0             	or     %rsi,%rax
static inline pgprotval_t massage_pgprot(pgprot_t pgprot)
{
	pgprotval_t protval = pgprot_val(pgprot);

	if (protval & _PAGE_PRESENT)
	  protval &= __supported_pte_mask;
    416c:	48 89 c7             	mov    %rax,%rdi
    416f:	48 ba 00 00 00 00 00 	movabs $0x160000000000,%rdx
    4176:	16 00 00 
    4179:	48 23 3d 00 00 00 00 	and    0x0(%rip),%rdi        # 4180 <do_swap_page.isra.68+0x2c0>
    4180:	4c 01 fa             	add    %r15,%rdx
    4183:	48 c1 fa 06          	sar    $0x6,%rdx
	return protval;
}

static inline pte_t pfn_pte(unsigned long page_nr, pgprot_t pgprot)
{
	return __pte(((phys_addr_t)page_nr << PAGE_SHIFT) |
    4187:	48 c1 e2 0c          	shl    $0xc,%rdx
static inline pgprotval_t massage_pgprot(pgprot_t pgprot)
{
	pgprotval_t protval = pgprot_val(pgprot);

	if (protval & _PAGE_PRESENT)
	  protval &= __supported_pte_mask;
    418b:	a8 01                	test   $0x1,%al
    418d:	48 0f 44 f8          	cmove  %rax,%rdi
	return protval;
}

static inline pte_t pfn_pte(unsigned long page_nr, pgprot_t pgprot)
{
	return __pte(((phys_addr_t)page_nr << PAGE_SHIFT) |
    4191:	48 09 d7             	or     %rdx,%rdi
	if (sizeof(pteval_t) > sizeof(long))
		ret = PVOP_CALLEE2(pteval_t,
				   pv_mmu_ops.make_pte,
				   val, (u64)val >> 32);
	else
		ret = PVOP_CALLEE1(pteval_t,
    4194:	ff 14 25 00 00 00 00 	callq  *0x0
    419b:	49 89 c6             	mov    %rax,%r14
	if ((flags & FAULT_FLAG_WRITE) && reuse_swap_page(page)) {
    419e:	8b 45 b4             	mov    -0x4c(%rbp),%eax
	struct page *page, *swapcache;
	swp_entry_t entry;
	pte_t pte;
	int locked;
	struct mem_cgroup *ptr;
	int exclusive = 0;
    41a1:	c7 45 90 00 00 00 00 	movl   $0x0,-0x70(%rbp)
	 */

	inc_mm_counter_fast(mm, MM_ANONPAGES);
	dec_mm_counter_fast(mm, MM_SWAPENTS);
	pte = mk_pte(page, vma->vm_page_prot);
	if ((flags & FAULT_FLAG_WRITE) && reuse_swap_page(page)) {
    41a8:	83 e0 01             	and    $0x1,%eax
    41ab:	89 45 b4             	mov    %eax,-0x4c(%rbp)
    41ae:	0f 85 f4 01 00 00    	jne    43a8 <do_swap_page.isra.68+0x4e8>

static inline pte_t pte_set_flags(pte_t pte, pteval_t set)
{
	pteval_t v = native_pte_val(pte);

	return native_make_pte(v | set);
    41b4:	4c 89 f0             	mov    %r14,%rax
{
	if (sizeof(pteval_t) > sizeof(long))
		/* 5 arg words */
		pv_mmu_ops.set_pte_at(mm, addr, ptep, pte);
	else
		PVOP_VCALL4(pv_mmu_ops.set_pte_at, mm, addr, ptep, pte.pte);
    41b7:	48 8b 7d b8          	mov    -0x48(%rbp),%rdi
    41bb:	48 8b 75 c8          	mov    -0x38(%rbp),%rsi
    41bf:	80 cc 08             	or     $0x8,%ah
    41c2:	81 e1 80 00 00 00    	and    $0x80,%ecx
    41c8:	48 8b 55 a8          	mov    -0x58(%rbp),%rdx
    41cc:	4c 0f 45 f0          	cmovne %rax,%r14
    41d0:	4c 89 f1             	mov    %r14,%rcx
    41d3:	ff 14 25 00 00 00 00 	callq  *0x0
	raw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \
} while (0)

static inline void spin_unlock(spinlock_t *lock)
{
	raw_spin_unlock(&lock->rlock);
    41da:	48 8b 7d 98          	mov    -0x68(%rbp),%rdi
    41de:	e8 00 00 00 00       	callq  41e3 <do_swap_page.isra.68+0x323>
	if (pte_swp_soft_dirty(orig_pte))
	  pte = pte_mksoft_dirty(pte);
	set_pte_at(mm, address, page_table, pte);
	pte_unmap_unlock(page_table, ptl);

	if (page == swapcache)
    41e3:	4c 39 fb             	cmp    %r15,%rbx
    41e6:	0f 84 6c 02 00 00    	je     4458 <do_swap_page.isra.68+0x598>
	  do_page_add_anon_rmap(page, vma, address, exclusive);
	else /* ksm created a completely new copy */
	  page_add_new_anon_rmap(page, vma, address);
    41ec:	48 8b 55 c8          	mov    -0x38(%rbp),%rdx
    41f0:	48 8b 75 c0          	mov    -0x40(%rbp),%rsi
    41f4:	4c 89 ff             	mov    %r15,%rdi
    41f7:	e8 00 00 00 00       	callq  41fc <do_swap_page.isra.68+0x33c>

	if (sizeof(pmdval_t) > sizeof(long))
		ret =  PVOP_CALLEE2(pmdval_t, pv_mmu_ops.pmd_val,
				    pmd.pmd, (u64)pmd.pmd >> 32);
	else
		ret =  PVOP_CALLEE1(pmdval_t, pv_mmu_ops.pmd_val,
    41fc:	49 8b 7d 00          	mov    0x0(%r13),%rdi
    4200:	ff 14 25 00 00 00 00 	callq  *0x0
    4207:	48 c1 e0 12          	shl    $0x12,%rax
    420b:	48 ba 00 00 00 00 00 	movabs $0xffffea0000000000,%rdx
    4212:	ea ff ff 
    4215:	49 8b 7d 00          	mov    0x0(%r13),%rdi
    4219:	48 c1 e8 1e          	shr    $0x1e,%rax
    421d:	48 c1 e0 06          	shl    $0x6,%rax
    4221:	48 8b 44 10 30       	mov    0x30(%rax,%rdx,1),%rax
    4226:	48 89 c6             	mov    %rax,%rsi
    4229:	48 89 45 98          	mov    %rax,-0x68(%rbp)
    422d:	ff 14 25 00 00 00 00 	callq  *0x0
	raw_spin_lock_init(&(_lock)->rlock);		\
} while (0)

static inline void spin_lock(spinlock_t *lock)
{
	raw_spin_lock(&lock->rlock);
    4234:	48 89 f7             	mov    %rsi,%rdi
    4237:	48 89 45 90          	mov    %rax,-0x70(%rbp)
    423b:	48 89 75 a8          	mov    %rsi,-0x58(%rbp)
    423f:	e8 00 00 00 00       	callq  4244 <do_swap_page.isra.68+0x384>
	page_table = pte_offset_map_lock(mm, pmd, address, &ptl);

	/* It's better to call commit-charge after rmap is established */
	mem_cgroup_commit_charge_swapin(page, ptr);
    4244:	48 8b 75 d0          	mov    -0x30(%rbp),%rsi
    4248:	4c 89 ff             	mov    %r15,%rdi
    424b:	e8 00 00 00 00       	callq  4250 <do_swap_page.isra.68+0x390>

	swap_free(entry);
    4250:	48 8b 7d a0          	mov    -0x60(%rbp),%rdi
    4254:	e8 00 00 00 00       	callq  4259 <do_swap_page.isra.68+0x399>
    4259:	48 8b 05 00 00 00 00 	mov    0x0(%rip),%rax        # 4260 <do_swap_page.isra.68+0x3a0>
extern long total_swap_pages;

/* Swap 50% full? Release swapcache more aggressively.. */
static inline bool vm_swap_full(void)
{
	return atomic_long_read(&nr_swap_pages) * 2 < total_swap_pages;
    4260:	48 01 c0             	add    %rax,%rax
	if (vm_swap_full() || (vma->vm_flags & VM_LOCKED) || PageMlocked(page))
    4263:	48 3b 05 00 00 00 00 	cmp    0x0(%rip),%rax        # 426a <do_swap_page.isra.68+0x3aa>
    426a:	0f 8c c0 00 00 00    	jl     4330 <do_swap_page.isra.68+0x470>
    4270:	48 8b 45 c0          	mov    -0x40(%rbp),%rax
    4274:	f6 40 51 20          	testb  $0x20,0x51(%rax)
    4278:	0f 85 b2 00 00 00    	jne    4330 <do_swap_page.isra.68+0x470>
    427e:	49 8b 07             	mov    (%r15),%rax
    4281:	a9 00 00 20 00       	test   $0x200000,%eax
    4286:	0f 85 a4 00 00 00    	jne    4330 <do_swap_page.isra.68+0x470>
    428c:	0f 1f 40 00          	nopl   0x0(%rax)
	  try_to_free_swap(page);
	unlock_page(page);
    4290:	4c 89 ff             	mov    %r15,%rdi
    4293:	e8 00 00 00 00       	callq  4298 <do_swap_page.isra.68+0x3d8>
	if (page != swapcache) {
    4298:	4c 39 fb             	cmp    %r15,%rbx
    429b:	74 10                	je     42ad <do_swap_page.isra.68+0x3ed>
		 * (to avoid false positives from pte_same). For
		 * further safety release the lock after the swap_free
		 * so that the swap count won't change under a
		 * parallel locked swapcache.
		 */
		unlock_page(swapcache);
    429d:	48 89 df             	mov    %rbx,%rdi
    42a0:	e8 00 00 00 00       	callq  42a5 <do_swap_page.isra.68+0x3e5>
		page_cache_release(swapcache);
    42a5:	48 89 df             	mov    %rbx,%rdi
    42a8:	e8 00 00 00 00       	callq  42ad <do_swap_page.isra.68+0x3ed>
	}

	if (flags & FAULT_FLAG_WRITE) {
    42ad:	8b 45 b4             	mov    -0x4c(%rbp),%eax
    42b0:	85 c0                	test   %eax,%eax
    42b2:	0f 85 a8 00 00 00    	jne    4360 <do_swap_page.isra.68+0x4a0>
	raw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \
} while (0)

static inline void spin_unlock(spinlock_t *lock)
{
	raw_spin_unlock(&lock->rlock);
    42b8:	48 8b 7d a8          	mov    -0x58(%rbp),%rdi
    42bc:	e8 00 00 00 00       	callq  42c1 <do_swap_page.isra.68+0x401>
    42c1:	e9 0c fd ff ff       	jmpq   3fd2 <do_swap_page.isra.68+0x112>
    42c6:	66 2e 0f 1f 84 00 00 	nopw   %cs:0x0(%rax,%rax,1)
    42cd:	00 00 00 
	page_table = pte_offset_map_lock(mm, pmd, address, &ptl);
	if (unlikely(!pte_same(*page_table, orig_pte)))
	  goto out_nomap;

	if (unlikely(!PageUptodate(page))) {
		ret = VM_FAULT_SIGBUS;
    42d0:	41 bc 02 00 00 00    	mov    $0x2,%r12d
unlock:
	pte_unmap_unlock(page_table, ptl);
out:
	return ret;
out_nomap:
	mem_cgroup_cancel_charge_swapin(ptr);
    42d6:	48 8b 7d d0          	mov    -0x30(%rbp),%rdi
    42da:	e8 00 00 00 00       	callq  42df <do_swap_page.isra.68+0x41f>
    42df:	48 8b 7d 98          	mov    -0x68(%rbp),%rdi
    42e3:	e8 00 00 00 00       	callq  42e8 <do_swap_page.isra.68+0x428>
	pte_unmap_unlock(page_table, ptl);
out_page:
	unlock_page(page);
    42e8:	4c 89 ff             	mov    %r15,%rdi
    42eb:	e8 00 00 00 00       	callq  42f0 <do_swap_page.isra.68+0x430>
out_release:
	page_cache_release(page);
    42f0:	4c 89 ff             	mov    %r15,%rdi
    42f3:	e8 00 00 00 00       	callq  42f8 <do_swap_page.isra.68+0x438>
	if (page != swapcache) {
    42f8:	4c 39 fb             	cmp    %r15,%rbx
    42fb:	0f 84 d1 fc ff ff    	je     3fd2 <do_swap_page.isra.68+0x112>
	return ret;
out_nomap:
	mem_cgroup_cancel_charge_swapin(ptr);
	pte_unmap_unlock(page_table, ptl);
out_page:
	unlock_page(page);
    4301:	48 89 df             	mov    %rbx,%rdi
    4304:	e8 00 00 00 00       	callq  4309 <do_swap_page.isra.68+0x449>
    4309:	e9 bc fc ff ff       	jmpq   3fca <do_swap_page.isra.68+0x10a>
    430e:	66 90                	xchg   %ax,%ax
    4310:	8b 55 b4             	mov    -0x4c(%rbp),%edx
    4313:	48 8b 75 b8          	mov    -0x48(%rbp),%rsi
    4317:	48 89 df             	mov    %rbx,%rdi
    431a:	e8 00 00 00 00       	callq  431f <do_swap_page.isra.68+0x45f>
    431f:	31 d2                	xor    %edx,%edx
    4321:	85 c0                	test   %eax,%eax
    4323:	0f 95 c2             	setne  %dl
    4326:	e9 76 fc ff ff       	jmpq   3fa1 <do_swap_page.isra.68+0xe1>
    432b:	0f 1f 44 00 00       	nopl   0x0(%rax,%rax,1)
	/* It's better to call commit-charge after rmap is established */
	mem_cgroup_commit_charge_swapin(page, ptr);

	swap_free(entry);
	if (vm_swap_full() || (vma->vm_flags & VM_LOCKED) || PageMlocked(page))
	  try_to_free_swap(page);
    4330:	4c 89 ff             	mov    %r15,%rdi
    4333:	e8 00 00 00 00       	callq  4338 <do_swap_page.isra.68+0x478>
    4338:	e9 53 ff ff ff       	jmpq   4290 <do_swap_page.isra.68+0x3d0>
    433d:	0f 1f 00             	nopl   (%rax)
    4340:	49 8b 3f             	mov    (%r15),%rdi
	 * discarded at swap_free().
	 */

	inc_mm_counter_fast(mm, MM_ANONPAGES);
	dec_mm_counter_fast(mm, MM_SWAPENTS);
	pte = mk_pte(page, vma->vm_page_prot);
    4343:	48 b8 00 00 00 00 00 	movabs $0x8000000000000,%rax
    434a:	00 08 00 
    434d:	f7 c7 00 00 00 02    	test   $0x2000000,%edi
    4353:	48 0f 44 c6          	cmove  %rsi,%rax
    4357:	48 09 d0             	or     %rdx,%rax
    435a:	e9 0d fe ff ff       	jmpq   416c <do_swap_page.isra.68+0x2ac>
    435f:	90                   	nop
	return (unsigned long)native_pmd_val(pmd) == 0;
}

static inline unsigned long pmd_page_vaddr(pmd_t pmd)
{
	return (unsigned long)__va(pmd_val(pmd) & PTE_PFN_MASK);
    4360:	48 b9 00 f0 ff ff ff 	movabs $0x3ffffffff000,%rcx
    4367:	3f 00 00 
    436a:	48 23 4d 90          	and    -0x70(%rbp),%rcx
		unlock_page(swapcache);
		page_cache_release(swapcache);
	}

	if (flags & FAULT_FLAG_WRITE) {
		ret |= do_wp_page(mm, vma, address, page_table, pmd, ptl, pte);
    436e:	4c 8b 4d 98          	mov    -0x68(%rbp),%r9
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
}

static inline pte_t *pte_offset_kernel(pmd_t *pmd, unsigned long address)
{
	return (pte_t *)pmd_page_vaddr(*pmd) + pte_index(address);
    4372:	48 03 4d 88          	add    -0x78(%rbp),%rcx
    4376:	48 8b 55 c8          	mov    -0x38(%rbp),%rdx
    437a:	4d 89 e8             	mov    %r13,%r8
    437d:	48 8b 75 c0          	mov    -0x40(%rbp),%rsi
    4381:	48 8b 7d b8          	mov    -0x48(%rbp),%rdi
    4385:	4c 89 34 24          	mov    %r14,(%rsp)
    4389:	e8 02 f3 ff ff       	callq  3690 <do_wp_page>
    438e:	44 09 e0             	or     %r12d,%eax
		if (ret & VM_FAULT_ERROR)
    4391:	41 89 c4             	mov    %eax,%r12d
    4394:	41 81 e4 33 08 00 00 	and    $0x833,%r12d
    439b:	44 0f 44 e0          	cmove  %eax,%r12d
    439f:	e9 2e fc ff ff       	jmpq   3fd2 <do_swap_page.isra.68+0x112>
    43a4:	0f 1f 40 00          	nopl   0x0(%rax)
	 */

	inc_mm_counter_fast(mm, MM_ANONPAGES);
	dec_mm_counter_fast(mm, MM_SWAPENTS);
	pte = mk_pte(page, vma->vm_page_prot);
	if ((flags & FAULT_FLAG_WRITE) && reuse_swap_page(page)) {
    43a8:	4c 89 ff             	mov    %r15,%rdi
    43ab:	48 89 4d 80          	mov    %rcx,-0x80(%rbp)
    43af:	e8 00 00 00 00       	callq  43b4 <do_swap_page.isra.68+0x4f4>
    43b4:	85 c0                	test   %eax,%eax
    43b6:	c7 45 b4 01 00 00 00 	movl   $0x1,-0x4c(%rbp)
    43bd:	48 8b 4d 80          	mov    -0x80(%rbp),%rcx
    43c1:	0f 84 ed fd ff ff    	je     41b4 <do_swap_page.isra.68+0x2f4>
 * pte_mkwrite.  But get_user_pages can cause write faults for mappings
 * that do not have writing enabled, when used by access_process_vm.
 */
static inline pte_t maybe_mkwrite(pte_t pte, struct vm_area_struct *vma)
{
	if (likely(vma->vm_flags & VM_WRITE))
    43c7:	48 8b 45 c0          	mov    -0x40(%rbp),%rax
    43cb:	f6 40 50 02          	testb  $0x2,0x50(%rax)
    43cf:	0f 84 b4 01 00 00    	je     4589 <do_swap_page.isra.68+0x6c9>

static inline pte_t pte_set_flags(pte_t pte, pteval_t set)
{
	pteval_t v = native_pte_val(pte);

	return native_make_pte(v | set);
    43d5:	49 81 ce 42 08 00 00 	or     $0x842,%r14
		pte = maybe_mkwrite(pte_mkdirty(pte), vma);
		flags &= ~FAULT_FLAG_WRITE;
		ret |= VM_FAULT_WRITE;
    43dc:	41 83 cc 08          	or     $0x8,%r12d
    43e0:	c7 45 b4 00 00 00 00 	movl   $0x0,-0x4c(%rbp)
		exclusive = 1;
    43e7:	c7 45 90 01 00 00 00 	movl   $0x1,-0x70(%rbp)
    43ee:	e9 c1 fd ff ff       	jmpq   41b4 <do_swap_page.isra.68+0x2f4>
    43f3:	0f 1f 44 00 00       	nopl   0x0(%rax,%rax,1)
	} else if (PageHWPoison(page)) {
		/*
		 * hwpoisoned dirty swapcache pages are kept for killing
		 * owner processes (which may be unknown at hwpoison time)
		 */
		ret = VM_FAULT_HWPOISON;
    43f8:	41 bc 10 00 00 00    	mov    $0x10,%r12d
    43fe:	e9 c7 fb ff ff       	jmpq   3fca <do_swap_page.isra.68+0x10a>
    4403:	0f 1f 44 00 00       	nopl   0x0(%rax,%rax,1)
		goto out;
	}
	delayacct_set_flag(DELAYACCT_PF_SWAPIN);
	page = lookup_swap_cache(entry);
	if (!page) {
		page = swapin_readahead(entry,
    4408:	48 8b 4d c8          	mov    -0x38(%rbp),%rcx
    440c:	48 8b 55 c0          	mov    -0x40(%rbp),%rdx
    4410:	be da 00 02 00       	mov    $0x200da,%esi
    4415:	4c 89 ff             	mov    %r15,%rdi
    4418:	e8 00 00 00 00       	callq  441d <do_swap_page.isra.68+0x55d>
					GFP_HIGHUSER_MOVABLE, vma, address);
		if (!page) {
    441d:	48 85 c0             	test   %rax,%rax
		goto out;
	}
	delayacct_set_flag(DELAYACCT_PF_SWAPIN);
	page = lookup_swap_cache(entry);
	if (!page) {
		page = swapin_readahead(entry,
    4420:	48 89 c3             	mov    %rax,%rbx
					GFP_HIGHUSER_MOVABLE, vma, address);
		if (!page) {
    4423:	74 4b                	je     4470 <do_swap_page.isra.68+0x5b0>
extern int do_swap_account;
#endif

static inline bool mem_cgroup_disabled(void)
{
	if (mem_cgroup_subsys.disabled)
    4425:	8b 15 00 00 00 00    	mov    0x0(%rip),%edx        # 442b <do_swap_page.isra.68+0x56b>
				delayacct_clear_flag(DELAYACCT_PF_SWAPIN);
			goto unlock;
		}

		/* Had to read the page from swap area: Major fault */
		ret = VM_FAULT_MAJOR;
    442b:	41 bc 04 00 00 00    	mov    $0x4,%r12d
	__this_cpu_inc(vm_event_states.event[item]);
}

static inline void count_vm_event(enum vm_event_item item)
{
	this_cpu_inc(vm_event_states.event[item]);
    4431:	65 48 ff 04 25 00 00 	incq   %gs:0x0
    4438:	00 00 
    443a:	85 d2                	test   %edx,%edx
    443c:	0f 85 4f fb ff ff    	jne    3f91 <do_swap_page.isra.68+0xd1>
static inline void mem_cgroup_count_vm_event(struct mm_struct *mm,
					     enum vm_event_item idx)
{
	if (mem_cgroup_disabled())
		return;
	__mem_cgroup_count_vm_event(mm, idx);
    4442:	48 8b 7d b8          	mov    -0x48(%rbp),%rdi
    4446:	be 0c 00 00 00       	mov    $0xc,%esi
    444b:	e8 00 00 00 00       	callq  4450 <do_swap_page.isra.68+0x590>
    4450:	e9 3c fb ff ff       	jmpq   3f91 <do_swap_page.isra.68+0xd1>
    4455:	0f 1f 00             	nopl   (%rax)
	  pte = pte_mksoft_dirty(pte);
	set_pte_at(mm, address, page_table, pte);
	pte_unmap_unlock(page_table, ptl);

	if (page == swapcache)
	  do_page_add_anon_rmap(page, vma, address, exclusive);
    4458:	8b 4d 90             	mov    -0x70(%rbp),%ecx
    445b:	48 8b 55 c8          	mov    -0x38(%rbp),%rdx
    445f:	48 89 df             	mov    %rbx,%rdi
    4462:	48 8b 75 c0          	mov    -0x40(%rbp),%rsi
    4466:	e8 00 00 00 00       	callq  446b <do_swap_page.isra.68+0x5ab>
    446b:	e9 8c fd ff ff       	jmpq   41fc <do_swap_page.isra.68+0x33c>
    4470:	49 8b 7d 00          	mov    0x0(%r13),%rdi
    4474:	ff 14 25 00 00 00 00 	callq  *0x0
}
#endif /* ALLOC_SPLIT_PTLOCKS */

static inline spinlock_t *pte_lockptr(struct mm_struct *mm, pmd_t *pmd)
{
	return ptlock_ptr(pmd_page(*pmd));
    447b:	49 bc 00 f0 ff ff ff 	movabs $0x3ffffffff000,%r12
    4482:	3f 00 00 
    4485:	48 ba 00 00 00 00 00 	movabs $0xffffea0000000000,%rdx
    448c:	ea ff ff 
    448f:	49 8b 7d 00          	mov    0x0(%r13),%rdi
    4493:	4c 21 e0             	and    %r12,%rax
    4496:	48 c1 e8 06          	shr    $0x6,%rax
    449a:	4c 8b 7c 10 30       	mov    0x30(%rax,%rdx,1),%r15
    449f:	ff 14 25 00 00 00 00 	callq  *0x0
	raw_spin_lock_init(&(_lock)->rlock);		\
} while (0)

static inline void spin_lock(spinlock_t *lock)
{
	raw_spin_lock(&lock->rlock);
    44a6:	4c 89 ff             	mov    %r15,%rdi
    44a9:	48 89 c3             	mov    %rax,%rbx
    44ac:	4c 89 7d a8          	mov    %r15,-0x58(%rbp)
    44b0:	e8 00 00 00 00       	callq  44b5 <do_swap_page.isra.68+0x5f5>
 * this function returns the index of the entry in the pte page which would
 * control the given virtual address
 */
static inline unsigned long pte_index(unsigned long address)
{
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
    44b5:	48 8b 55 c8          	mov    -0x38(%rbp),%rdx
	return (unsigned long)native_pmd_val(pmd) == 0;
}

static inline unsigned long pmd_page_vaddr(pmd_t pmd)
{
	return (unsigned long)__va(pmd_val(pmd) & PTE_PFN_MASK);
    44b9:	4c 21 e3             	and    %r12,%rbx
			/*
			 * Back out if somebody else faulted in this pte
			 * while we released the pte lock.
			 */
			page_table = pte_offset_map_lock(mm, pmd, address, &ptl);
			if (likely(pte_same(*page_table, orig_pte)))
    44bc:	48 b8 00 00 00 00 00 	movabs $0xffff880000000000,%rax
    44c3:	88 ff ff 
 * this function returns the index of the entry in the pte page which would
 * control the given virtual address
 */
static inline unsigned long pte_index(unsigned long address)
{
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
    44c6:	48 c1 ea 09          	shr    $0x9,%rdx
    44ca:	81 e2 f8 0f 00 00    	and    $0xff8,%edx
    44d0:	48 01 da             	add    %rbx,%rdx
    44d3:	4c 39 34 02          	cmp    %r14,(%rdx,%rax,1)
    44d7:	0f 85 b8 00 00 00    	jne    4595 <do_swap_page.isra.68+0x6d5>
			{ ret = VM_FAULT_OOM;
				printk("not same pte %lx %lx",page_table->pte,orig_pte.pte);
    44dd:	4c 89 f2             	mov    %r14,%rdx
    44e0:	4c 89 f6             	mov    %r14,%rsi
    44e3:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
    44ea:	31 c0                	xor    %eax,%eax
			 * Back out if somebody else faulted in this pte
			 * while we released the pte lock.
			 */
			page_table = pte_offset_map_lock(mm, pmd, address, &ptl);
			if (likely(pte_same(*page_table, orig_pte)))
			{ ret = VM_FAULT_OOM;
    44ec:	41 bc 01 00 00 00    	mov    $0x1,%r12d
				printk("not same pte %lx %lx",page_table->pte,orig_pte.pte);
    44f2:	e8 00 00 00 00       	callq  44f7 <do_swap_page.isra.68+0x637>
    44f7:	65 48 8b 04 25 00 00 	mov    %gs:0x0,%rax
    44fe:	00 00 
		current->delays->flags |= flag;
}

static inline void delayacct_clear_flag(int flag)
{
	if (current->delays)
    4500:	48 83 b8 a0 08 00 00 	cmpq   $0x0,0x8a0(%rax)
    4507:	00 
    4508:	0f 84 aa fd ff ff    	je     42b8 <do_swap_page.isra.68+0x3f8>
		current->delays->flags &= ~flag;
    450e:	48 8b 80 a0 08 00 00 	mov    0x8a0(%rax),%rax
    4515:	83 60 18 fe          	andl   $0xfffffffe,0x18(%rax)
    4519:	e9 9a fd ff ff       	jmpq   42b8 <do_swap_page.isra.68+0x3f8>
			page_to_pfn(page));
}

static inline int is_migration_entry(swp_entry_t entry)
{
	return unlikely(swp_type(entry) == SWP_MIGRATION_READ ||
    451e:	8d 50 e2             	lea    -0x1e(%rax),%edx
    4521:	83 fa 01             	cmp    $0x1,%edx
    4524:	76 77                	jbe    459d <do_swap_page.isra.68+0x6dd>

	entry = pte_to_swp_entry(orig_pte);
	if (unlikely(non_swap_entry(entry))) {
		if (is_migration_entry(entry)) {
			migration_entry_wait(mm, pmd, address);
		} else if (is_hwpoison_entry(entry)) {
    4526:	83 f8 1d             	cmp    $0x1d,%eax
			ret = VM_FAULT_HWPOISON;
    4529:	41 bc 10 00 00 00    	mov    $0x10,%r12d

	entry = pte_to_swp_entry(orig_pte);
	if (unlikely(non_swap_entry(entry))) {
		if (is_migration_entry(entry)) {
			migration_entry_wait(mm, pmd, address);
		} else if (is_hwpoison_entry(entry)) {
    452f:	0f 84 9d fa ff ff    	je     3fd2 <do_swap_page.isra.68+0x112>
			ret = VM_FAULT_HWPOISON;
		} else {
			print_bad_pte(vma, address, orig_pte, NULL);
    4535:	48 8b 75 c8          	mov    -0x38(%rbp),%rsi
    4539:	48 8b 7d c0          	mov    -0x40(%rbp),%rdi
    453d:	31 c9                	xor    %ecx,%ecx
    453f:	4c 89 f2             	mov    %r14,%rdx
			ret = VM_FAULT_SIGBUS;
    4542:	41 bc 02 00 00 00    	mov    $0x2,%r12d
		if (is_migration_entry(entry)) {
			migration_entry_wait(mm, pmd, address);
		} else if (is_hwpoison_entry(entry)) {
			ret = VM_FAULT_HWPOISON;
		} else {
			print_bad_pte(vma, address, orig_pte, NULL);
    4548:	e8 23 c2 ff ff       	callq  770 <print_bad_pte>
    454d:	e9 80 fa ff ff       	jmpq   3fd2 <do_swap_page.isra.68+0x112>
    4552:	e8 00 00 00 00       	callq  4557 <do_swap_page.isra.68+0x697>
	  goto out_page;

	page = ksm_might_need_to_copy(page, vma, address);
	if (unlikely(!page)) {
		ret = VM_FAULT_OOM;
		printk("swap fault due to !page\n");
    4557:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
    455e:	31 c0                	xor    %eax,%eax
	if (unlikely(!PageSwapCache(page) || page_private(page) != entry.val))
	  goto out_page;

	page = ksm_might_need_to_copy(page, vma, address);
	if (unlikely(!page)) {
		ret = VM_FAULT_OOM;
    4560:	41 bc 01 00 00 00    	mov    $0x1,%r12d
		printk("swap fault due to !page\n");
    4566:	e8 00 00 00 00       	callq  456b <do_swap_page.isra.68+0x6ab>
    456b:	e9 91 fd ff ff       	jmpq   4301 <do_swap_page.isra.68+0x441>
		page = swapcache;
		goto out_page;
	}

	if (mem_cgroup_try_charge_swapin(mm, page, GFP_KERNEL, &ptr)) {
		printk("swap fault due to mem_cgroup_try_charge_swapin\n");	
    4570:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
    4577:	31 c0                	xor    %eax,%eax
		ret = VM_FAULT_OOM;
    4579:	41 bc 01 00 00 00    	mov    $0x1,%r12d
		page = swapcache;
		goto out_page;
	}

	if (mem_cgroup_try_charge_swapin(mm, page, GFP_KERNEL, &ptr)) {
		printk("swap fault due to mem_cgroup_try_charge_swapin\n");	
    457f:	e8 00 00 00 00       	callq  4584 <do_swap_page.isra.68+0x6c4>
    4584:	e9 5f fd ff ff       	jmpq   42e8 <do_swap_page.isra.68+0x428>

static inline pte_t pte_set_flags(pte_t pte, pteval_t set)
{
	pteval_t v = native_pte_val(pte);

	return native_make_pte(v | set);
    4589:	49 81 ce 40 08 00 00 	or     $0x840,%r14
    4590:	e9 47 fe ff ff       	jmpq   43dc <do_swap_page.isra.68+0x51c>
	swp_entry_t entry;
	pte_t pte;
	int locked;
	struct mem_cgroup *ptr;
	int exclusive = 0;
	int ret = 0;
    4595:	45 31 e4             	xor    %r12d,%r12d
    4598:	e9 5a ff ff ff       	jmpq   44f7 <do_swap_page.isra.68+0x637>
	  goto out;

	entry = pte_to_swp_entry(orig_pte);
	if (unlikely(non_swap_entry(entry))) {
		if (is_migration_entry(entry)) {
			migration_entry_wait(mm, pmd, address);
    459d:	48 8b 55 c8          	mov    -0x38(%rbp),%rdx
    45a1:	48 8b 7d b8          	mov    -0x48(%rbp),%rdi
    45a5:	48 89 ce             	mov    %rcx,%rsi
	swp_entry_t entry;
	pte_t pte;
	int locked;
	struct mem_cgroup *ptr;
	int exclusive = 0;
	int ret = 0;
    45a8:	45 31 e4             	xor    %r12d,%r12d
	  goto out;

	entry = pte_to_swp_entry(orig_pte);
	if (unlikely(non_swap_entry(entry))) {
		if (is_migration_entry(entry)) {
			migration_entry_wait(mm, pmd, address);
    45ab:	e8 00 00 00 00       	callq  45b0 <do_swap_page.isra.68+0x6f0>
    45b0:	e9 1d fa ff ff       	jmpq   3fd2 <do_swap_page.isra.68+0x112>
    45b5:	66 66 2e 0f 1f 84 00 	data32 nopw %cs:0x0(%rax,%rax,1)
    45bc:	00 00 00 00 

00000000000045c0 <numa_migrate_prep>:
}

int numa_migrate_prep(struct page *page, struct vm_area_struct *vma,
			unsigned long addr, int page_nid,
			int *flags)
{
    45c0:	e8 00 00 00 00       	callq  45c5 <numa_migrate_prep+0x5>
    45c5:	55                   	push   %rbp
    45c6:	48 89 e5             	mov    %rsp,%rbp
    45c9:	53                   	push   %rbx
    45ca:	48 89 fb             	mov    %rdi,%rbx
    45cd:	48 83 ec 20          	sub    $0x20,%rsp
    45d1:	48 8b 07             	mov    (%rdi),%rax
void get_huge_page_tail(struct page *page);/*ziqiao*/
extern bool __get_page_tail(struct page *page);

static inline void get_page(struct page *page)
{
	if (unlikely(PageTail(page)))
    45d4:	f6 c4 80             	test   $0x80,%ah
    45d7:	75 35                	jne    460e <numa_migrate_prep+0x4e>
    45d9:	f0 ff 43 1c          	lock incl 0x1c(%rbx)
    45dd:	65 48 ff 04 25 00 00 	incq   %gs:0x0
    45e4:	00 00 

#ifndef numa_node_id
/* Returns the number of the current Node. */
static inline int numa_node_id(void)
{
	return __this_cpu_read(numa_node);
    45e6:	65 8b 04 25 00 00 00 	mov    %gs:0x0,%eax
    45ed:	00 
	get_page(page);

	count_vm_numa_event(NUMA_HINT_FAULTS);
	if (page_nid == numa_node_id()) {
    45ee:	39 c1                	cmp    %eax,%ecx
    45f0:	75 0d                	jne    45ff <numa_migrate_prep+0x3f>
    45f2:	65 48 ff 04 25 00 00 	incq   %gs:0x0
    45f9:	00 00 
		count_vm_numa_event(NUMA_HINT_FAULTS_LOCAL);
		*flags |= TNF_FAULT_LOCAL;
    45fb:	41 83 08 08          	orl    $0x8,(%r8)
	}

	return mpol_misplaced(page, vma, addr);
    45ff:	48 89 df             	mov    %rbx,%rdi
    4602:	e8 00 00 00 00       	callq  4607 <numa_migrate_prep+0x47>
}
    4607:	48 83 c4 20          	add    $0x20,%rsp
    460b:	5b                   	pop    %rbx
    460c:	5d                   	pop    %rbp
    460d:	c3                   	retq   
    460e:	4c 89 45 d8          	mov    %r8,-0x28(%rbp)
    4612:	89 4d e4             	mov    %ecx,-0x1c(%rbp)
    4615:	48 89 55 e8          	mov    %rdx,-0x18(%rbp)
    4619:	48 89 75 f0          	mov    %rsi,-0x10(%rbp)
		if (likely(__get_page_tail(page)))
    461d:	e8 00 00 00 00       	callq  4622 <numa_migrate_prep+0x62>
    4622:	84 c0                	test   %al,%al
    4624:	48 8b 75 f0          	mov    -0x10(%rbp),%rsi
    4628:	48 8b 55 e8          	mov    -0x18(%rbp),%rdx
    462c:	8b 4d e4             	mov    -0x1c(%rbp),%ecx
    462f:	4c 8b 45 d8          	mov    -0x28(%rbp),%r8
    4633:	75 a8                	jne    45dd <numa_migrate_prep+0x1d>
    4635:	eb a2                	jmp    45d9 <numa_migrate_prep+0x19>
    4637:	66 0f 1f 84 00 00 00 	nopw   0x0(%rax,%rax,1)
    463e:	00 00 

0000000000004640 <do_numa_page>:

int do_numa_page(struct mm_struct *mm, struct vm_area_struct *vma,
			unsigned long addr, pte_t pte, pte_t *ptep, pmd_t *pmd)
{
    4640:	e8 00 00 00 00       	callq  4645 <do_numa_page+0x5>
    4645:	55                   	push   %rbp
    4646:	48 89 e5             	mov    %rsp,%rbp
    4649:	41 57                	push   %r15
    464b:	4d 89 c7             	mov    %r8,%r15
    464e:	41 56                	push   %r14
    4650:	49 89 ce             	mov    %rcx,%r14
    4653:	41 55                	push   %r13
    4655:	41 54                	push   %r12
    4657:	49 89 f4             	mov    %rsi,%r12
    465a:	53                   	push   %rbx
    465b:	48 89 d3             	mov    %rdx,%rbx
    465e:	48 83 ec 10          	sub    $0x10,%rsp
    4662:	48 89 7d c8          	mov    %rdi,-0x38(%rbp)
	spinlock_t *ptl;
	int page_nid = -1;
	int last_cpupid;
	int target_nid;
	bool migrated = false;
	int flags = 0;
    4666:	c7 45 d4 00 00 00 00 	movl   $0x0,-0x2c(%rbp)
    466d:	49 8b 39             	mov    (%r9),%rdi
    4670:	ff 14 25 00 00 00 00 	callq  *0x0
}
#endif /* ALLOC_SPLIT_PTLOCKS */

static inline spinlock_t *pte_lockptr(struct mm_struct *mm, pmd_t *pmd)
{
	return ptlock_ptr(pmd_page(*pmd));
    4677:	48 c1 e0 12          	shl    $0x12,%rax
    467b:	48 b9 00 00 00 00 00 	movabs $0xffffea0000000000,%rcx
    4682:	ea ff ff 
    4685:	48 c1 e8 1e          	shr    $0x1e,%rax
    4689:	48 c1 e0 06          	shl    $0x6,%rax
    468d:	4c 8b 6c 08 30       	mov    0x30(%rax,%rcx,1),%r13
    4692:	4c 89 ef             	mov    %r13,%rdi
    4695:	e8 00 00 00 00       	callq  469a <do_numa_page+0x5a>
    469a:	49 8b 3f             	mov    (%r15),%rdi
	 * the _PAGE_NUMA bit and it is not really expected that there
	 * would be concurrent hardware modifications to the PTE.
	 */
	ptl = pte_lockptr(mm, pmd);
	spin_lock(ptl);
	if (unlikely(!pte_same(*ptep, pte))) {
    469d:	4c 39 f7             	cmp    %r14,%rdi
    46a0:	0f 85 3a 01 00 00    	jne    47e0 <do_numa_page+0x1a0>

	if (sizeof(pteval_t) > sizeof(long))
		ret = PVOP_CALLEE2(pteval_t, pv_mmu_ops.pte_val,
				   pte.pte, (u64)pte.pte >> 32);
	else
		ret = PVOP_CALLEE1(pteval_t, pv_mmu_ops.pte_val,
    46a6:	ff 14 25 00 00 00 00 	callq  *0x0
    46ad:	48 89 c7             	mov    %rax,%rdi
#ifndef pte_mknonnuma
static inline pte_t pte_mknonnuma(pte_t pte)
{
	pteval_t val = pte_val(pte);

	val &= ~_PAGE_NUMA;
    46b0:	48 81 e7 ff fe ff ff 	and    $0xfffffffffffffeff,%rdi
	val |= (_PAGE_PRESENT|_PAGE_ACCESSED);
    46b7:	48 83 cf 21          	or     $0x21,%rdi
	if (sizeof(pteval_t) > sizeof(long))
		ret = PVOP_CALLEE2(pteval_t,
				   pv_mmu_ops.make_pte,
				   val, (u64)val >> 32);
	else
		ret = PVOP_CALLEE1(pteval_t,
    46bb:	ff 14 25 00 00 00 00 	callq  *0x0
    46c2:	49 89 c6             	mov    %rax,%r14
{
	if (sizeof(pteval_t) > sizeof(long))
		/* 5 arg words */
		pv_mmu_ops.set_pte_at(mm, addr, ptep, pte);
	else
		PVOP_VCALL4(pv_mmu_ops.set_pte_at, mm, addr, ptep, pte.pte);
    46c5:	48 8b 7d c8          	mov    -0x38(%rbp),%rdi
    46c9:	48 89 de             	mov    %rbx,%rsi
    46cc:	4c 89 fa             	mov    %r15,%rdx
    46cf:	48 89 c1             	mov    %rax,%rcx
    46d2:	ff 14 25 00 00 00 00 	callq  *0x0

	pte = pte_mknonnuma(pte);
	set_pte_at(mm, addr, ptep, pte);
	update_mmu_cache(vma, addr, ptep);

	page = vm_normal_page(vma, addr, pte);
    46d9:	4c 89 f2             	mov    %r14,%rdx
    46dc:	48 89 de             	mov    %rbx,%rsi
    46df:	4c 89 e7             	mov    %r12,%rdi
    46e2:	e8 00 00 00 00       	callq  46e7 <do_numa_page+0xa7>
	if (!page) {
    46e7:	48 85 c0             	test   %rax,%rax

	pte = pte_mknonnuma(pte);
	set_pte_at(mm, addr, ptep, pte);
	update_mmu_cache(vma, addr, ptep);

	page = vm_normal_page(vma, addr, pte);
    46ea:	49 89 c7             	mov    %rax,%r15
	if (!page) {
    46ed:	0f 84 ed 00 00 00    	je     47e0 <do_numa_page+0x1a0>
		pte_unmap_unlock(ptep, ptl);
		return 0;
	}
	BUG_ON(is_zero_pfn(page_to_pfn(page)));
    46f3:	48 b8 00 00 00 00 00 	movabs $0x160000000000,%rax
    46fa:	16 00 00 
    46fd:	4c 01 f8             	add    %r15,%rax
    4700:	48 c1 f8 06          	sar    $0x6,%rax
    4704:	48 3b 05 00 00 00 00 	cmp    0x0(%rip),%rax        # 470b <do_numa_page+0xcb>
    470b:	0f 84 d9 00 00 00    	je     47ea <do_numa_page+0x1aa>
	/*
	 * Avoid grouping on DSO/COW pages in specific and RO pages
	 * in general, RO pages shouldn't hurt as much anyway since
	 * they can be in shared cache state.
	 */
	if (!pte_write(pte))
    4711:	41 83 e6 02          	and    $0x2,%r14d
    4715:	0f 84 85 00 00 00    	je     47a0 <do_numa_page+0x160>
 *
 * Atomically reads the value of @v.
 */
static inline int atomic_read(const atomic_t *v)
{
	return (*(volatile int *)&(v)->counter);
    471b:	41 8b 47 18          	mov    0x18(%r15),%eax
	reset_page_counter_in_ns(page);
}

static inline int page_mapcount(struct page *page)
{
	return atomic_read(&(page)->_mapcount) + 1;
    471f:	83 c0 01             	add    $0x1,%eax

	/*
	 * Flag if the page is shared between multiple address spaces. This
	 * is later used when determining whether to group tasks together
	 */
	if (page_mapcount(page) > 1 && (vma->vm_flags & VM_SHARED))
    4722:	83 f8 01             	cmp    $0x1,%eax
    4725:	7e 0c                	jle    4733 <do_numa_page+0xf3>
    4727:	41 f6 44 24 50 08    	testb  $0x8,0x50(%r12)
    472d:	0f 85 7d 00 00 00    	jne    47b0 <do_numa_page+0x170>
}

int do_numa_page(struct mm_struct *mm, struct vm_area_struct *vma,
			unsigned long addr, pte_t pte, pte_t *ptep, pmd_t *pmd)
{
	struct page *page = NULL;
    4733:	4d 8b 37             	mov    (%r15),%r14
	if (page_mapcount(page) > 1 && (vma->vm_flags & VM_SHARED))
	  flags |= TNF_SHARED;

	last_cpupid = page_cpupid_last(page);
	page_nid = page_to_nid(page);
	target_nid = numa_migrate_prep(page, vma, addr, page_nid, &flags);
    4736:	4c 8d 45 d4          	lea    -0x2c(%rbp),%r8
    473a:	48 89 da             	mov    %rbx,%rdx
    473d:	4c 89 e6             	mov    %r12,%rsi
    4740:	4c 89 ff             	mov    %r15,%rdi
#ifdef NODE_NOT_IN_PAGE_FLAGS
extern int page_to_nid(const struct page *page);
#else
static inline int page_to_nid(const struct page *page)
{
	return (page->flags >> NODES_PGSHIFT) & NODES_MASK;
    4743:	4c 89 f1             	mov    %r14,%rcx
    4746:	48 c1 e9 3a          	shr    $0x3a,%rcx
    474a:	89 4d c8             	mov    %ecx,-0x38(%rbp)
    474d:	e8 00 00 00 00       	callq  4752 <do_numa_page+0x112>
	raw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \
} while (0)

static inline void spin_unlock(spinlock_t *lock)
{
	raw_spin_unlock(&lock->rlock);
    4752:	4c 89 ef             	mov    %r13,%rdi
    4755:	89 c3                	mov    %eax,%ebx
    4757:	e8 00 00 00 00       	callq  475c <do_numa_page+0x11c>
	pte_unmap_unlock(ptep, ptl);
	if (target_nid == -1) {
    475c:	83 fb ff             	cmp    $0xffffffff,%ebx
    475f:	74 6f                	je     47d0 <do_numa_page+0x190>
		put_page(page);
		goto out;
	}

	/* Migrate to the requested node */
	migrated = migrate_misplaced_page(page, vma, target_nid);
    4761:	89 da                	mov    %ebx,%edx
    4763:	4c 89 e6             	mov    %r12,%rsi
    4766:	4c 89 ff             	mov    %r15,%rdi
    4769:	e8 00 00 00 00       	callq  476e <do_numa_page+0x12e>
	if (migrated) {
    476e:	85 c0                	test   %eax,%eax
    4770:	75 4e                	jne    47c0 <do_numa_page+0x180>
    4772:	8b 4d d4             	mov    -0x2c(%rbp),%ecx
		flags |= TNF_MIGRATED;
	}

out:
	if (page_nid != -1)
	  task_numa_fault(last_cpupid, page_nid, 1, flags);
    4775:	8b 75 c8             	mov    -0x38(%rbp),%esi
	page->_last_cpupid = -1;
}
#else
static inline int page_cpupid_last(struct page *page)
{
	return (page->flags >> LAST_CPUPID_PGSHIFT) & LAST_CPUPID_MASK;
    4778:	49 c1 ee 28          	shr    $0x28,%r14
    477c:	ba 01 00 00 00       	mov    $0x1,%edx
    4781:	41 0f b7 fe          	movzwl %r14w,%edi
    4785:	e8 00 00 00 00       	callq  478a <do_numa_page+0x14a>
		return handle_double_cache_pte_fault(mm, vma, addr,
				ptep, pmd, ptl, pte,true);
	}*/

	return 0;
}
    478a:	48 83 c4 10          	add    $0x10,%rsp
    478e:	31 c0                	xor    %eax,%eax
    4790:	5b                   	pop    %rbx
    4791:	41 5c                	pop    %r12
    4793:	41 5d                	pop    %r13
    4795:	41 5e                	pop    %r14
    4797:	41 5f                	pop    %r15
    4799:	5d                   	pop    %rbp
    479a:	c3                   	retq   
    479b:	0f 1f 44 00 00       	nopl   0x0(%rax,%rax,1)
	 * Avoid grouping on DSO/COW pages in specific and RO pages
	 * in general, RO pages shouldn't hurt as much anyway since
	 * they can be in shared cache state.
	 */
	if (!pte_write(pte))
	  flags |= TNF_NO_GROUP;
    47a0:	83 4d d4 02          	orl    $0x2,-0x2c(%rbp)
    47a4:	e9 72 ff ff ff       	jmpq   471b <do_numa_page+0xdb>
    47a9:	0f 1f 80 00 00 00 00 	nopl   0x0(%rax)
	/*
	 * Flag if the page is shared between multiple address spaces. This
	 * is later used when determining whether to group tasks together
	 */
	if (page_mapcount(page) > 1 && (vma->vm_flags & VM_SHARED))
	  flags |= TNF_SHARED;
    47b0:	83 4d d4 04          	orl    $0x4,-0x2c(%rbp)
    47b4:	e9 7a ff ff ff       	jmpq   4733 <do_numa_page+0xf3>
    47b9:	0f 1f 80 00 00 00 00 	nopl   0x0(%rax)

	/* Migrate to the requested node */
	migrated = migrate_misplaced_page(page, vma, target_nid);
	if (migrated) {
		page_nid = target_nid;
		flags |= TNF_MIGRATED;
    47c0:	8b 4d d4             	mov    -0x2c(%rbp),%ecx
    47c3:	89 5d c8             	mov    %ebx,-0x38(%rbp)
    47c6:	83 c9 01             	or     $0x1,%ecx
    47c9:	89 4d d4             	mov    %ecx,-0x2c(%rbp)
    47cc:	eb a7                	jmp    4775 <do_numa_page+0x135>
    47ce:	66 90                	xchg   %ax,%ax
	last_cpupid = page_cpupid_last(page);
	page_nid = page_to_nid(page);
	target_nid = numa_migrate_prep(page, vma, addr, page_nid, &flags);
	pte_unmap_unlock(ptep, ptl);
	if (target_nid == -1) {
		put_page(page);
    47d0:	4c 89 ff             	mov    %r15,%rdi
    47d3:	e8 00 00 00 00       	callq  47d8 <do_numa_page+0x198>
		goto out;
    47d8:	eb 98                	jmp    4772 <do_numa_page+0x132>
    47da:	66 0f 1f 44 00 00    	nopw   0x0(%rax,%rax,1)
    47e0:	4c 89 ef             	mov    %r13,%rdi
    47e3:	e8 00 00 00 00       	callq  47e8 <do_numa_page+0x1a8>
	update_mmu_cache(vma, addr, ptep);

	page = vm_normal_page(vma, addr, pte);
	if (!page) {
		pte_unmap_unlock(ptep, ptl);
		return 0;
    47e8:	eb a0                	jmp    478a <do_numa_page+0x14a>
	}
	BUG_ON(is_zero_pfn(page_to_pfn(page)));
    47ea:	0f 0b                	ud2    
    47ec:	0f 1f 40 00          	nopl   0x0(%rax)

00000000000047f0 <__pud_alloc>:
/*
 * Allocate page upper directory.
 * We've already handled the fast-path in-line.
 */
int __pud_alloc(struct mm_struct *mm, pgd_t *pgd, unsigned long address)
{
    47f0:	e8 00 00 00 00       	callq  47f5 <__pud_alloc+0x5>
    47f5:	55                   	push   %rbp
    47f6:	48 89 e5             	mov    %rsp,%rbp
    47f9:	41 57                	push   %r15
    47fb:	49 89 ff             	mov    %rdi,%r15
	set_pgd(pgd, __pgd(_PAGE_TABLE | __pa(pud)));
}

static inline pud_t *pud_alloc_one(struct mm_struct *mm, unsigned long addr)
{
	return (pud_t *)get_zeroed_page(GFP_KERNEL|__GFP_REPEAT);
    47fe:	bf d0 04 00 00       	mov    $0x4d0,%edi
    4803:	41 56                	push   %r14
    4805:	41 55                	push   %r13
    4807:	41 54                	push   %r12
    4809:	49 89 f4             	mov    %rsi,%r12
    480c:	53                   	push   %rbx
    480d:	48 83 ec 08          	sub    $0x8,%rsp
    4811:	e8 00 00 00 00       	callq  4816 <__pud_alloc+0x26>
	pud_t *new = pud_alloc_one(mm, address);
	if (!new)
    4816:	48 85 c0             	test   %rax,%rax
    4819:	48 89 c3             	mov    %rax,%rbx
    481c:	0f 84 a6 00 00 00    	je     48c8 <__pud_alloc+0xd8>
	raw_spin_lock_init(&(_lock)->rlock);		\
} while (0)

static inline void spin_lock(spinlock_t *lock)
{
	raw_spin_lock(&lock->rlock);
    4822:	4d 8d 6f 60          	lea    0x60(%r15),%r13
    4826:	4c 89 ef             	mov    %r13,%rdi
    4829:	e8 00 00 00 00       	callq  482e <__pud_alloc+0x3e>
	  return -ENOMEM;

	smp_wmb(); /* See comment in __pte_alloc */

	spin_lock(&mm->page_table_lock);
	if (pgd_present(*pgd))		/* Another has populated it */
    482e:	41 f6 04 24 01       	testb  $0x1,(%r12)
    4833:	75 7b                	jne    48b0 <__pud_alloc+0xc0>
    4835:	49 be 00 00 00 80 ff 	movabs $0x77ff80000000,%r14
    483c:	77 00 00 
extern unsigned long max_pfn;
extern unsigned long phys_base;

static inline unsigned long __phys_addr_nodebug(unsigned long x)
{
	unsigned long y = x - __START_KERNEL_map;
    483f:	b8 00 00 00 80       	mov    $0x80000000,%eax
	PVOP_VCALL1(pv_mmu_ops.release_pmd, pfn);
}

static inline void paravirt_alloc_pud(struct mm_struct *mm, unsigned long pfn)
{
	PVOP_VCALL2(pv_mmu_ops.alloc_pud, mm, pfn);
    4844:	4c 89 ff             	mov    %r15,%rdi

	/* use the carry flag to determine if x was < __START_KERNEL_map */
	x = y + ((x > y) ? phys_base : (__START_KERNEL_map - PAGE_OFFSET));
    4847:	48 01 d8             	add    %rbx,%rax
    484a:	4c 89 f6             	mov    %r14,%rsi
    484d:	48 0f 42 35 00 00 00 	cmovb  0x0(%rip),%rsi        # 4855 <__pud_alloc+0x65>
    4854:	00 
    4855:	48 89 45 d0          	mov    %rax,-0x30(%rbp)
    4859:	48 01 c6             	add    %rax,%rsi
#endif	/* CONFIG_X86_PAE */

#if PAGETABLE_LEVELS > 3
static inline void pgd_populate(struct mm_struct *mm, pgd_t *pgd, pud_t *pud)
{
	paravirt_alloc_pud(mm, __pa(pud) >> PAGE_SHIFT);
    485c:	48 c1 ee 0c          	shr    $0xc,%rsi
    4860:	ff 14 25 00 00 00 00 	callq  *0x0
    4867:	48 8b 45 d0          	mov    -0x30(%rbp),%rax
    486b:	48 39 c3             	cmp    %rax,%rbx
    486e:	4c 0f 47 35 00 00 00 	cmova  0x0(%rip),%r14        # 4876 <__pud_alloc+0x86>
    4875:	00 
    4876:	49 8d 3c 06          	lea    (%r14,%rax,1),%rdi
	set_pgd(pgd, __pgd(_PAGE_TABLE | __pa(pud)));
    487a:	48 83 cf 67          	or     $0x67,%rdi

	if (sizeof(pgdval_t) > sizeof(long))
		ret = PVOP_CALLEE2(pgdval_t, pv_mmu_ops.make_pgd,
				   val, (u64)val >> 32);
	else
		ret = PVOP_CALLEE1(pgdval_t, pv_mmu_ops.make_pgd,
    487e:	ff 14 25 00 00 00 00 	callq  *0x0
    4885:	48 89 c6             	mov    %rax,%rsi

	if (sizeof(pgdval_t) > sizeof(long))
		PVOP_VCALL3(pv_mmu_ops.set_pgd, pgdp,
			    val, (u64)val >> 32);
	else
		PVOP_VCALL2(pv_mmu_ops.set_pgd, pgdp,
    4888:	4c 89 e7             	mov    %r12,%rdi
    488b:	ff 14 25 00 00 00 00 	callq  *0x0
	raw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \
} while (0)

static inline void spin_unlock(spinlock_t *lock)
{
	raw_spin_unlock(&lock->rlock);
    4892:	4c 89 ef             	mov    %r13,%rdi
    4895:	e8 00 00 00 00       	callq  489a <__pud_alloc+0xaa>
	  pud_free(mm, new);
	else
	  pgd_populate(mm, pgd, new);
	spin_unlock(&mm->page_table_lock);
	return 0;
    489a:	31 c0                	xor    %eax,%eax
}
    489c:	48 83 c4 08          	add    $0x8,%rsp
    48a0:	5b                   	pop    %rbx
    48a1:	41 5c                	pop    %r12
    48a3:	41 5d                	pop    %r13
    48a5:	41 5e                	pop    %r14
    48a7:	41 5f                	pop    %r15
    48a9:	5d                   	pop    %rbp
    48aa:	c3                   	retq   
    48ab:	0f 1f 44 00 00       	nopl   0x0(%rax,%rax,1)
	return (pud_t *)get_zeroed_page(GFP_KERNEL|__GFP_REPEAT);
}

static inline void pud_free(struct mm_struct *mm, pud_t *pud)
{
	BUG_ON((unsigned long)pud & (PAGE_SIZE-1));
    48b0:	f7 c3 ff 0f 00 00    	test   $0xfff,%ebx
    48b6:	75 17                	jne    48cf <__pud_alloc+0xdf>
	free_page((unsigned long)pud);
    48b8:	31 f6                	xor    %esi,%esi
    48ba:	48 89 df             	mov    %rbx,%rdi
    48bd:	e8 00 00 00 00       	callq  48c2 <__pud_alloc+0xd2>
    48c2:	eb ce                	jmp    4892 <__pud_alloc+0xa2>
    48c4:	0f 1f 40 00          	nopl   0x0(%rax)
 */
int __pud_alloc(struct mm_struct *mm, pgd_t *pgd, unsigned long address)
{
	pud_t *new = pud_alloc_one(mm, address);
	if (!new)
	  return -ENOMEM;
    48c8:	b8 f4 ff ff ff       	mov    $0xfffffff4,%eax
    48cd:	eb cd                	jmp    489c <__pud_alloc+0xac>
	return (pud_t *)get_zeroed_page(GFP_KERNEL|__GFP_REPEAT);
}

static inline void pud_free(struct mm_struct *mm, pud_t *pud)
{
	BUG_ON((unsigned long)pud & (PAGE_SIZE-1));
    48cf:	0f 0b                	ud2    
    48d1:	66 66 66 66 66 66 2e 	data32 data32 data32 data32 data32 nopw %cs:0x0(%rax,%rax,1)
    48d8:	0f 1f 84 00 00 00 00 
    48df:	00 

00000000000048e0 <follow_phys>:

#ifdef CONFIG_HAVE_IOREMAP_PROT
int follow_phys(struct vm_area_struct *vma,
			unsigned long address, unsigned int flags,
			unsigned long *prot, resource_size_t *phys)
{
    48e0:	e8 00 00 00 00       	callq  48e5 <follow_phys+0x5>
    48e5:	55                   	push   %rbp
    48e6:	48 89 e5             	mov    %rsp,%rbp
    48e9:	41 55                	push   %r13
    48eb:	41 54                	push   %r12
    48ed:	53                   	push   %rbx
    48ee:	48 83 ec 10          	sub    $0x10,%rsp
	int ret = -EINVAL;
	pte_t *ptep, pte;
	spinlock_t *ptl;

	if (!(vma->vm_flags & (VM_IO | VM_PFNMAP)))
    48f2:	48 f7 47 50 00 44 00 	testq  $0x4400,0x50(%rdi)
    48f9:	00 
    48fa:	0f 84 80 00 00 00    	je     4980 <follow_phys+0xa0>
	pte_unmap_unlock(ptep, *ptlp);
out:
	return -EINVAL;
}

static inline int follow_pte(struct mm_struct *mm, unsigned long address,
    4900:	48 8b 47 40          	mov    0x40(%rdi),%rax
    4904:	89 d3                	mov    %edx,%ebx
    4906:	49 89 cc             	mov    %rcx,%r12
			pte_t **ptepp, spinlock_t **ptlp)
{
	int res;

	/* (void) is needed to make gcc happy */
	(void) __cond_lock(*ptlp,
    4909:	48 8d 55 d8          	lea    -0x28(%rbp),%rdx
    490d:	48 8d 4d e0          	lea    -0x20(%rbp),%rcx
    4911:	4d 89 c5             	mov    %r8,%r13
    4914:	48 8b 78 40          	mov    0x40(%rax),%rdi
    4918:	e8 a3 c0 ff ff       	callq  9c0 <__follow_pte.isra.40>
	spinlock_t *ptl;

	if (!(vma->vm_flags & (VM_IO | VM_PFNMAP)))
	  goto out;

	if (follow_pte(vma->vm_mm, address, &ptep, &ptl))
    491d:	85 c0                	test   %eax,%eax
    491f:	75 5f                	jne    4980 <follow_phys+0xa0>
	  goto out;
	pte = *ptep;
    4921:	48 8b 45 d8          	mov    -0x28(%rbp),%rax
    4925:	48 8b 38             	mov    (%rax),%rdi
    4928:	48 b8 ff 0f 00 00 00 	movabs $0xffffc00000000fff,%rax
    492f:	c0 ff ff 
    4932:	48 21 f8             	and    %rdi,%rax

	if ((flags & FOLL_WRITE) && !pte_write(pte))
    4935:	83 e3 01             	and    $0x1,%ebx
    4938:	75 36                	jne    4970 <follow_phys+0x90>
	  goto unlock;

	*prot = pgprot_val(pte_pgprot(pte));
    493a:	49 89 04 24          	mov    %rax,(%r12)

	if (sizeof(pteval_t) > sizeof(long))
		ret = PVOP_CALLEE2(pteval_t, pv_mmu_ops.pte_val,
				   pte.pte, (u64)pte.pte >> 32);
	else
		ret = PVOP_CALLEE1(pteval_t, pv_mmu_ops.pte_val,
    493e:	ff 14 25 00 00 00 00 	callq  *0x0
	return pte_flags(pte) & _PAGE_SPECIAL;
}

static inline unsigned long pte_pfn(pte_t pte)
{
	return (pte_val(pte) & PTE_PFN_MASK) >> PAGE_SHIFT;
    4945:	48 ba 00 f0 ff ff ff 	movabs $0x3ffffffff000,%rdx
    494c:	3f 00 00 
	*phys = (resource_size_t)pte_pfn(pte) << PAGE_SHIFT;

	ret = 0;
    494f:	31 db                	xor    %ebx,%ebx

	if ((flags & FOLL_WRITE) && !pte_write(pte))
	  goto unlock;

	*prot = pgprot_val(pte_pgprot(pte));
	*phys = (resource_size_t)pte_pfn(pte) << PAGE_SHIFT;
    4951:	48 21 d0             	and    %rdx,%rax
    4954:	49 89 45 00          	mov    %rax,0x0(%r13)
    4958:	48 8b 7d e0          	mov    -0x20(%rbp),%rdi
    495c:	e8 00 00 00 00       	callq  4961 <follow_phys+0x81>
	ret = 0;
unlock:
	pte_unmap_unlock(ptep, ptl);
out:
	return ret;
}
    4961:	48 83 c4 10          	add    $0x10,%rsp
    4965:	89 d8                	mov    %ebx,%eax
    4967:	5b                   	pop    %rbx
    4968:	41 5c                	pop    %r12
    496a:	41 5d                	pop    %r13
    496c:	5d                   	pop    %rbp
    496d:	c3                   	retq   
    496e:	66 90                	xchg   %ax,%ax

	if (follow_pte(vma->vm_mm, address, &ptep, &ptl))
	  goto out;
	pte = *ptep;

	if ((flags & FOLL_WRITE) && !pte_write(pte))
    4970:	a8 02                	test   $0x2,%al
#ifdef CONFIG_HAVE_IOREMAP_PROT
int follow_phys(struct vm_area_struct *vma,
			unsigned long address, unsigned int flags,
			unsigned long *prot, resource_size_t *phys)
{
	int ret = -EINVAL;
    4972:	bb ea ff ff ff       	mov    $0xffffffea,%ebx

	if (follow_pte(vma->vm_mm, address, &ptep, &ptl))
	  goto out;
	pte = *ptep;

	if ((flags & FOLL_WRITE) && !pte_write(pte))
    4977:	74 df                	je     4958 <follow_phys+0x78>
    4979:	eb bf                	jmp    493a <follow_phys+0x5a>
    497b:	0f 1f 44 00 00       	nopl   0x0(%rax,%rax,1)
	ret = 0;
unlock:
	pte_unmap_unlock(ptep, ptl);
out:
	return ret;
}
    4980:	48 83 c4 10          	add    $0x10,%rsp
#ifdef CONFIG_HAVE_IOREMAP_PROT
int follow_phys(struct vm_area_struct *vma,
			unsigned long address, unsigned int flags,
			unsigned long *prot, resource_size_t *phys)
{
	int ret = -EINVAL;
    4984:	bb ea ff ff ff       	mov    $0xffffffea,%ebx
	ret = 0;
unlock:
	pte_unmap_unlock(ptep, ptl);
out:
	return ret;
}
    4989:	89 d8                	mov    %ebx,%eax
    498b:	5b                   	pop    %rbx
    498c:	41 5c                	pop    %r12
    498e:	41 5d                	pop    %r13
    4990:	5d                   	pop    %rbp
    4991:	c3                   	retq   
    4992:	66 66 66 66 66 2e 0f 	data32 data32 data32 data32 nopw %cs:0x0(%rax,%rax,1)
    4999:	1f 84 00 00 00 00 00 

00000000000049a0 <generic_access_phys>:

int generic_access_phys(struct vm_area_struct *vma, unsigned long addr,
			void *buf, int len, int write)
{
    49a0:	e8 00 00 00 00       	callq  49a5 <generic_access_phys+0x5>
    49a5:	55                   	push   %rbp
    49a6:	48 89 e5             	mov    %rsp,%rbp
    49a9:	41 57                	push   %r15
    49ab:	41 56                	push   %r14
	resource_size_t phys_addr;
	unsigned long prot = 0;
	void __iomem *maddr;
	int offset = addr & (PAGE_SIZE-1);
    49ad:	41 89 f6             	mov    %esi,%r14d
    49b0:	41 81 e6 ff 0f 00 00 	and    $0xfff,%r14d
	return ret;
}

int generic_access_phys(struct vm_area_struct *vma, unsigned long addr,
			void *buf, int len, int write)
{
    49b7:	41 55                	push   %r13
    49b9:	49 89 d5             	mov    %rdx,%r13
    49bc:	41 54                	push   %r12
    49be:	41 89 cc             	mov    %ecx,%r12d
	resource_size_t phys_addr;
	unsigned long prot = 0;
	void __iomem *maddr;
	int offset = addr & (PAGE_SIZE-1);

	if (follow_phys(vma, addr, write, &prot, &phys_addr))
    49c1:	48 8d 4d d0          	lea    -0x30(%rbp),%rcx
	return ret;
}

int generic_access_phys(struct vm_area_struct *vma, unsigned long addr,
			void *buf, int len, int write)
{
    49c5:	53                   	push   %rbx
    49c6:	44 89 c3             	mov    %r8d,%ebx
	resource_size_t phys_addr;
	unsigned long prot = 0;
	void __iomem *maddr;
	int offset = addr & (PAGE_SIZE-1);

	if (follow_phys(vma, addr, write, &prot, &phys_addr))
    49c9:	4c 8d 45 c8          	lea    -0x38(%rbp),%r8
    49cd:	89 da                	mov    %ebx,%edx
	return ret;
}

int generic_access_phys(struct vm_area_struct *vma, unsigned long addr,
			void *buf, int len, int write)
{
    49cf:	48 83 ec 10          	sub    $0x10,%rsp
	resource_size_t phys_addr;
	unsigned long prot = 0;
    49d3:	48 c7 45 d0 00 00 00 	movq   $0x0,-0x30(%rbp)
    49da:	00 
	void __iomem *maddr;
	int offset = addr & (PAGE_SIZE-1);

	if (follow_phys(vma, addr, write, &prot, &phys_addr))
    49db:	e8 00 00 00 00       	callq  49e0 <generic_access_phys+0x40>
    49e0:	85 c0                	test   %eax,%eax
	  return -EINVAL;
    49e2:	ba ea ff ff ff       	mov    $0xffffffea,%edx
	resource_size_t phys_addr;
	unsigned long prot = 0;
	void __iomem *maddr;
	int offset = addr & (PAGE_SIZE-1);

	if (follow_phys(vma, addr, write, &prot, &phys_addr))
    49e7:	75 36                	jne    4a1f <generic_access_phys+0x7f>
	  return -EINVAL;

	maddr = ioremap_prot(phys_addr, PAGE_SIZE, prot);
    49e9:	48 8b 55 d0          	mov    -0x30(%rbp),%rdx
    49ed:	48 8b 7d c8          	mov    -0x38(%rbp),%rdi
    49f1:	be 00 10 00 00       	mov    $0x1000,%esi
    49f6:	e8 00 00 00 00       	callq  49fb <generic_access_phys+0x5b>
	if (write)
    49fb:	85 db                	test   %ebx,%ebx
	int offset = addr & (PAGE_SIZE-1);

	if (follow_phys(vma, addr, write, &prot, &phys_addr))
	  return -EINVAL;

	maddr = ioremap_prot(phys_addr, PAGE_SIZE, prot);
    49fd:	49 89 c7             	mov    %rax,%r15
	if (write)
    4a00:	75 2e                	jne    4a30 <generic_access_phys+0x90>
	  memcpy_toio(maddr + offset, buf, len);
	else
	  memcpy_fromio(buf, maddr + offset, len);
    4a02:	4d 63 f6             	movslq %r14d,%r14
    4a05:	49 63 d4             	movslq %r12d,%rdx
}

static inline void
memcpy_fromio(void *dst, const volatile void __iomem *src, size_t count)
{
	memcpy(dst, (const void __force *)src, count);
    4a08:	4c 89 ef             	mov    %r13,%rdi
    4a0b:	4a 8d 34 30          	lea    (%rax,%r14,1),%rsi
    4a0f:	e8 00 00 00 00       	callq  4a14 <generic_access_phys+0x74>
	iounmap(maddr);
    4a14:	4c 89 ff             	mov    %r15,%rdi
    4a17:	e8 00 00 00 00       	callq  4a1c <generic_access_phys+0x7c>

	return len;
    4a1c:	44 89 e2             	mov    %r12d,%edx
}
    4a1f:	48 83 c4 10          	add    $0x10,%rsp
    4a23:	89 d0                	mov    %edx,%eax
    4a25:	5b                   	pop    %rbx
    4a26:	41 5c                	pop    %r12
    4a28:	41 5d                	pop    %r13
    4a2a:	41 5e                	pop    %r14
    4a2c:	41 5f                	pop    %r15
    4a2e:	5d                   	pop    %rbp
    4a2f:	c3                   	retq   
	if (follow_phys(vma, addr, write, &prot, &phys_addr))
	  return -EINVAL;

	maddr = ioremap_prot(phys_addr, PAGE_SIZE, prot);
	if (write)
	  memcpy_toio(maddr + offset, buf, len);
    4a30:	4d 63 f6             	movslq %r14d,%r14
    4a33:	49 63 d4             	movslq %r12d,%rdx
}

static inline void
memcpy_toio(volatile void __iomem *dst, const void *src, size_t count)
{
	memcpy((void __force *)dst, src, count);
    4a36:	4c 89 ee             	mov    %r13,%rsi
    4a39:	4a 8d 3c 30          	lea    (%rax,%r14,1),%rdi
    4a3d:	e8 00 00 00 00       	callq  4a42 <generic_access_phys+0xa2>
    4a42:	eb d0                	jmp    4a14 <generic_access_phys+0x74>
    4a44:	66 66 66 2e 0f 1f 84 	data32 data32 nopw %cs:0x0(%rax,%rax,1)
    4a4b:	00 00 00 00 00 

0000000000004a50 <print_vma_addr>:

/*
 * Print the name of a VMA.
 */
void print_vma_addr(char *prefix, unsigned long ip)
{
    4a50:	e8 00 00 00 00       	callq  4a55 <print_vma_addr+0x5>
    4a55:	55                   	push   %rbp
    4a56:	65 48 8b 04 25 00 00 	mov    %gs:0x0,%rax
    4a5d:	00 00 
    4a5f:	48 89 e5             	mov    %rsp,%rbp
    4a62:	41 57                	push   %r15
    4a64:	41 56                	push   %r14
    4a66:	41 55                	push   %r13
    4a68:	41 54                	push   %r12
    4a6a:	53                   	push   %rbx
    4a6b:	48 83 ec 08          	sub    $0x8,%rsp
	struct mm_struct *mm = current->mm;
    4a6f:	48 8b 98 a8 02 00 00 	mov    0x2a8(%rax),%rbx
 * We mask the PREEMPT_NEED_RESCHED bit so as not to confuse all current users
 * that think a non-zero value indicates we cannot preempt.
 */
static __always_inline int preempt_count(void)
{
	return __this_cpu_read_4(__preempt_count) & ~PREEMPT_NEED_RESCHED;
    4a76:	65 8b 04 25 00 00 00 	mov    %gs:0x0,%eax
    4a7d:	00 

	/*
	 * Do not print if we are in atomic
	 * contexts (in exception stacks, etc.):
	 */
	if (preempt_count())
    4a7e:	a9 ff ff ff 7f       	test   $0x7fffffff,%eax
    4a83:	74 13                	je     4a98 <print_vma_addr+0x48>
						vma->vm_end - vma->vm_start);
			free_page((unsigned long)buf);
		}
	}
	up_read(&mm->mmap_sem);
}
    4a85:	48 83 c4 08          	add    $0x8,%rsp
    4a89:	5b                   	pop    %rbx
    4a8a:	41 5c                	pop    %r12
    4a8c:	41 5d                	pop    %r13
    4a8e:	41 5e                	pop    %r14
    4a90:	41 5f                	pop    %r15
    4a92:	5d                   	pop    %rbp
    4a93:	c3                   	retq   
    4a94:	0f 1f 40 00          	nopl   0x0(%rax)
	 * contexts (in exception stacks, etc.):
	 */
	if (preempt_count())
	return;

	down_read(&mm->mmap_sem);
    4a98:	4c 8d 6b 78          	lea    0x78(%rbx),%r13
    4a9c:	49 89 fc             	mov    %rdi,%r12
    4a9f:	48 89 75 d0          	mov    %rsi,-0x30(%rbp)
    4aa3:	4c 89 ef             	mov    %r13,%rdi
    4aa6:	e8 00 00 00 00       	callq  4aab <print_vma_addr+0x5b>
	vma = find_vma(mm, ip);
    4aab:	48 8b 75 d0          	mov    -0x30(%rbp),%rsi
    4aaf:	48 89 df             	mov    %rbx,%rdi
    4ab2:	e8 00 00 00 00       	callq  4ab7 <print_vma_addr+0x67>
	if (vma && vma->vm_file) {
    4ab7:	48 85 c0             	test   %rax,%rax
	 */
	if (preempt_count())
	return;

	down_read(&mm->mmap_sem);
	vma = find_vma(mm, ip);
    4aba:	48 89 c3             	mov    %rax,%rbx
	if (vma && vma->vm_file) {
    4abd:	74 20                	je     4adf <print_vma_addr+0x8f>
    4abf:	4c 8b b0 a8 00 00 00 	mov    0xa8(%rax),%r14
    4ac6:	4d 85 f6             	test   %r14,%r14
    4ac9:	74 14                	je     4adf <print_vma_addr+0x8f>
		struct file *f = vma->vm_file;
		char *buf = (char *)__get_free_page(GFP_KERNEL);
    4acb:	31 f6                	xor    %esi,%esi
    4acd:	bf d0 00 00 00       	mov    $0xd0,%edi
    4ad2:	e8 00 00 00 00       	callq  4ad7 <print_vma_addr+0x87>
		if (buf) {
    4ad7:	48 85 c0             	test   %rax,%rax

	down_read(&mm->mmap_sem);
	vma = find_vma(mm, ip);
	if (vma && vma->vm_file) {
		struct file *f = vma->vm_file;
		char *buf = (char *)__get_free_page(GFP_KERNEL);
    4ada:	49 89 c7             	mov    %rax,%r15
		if (buf) {
    4add:	75 17                	jne    4af6 <print_vma_addr+0xa6>
						vma->vm_start,
						vma->vm_end - vma->vm_start);
			free_page((unsigned long)buf);
		}
	}
	up_read(&mm->mmap_sem);
    4adf:	4c 89 ef             	mov    %r13,%rdi
    4ae2:	e8 00 00 00 00       	callq  4ae7 <print_vma_addr+0x97>
}
    4ae7:	48 83 c4 08          	add    $0x8,%rsp
    4aeb:	5b                   	pop    %rbx
    4aec:	41 5c                	pop    %r12
    4aee:	41 5d                	pop    %r13
    4af0:	41 5e                	pop    %r14
    4af2:	41 5f                	pop    %r15
    4af4:	5d                   	pop    %rbp
    4af5:	c3                   	retq   
		struct file *f = vma->vm_file;
		char *buf = (char *)__get_free_page(GFP_KERNEL);
		if (buf) {
			char *p;

			p = d_path(&f->f_path, buf, PAGE_SIZE);
    4af6:	49 8d 7e 10          	lea    0x10(%r14),%rdi
    4afa:	ba 00 10 00 00       	mov    $0x1000,%edx
    4aff:	48 89 c6             	mov    %rax,%rsi
    4b02:	e8 00 00 00 00       	callq  4b07 <print_vma_addr+0xb7>
			if (IS_ERR(p))
			  p = "?";
			printk(KERN_DEBUG"%s%s[%lx+%lx]", prefix, kbasename(p),
						vma->vm_start,
						vma->vm_end - vma->vm_start);
    4b07:	4c 8b 33             	mov    (%rbx),%r14
			char *p;

			p = d_path(&f->f_path, buf, PAGE_SIZE);
			if (IS_ERR(p))
			  p = "?";
			printk(KERN_DEBUG"%s%s[%lx+%lx]", prefix, kbasename(p),
    4b0a:	48 8b 5b 08          	mov    0x8(%rbx),%rbx
    4b0e:	4c 29 f3             	sub    %r14,%rbx
		char *buf = (char *)__get_free_page(GFP_KERNEL);
		if (buf) {
			char *p;

			p = d_path(&f->f_path, buf, PAGE_SIZE);
			if (IS_ERR(p))
    4b11:	48 3d 00 f0 ff ff    	cmp    $0xfffffffffffff000,%rax
    4b17:	77 43                	ja     4b5c <print_vma_addr+0x10c>
 *
 * @path: path to extract the filename from.
 */
static inline const char *kbasename(const char *path)
{
	const char *tail = strrchr(path, '/');
    4b19:	be 2f 00 00 00       	mov    $0x2f,%esi
    4b1e:	48 89 c7             	mov    %rax,%rdi
    4b21:	48 89 45 d0          	mov    %rax,-0x30(%rbp)
    4b25:	e8 00 00 00 00       	callq  4b2a <print_vma_addr+0xda>
	return tail ? tail + 1 : path;
    4b2a:	48 8b 4d d0          	mov    -0x30(%rbp),%rcx
    4b2e:	48 8d 50 01          	lea    0x1(%rax),%rdx
    4b32:	48 85 c0             	test   %rax,%rax
    4b35:	48 0f 44 d1          	cmove  %rcx,%rdx
			  p = "?";
			printk(KERN_DEBUG"%s%s[%lx+%lx]", prefix, kbasename(p),
    4b39:	4c 89 e6             	mov    %r12,%rsi
    4b3c:	49 89 d8             	mov    %rbx,%r8
    4b3f:	4c 89 f1             	mov    %r14,%rcx
    4b42:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
    4b49:	31 c0                	xor    %eax,%eax
    4b4b:	e8 00 00 00 00       	callq  4b50 <print_vma_addr+0x100>
						vma->vm_start,
						vma->vm_end - vma->vm_start);
			free_page((unsigned long)buf);
    4b50:	31 f6                	xor    %esi,%esi
    4b52:	4c 89 ff             	mov    %r15,%rdi
    4b55:	e8 00 00 00 00       	callq  4b5a <print_vma_addr+0x10a>
    4b5a:	eb 83                	jmp    4adf <print_vma_addr+0x8f>
		if (buf) {
			char *p;

			p = d_path(&f->f_path, buf, PAGE_SIZE);
			if (IS_ERR(p))
			  p = "?";
    4b5c:	48 c7 c2 00 00 00 00 	mov    $0x0,%rdx
    4b63:	eb d4                	jmp    4b39 <print_vma_addr+0xe9>
    4b65:	66 66 2e 0f 1f 84 00 	data32 nopw %cs:0x0(%rax,%rax,1)
    4b6c:	00 00 00 00 

0000000000004b70 <ptlock_alloc>:
}
#endif /* CONFIG_TRANSPARENT_HUGEPAGE || CONFIG_HUGETLBFS */

#if USE_SPLIT_PTE_PTLOCKS && ALLOC_SPLIT_PTLOCKS
bool ptlock_alloc(struct page *page)
{
    4b70:	e8 00 00 00 00       	callq  4b75 <ptlock_alloc+0x5>
    4b75:	55                   	push   %rbp
			int index = kmalloc_index(size);

			if (!index)
				return ZERO_SIZE_PTR;

			return kmem_cache_alloc_trace(kmalloc_caches[index],
    4b76:	ba 18 00 00 00       	mov    $0x18,%edx
    4b7b:	be d0 00 00 00       	mov    $0xd0,%esi
    4b80:	48 89 e5             	mov    %rsp,%rbp
    4b83:	53                   	push   %rbx
    4b84:	48 89 fb             	mov    %rdi,%rbx
    4b87:	48 8b 3d 00 00 00 00 	mov    0x0(%rip),%rdi        # 4b8e <ptlock_alloc+0x1e>
    4b8e:	e8 00 00 00 00       	callq  4b93 <ptlock_alloc+0x23>
	spinlock_t *ptl;

	ptl = kmalloc(sizeof(spinlock_t), GFP_KERNEL);
	if (!ptl)
    4b93:	48 85 c0             	test   %rax,%rax
    4b96:	74 10                	je     4ba8 <ptlock_alloc+0x38>
	  return false;
	page->ptl = ptl;
    4b98:	48 89 43 30          	mov    %rax,0x30(%rbx)
	return true;
    4b9c:	b8 01 00 00 00       	mov    $0x1,%eax
}
    4ba1:	5b                   	pop    %rbx
    4ba2:	5d                   	pop    %rbp
    4ba3:	c3                   	retq   
    4ba4:	0f 1f 40 00          	nopl   0x0(%rax)
    4ba8:	5b                   	pop    %rbx
{
	spinlock_t *ptl;

	ptl = kmalloc(sizeof(spinlock_t), GFP_KERNEL);
	if (!ptl)
	  return false;
    4ba9:	31 c0                	xor    %eax,%eax
	page->ptl = ptl;
	return true;
}
    4bab:	5d                   	pop    %rbp
    4bac:	c3                   	retq   
    4bad:	0f 1f 00             	nopl   (%rax)

0000000000004bb0 <__pmd_alloc>:
/*
 * Allocate page middle directory.
 * We've already handled the fast-path in-line.
 */
int __pmd_alloc(struct mm_struct *mm, pud_t *pud, unsigned long address)
{
    4bb0:	e8 00 00 00 00       	callq  4bb5 <__pmd_alloc+0x5>
    4bb5:	55                   	push   %rbp
    4bb6:	48 89 e5             	mov    %rsp,%rbp
    4bb9:	41 57                	push   %r15
    4bbb:	49 89 ff             	mov    %rdi,%r15
extern struct page *alloc_pages_current(gfp_t gfp_mask, unsigned order);

static inline struct page *
alloc_pages(gfp_t gfp_mask, unsigned int order)
{
	return alloc_pages_current(gfp_mask, order);
    4bbe:	bf d0 84 00 00       	mov    $0x84d0,%edi
    4bc3:	41 56                	push   %r14
    4bc5:	41 55                	push   %r13
    4bc7:	41 54                	push   %r12
    4bc9:	53                   	push   %rbx
    4bca:	48 89 f3             	mov    %rsi,%rbx
    4bcd:	31 f6                	xor    %esi,%esi
    4bcf:	e8 00 00 00 00       	callq  4bd4 <__pmd_alloc+0x24>
#if PAGETABLE_LEVELS > 2
static inline pmd_t *pmd_alloc_one(struct mm_struct *mm, unsigned long addr)
{
	struct page *page;
	page = alloc_pages(GFP_KERNEL | __GFP_REPEAT | __GFP_ZERO, 0);
	if (!page)
    4bd4:	48 85 c0             	test   %rax,%rax
    4bd7:	49 89 c4             	mov    %rax,%r12
    4bda:	74 16                	je     4bf2 <__pmd_alloc+0x42>
	 * It can happen if arch try to use slab for page table allocation:
	 * slab code uses page->slab_cache and page->first_page (for tail
	 * pages), which share storage with page->ptl.
	 */
	VM_BUG_ON(*(unsigned long *)&page->ptl);
	if (!ptlock_alloc(page))
    4bdc:	48 89 c7             	mov    %rax,%rdi
    4bdf:	e8 00 00 00 00       	callq  4be4 <__pmd_alloc+0x34>
    4be4:	84 c0                	test   %al,%al
    4be6:	75 20                	jne    4c08 <__pmd_alloc+0x58>
		return NULL;
	if (!pgtable_pmd_page_ctor(page)) {
		__free_pages(page, 0);
    4be8:	31 f6                	xor    %esi,%esi
    4bea:	4c 89 e7             	mov    %r12,%rdi
    4bed:	e8 00 00 00 00       	callq  4bf2 <__pmd_alloc+0x42>
	else
	  pgd_populate(mm, pud, new);
#endif /* __ARCH_HAS_4LEVEL_HACK */
	spin_unlock(&mm->page_table_lock);
	return 0;
}
    4bf2:	5b                   	pop    %rbx
    4bf3:	41 5c                	pop    %r12
    4bf5:	41 5d                	pop    %r13
    4bf7:	41 5e                	pop    %r14
    4bf9:	41 5f                	pop    %r15
 */
int __pmd_alloc(struct mm_struct *mm, pud_t *pud, unsigned long address)
{
	pmd_t *new = pmd_alloc_one(mm, address);
	if (!new)
	  return -ENOMEM;
    4bfb:	b8 f4 ff ff ff       	mov    $0xfffffff4,%eax
	else
	  pgd_populate(mm, pud, new);
#endif /* __ARCH_HAS_4LEVEL_HACK */
	spin_unlock(&mm->page_table_lock);
	return 0;
}
    4c00:	5d                   	pop    %rbp
    4c01:	c3                   	retq   
    4c02:	66 0f 1f 44 00 00    	nopw   0x0(%rax,%rax,1)
		return false;
	spin_lock_init(ptlock_ptr(page));
    4c08:	49 8b 7c 24 30       	mov    0x30(%r12),%rdi
    4c0d:	48 c7 c2 00 00 00 00 	mov    $0x0,%rdx
    4c14:	48 c7 c6 00 00 00 00 	mov    $0x0,%rsi
 */
#include <linux/vmstat.h>

static __always_inline void *lowmem_page_address(const struct page *page)
{
	return __va(PFN_PHYS(page_to_pfn(page)));
    4c1b:	49 be 00 00 00 00 00 	movabs $0xffff880000000000,%r14
    4c22:	88 ff ff 
	 * pages), which share storage with page->ptl.
	 */
	VM_BUG_ON(*(unsigned long *)&page->ptl);
	if (!ptlock_alloc(page))
		return false;
	spin_lock_init(ptlock_ptr(page));
    4c25:	e8 00 00 00 00       	callq  4c2a <__pmd_alloc+0x7a>
 */
#include <linux/vmstat.h>

static __always_inline void *lowmem_page_address(const struct page *page)
{
	return __va(PFN_PHYS(page_to_pfn(page)));
    4c2a:	48 b8 00 00 00 00 00 	movabs $0x160000000000,%rax
    4c31:	16 00 00 
    4c34:	4d 8d 2c 04          	lea    (%r12,%rax,1),%r13
    4c38:	49 c1 fd 06          	sar    $0x6,%r13
    4c3c:	49 c1 e5 0c          	shl    $0xc,%r13
 * We've already handled the fast-path in-line.
 */
int __pmd_alloc(struct mm_struct *mm, pud_t *pud, unsigned long address)
{
	pmd_t *new = pmd_alloc_one(mm, address);
	if (!new)
    4c40:	4d 01 ee             	add    %r13,%r14
    4c43:	74 ad                	je     4bf2 <__pmd_alloc+0x42>
	raw_spin_lock_init(&(_lock)->rlock);		\
} while (0)

static inline void spin_lock(spinlock_t *lock)
{
	raw_spin_lock(&lock->rlock);
    4c45:	4d 8d 67 60          	lea    0x60(%r15),%r12
    4c49:	4c 89 e7             	mov    %r12,%rdi
    4c4c:	e8 00 00 00 00       	callq  4c51 <__pmd_alloc+0xa1>

	smp_wmb(); /* See comment in __pte_alloc */

	spin_lock(&mm->page_table_lock);
#ifndef __ARCH_HAS_4LEVEL_HACK
	if (pud_present(*pud))		/* Another has populated it */
    4c51:	f6 03 01             	testb  $0x1,(%rbx)
    4c54:	74 6a                	je     4cc0 <__pmd_alloc+0x110>
extern unsigned long max_pfn;
extern unsigned long phys_base;

static inline unsigned long __phys_addr_nodebug(unsigned long x)
{
	unsigned long y = x - __START_KERNEL_map;
    4c56:	48 ba 00 00 00 80 00 	movabs $0xffff880080000000,%rdx
    4c5d:	88 ff ff 
}

static inline void pmd_free(struct mm_struct *mm, pmd_t *pmd)
{
	BUG_ON((unsigned long)pmd & (PAGE_SIZE-1));
	pgtable_pmd_page_dtor(virt_to_page(pmd));
    4c60:	48 b9 00 00 00 00 00 	movabs $0x160000000000,%rcx
    4c67:	16 00 00 
    4c6a:	49 8d 44 15 00       	lea    0x0(%r13,%rdx,1),%rax

	/* use the carry flag to determine if x was < __START_KERNEL_map */
	x = y + ((x > y) ? phys_base : (__START_KERNEL_map - PAGE_OFFSET));
    4c6f:	48 ba 00 00 00 80 ff 	movabs $0x77ff80000000,%rdx
    4c76:	77 00 00 
    4c79:	49 39 c6             	cmp    %rax,%r14
    4c7c:	48 0f 47 15 00 00 00 	cmova  0x0(%rip),%rdx        # 4c84 <__pmd_alloc+0xd4>
    4c83:	00 
    4c84:	48 01 d0             	add    %rdx,%rax
    4c87:	48 c1 e8 0c          	shr    $0xc,%rax
    4c8b:	48 c1 e0 06          	shl    $0x6,%rax
    4c8f:	48 29 c8             	sub    %rcx,%rax
	return true;
}

void ptlock_free(struct page *page)
{
	kfree(page->ptl);
    4c92:	48 8b 78 30          	mov    0x30(%rax),%rdi
    4c96:	e8 00 00 00 00       	callq  4c9b <__pmd_alloc+0xeb>
	free_page((unsigned long)pmd);
    4c9b:	31 f6                	xor    %esi,%esi
    4c9d:	4c 89 f7             	mov    %r14,%rdi
    4ca0:	e8 00 00 00 00       	callq  4ca5 <__pmd_alloc+0xf5>
	raw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \
} while (0)

static inline void spin_unlock(spinlock_t *lock)
{
	raw_spin_unlock(&lock->rlock);
    4ca5:	4c 89 e7             	mov    %r12,%rdi
    4ca8:	e8 00 00 00 00       	callq  4cad <__pmd_alloc+0xfd>
	else
	  pgd_populate(mm, pud, new);
#endif /* __ARCH_HAS_4LEVEL_HACK */
	spin_unlock(&mm->page_table_lock);
	return 0;
}
    4cad:	5b                   	pop    %rbx
    4cae:	41 5c                	pop    %r12
    4cb0:	41 5d                	pop    %r13
    4cb2:	41 5e                	pop    %r14
    4cb4:	41 5f                	pop    %r15
	  pmd_free(mm, new);
	else
	  pgd_populate(mm, pud, new);
#endif /* __ARCH_HAS_4LEVEL_HACK */
	spin_unlock(&mm->page_table_lock);
	return 0;
    4cb6:	31 c0                	xor    %eax,%eax
}
    4cb8:	5d                   	pop    %rbp
    4cb9:	c3                   	retq   
    4cba:	66 0f 1f 44 00 00    	nopw   0x0(%rax,%rax,1)
extern unsigned long max_pfn;
extern unsigned long phys_base;

static inline unsigned long __phys_addr_nodebug(unsigned long x)
{
	unsigned long y = x - __START_KERNEL_map;
    4cc0:	48 b8 00 00 00 80 00 	movabs $0xffff880080000000,%rax
    4cc7:	88 ff ff 

	/* use the carry flag to determine if x was < __START_KERNEL_map */
	x = y + ((x > y) ? phys_base : (__START_KERNEL_map - PAGE_OFFSET));
    4cca:	48 be 00 00 00 80 ff 	movabs $0x77ff80000000,%rsi
    4cd1:	77 00 00 
	PVOP_VCALL1(pv_mmu_ops.release_pte, pfn);
}

static inline void paravirt_alloc_pmd(struct mm_struct *mm, unsigned long pfn)
{
	PVOP_VCALL2(pv_mmu_ops.alloc_pmd, mm, pfn);
    4cd4:	4c 89 ff             	mov    %r15,%rdi
extern unsigned long max_pfn;
extern unsigned long phys_base;

static inline unsigned long __phys_addr_nodebug(unsigned long x)
{
	unsigned long y = x - __START_KERNEL_map;
    4cd7:	49 01 c5             	add    %rax,%r13

	/* use the carry flag to determine if x was < __START_KERNEL_map */
	x = y + ((x > y) ? phys_base : (__START_KERNEL_map - PAGE_OFFSET));
    4cda:	4d 39 ee             	cmp    %r13,%r14
    4cdd:	48 0f 47 35 00 00 00 	cmova  0x0(%rip),%rsi        # 4ce5 <__pmd_alloc+0x135>
    4ce4:	00 
    4ce5:	4c 01 ee             	add    %r13,%rsi
#ifdef CONFIG_X86_PAE
extern void pud_populate(struct mm_struct *mm, pud_t *pudp, pmd_t *pmd);
#else	/* !CONFIG_X86_PAE */
static inline void pud_populate(struct mm_struct *mm, pud_t *pud, pmd_t *pmd)
{
	paravirt_alloc_pmd(mm, __pa(pmd) >> PAGE_SHIFT);
    4ce8:	48 c1 ee 0c          	shr    $0xc,%rsi
    4cec:	ff 14 25 00 00 00 00 	callq  *0x0
    4cf3:	48 bf 00 00 00 80 ff 	movabs $0x77ff80000000,%rdi
    4cfa:	77 00 00 
    4cfd:	4d 39 ee             	cmp    %r13,%r14
    4d00:	48 0f 47 3d 00 00 00 	cmova  0x0(%rip),%rdi        # 4d08 <__pmd_alloc+0x158>
    4d07:	00 
    4d08:	4c 01 ef             	add    %r13,%rdi
	set_pud(pud, __pud(_PAGE_TABLE | __pa(pmd)));
    4d0b:	48 83 cf 67          	or     $0x67,%rdi

	if (sizeof(pudval_t) > sizeof(long))
		ret = PVOP_CALLEE2(pudval_t, pv_mmu_ops.make_pud,
				   val, (u64)val >> 32);
	else
		ret = PVOP_CALLEE1(pudval_t, pv_mmu_ops.make_pud,
    4d0f:	ff 14 25 00 00 00 00 	callq  *0x0
    4d16:	48 89 c6             	mov    %rax,%rsi

	if (sizeof(pudval_t) > sizeof(long))
		PVOP_VCALL3(pv_mmu_ops.set_pud, pudp,
			    val, (u64)val >> 32);
	else
		PVOP_VCALL2(pv_mmu_ops.set_pud, pudp,
    4d19:	48 89 df             	mov    %rbx,%rdi
    4d1c:	ff 14 25 00 00 00 00 	callq  *0x0
    4d23:	eb 80                	jmp    4ca5 <__pmd_alloc+0xf5>
    4d25:	66 66 2e 0f 1f 84 00 	data32 nopw %cs:0x0(%rax,%rax,1)
    4d2c:	00 00 00 00 

0000000000004d30 <remap_pfn_range>:
 *
 *  Note: this is only safe if the mm semaphore is held when called.
 */
int remap_pfn_range(struct vm_area_struct *vma, unsigned long addr,
			unsigned long pfn, unsigned long size, pgprot_t prot)
{
    4d30:	e8 00 00 00 00       	callq  4d35 <remap_pfn_range+0x5>
    4d35:	55                   	push   %rbp
    4d36:	48 89 e5             	mov    %rsp,%rbp
    4d39:	41 57                	push   %r15
    4d3b:	49 89 f7             	mov    %rsi,%r15
	pgd_t *pgd;
	unsigned long next;
	unsigned long end = addr + PAGE_ALIGN(size);
    4d3e:	48 8d b1 ff 0f 00 00 	lea    0xfff(%rcx),%rsi
 *
 *  Note: this is only safe if the mm semaphore is held when called.
 */
int remap_pfn_range(struct vm_area_struct *vma, unsigned long addr,
			unsigned long pfn, unsigned long size, pgprot_t prot)
{
    4d45:	41 56                	push   %r14
    4d47:	41 55                	push   %r13
    4d49:	41 54                	push   %r12
    4d4b:	53                   	push   %rbx
    4d4c:	48 89 d3             	mov    %rdx,%rbx
    4d4f:	48 81 ec 88 00 00 00 	sub    $0x88,%rsp
	pgd_t *pgd;
	unsigned long next;
	unsigned long end = addr + PAGE_ALIGN(size);
    4d56:	48 89 b5 68 ff ff ff 	mov    %rsi,-0x98(%rbp)
    4d5d:	48 81 a5 68 ff ff ff 	andq   $0xfffffffffffff000,-0x98(%rbp)
    4d64:	00 f0 ff ff 
    4d68:	48 8b b5 68 ff ff ff 	mov    -0x98(%rbp),%rsi
	add_taint(TAINT_BAD_PAGE, LOCKDEP_NOW_UNRELIABLE);
}

static inline bool is_cow_mapping(vm_flags_t flags)
{
	return (flags & (VM_SHARED | VM_MAYWRITE)) == VM_MAYWRITE;
    4d6f:	48 8b 47 50          	mov    0x50(%rdi),%rax
 *
 *  Note: this is only safe if the mm semaphore is held when called.
 */
int remap_pfn_range(struct vm_area_struct *vma, unsigned long addr,
			unsigned long pfn, unsigned long size, pgprot_t prot)
{
    4d73:	48 89 bd 58 ff ff ff 	mov    %rdi,-0xa8(%rbp)
    4d7a:	4c 89 45 d0          	mov    %r8,-0x30(%rbp)
	pgd_t *pgd;
	unsigned long next;
	unsigned long end = addr + PAGE_ALIGN(size);
    4d7e:	4c 01 fe             	add    %r15,%rsi
	add_taint(TAINT_BAD_PAGE, LOCKDEP_NOW_UNRELIABLE);
}

static inline bool is_cow_mapping(vm_flags_t flags)
{
	return (flags & (VM_SHARED | VM_MAYWRITE)) == VM_MAYWRITE;
    4d81:	83 e0 28             	and    $0x28,%eax
int remap_pfn_range(struct vm_area_struct *vma, unsigned long addr,
			unsigned long pfn, unsigned long size, pgprot_t prot)
{
	pgd_t *pgd;
	unsigned long next;
	unsigned long end = addr + PAGE_ALIGN(size);
    4d84:	48 89 b5 70 ff ff ff 	mov    %rsi,-0x90(%rbp)
	struct mm_struct *mm = vma->vm_mm;
    4d8b:	48 8b 77 40          	mov    0x40(%rdi),%rsi
	 * There's a horrible special case to handle copy-on-write
	 * behaviour that some programs depend on. We mark the "original"
	 * un-COW'ed pages by matching them up with "vma->vm_pgoff".
	 * See vm_normal_page() for details.
	 */
	if (is_cow_mapping(vma->vm_flags)) {
    4d8f:	48 83 f8 20          	cmp    $0x20,%rax
			unsigned long pfn, unsigned long size, pgprot_t prot)
{
	pgd_t *pgd;
	unsigned long next;
	unsigned long end = addr + PAGE_ALIGN(size);
	struct mm_struct *mm = vma->vm_mm;
    4d93:	48 89 75 c0          	mov    %rsi,-0x40(%rbp)
	 * There's a horrible special case to handle copy-on-write
	 * behaviour that some programs depend on. We mark the "original"
	 * un-COW'ed pages by matching them up with "vma->vm_pgoff".
	 * See vm_normal_page() for details.
	 */
	if (is_cow_mapping(vma->vm_flags)) {
    4d97:	75 24                	jne    4dbd <remap_pfn_range+0x8d>
		if (addr != vma->vm_start || end != vma->vm_end)
    4d99:	4c 39 3f             	cmp    %r15,(%rdi)
    4d9c:	48 89 f8             	mov    %rdi,%rax
    4d9f:	0f 85 8b 03 00 00    	jne    5130 <remap_pfn_range+0x400>
    4da5:	48 8b b5 70 ff ff ff 	mov    -0x90(%rbp),%rsi
    4dac:	48 39 77 08          	cmp    %rsi,0x8(%rdi)
    4db0:	0f 85 7a 03 00 00    	jne    5130 <remap_pfn_range+0x400>
		  return -EINVAL;
		vma->vm_pgoff = pfn;
    4db6:	48 89 90 a0 00 00 00 	mov    %rdx,0xa0(%rax)
	}

	err = track_pfn_remap(vma, &prot, pfn, addr, PAGE_ALIGN(size));
    4dbd:	4c 8b b5 58 ff ff ff 	mov    -0xa8(%rbp),%r14
    4dc4:	4c 8b 85 68 ff ff ff 	mov    -0x98(%rbp),%r8
    4dcb:	48 8d 75 d0          	lea    -0x30(%rbp),%rsi
    4dcf:	4c 89 f9             	mov    %r15,%rcx
    4dd2:	48 89 da             	mov    %rbx,%rdx
    4dd5:	4c 89 f7             	mov    %r14,%rdi
    4dd8:	e8 00 00 00 00       	callq  4ddd <remap_pfn_range+0xad>
	if (err)
    4ddd:	85 c0                	test   %eax,%eax
    4ddf:	0f 85 4b 03 00 00    	jne    5130 <remap_pfn_range+0x400>
	  return -EINVAL;

	vma->vm_flags |= VM_IO | VM_PFNMAP | VM_DONTEXPAND | VM_DONTDUMP;
    4de5:	49 81 4e 50 00 44 04 	orq    $0x4044400,0x50(%r14)
    4dec:	04 

	BUG_ON(addr >= end);
    4ded:	4c 3b bd 70 ff ff ff 	cmp    -0x90(%rbp),%r15
    4df4:	0f 83 3d 03 00 00    	jae    5137 <remap_pfn_range+0x407>
	pfn -= addr >> PAGE_SHIFT;
    4dfa:	4c 89 f8             	mov    %r15,%rax
	return pgd_flags(pgd) & _PAGE_PRESENT;
}

static inline unsigned long pgd_page_vaddr(pgd_t pgd)
{
	return (unsigned long)__va((unsigned long)pgd_val(pgd) & PTE_PFN_MASK);
    4dfd:	4d 89 fe             	mov    %r15,%r14
    4e00:	48 c1 e8 0c          	shr    $0xc,%rax
    4e04:	48 29 c3             	sub    %rax,%rbx
	pgd = pgd_offset(mm, addr);
    4e07:	4c 89 f8             	mov    %r15,%rax
    4e0a:	48 c1 e8 24          	shr    $0x24,%rax
	  return -EINVAL;

	vma->vm_flags |= VM_IO | VM_PFNMAP | VM_DONTEXPAND | VM_DONTDUMP;

	BUG_ON(addr >= end);
	pfn -= addr >> PAGE_SHIFT;
    4e0e:	48 89 5d 98          	mov    %rbx,-0x68(%rbp)
	pgd = pgd_offset(mm, addr);
    4e12:	48 89 85 78 ff ff ff 	mov    %rax,-0x88(%rbp)
    4e19:	48 8b 45 c0          	mov    -0x40(%rbp),%rax
    4e1d:	48 81 a5 78 ff ff ff 	andq   $0xff8,-0x88(%rbp)
    4e24:	f8 0f 00 00 
    4e28:	48 8b 40 40          	mov    0x40(%rax),%rax
    4e2c:	48 01 85 78 ff ff ff 	add    %rax,-0x88(%rbp)
    4e33:	48 8b 85 70 ff ff ff 	mov    -0x90(%rbp),%rax
    4e3a:	48 83 e8 01          	sub    $0x1,%rax
    4e3e:	48 89 85 50 ff ff ff 	mov    %rax,-0xb0(%rbp)
	flush_cache_range(vma, addr, end);
	do {
		next = pgd_addr_end(addr, end);
    4e45:	48 b8 00 00 00 00 80 	movabs $0x8000000000,%rax
    4e4c:	00 00 00 
    4e4f:	4c 8b 7d d0          	mov    -0x30(%rbp),%r15
    4e53:	4c 01 f0             	add    %r14,%rax
    4e56:	48 89 45 88          	mov    %rax,-0x78(%rbp)
    4e5a:	48 b8 00 00 00 00 80 	movabs $0xffffff8000000000,%rax
    4e61:	ff ff ff 
    4e64:	48 21 45 88          	and    %rax,-0x78(%rbp)
    4e68:	48 8b 75 88          	mov    -0x78(%rbp),%rsi
    4e6c:	48 89 f0             	mov    %rsi,%rax
    4e6f:	48 83 e8 01          	sub    $0x1,%rax
    4e73:	48 3b 85 50 ff ff ff 	cmp    -0xb0(%rbp),%rax
    4e7a:	48 89 f0             	mov    %rsi,%rax
    4e7d:	48 0f 43 85 70 ff ff 	cmovae -0x90(%rbp),%rax
    4e84:	ff 
    4e85:	48 89 45 88          	mov    %rax,-0x78(%rbp)
    4e89:	48 8b 85 78 ff ff ff 	mov    -0x88(%rbp),%rax
    4e90:	48 8b 00             	mov    (%rax),%rax
 */
#if defined(CONFIG_MMU) && !defined(__ARCH_HAS_4LEVEL_HACK)
static inline pud_t *pud_alloc(struct mm_struct *mm, pgd_t *pgd, unsigned long address)
{
	return (unlikely(pgd_none(*pgd)) && __pud_alloc(mm, pgd, address))?
		NULL: pud_offset(pgd, address);
    4e93:	48 85 c0             	test   %rax,%rax
    4e96:	0f 84 9d 02 00 00    	je     5139 <remap_pfn_range+0x409>
    4e9c:	48 89 c7             	mov    %rax,%rdi

	if (sizeof(pgdval_t) > sizeof(long))
		ret =  PVOP_CALLEE2(pgdval_t, pv_mmu_ops.pgd_val,
				    pgd.pgd, (u64)pgd.pgd >> 32);
	else
		ret =  PVOP_CALLEE1(pgdval_t, pv_mmu_ops.pgd_val,
    4e9f:	ff 14 25 00 00 00 00 	callq  *0x0
#define pgd_page(pgd)		pfn_to_page(pgd_val(pgd) >> PAGE_SHIFT)

/* to find an entry in a page-table-directory. */
static inline unsigned long pud_index(unsigned long address)
{
	return (address >> PUD_SHIFT) & (PTRS_PER_PUD - 1);
    4ea6:	4c 89 f2             	mov    %r14,%rdx
	return pgd_flags(pgd) & _PAGE_PRESENT;
}

static inline unsigned long pgd_page_vaddr(pgd_t pgd)
{
	return (unsigned long)__va((unsigned long)pgd_val(pgd) & PTE_PFN_MASK);
    4ea9:	48 be 00 00 00 00 00 	movabs $0xffff880000000000,%rsi
    4eb0:	88 ff ff 
#define pgd_page(pgd)		pfn_to_page(pgd_val(pgd) >> PAGE_SHIFT)

/* to find an entry in a page-table-directory. */
static inline unsigned long pud_index(unsigned long address)
{
	return (address >> PUD_SHIFT) & (PTRS_PER_PUD - 1);
    4eb3:	48 c1 ea 1b          	shr    $0x1b,%rdx
}

static inline pud_t *pud_offset(pgd_t *pgd, unsigned long address)
{
	return (pud_t *)pgd_page_vaddr(*pgd) + pud_index(address);
    4eb7:	81 e2 f8 0f 00 00    	and    $0xff8,%edx
	return pgd_flags(pgd) & _PAGE_PRESENT;
}

static inline unsigned long pgd_page_vaddr(pgd_t pgd)
{
	return (unsigned long)__va((unsigned long)pgd_val(pgd) & PTE_PFN_MASK);
    4ebd:	48 01 f2             	add    %rsi,%rdx
    4ec0:	48 be 00 f0 ff ff ff 	movabs $0x3ffffffff000,%rsi
    4ec7:	3f 00 00 
    4eca:	48 21 f0             	and    %rsi,%rax
	pud_t *pud;
	unsigned long next;

	pfn -= addr >> PAGE_SHIFT;
	pud = pud_alloc(mm, pgd, addr);
	if (!pud)
    4ecd:	48 01 c2             	add    %rax,%rdx
    4ed0:	48 89 55 80          	mov    %rdx,-0x80(%rbp)
    4ed4:	0f 84 28 02 00 00    	je     5102 <remap_pfn_range+0x3d2>
    4eda:	48 8b 45 88          	mov    -0x78(%rbp),%rax
    4ede:	48 83 e8 01          	sub    $0x1,%rax
    4ee2:	48 89 85 60 ff ff ff 	mov    %rax,-0xa0(%rbp)

static inline pgprotval_t massage_pgprot(pgprot_t pgprot)
{
	pgprotval_t protval = pgprot_val(pgprot);

	if (protval & _PAGE_PRESENT)
    4ee9:	4c 89 f8             	mov    %r15,%rax
    4eec:	83 e0 01             	and    $0x1,%eax
    4eef:	48 89 45 90          	mov    %rax,-0x70(%rbp)
    4ef3:	4c 89 f0             	mov    %r14,%rax
    4ef6:	4d 89 fe             	mov    %r15,%r14
    4ef9:	49 89 c7             	mov    %rax,%r15
	  return -ENOMEM;
	do {
		next = pud_addr_end(addr, end);
    4efc:	49 8d 87 00 00 00 40 	lea    0x40000000(%r15),%rax
    4f03:	48 89 45 b0          	mov    %rax,-0x50(%rbp)
    4f07:	48 81 65 b0 00 00 00 	andq   $0xffffffffc0000000,-0x50(%rbp)
    4f0e:	c0 
    4f0f:	48 8b 75 b0          	mov    -0x50(%rbp),%rsi
    4f13:	48 89 f0             	mov    %rsi,%rax
    4f16:	48 83 e8 01          	sub    $0x1,%rax
    4f1a:	48 3b 85 60 ff ff ff 	cmp    -0xa0(%rbp),%rax
    4f21:	48 89 f0             	mov    %rsi,%rax
    4f24:	48 0f 43 45 88       	cmovae -0x78(%rbp),%rax
    4f29:	48 89 45 b0          	mov    %rax,-0x50(%rbp)
    4f2d:	48 8b 45 80          	mov    -0x80(%rbp),%rax
    4f31:	48 8b 00             	mov    (%rax),%rax
}

static inline pmd_t *pmd_alloc(struct mm_struct *mm, pud_t *pud, unsigned long address)
{
	return (unlikely(pud_none(*pud)) && __pmd_alloc(mm, pud, address))?
		NULL: pmd_offset(pud, address);
    4f34:	48 85 c0             	test   %rax,%rax
    4f37:	0f 84 1e 02 00 00    	je     515b <remap_pfn_range+0x42b>
    4f3d:	48 89 c7             	mov    %rax,%rdi

	if (sizeof(pudval_t) > sizeof(long))
		ret =  PVOP_CALLEE2(pudval_t, pv_mmu_ops.pud_val,
				    pud.pud, (u64)pud.pud >> 32);
	else
		ret =  PVOP_CALLEE1(pudval_t, pv_mmu_ops.pud_val,
    4f40:	ff 14 25 00 00 00 00 	callq  *0x0
 * this macro returns the index of the entry in the pmd page which would
 * control the given virtual address
 */
static inline unsigned long pmd_index(unsigned long address)
{
	return (address >> PMD_SHIFT) & (PTRS_PER_PMD - 1);
    4f47:	4c 89 fa             	mov    %r15,%rdx
	return pud_flags(pud) & _PAGE_PRESENT;
}

static inline unsigned long pud_page_vaddr(pud_t pud)
{
	return (unsigned long)__va((unsigned long)pud_val(pud) & PTE_PFN_MASK);
    4f4a:	48 be 00 00 00 00 00 	movabs $0xffff880000000000,%rsi
    4f51:	88 ff ff 
 * this macro returns the index of the entry in the pmd page which would
 * control the given virtual address
 */
static inline unsigned long pmd_index(unsigned long address)
{
	return (address >> PMD_SHIFT) & (PTRS_PER_PMD - 1);
    4f54:	48 c1 ea 12          	shr    $0x12,%rdx
#define pud_page(pud)		pfn_to_page(pud_val(pud) >> PAGE_SHIFT)

/* Find an entry in the second-level page table.. */
static inline pmd_t *pmd_offset(pud_t *pud, unsigned long address)
{
	return (pmd_t *)pud_page_vaddr(*pud) + pmd_index(address);
    4f58:	81 e2 f8 0f 00 00    	and    $0xff8,%edx
	return pud_flags(pud) & _PAGE_PRESENT;
}

static inline unsigned long pud_page_vaddr(pud_t pud)
{
	return (unsigned long)__va((unsigned long)pud_val(pud) & PTE_PFN_MASK);
    4f5e:	48 01 f2             	add    %rsi,%rdx
    4f61:	48 be 00 f0 ff ff ff 	movabs $0x3ffffffff000,%rsi
    4f68:	3f 00 00 
    4f6b:	48 21 f0             	and    %rsi,%rax
	pmd_t *pmd;
	unsigned long next;

	pfn -= addr >> PAGE_SHIFT;
	pmd = pmd_alloc(mm, pud, addr);
	if (!pmd)
    4f6e:	48 01 c2             	add    %rax,%rdx
    4f71:	48 89 55 b8          	mov    %rdx,-0x48(%rbp)
    4f75:	0f 84 87 01 00 00    	je     5102 <remap_pfn_range+0x3d2>
    4f7b:	48 8b 45 b0          	mov    -0x50(%rbp),%rax
    4f7f:	48 83 e8 01          	sub    $0x1,%rax
    4f83:	48 89 45 a0          	mov    %rax,-0x60(%rbp)
    4f87:	66 0f 1f 84 00 00 00 	nopw   0x0(%rax,%rax,1)
    4f8e:	00 00 
	  return -ENOMEM;
	VM_BUG_ON(pmd_trans_huge(*pmd));
	do {
		next = pmd_addr_end(addr, end);
    4f90:	4d 8d af 00 00 20 00 	lea    0x200000(%r15),%r13
		if (remap_pte_range(mm, pmd, addr, next,
						pfn + (addr >> PAGE_SHIFT), prot))
    4f97:	4c 89 fb             	mov    %r15,%rbx
	pmd = pmd_alloc(mm, pud, addr);
	if (!pmd)
	  return -ENOMEM;
	VM_BUG_ON(pmd_trans_huge(*pmd));
	do {
		next = pmd_addr_end(addr, end);
    4f9a:	49 81 e5 00 00 e0 ff 	and    $0xffffffffffe00000,%r13
    4fa1:	49 8d 45 ff          	lea    -0x1(%r13),%rax
    4fa5:	48 3b 45 a0          	cmp    -0x60(%rbp),%rax
		if (remap_pte_range(mm, pmd, addr, next,
    4fa9:	48 8b 45 98          	mov    -0x68(%rbp),%rax
	pmd = pmd_alloc(mm, pud, addr);
	if (!pmd)
	  return -ENOMEM;
	VM_BUG_ON(pmd_trans_huge(*pmd));
	do {
		next = pmd_addr_end(addr, end);
    4fad:	4c 0f 43 6d b0       	cmovae -0x50(%rbp),%r13
		if (remap_pte_range(mm, pmd, addr, next,
						pfn + (addr >> PAGE_SHIFT), prot))
    4fb2:	48 c1 eb 0c          	shr    $0xc,%rbx
	if (!pmd)
	  return -ENOMEM;
	VM_BUG_ON(pmd_trans_huge(*pmd));
	do {
		next = pmd_addr_end(addr, end);
		if (remap_pte_range(mm, pmd, addr, next,
    4fb6:	4c 8d 24 03          	lea    (%rbx,%rax,1),%r12
    4fba:	48 8b 45 b8          	mov    -0x48(%rbp),%rax
    4fbe:	48 8b 00             	mov    (%rax),%rax
			unsigned long pfn, pgprot_t prot)
{
	pte_t *pte;
	spinlock_t *ptl;

	pte = pte_alloc_map_lock(mm, pmd, addr, &ptl);
    4fc1:	48 85 c0             	test   %rax,%rax
    4fc4:	0f 84 b0 01 00 00    	je     517a <remap_pfn_range+0x44a>
    4fca:	48 89 c7             	mov    %rax,%rdi

	if (sizeof(pmdval_t) > sizeof(long))
		ret =  PVOP_CALLEE2(pmdval_t, pv_mmu_ops.pmd_val,
				    pmd.pmd, (u64)pmd.pmd >> 32);
	else
		ret =  PVOP_CALLEE1(pmdval_t, pv_mmu_ops.pmd_val,
    4fcd:	ff 14 25 00 00 00 00 	callq  *0x0
}
#endif /* ALLOC_SPLIT_PTLOCKS */

static inline spinlock_t *pte_lockptr(struct mm_struct *mm, pmd_t *pmd)
{
	return ptlock_ptr(pmd_page(*pmd));
    4fd4:	48 ba 00 f0 ff ff ff 	movabs $0x3ffffffff000,%rdx
    4fdb:	3f 00 00 
    4fde:	48 21 d0             	and    %rdx,%rax
    4fe1:	48 ba 00 00 00 00 00 	movabs $0xffffea0000000000,%rdx
    4fe8:	ea ff ff 
    4feb:	48 c1 e8 06          	shr    $0x6,%rax
    4fef:	48 8b 44 10 30       	mov    0x30(%rax,%rdx,1),%rax
    4ff4:	48 89 c6             	mov    %rax,%rsi
    4ff7:	48 89 45 a8          	mov    %rax,-0x58(%rbp)
    4ffb:	48 8b 45 b8          	mov    -0x48(%rbp),%rax
    4fff:	48 8b 38             	mov    (%rax),%rdi
    5002:	ff 14 25 00 00 00 00 	callq  *0x0
	return (unsigned long)native_pmd_val(pmd) == 0;
}

static inline unsigned long pmd_page_vaddr(pmd_t pmd)
{
	return (unsigned long)__va(pmd_val(pmd) & PTE_PFN_MASK);
    5009:	48 ba 00 00 00 00 00 	movabs $0xffff880000000000,%rdx
    5010:	88 ff ff 
 * this function returns the index of the entry in the pte page which would
 * control the given virtual address
 */
static inline unsigned long pte_index(unsigned long address)
{
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
    5013:	81 e3 ff 01 00 00    	and    $0x1ff,%ebx
	raw_spin_lock_init(&(_lock)->rlock);		\
} while (0)

static inline void spin_lock(spinlock_t *lock)
{
	raw_spin_lock(&lock->rlock);
    5019:	48 89 f7             	mov    %rsi,%rdi
	return (unsigned long)native_pmd_val(pmd) == 0;
}

static inline unsigned long pmd_page_vaddr(pmd_t pmd)
{
	return (unsigned long)__va(pmd_val(pmd) & PTE_PFN_MASK);
    501c:	48 8d 1c da          	lea    (%rdx,%rbx,8),%rbx
    5020:	48 ba 00 f0 ff ff ff 	movabs $0x3ffffffff000,%rdx
    5027:	3f 00 00 
    502a:	48 21 d0             	and    %rdx,%rax
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
}

static inline pte_t *pte_offset_kernel(pmd_t *pmd, unsigned long address)
{
	return (pte_t *)pmd_page_vaddr(*pmd) + pte_index(address);
    502d:	48 01 c3             	add    %rax,%rbx
    5030:	e8 00 00 00 00       	callq  5035 <remap_pfn_range+0x305>
	if (!pte)
    5035:	48 85 db             	test   %rbx,%rbx
    5038:	0f 84 c4 00 00 00    	je     5102 <remap_pfn_range+0x3d2>
}

#define  __HAVE_ARCH_ENTER_LAZY_MMU_MODE
static inline void arch_enter_lazy_mmu_mode(void)
{
	PVOP_VCALL0(pv_mmu_ops.lazy_mode.enter);
    503e:	ff 14 25 00 00 00 00 	callq  *0x0
    5045:	4c 89 6d c8          	mov    %r13,-0x38(%rbp)
    5049:	4c 8b 6d 90          	mov    -0x70(%rbp),%r13
    504d:	0f 1f 00             	nopl   (%rax)
	  return -ENOMEM;
	arch_enter_lazy_mmu_mode();
	do {
		BUG_ON(!pte_none(*pte));
    5050:	48 83 3b 00          	cmpq   $0x0,(%rbx)
    5054:	0f 85 a6 00 00 00    	jne    5100 <remap_pfn_range+0x3d0>
static inline pgprotval_t massage_pgprot(pgprot_t pgprot)
{
	pgprotval_t protval = pgprot_val(pgprot);

	if (protval & _PAGE_PRESENT)
	  protval &= __supported_pte_mask;
    505a:	4c 89 f7             	mov    %r14,%rdi
    505d:	48 23 3d 00 00 00 00 	and    0x0(%rip),%rdi        # 5064 <remap_pfn_range+0x334>
	return protval;
}

static inline pte_t pfn_pte(unsigned long page_nr, pgprot_t pgprot)
{
	return __pte(((phys_addr_t)page_nr << PAGE_SHIFT) |
    5064:	4c 89 e0             	mov    %r12,%rax
    5067:	48 c1 e0 0c          	shl    $0xc,%rax
static inline pgprotval_t massage_pgprot(pgprot_t pgprot)
{
	pgprotval_t protval = pgprot_val(pgprot);

	if (protval & _PAGE_PRESENT)
	  protval &= __supported_pte_mask;
    506b:	4d 85 ed             	test   %r13,%r13
    506e:	49 0f 44 fe          	cmove  %r14,%rdi
	return protval;
}

static inline pte_t pfn_pte(unsigned long page_nr, pgprot_t pgprot)
{
	return __pte(((phys_addr_t)page_nr << PAGE_SHIFT) |
    5072:	48 09 c7             	or     %rax,%rdi
	if (sizeof(pteval_t) > sizeof(long))
		ret = PVOP_CALLEE2(pteval_t,
				   pv_mmu_ops.make_pte,
				   val, (u64)val >> 32);
	else
		ret = PVOP_CALLEE1(pteval_t,
    5075:	ff 14 25 00 00 00 00 	callq  *0x0
    507c:	48 89 c1             	mov    %rax,%rcx
{
	if (sizeof(pteval_t) > sizeof(long))
		/* 5 arg words */
		pv_mmu_ops.set_pte_at(mm, addr, ptep, pte);
	else
		PVOP_VCALL4(pv_mmu_ops.set_pte_at, mm, addr, ptep, pte.pte);
    507f:	48 8b 7d c0          	mov    -0x40(%rbp),%rdi
    5083:	4c 89 fe             	mov    %r15,%rsi

static inline pte_t pte_set_flags(pte_t pte, pteval_t set)
{
	pteval_t v = native_pte_val(pte);

	return native_make_pte(v | set);
    5086:	80 cd 02             	or     $0x2,%ch
    5089:	48 89 da             	mov    %rbx,%rdx
    508c:	ff 14 25 00 00 00 00 	callq  *0x0
		set_pte_at(mm, addr, pte, pte_mkspecial(pfn_pte(pfn, prot)));
		pfn++;
    5093:	49 83 c4 01          	add    $0x1,%r12
	} while (pte++, addr += PAGE_SIZE, addr != end);
    5097:	48 83 c3 08          	add    $0x8,%rbx
    509b:	49 81 c7 00 10 00 00 	add    $0x1000,%r15
    50a2:	4c 39 7d c8          	cmp    %r15,-0x38(%rbp)
    50a6:	75 a8                	jne    5050 <remap_pfn_range+0x320>
    50a8:	4c 8b 6d c8          	mov    -0x38(%rbp),%r13
	PVOP_VCALL0(pv_mmu_ops.lazy_mode.enter);
}

static inline void arch_leave_lazy_mmu_mode(void)
{
	PVOP_VCALL0(pv_mmu_ops.lazy_mode.leave);
    50ac:	ff 14 25 00 00 00 00 	callq  *0x0
	raw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \
} while (0)

static inline void spin_unlock(spinlock_t *lock)
{
	raw_spin_unlock(&lock->rlock);
    50b3:	48 8b 7d a8          	mov    -0x58(%rbp),%rdi
    50b7:	e8 00 00 00 00       	callq  50bc <remap_pfn_range+0x38c>
	do {
		next = pmd_addr_end(addr, end);
		if (remap_pte_range(mm, pmd, addr, next,
						pfn + (addr >> PAGE_SHIFT), prot))
		  return -ENOMEM;
	} while (pmd++, addr = next, addr != end);
    50bc:	48 83 45 b8 08       	addq   $0x8,-0x48(%rbp)
    50c1:	4c 39 6d b0          	cmp    %r13,-0x50(%rbp)
    50c5:	0f 85 c5 fe ff ff    	jne    4f90 <remap_pfn_range+0x260>
	do {
		next = pud_addr_end(addr, end);
		if (remap_pmd_range(mm, pud, addr, next,
						pfn + (addr >> PAGE_SHIFT), prot))
		  return -ENOMEM;
	} while (pud++, addr = next, addr != end);
    50cb:	48 83 45 80 08       	addq   $0x8,-0x80(%rbp)
    50d0:	48 8b 45 b0          	mov    -0x50(%rbp),%rax
    50d4:	48 39 45 88          	cmp    %rax,-0x78(%rbp)
    50d8:	0f 85 1e fe ff ff    	jne    4efc <remap_pfn_range+0x1cc>
		next = pgd_addr_end(addr, end);
		err = remap_pud_range(mm, pgd, addr, next,
					pfn + (addr >> PAGE_SHIFT), prot);
		if (err)
		  break;
	} while (pgd++, addr = next, addr != end);
    50de:	48 83 85 78 ff ff ff 	addq   $0x8,-0x88(%rbp)
    50e5:	08 
    50e6:	48 8b 85 70 ff ff ff 	mov    -0x90(%rbp),%rax
    50ed:	4d 89 fe             	mov    %r15,%r14
    50f0:	48 39 45 88          	cmp    %rax,-0x78(%rbp)
    50f4:	0f 85 4b fd ff ff    	jne    4e45 <remap_pfn_range+0x115>
    50fa:	31 c0                	xor    %eax,%eax
    50fc:	eb 20                	jmp    511e <remap_pfn_range+0x3ee>
    50fe:	66 90                	xchg   %ax,%ax
	pte = pte_alloc_map_lock(mm, pmd, addr, &ptl);
	if (!pte)
	  return -ENOMEM;
	arch_enter_lazy_mmu_mode();
	do {
		BUG_ON(!pte_none(*pte));
    5100:	0f 0b                	ud2    
		if (err)
		  break;
	} while (pgd++, addr = next, addr != end);

	if (err)
	  untrack_pfn(vma, pfn, PAGE_ALIGN(size));
    5102:	48 8b 95 68 ff ff ff 	mov    -0x98(%rbp),%rdx
    5109:	48 8b 75 98          	mov    -0x68(%rbp),%rsi
    510d:	48 8b bd 58 ff ff ff 	mov    -0xa8(%rbp),%rdi
    5114:	e8 00 00 00 00       	callq  5119 <remap_pfn_range+0x3e9>
    5119:	b8 f4 ff ff ff       	mov    $0xfffffff4,%eax

	return err;
}
    511e:	48 81 c4 88 00 00 00 	add    $0x88,%rsp
    5125:	5b                   	pop    %rbx
    5126:	41 5c                	pop    %r12
    5128:	41 5d                	pop    %r13
    512a:	41 5e                	pop    %r14
    512c:	41 5f                	pop    %r15
    512e:	5d                   	pop    %rbp
    512f:	c3                   	retq   
	 * un-COW'ed pages by matching them up with "vma->vm_pgoff".
	 * See vm_normal_page() for details.
	 */
	if (is_cow_mapping(vma->vm_flags)) {
		if (addr != vma->vm_start || end != vma->vm_end)
		  return -EINVAL;
    5130:	b8 ea ff ff ff       	mov    $0xffffffea,%eax
    5135:	eb e7                	jmp    511e <remap_pfn_range+0x3ee>
	if (err)
	  return -EINVAL;

	vma->vm_flags |= VM_IO | VM_PFNMAP | VM_DONTEXPAND | VM_DONTDUMP;

	BUG_ON(addr >= end);
    5137:	0f 0b                	ud2    
 * Remove it when 4level-fixup.h has been removed.
 */
#if defined(CONFIG_MMU) && !defined(__ARCH_HAS_4LEVEL_HACK)
static inline pud_t *pud_alloc(struct mm_struct *mm, pgd_t *pgd, unsigned long address)
{
	return (unlikely(pgd_none(*pgd)) && __pud_alloc(mm, pgd, address))?
    5139:	48 8b 9d 78 ff ff ff 	mov    -0x88(%rbp),%rbx
    5140:	48 8b 7d c0          	mov    -0x40(%rbp),%rdi
    5144:	4c 89 f2             	mov    %r14,%rdx
    5147:	48 89 de             	mov    %rbx,%rsi
    514a:	e8 00 00 00 00       	callq  514f <remap_pfn_range+0x41f>
    514f:	85 c0                	test   %eax,%eax
    5151:	75 af                	jne    5102 <remap_pfn_range+0x3d2>
    5153:	48 8b 3b             	mov    (%rbx),%rdi
    5156:	e9 44 fd ff ff       	jmpq   4e9f <remap_pfn_range+0x16f>
		NULL: pud_offset(pgd, address);
}

static inline pmd_t *pmd_alloc(struct mm_struct *mm, pud_t *pud, unsigned long address)
{
	return (unlikely(pud_none(*pud)) && __pmd_alloc(mm, pud, address))?
    515b:	48 8b 5d 80          	mov    -0x80(%rbp),%rbx
    515f:	48 8b 7d c0          	mov    -0x40(%rbp),%rdi
    5163:	4c 89 fa             	mov    %r15,%rdx
    5166:	48 89 de             	mov    %rbx,%rsi
    5169:	e8 00 00 00 00       	callq  516e <remap_pfn_range+0x43e>
    516e:	85 c0                	test   %eax,%eax
    5170:	75 90                	jne    5102 <remap_pfn_range+0x3d2>
    5172:	48 8b 3b             	mov    (%rbx),%rdi
    5175:	e9 c6 fd ff ff       	jmpq   4f40 <remap_pfn_range+0x210>
			unsigned long pfn, pgprot_t prot)
{
	pte_t *pte;
	spinlock_t *ptl;

	pte = pte_alloc_map_lock(mm, pmd, addr, &ptl);
    517a:	48 8b 55 b8          	mov    -0x48(%rbp),%rdx
    517e:	48 8b 7d c0          	mov    -0x40(%rbp),%rdi
    5182:	31 f6                	xor    %esi,%esi
    5184:	4c 89 f9             	mov    %r15,%rcx
    5187:	e8 00 00 00 00       	callq  518c <remap_pfn_range+0x45c>
    518c:	85 c0                	test   %eax,%eax
    518e:	0f 85 6e ff ff ff    	jne    5102 <remap_pfn_range+0x3d2>
    5194:	48 8b 45 b8          	mov    -0x48(%rbp),%rax
    5198:	48 8b 38             	mov    (%rax),%rdi
    519b:	e9 2d fe ff ff       	jmpq   4fcd <remap_pfn_range+0x29d>

00000000000051a0 <vm_iomap_memory>:
 *
 * NOTE! Some drivers might want to tweak vma->vm_page_prot first to get
 * whatever write-combining details or similar.
 */
int vm_iomap_memory(struct vm_area_struct *vma, phys_addr_t start, unsigned long len)
{
    51a0:	e8 00 00 00 00       	callq  51a5 <vm_iomap_memory+0x5>
	unsigned long vm_len, pfn, pages;

	/* Check that the physical memory area passed in looks valid */
	if (start + len < start)
    51a5:	48 89 f0             	mov    %rsi,%rax
    51a8:	48 01 d0             	add    %rdx,%rax
    51ab:	72 63                	jb     5210 <vm_iomap_memory+0x70>
	 * You *really* shouldn't map things that aren't page-aligned,
	 * but we've historically allowed it because IO memory might
	 * just have smaller alignment.
	 */
	len += start & ~PAGE_MASK;
	pfn = start >> PAGE_SHIFT;
    51ad:	48 89 f1             	mov    %rsi,%rcx
	/*
	 * You *really* shouldn't map things that aren't page-aligned,
	 * but we've historically allowed it because IO memory might
	 * just have smaller alignment.
	 */
	len += start & ~PAGE_MASK;
    51b0:	81 e6 ff 0f 00 00    	and    $0xfff,%esi
	pfn = start >> PAGE_SHIFT;
	pages = (len + ~PAGE_MASK) >> PAGE_SHIFT;
    51b6:	48 8d 84 32 ff 0f 00 	lea    0xfff(%rdx,%rsi,1),%rax
    51bd:	00 
	 * You *really* shouldn't map things that aren't page-aligned,
	 * but we've historically allowed it because IO memory might
	 * just have smaller alignment.
	 */
	len += start & ~PAGE_MASK;
	pfn = start >> PAGE_SHIFT;
    51be:	48 c1 e9 0c          	shr    $0xc,%rcx
	pages = (len + ~PAGE_MASK) >> PAGE_SHIFT;
    51c2:	48 c1 e8 0c          	shr    $0xc,%rax
	if (pfn + pages < pfn)
    51c6:	48 89 c2             	mov    %rax,%rdx
    51c9:	48 01 ca             	add    %rcx,%rdx
    51cc:	72 42                	jb     5210 <vm_iomap_memory+0x70>
	  return -EINVAL;

	/* We start the mapping 'vm_pgoff' pages into the area */
	if (vma->vm_pgoff > pages)
    51ce:	4c 8b 87 a0 00 00 00 	mov    0xa0(%rdi),%r8
    51d5:	4c 39 c0             	cmp    %r8,%rax
    51d8:	72 36                	jb     5210 <vm_iomap_memory+0x70>
	  return -EINVAL;
	pfn += vma->vm_pgoff;
    51da:	4a 8d 14 01          	lea    (%rcx,%r8,1),%rdx
	pages -= vma->vm_pgoff;

	/* Can we fit all of the mapping? */
	vm_len = vma->vm_end - vma->vm_start;
    51de:	48 8b 37             	mov    (%rdi),%rsi
    51e1:	48 8b 4f 08          	mov    0x8(%rdi),%rcx

	/* We start the mapping 'vm_pgoff' pages into the area */
	if (vma->vm_pgoff > pages)
	  return -EINVAL;
	pfn += vma->vm_pgoff;
	pages -= vma->vm_pgoff;
    51e5:	4c 29 c0             	sub    %r8,%rax
    51e8:	49 89 c1             	mov    %rax,%r9
{
	unsigned long vm_len, pfn, pages;

	/* Check that the physical memory area passed in looks valid */
	if (start + len < start)
	  return -EINVAL;
    51eb:	b8 ea ff ff ff       	mov    $0xffffffea,%eax
	  return -EINVAL;
	pfn += vma->vm_pgoff;
	pages -= vma->vm_pgoff;

	/* Can we fit all of the mapping? */
	vm_len = vma->vm_end - vma->vm_start;
    51f0:	48 29 f1             	sub    %rsi,%rcx
	if (vm_len >> PAGE_SHIFT > pages)
    51f3:	49 89 c8             	mov    %rcx,%r8
    51f6:	49 c1 e8 0c          	shr    $0xc,%r8
    51fa:	4d 39 c1             	cmp    %r8,%r9
    51fd:	72 0e                	jb     520d <vm_iomap_memory+0x6d>
 *
 * NOTE! Some drivers might want to tweak vma->vm_page_prot first to get
 * whatever write-combining details or similar.
 */
int vm_iomap_memory(struct vm_area_struct *vma, phys_addr_t start, unsigned long len)
{
    51ff:	55                   	push   %rbp
	vm_len = vma->vm_end - vma->vm_start;
	if (vm_len >> PAGE_SHIFT > pages)
	  return -EINVAL;

	/* Ok, let it rip */
	return io_remap_pfn_range(vma, vma->vm_start, pfn, vm_len, vma->vm_page_prot);
    5200:	4c 8b 47 48          	mov    0x48(%rdi),%r8
 *
 * NOTE! Some drivers might want to tweak vma->vm_page_prot first to get
 * whatever write-combining details or similar.
 */
int vm_iomap_memory(struct vm_area_struct *vma, phys_addr_t start, unsigned long len)
{
    5204:	48 89 e5             	mov    %rsp,%rbp
	vm_len = vma->vm_end - vma->vm_start;
	if (vm_len >> PAGE_SHIFT > pages)
	  return -EINVAL;

	/* Ok, let it rip */
	return io_remap_pfn_range(vma, vma->vm_start, pfn, vm_len, vma->vm_page_prot);
    5207:	e8 00 00 00 00       	callq  520c <vm_iomap_memory+0x6c>
}
    520c:	5d                   	pop    %rbp
    520d:	f3 c3                	repz retq 
    520f:	90                   	nop
{
	unsigned long vm_len, pfn, pages;

	/* Check that the physical memory area passed in looks valid */
	if (start + len < start)
	  return -EINVAL;
    5210:	b8 ea ff ff ff       	mov    $0xffffffea,%eax
    5215:	c3                   	retq   
    5216:	66 2e 0f 1f 84 00 00 	nopw   %cs:0x0(%rax,%rax,1)
    521d:	00 00 00 

0000000000005220 <__get_locked_pte>:
}
#endif /* CONFIG_ELF_CORE */

pte_t *__get_locked_pte(struct mm_struct *mm, unsigned long addr,
			spinlock_t **ptl)
{
    5220:	e8 00 00 00 00       	callq  5225 <__get_locked_pte+0x5>
    5225:	55                   	push   %rbp
    5226:	48 89 e5             	mov    %rsp,%rbp
    5229:	41 56                	push   %r14
    522b:	49 89 fe             	mov    %rdi,%r14
    522e:	41 55                	push   %r13
    5230:	49 89 d5             	mov    %rdx,%r13
    5233:	41 54                	push   %r12
    5235:	49 89 f4             	mov    %rsi,%r12
    5238:	53                   	push   %rbx
	pgd_t * pgd = pgd_offset(mm, addr);
    5239:	48 89 f3             	mov    %rsi,%rbx
    523c:	48 c1 eb 24          	shr    $0x24,%rbx
    5240:	81 e3 f8 0f 00 00    	and    $0xff8,%ebx
    5246:	48 03 5f 40          	add    0x40(%rdi),%rbx
    524a:	48 8b 03             	mov    (%rbx),%rax
 */
#if defined(CONFIG_MMU) && !defined(__ARCH_HAS_4LEVEL_HACK)
static inline pud_t *pud_alloc(struct mm_struct *mm, pgd_t *pgd, unsigned long address)
{
	return (unlikely(pgd_none(*pgd)) && __pud_alloc(mm, pgd, address))?
		NULL: pud_offset(pgd, address);
    524d:	48 85 c0             	test   %rax,%rax
    5250:	0f 84 19 01 00 00    	je     536f <__get_locked_pte+0x14f>
    5256:	48 89 c7             	mov    %rax,%rdi

	if (sizeof(pgdval_t) > sizeof(long))
		ret =  PVOP_CALLEE2(pgdval_t, pv_mmu_ops.pgd_val,
				    pgd.pgd, (u64)pgd.pgd >> 32);
	else
		ret =  PVOP_CALLEE1(pgdval_t, pv_mmu_ops.pgd_val,
    5259:	ff 14 25 00 00 00 00 	callq  *0x0
#define pgd_page(pgd)		pfn_to_page(pgd_val(pgd) >> PAGE_SHIFT)

/* to find an entry in a page-table-directory. */
static inline unsigned long pud_index(unsigned long address)
{
	return (address >> PUD_SHIFT) & (PTRS_PER_PUD - 1);
    5260:	4c 89 e1             	mov    %r12,%rcx
	return pgd_flags(pgd) & _PAGE_PRESENT;
}

static inline unsigned long pgd_page_vaddr(pgd_t pgd)
{
	return (unsigned long)__va((unsigned long)pgd_val(pgd) & PTE_PFN_MASK);
    5263:	48 ba 00 00 00 00 00 	movabs $0xffff880000000000,%rdx
    526a:	88 ff ff 
#define pgd_page(pgd)		pfn_to_page(pgd_val(pgd) >> PAGE_SHIFT)

/* to find an entry in a page-table-directory. */
static inline unsigned long pud_index(unsigned long address)
{
	return (address >> PUD_SHIFT) & (PTRS_PER_PUD - 1);
    526d:	48 c1 e9 1b          	shr    $0x1b,%rcx
}

static inline pud_t *pud_offset(pgd_t *pgd, unsigned long address)
{
	return (pud_t *)pgd_page_vaddr(*pgd) + pud_index(address);
    5271:	81 e1 f8 0f 00 00    	and    $0xff8,%ecx
	return pgd_flags(pgd) & _PAGE_PRESENT;
}

static inline unsigned long pgd_page_vaddr(pgd_t pgd)
{
	return (unsigned long)__va((unsigned long)pgd_val(pgd) & PTE_PFN_MASK);
    5277:	48 01 d1             	add    %rdx,%rcx
    527a:	48 ba 00 f0 ff ff ff 	movabs $0x3ffffffff000,%rdx
    5281:	3f 00 00 
    5284:	48 21 d0             	and    %rdx,%rax
	pud_t * pud = pud_alloc(mm, pgd, addr);
	if (pud) {
    5287:	48 89 cb             	mov    %rcx,%rbx
    528a:	48 01 c3             	add    %rax,%rbx
    528d:	0f 84 bd 00 00 00    	je     5350 <__get_locked_pte+0x130>
    5293:	48 8b 3b             	mov    (%rbx),%rdi
}

static inline pmd_t *pmd_alloc(struct mm_struct *mm, pud_t *pud, unsigned long address)
{
	return (unlikely(pud_none(*pud)) && __pmd_alloc(mm, pud, address))?
		NULL: pmd_offset(pud, address);
    5296:	48 85 ff             	test   %rdi,%rdi
    5299:	0f 84 b6 00 00 00    	je     5355 <__get_locked_pte+0x135>

	if (sizeof(pudval_t) > sizeof(long))
		ret =  PVOP_CALLEE2(pudval_t, pv_mmu_ops.pud_val,
				    pud.pud, (u64)pud.pud >> 32);
	else
		ret =  PVOP_CALLEE1(pudval_t, pv_mmu_ops.pud_val,
    529f:	ff 14 25 00 00 00 00 	callq  *0x0
 * this macro returns the index of the entry in the pmd page which would
 * control the given virtual address
 */
static inline unsigned long pmd_index(unsigned long address)
{
	return (address >> PMD_SHIFT) & (PTRS_PER_PMD - 1);
    52a6:	4c 89 e1             	mov    %r12,%rcx
	return pud_flags(pud) & _PAGE_PRESENT;
}

static inline unsigned long pud_page_vaddr(pud_t pud)
{
	return (unsigned long)__va((unsigned long)pud_val(pud) & PTE_PFN_MASK);
    52a9:	48 ba 00 00 00 00 00 	movabs $0xffff880000000000,%rdx
    52b0:	88 ff ff 
 * this macro returns the index of the entry in the pmd page which would
 * control the given virtual address
 */
static inline unsigned long pmd_index(unsigned long address)
{
	return (address >> PMD_SHIFT) & (PTRS_PER_PMD - 1);
    52b3:	48 c1 e9 12          	shr    $0x12,%rcx
#define pud_page(pud)		pfn_to_page(pud_val(pud) >> PAGE_SHIFT)

/* Find an entry in the second-level page table.. */
static inline pmd_t *pmd_offset(pud_t *pud, unsigned long address)
{
	return (pmd_t *)pud_page_vaddr(*pud) + pmd_index(address);
    52b7:	81 e1 f8 0f 00 00    	and    $0xff8,%ecx
	return pud_flags(pud) & _PAGE_PRESENT;
}

static inline unsigned long pud_page_vaddr(pud_t pud)
{
	return (unsigned long)__va((unsigned long)pud_val(pud) & PTE_PFN_MASK);
    52bd:	48 01 d1             	add    %rdx,%rcx
    52c0:	48 ba 00 f0 ff ff ff 	movabs $0x3ffffffff000,%rdx
    52c7:	3f 00 00 
    52ca:	48 21 d0             	and    %rdx,%rax
		pmd_t * pmd = pmd_alloc(mm, pud, addr);
		if (pmd) {
    52cd:	48 89 cb             	mov    %rcx,%rbx
    52d0:	48 01 c3             	add    %rax,%rbx
    52d3:	74 7b                	je     5350 <__get_locked_pte+0x130>
    52d5:	48 8b 03             	mov    (%rbx),%rax
			VM_BUG_ON(pmd_trans_huge(*pmd));
			return pte_alloc_map_lock(mm, pmd, addr, ptl);
    52d8:	48 85 c0             	test   %rax,%rax
    52db:	0f 84 a5 00 00 00    	je     5386 <__get_locked_pte+0x166>
    52e1:	48 89 c7             	mov    %rax,%rdi

	if (sizeof(pmdval_t) > sizeof(long))
		ret =  PVOP_CALLEE2(pmdval_t, pv_mmu_ops.pmd_val,
				    pmd.pmd, (u64)pmd.pmd >> 32);
	else
		ret =  PVOP_CALLEE1(pmdval_t, pv_mmu_ops.pmd_val,
    52e4:	ff 14 25 00 00 00 00 	callq  *0x0
}
#endif /* ALLOC_SPLIT_PTLOCKS */

static inline spinlock_t *pte_lockptr(struct mm_struct *mm, pmd_t *pmd)
{
	return ptlock_ptr(pmd_page(*pmd));
    52eb:	48 b9 00 f0 ff ff ff 	movabs $0x3ffffffff000,%rcx
    52f2:	3f 00 00 
    52f5:	48 ba 00 00 00 00 00 	movabs $0xffffea0000000000,%rdx
    52fc:	ea ff ff 
    52ff:	48 8b 3b             	mov    (%rbx),%rdi
    5302:	48 21 c8             	and    %rcx,%rax
    5305:	48 c1 e8 06          	shr    $0x6,%rax
    5309:	48 8b 54 10 30       	mov    0x30(%rax,%rdx,1),%rdx
    530e:	ff 14 25 00 00 00 00 	callq  *0x0
 * this function returns the index of the entry in the pte page which would
 * control the given virtual address
 */
static inline unsigned long pte_index(unsigned long address)
{
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
    5315:	49 c1 ec 09          	shr    $0x9,%r12
	return (unsigned long)native_pmd_val(pmd) == 0;
}

static inline unsigned long pmd_page_vaddr(pmd_t pmd)
{
	return (unsigned long)__va(pmd_val(pmd) & PTE_PFN_MASK);
    5319:	48 be 00 00 00 00 00 	movabs $0xffff880000000000,%rsi
    5320:	88 ff ff 
    5323:	48 21 c8             	and    %rcx,%rax
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
}

static inline pte_t *pte_offset_kernel(pmd_t *pmd, unsigned long address)
{
	return (pte_t *)pmd_page_vaddr(*pmd) + pte_index(address);
    5326:	41 81 e4 f8 0f 00 00 	and    $0xff8,%r12d
    532d:	49 89 55 00          	mov    %rdx,0x0(%r13)
	raw_spin_lock_init(&(_lock)->rlock);		\
} while (0)

static inline void spin_lock(spinlock_t *lock)
{
	raw_spin_lock(&lock->rlock);
    5331:	48 89 d7             	mov    %rdx,%rdi
	return (unsigned long)native_pmd_val(pmd) == 0;
}

static inline unsigned long pmd_page_vaddr(pmd_t pmd)
{
	return (unsigned long)__va(pmd_val(pmd) & PTE_PFN_MASK);
    5334:	49 01 f4             	add    %rsi,%r12
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
}

static inline pte_t *pte_offset_kernel(pmd_t *pmd, unsigned long address)
{
	return (pte_t *)pmd_page_vaddr(*pmd) + pte_index(address);
    5337:	49 01 c4             	add    %rax,%r12
    533a:	e8 00 00 00 00       	callq  533f <__get_locked_pte+0x11f>
		}
	}
	return NULL;
}
    533f:	5b                   	pop    %rbx
    5340:	4c 89 e0             	mov    %r12,%rax
    5343:	41 5c                	pop    %r12
    5345:	41 5d                	pop    %r13
    5347:	41 5e                	pop    %r14
    5349:	5d                   	pop    %rbp
    534a:	c3                   	retq   
    534b:	0f 1f 44 00 00       	nopl   0x0(%rax,%rax,1)
		if (pmd) {
			VM_BUG_ON(pmd_trans_huge(*pmd));
			return pte_alloc_map_lock(mm, pmd, addr, ptl);
		}
	}
	return NULL;
    5350:	45 31 e4             	xor    %r12d,%r12d
    5353:	eb ea                	jmp    533f <__get_locked_pte+0x11f>
		NULL: pud_offset(pgd, address);
}

static inline pmd_t *pmd_alloc(struct mm_struct *mm, pud_t *pud, unsigned long address)
{
	return (unlikely(pud_none(*pud)) && __pmd_alloc(mm, pud, address))?
    5355:	4c 89 e2             	mov    %r12,%rdx
    5358:	48 89 de             	mov    %rbx,%rsi
    535b:	4c 89 f7             	mov    %r14,%rdi
    535e:	e8 00 00 00 00       	callq  5363 <__get_locked_pte+0x143>
    5363:	85 c0                	test   %eax,%eax
    5365:	75 e9                	jne    5350 <__get_locked_pte+0x130>
    5367:	48 8b 3b             	mov    (%rbx),%rdi
    536a:	e9 30 ff ff ff       	jmpq   529f <__get_locked_pte+0x7f>
 * Remove it when 4level-fixup.h has been removed.
 */
#if defined(CONFIG_MMU) && !defined(__ARCH_HAS_4LEVEL_HACK)
static inline pud_t *pud_alloc(struct mm_struct *mm, pgd_t *pgd, unsigned long address)
{
	return (unlikely(pgd_none(*pgd)) && __pud_alloc(mm, pgd, address))?
    536f:	48 89 f2             	mov    %rsi,%rdx
    5372:	48 89 de             	mov    %rbx,%rsi
    5375:	e8 00 00 00 00       	callq  537a <__get_locked_pte+0x15a>
    537a:	85 c0                	test   %eax,%eax
    537c:	75 d2                	jne    5350 <__get_locked_pte+0x130>
    537e:	48 8b 3b             	mov    (%rbx),%rdi
    5381:	e9 d3 fe ff ff       	jmpq   5259 <__get_locked_pte+0x39>
	pud_t * pud = pud_alloc(mm, pgd, addr);
	if (pud) {
		pmd_t * pmd = pmd_alloc(mm, pud, addr);
		if (pmd) {
			VM_BUG_ON(pmd_trans_huge(*pmd));
			return pte_alloc_map_lock(mm, pmd, addr, ptl);
    5386:	31 f6                	xor    %esi,%esi
    5388:	4c 89 e1             	mov    %r12,%rcx
    538b:	48 89 da             	mov    %rbx,%rdx
    538e:	4c 89 f7             	mov    %r14,%rdi
    5391:	e8 00 00 00 00       	callq  5396 <__get_locked_pte+0x176>
    5396:	85 c0                	test   %eax,%eax
    5398:	75 b6                	jne    5350 <__get_locked_pte+0x130>
    539a:	48 8b 3b             	mov    (%rbx),%rdi
    539d:	e9 42 ff ff ff       	jmpq   52e4 <__get_locked_pte+0xc4>
    53a2:	66 66 66 66 66 2e 0f 	data32 data32 data32 data32 nopw %cs:0x0(%rax,%rax,1)
    53a9:	1f 84 00 00 00 00 00 

00000000000053b0 <vm_insert_page>:
 * Caller must set VM_MIXEDMAP on vma if it wants to call this
 * function from other places, for example from page-fault handler.
 */
int vm_insert_page(struct vm_area_struct *vma, unsigned long addr,
			struct page *page)
{
    53b0:	e8 00 00 00 00       	callq  53b5 <vm_insert_page+0x5>
    53b5:	55                   	push   %rbp
    53b6:	48 89 e5             	mov    %rsp,%rbp
    53b9:	41 57                	push   %r15
    53bb:	41 56                	push   %r14
    53bd:	41 55                	push   %r13
    53bf:	49 89 f5             	mov    %rsi,%r13
    53c2:	41 54                	push   %r12
    53c4:	53                   	push   %rbx
    53c5:	48 89 fb             	mov    %rdi,%rbx
    53c8:	48 83 ec 10          	sub    $0x10,%rsp
	if (addr < vma->vm_start || addr >= vma->vm_end)
    53cc:	48 39 37             	cmp    %rsi,(%rdi)
    53cf:	0f 87 ab 01 00 00    	ja     5580 <vm_insert_page+0x1d0>
    53d5:	48 3b 77 08          	cmp    0x8(%rdi),%rsi
    53d9:	0f 83 a1 01 00 00    	jae    5580 <vm_insert_page+0x1d0>
    53df:	48 8b 02             	mov    (%rdx),%rax
    53e2:	49 89 d4             	mov    %rdx,%r12
#endif
}

static inline struct page *compound_head(struct page *page)
{
	if (unlikely(PageTail(page))) {
    53e5:	f6 c4 80             	test   $0x80,%ah
    53e8:	0f 85 cc 01 00 00    	jne    55ba <vm_insert_page+0x20a>
		 */
		smp_rmb();
		if (likely(PageTail(page)))
			return head;
	}
	return page;
    53ee:	48 89 d0             	mov    %rdx,%rax
    53f1:	8b 40 1c             	mov    0x1c(%rax),%eax
	  return -EFAULT;
	if (!page_count(page))
    53f4:	85 c0                	test   %eax,%eax
    53f6:	0f 84 b4 01 00 00    	je     55b0 <vm_insert_page+0x200>
	  return -EINVAL;
	if (!(vma->vm_flags & VM_MIXEDMAP)) {
    53fc:	f6 43 53 10          	testb  $0x10,0x53(%rbx)
    5400:	75 2c                	jne    542e <vm_insert_page+0x7e>
		BUG_ON(down_read_trylock(&vma->vm_mm->mmap_sem));
    5402:	48 8b 43 40          	mov    0x40(%rbx),%rax
    5406:	48 8d 78 78          	lea    0x78(%rax),%rdi
    540a:	e8 00 00 00 00       	callq  540f <vm_insert_page+0x5f>
    540f:	85 c0                	test   %eax,%eax
    5411:	0f 85 b8 01 00 00    	jne    55cf <vm_insert_page+0x21f>
		BUG_ON(vma->vm_flags & VM_PFNMAP);
    5417:	48 8b 43 50          	mov    0x50(%rbx),%rax
    541b:	f6 c4 04             	test   $0x4,%ah
    541e:	0f 85 a9 01 00 00    	jne    55cd <vm_insert_page+0x21d>
		vma->vm_flags |= VM_MIXEDMAP;
    5424:	48 0d 00 00 00 10    	or     $0x10000000,%rax
    542a:	48 89 43 50          	mov    %rax,0x50(%rbx)
	int retval;
	pte_t *pte;
	spinlock_t *ptl;

	retval = -EINVAL;
	if (PageAnon(page))
    542e:	41 f6 44 24 08 01    	testb  $0x1,0x8(%r12)
    5434:	48 8b 4b 48          	mov    0x48(%rbx),%rcx
	struct mm_struct *mm = vma->vm_mm;
	int retval;
	pte_t *pte;
	spinlock_t *ptl;

	retval = -EINVAL;
    5438:	b8 ea ff ff ff       	mov    $0xffffffea,%eax
 * pages reserved for the old functions anyway.
 */
static int insert_page(struct vm_area_struct *vma, unsigned long addr,
			struct page *page, pgprot_t prot)
{
	struct mm_struct *mm = vma->vm_mm;
    543d:	4c 8b 73 40          	mov    0x40(%rbx),%r14
	int retval;
	pte_t *pte;
	spinlock_t *ptl;

	retval = -EINVAL;
	if (PageAnon(page))
    5441:	74 15                	je     5458 <vm_insert_page+0xa8>
		BUG_ON(down_read_trylock(&vma->vm_mm->mmap_sem));
		BUG_ON(vma->vm_flags & VM_PFNMAP);
		vma->vm_flags |= VM_MIXEDMAP;
	}
	return insert_page(vma, addr, page, vma->vm_page_prot);
}
    5443:	48 83 c4 10          	add    $0x10,%rsp
    5447:	5b                   	pop    %rbx
    5448:	41 5c                	pop    %r12
    544a:	41 5d                	pop    %r13
    544c:	41 5e                	pop    %r14
    544e:	41 5f                	pop    %r15
    5450:	5d                   	pop    %rbp
    5451:	c3                   	retq   
    5452:	66 0f 1f 44 00 00    	nopw   0x0(%rax,%rax,1)
			       spinlock_t **ptl);
static inline pte_t *get_locked_pte(struct mm_struct *mm, unsigned long addr,
				    spinlock_t **ptl)
{
	pte_t *ptep;
	__cond_lock(*ptl, ptep = __get_locked_pte(mm, addr, ptl));
    5458:	48 8d 55 d0          	lea    -0x30(%rbp),%rdx
    545c:	4c 89 ee             	mov    %r13,%rsi
    545f:	4c 89 f7             	mov    %r14,%rdi
    5462:	48 89 4d c8          	mov    %rcx,-0x38(%rbp)
    5466:	e8 00 00 00 00       	callq  546b <vm_insert_page+0xbb>
    546b:	49 89 c7             	mov    %rax,%r15
	spinlock_t *ptl;

	retval = -EINVAL;
	if (PageAnon(page))
	  goto out;
	retval = -ENOMEM;
    546e:	b8 f4 ff ff ff       	mov    $0xfffffff4,%eax
	flush_dcache_page(page);
	pte = get_locked_pte(mm, addr, &ptl);
	if (!pte)
    5473:	4d 85 ff             	test   %r15,%r15
    5476:	74 cb                	je     5443 <vm_insert_page+0x93>
	  goto out;
	retval = -EBUSY;
	if (!pte_none(*pte))
    5478:	49 83 3f 00          	cmpq   $0x0,(%r15)
    547c:	48 8b 4d c8          	mov    -0x38(%rbp),%rcx
    5480:	0f 85 da 00 00 00    	jne    5560 <vm_insert_page+0x1b0>
    5486:	49 8b 04 24          	mov    (%r12),%rax
void get_huge_page_tail(struct page *page);/*ziqiao*/
extern bool __get_page_tail(struct page *page);

static inline void get_page(struct page *page)
{
	if (unlikely(PageTail(page)))
    548a:	f6 c4 80             	test   $0x80,%ah
    548d:	0f 85 3e 01 00 00    	jne    55d1 <vm_insert_page+0x221>
 *
 * Atomically increments @v by 1.
 */
static inline void atomic_inc(atomic_t *v)
{
	asm volatile(LOCK_PREFIX "incl %0"
    5493:	f0 41 ff 44 24 1c    	lock incl 0x1c(%r12)
	  goto out_unlock;

	/* Ok, finally just insert the thing.. */
	get_page(page);
	inc_mm_counter_fast(mm, MM_FILEPAGES);
    5499:	ba 01 00 00 00       	mov    $0x1,%edx
    549e:	31 f6                	xor    %esi,%esi
    54a0:	4c 89 f7             	mov    %r14,%rdi
    54a3:	48 89 4d c8          	mov    %rcx,-0x38(%rbp)
    54a7:	e8 b4 ac ff ff       	callq  160 <add_mm_counter_fast>
	page_add_file_rmap(page);
    54ac:	4c 89 e7             	mov    %r12,%rdi
    54af:	e8 00 00 00 00       	callq  54b4 <vm_insert_page+0x104>
	set_pte_at(mm, addr, pte, mk_pte(page, prot));
    54b4:	48 8b 4d c8          	mov    -0x38(%rbp),%rcx
    54b8:	48 b8 00 00 00 00 00 	movabs $0x4000000000000,%rax
    54bf:	00 04 00 
    54c2:	48 21 c8             	and    %rcx,%rax
    54c5:	48 89 c2             	mov    %rax,%rdx
    54c8:	0f 84 c2 00 00 00    	je     5590 <vm_insert_page+0x1e0>
    54ce:	49 8b 04 24          	mov    (%r12),%rax
    54d2:	48 ba 00 00 00 00 00 	movabs $0x8000000000000,%rdx
    54d9:	00 08 00 
    54dc:	a9 00 00 00 02       	test   $0x2000000,%eax
    54e1:	b8 00 00 00 00       	mov    $0x0,%eax
    54e6:	48 0f 44 d0          	cmove  %rax,%rdx
    54ea:	48 89 c8             	mov    %rcx,%rax
    54ed:	48 83 c8 10          	or     $0x10,%rax
    54f1:	48 09 d0             	or     %rdx,%rax
static inline pgprotval_t massage_pgprot(pgprot_t pgprot)
{
	pgprotval_t protval = pgprot_val(pgprot);

	if (protval & _PAGE_PRESENT)
	  protval &= __supported_pte_mask;
    54f4:	48 89 c7             	mov    %rax,%rdi
    54f7:	49 b8 00 00 00 00 00 	movabs $0x160000000000,%r8
    54fe:	16 00 00 
    5501:	48 23 3d 00 00 00 00 	and    0x0(%rip),%rdi        # 5508 <vm_insert_page+0x158>
    5508:	4d 01 e0             	add    %r12,%r8
    550b:	49 c1 f8 06          	sar    $0x6,%r8
	return protval;
}

static inline pte_t pfn_pte(unsigned long page_nr, pgprot_t pgprot)
{
	return __pte(((phys_addr_t)page_nr << PAGE_SHIFT) |
    550f:	49 c1 e0 0c          	shl    $0xc,%r8
static inline pgprotval_t massage_pgprot(pgprot_t pgprot)
{
	pgprotval_t protval = pgprot_val(pgprot);

	if (protval & _PAGE_PRESENT)
	  protval &= __supported_pte_mask;
    5513:	a8 01                	test   $0x1,%al
    5515:	48 0f 44 f8          	cmove  %rax,%rdi
	return protval;
}

static inline pte_t pfn_pte(unsigned long page_nr, pgprot_t pgprot)
{
	return __pte(((phys_addr_t)page_nr << PAGE_SHIFT) |
    5519:	4c 09 c7             	or     %r8,%rdi
	if (sizeof(pteval_t) > sizeof(long))
		ret = PVOP_CALLEE2(pteval_t,
				   pv_mmu_ops.make_pte,
				   val, (u64)val >> 32);
	else
		ret = PVOP_CALLEE1(pteval_t,
    551c:	ff 14 25 00 00 00 00 	callq  *0x0
    5523:	48 89 c1             	mov    %rax,%rcx
{
	if (sizeof(pteval_t) > sizeof(long))
		/* 5 arg words */
		pv_mmu_ops.set_pte_at(mm, addr, ptep, pte);
	else
		PVOP_VCALL4(pv_mmu_ops.set_pte_at, mm, addr, ptep, pte.pte);
    5526:	4c 89 f7             	mov    %r14,%rdi
    5529:	4c 89 ee             	mov    %r13,%rsi
    552c:	4c 89 fa             	mov    %r15,%rdx
    552f:	ff 14 25 00 00 00 00 	callq  *0x0
	raw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \
} while (0)

static inline void spin_unlock(spinlock_t *lock)
{
	raw_spin_unlock(&lock->rlock);
    5536:	48 8b 7d d0          	mov    -0x30(%rbp),%rdi
    553a:	e8 00 00 00 00       	callq  553f <vm_insert_page+0x18f>

	retval = 0;
	pte_unmap_unlock(pte, ptl);
	inc_page_counter_in_ns(page,vma);
    553f:	48 89 de             	mov    %rbx,%rsi
    5542:	4c 89 e7             	mov    %r12,%rdi
    5545:	e8 00 00 00 00       	callq  554a <vm_insert_page+0x19a>
		BUG_ON(down_read_trylock(&vma->vm_mm->mmap_sem));
		BUG_ON(vma->vm_flags & VM_PFNMAP);
		vma->vm_flags |= VM_MIXEDMAP;
	}
	return insert_page(vma, addr, page, vma->vm_page_prot);
}
    554a:	48 83 c4 10          	add    $0x10,%rsp

	retval = 0;
	pte_unmap_unlock(pte, ptl);
	inc_page_counter_in_ns(page,vma);

	return retval;
    554e:	31 c0                	xor    %eax,%eax
		BUG_ON(down_read_trylock(&vma->vm_mm->mmap_sem));
		BUG_ON(vma->vm_flags & VM_PFNMAP);
		vma->vm_flags |= VM_MIXEDMAP;
	}
	return insert_page(vma, addr, page, vma->vm_page_prot);
}
    5550:	5b                   	pop    %rbx
    5551:	41 5c                	pop    %r12
    5553:	41 5d                	pop    %r13
    5555:	41 5e                	pop    %r14
    5557:	41 5f                	pop    %r15
    5559:	5d                   	pop    %rbp
    555a:	c3                   	retq   
    555b:	0f 1f 44 00 00       	nopl   0x0(%rax,%rax,1)
    5560:	48 8b 7d d0          	mov    -0x30(%rbp),%rdi
    5564:	e8 00 00 00 00       	callq  5569 <vm_insert_page+0x1b9>
    5569:	48 83 c4 10          	add    $0x10,%rsp
	retval = -ENOMEM;
	flush_dcache_page(page);
	pte = get_locked_pte(mm, addr, &ptl);
	if (!pte)
	  goto out;
	retval = -EBUSY;
    556d:	b8 f0 ff ff ff       	mov    $0xfffffff0,%eax
		BUG_ON(down_read_trylock(&vma->vm_mm->mmap_sem));
		BUG_ON(vma->vm_flags & VM_PFNMAP);
		vma->vm_flags |= VM_MIXEDMAP;
	}
	return insert_page(vma, addr, page, vma->vm_page_prot);
}
    5572:	5b                   	pop    %rbx
    5573:	41 5c                	pop    %r12
    5575:	41 5d                	pop    %r13
    5577:	41 5e                	pop    %r14
    5579:	41 5f                	pop    %r15
    557b:	5d                   	pop    %rbp
    557c:	c3                   	retq   
    557d:	0f 1f 00             	nopl   (%rax)
 */
int vm_insert_page(struct vm_area_struct *vma, unsigned long addr,
			struct page *page)
{
	if (addr < vma->vm_start || addr >= vma->vm_end)
	  return -EFAULT;
    5580:	b8 f2 ff ff ff       	mov    $0xfffffff2,%eax
    5585:	e9 b9 fe ff ff       	jmpq   5443 <vm_insert_page+0x93>
    558a:	66 0f 1f 44 00 00    	nopw   0x0(%rax,%rax,1)
    5590:	49 8b 34 24          	mov    (%r12),%rsi

	/* Ok, finally just insert the thing.. */
	get_page(page);
	inc_mm_counter_fast(mm, MM_FILEPAGES);
	page_add_file_rmap(page);
	set_pte_at(mm, addr, pte, mk_pte(page, prot));
    5594:	48 b8 00 00 00 00 00 	movabs $0x8000000000000,%rax
    559b:	00 08 00 
    559e:	f7 c6 00 00 00 02    	test   $0x2000000,%esi
    55a4:	48 0f 44 c2          	cmove  %rdx,%rax
    55a8:	48 09 c8             	or     %rcx,%rax
    55ab:	e9 44 ff ff ff       	jmpq   54f4 <vm_insert_page+0x144>
			struct page *page)
{
	if (addr < vma->vm_start || addr >= vma->vm_end)
	  return -EFAULT;
	if (!page_count(page))
	  return -EINVAL;
    55b0:	b8 ea ff ff ff       	mov    $0xffffffea,%eax
    55b5:	e9 89 fe ff ff       	jmpq   5443 <vm_insert_page+0x93>
}

static inline struct page *compound_head(struct page *page)
{
	if (unlikely(PageTail(page))) {
		struct page *head = page->first_page;
    55ba:	48 8b 42 30          	mov    0x30(%rdx),%rax
    55be:	48 8b 12             	mov    (%rdx),%rdx
		 * compound page, so recheck that it is still a tail
		 * page before returning.
		 */
		smp_rmb();
		if (likely(PageTail(page)))
			return head;
    55c1:	80 e6 80             	and    $0x80,%dh
    55c4:	49 0f 44 c4          	cmove  %r12,%rax
    55c8:	e9 24 fe ff ff       	jmpq   53f1 <vm_insert_page+0x41>
	if (!(vma->vm_flags & VM_MIXEDMAP)) {
		BUG_ON(down_read_trylock(&vma->vm_mm->mmap_sem));
		BUG_ON(vma->vm_flags & VM_PFNMAP);
    55cd:	0f 0b                	ud2    
	if (addr < vma->vm_start || addr >= vma->vm_end)
	  return -EFAULT;
	if (!page_count(page))
	  return -EINVAL;
	if (!(vma->vm_flags & VM_MIXEDMAP)) {
		BUG_ON(down_read_trylock(&vma->vm_mm->mmap_sem));
    55cf:	0f 0b                	ud2    
extern bool __get_page_tail(struct page *page);

static inline void get_page(struct page *page)
{
	if (unlikely(PageTail(page)))
		if (likely(__get_page_tail(page)))
    55d1:	4c 89 e7             	mov    %r12,%rdi
    55d4:	48 89 4d c8          	mov    %rcx,-0x38(%rbp)
    55d8:	e8 00 00 00 00       	callq  55dd <vm_insert_page+0x22d>
    55dd:	84 c0                	test   %al,%al
    55df:	48 8b 4d c8          	mov    -0x38(%rbp),%rcx
    55e3:	0f 85 b0 fe ff ff    	jne    5499 <vm_insert_page+0xe9>
    55e9:	e9 a5 fe ff ff       	jmpq   5493 <vm_insert_page+0xe3>
    55ee:	66 90                	xchg   %ax,%ax

00000000000055f0 <insert_pfn.isra.65>:
	}
	return insert_page(vma, addr, page, vma->vm_page_prot);
}
EXPORT_SYMBOL(vm_insert_page);

static int insert_pfn(struct vm_area_struct *vma, unsigned long addr,
    55f0:	e8 00 00 00 00       	callq  55f5 <insert_pfn.isra.65+0x5>
    55f5:	55                   	push   %rbp
    55f6:	48 89 e5             	mov    %rsp,%rbp
    55f9:	41 57                	push   %r15
    55fb:	49 89 d7             	mov    %rdx,%r15
			       spinlock_t **ptl);
static inline pte_t *get_locked_pte(struct mm_struct *mm, unsigned long addr,
				    spinlock_t **ptl)
{
	pte_t *ptep;
	__cond_lock(*ptl, ptep = __get_locked_pte(mm, addr, ptl));
    55fe:	48 8d 55 d0          	lea    -0x30(%rbp),%rdx
    5602:	41 56                	push   %r14
    5604:	41 55                	push   %r13
    5606:	49 89 f5             	mov    %rsi,%r13
    5609:	41 54                	push   %r12
    560b:	49 89 fc             	mov    %rdi,%r12
    560e:	53                   	push   %rbx
    560f:	48 89 cb             	mov    %rcx,%rbx
    5612:	48 83 ec 08          	sub    $0x8,%rsp
    5616:	e8 00 00 00 00       	callq  561b <insert_pfn.isra.65+0x2b>
	pte_t *pte, entry;
	spinlock_t *ptl;

	retval = -ENOMEM;
	pte = get_locked_pte(mm, addr, &ptl);
	if (!pte)
    561b:	48 85 c0             	test   %rax,%rax
    561e:	48 89 c2             	mov    %rax,%rdx
    5621:	74 64                	je     5687 <insert_pfn.isra.65+0x97>
	  goto out;
	retval = -EBUSY;
	if (!pte_none(*pte))
    5623:	48 83 38 00          	cmpq   $0x0,(%rax)

	retval = -ENOMEM;
	pte = get_locked_pte(mm, addr, &ptl);
	if (!pte)
	  goto out;
	retval = -EBUSY;
    5627:	41 be f0 ff ff ff    	mov    $0xfffffff0,%r14d
	if (!pte_none(*pte))
    562d:	74 21                	je     5650 <insert_pfn.isra.65+0x60>
    562f:	48 8b 7d d0          	mov    -0x30(%rbp),%rdi
    5633:	e8 00 00 00 00       	callq  5638 <insert_pfn.isra.65+0x48>
	retval = 0;
out_unlock:
	pte_unmap_unlock(pte, ptl);
out:
	return retval;
}
    5638:	48 83 c4 08          	add    $0x8,%rsp
    563c:	44 89 f0             	mov    %r14d,%eax
    563f:	5b                   	pop    %rbx
    5640:	41 5c                	pop    %r12
    5642:	41 5d                	pop    %r13
    5644:	41 5e                	pop    %r14
    5646:	41 5f                	pop    %r15
    5648:	5d                   	pop    %rbp
    5649:	c3                   	retq   
    564a:	66 0f 1f 44 00 00    	nopw   0x0(%rax,%rax,1)
static inline pgprotval_t massage_pgprot(pgprot_t pgprot)
{
	pgprotval_t protval = pgprot_val(pgprot);

	if (protval & _PAGE_PRESENT)
	  protval &= __supported_pte_mask;
    5650:	48 89 df             	mov    %rbx,%rdi
    5653:	48 23 3d 00 00 00 00 	and    0x0(%rip),%rdi        # 565a <insert_pfn.isra.65+0x6a>
	return protval;
}

static inline pte_t pfn_pte(unsigned long page_nr, pgprot_t pgprot)
{
	return __pte(((phys_addr_t)page_nr << PAGE_SHIFT) |
    565a:	49 c1 e7 0c          	shl    $0xc,%r15
static inline pgprotval_t massage_pgprot(pgprot_t pgprot)
{
	pgprotval_t protval = pgprot_val(pgprot);

	if (protval & _PAGE_PRESENT)
	  protval &= __supported_pte_mask;
    565e:	f6 c3 01             	test   $0x1,%bl
    5661:	48 0f 44 fb          	cmove  %rbx,%rdi
	return protval;
}

static inline pte_t pfn_pte(unsigned long page_nr, pgprot_t pgprot)
{
	return __pte(((phys_addr_t)page_nr << PAGE_SHIFT) |
    5665:	4c 09 ff             	or     %r15,%rdi
	if (sizeof(pteval_t) > sizeof(long))
		ret = PVOP_CALLEE2(pteval_t,
				   pv_mmu_ops.make_pte,
				   val, (u64)val >> 32);
	else
		ret = PVOP_CALLEE1(pteval_t,
    5668:	ff 14 25 00 00 00 00 	callq  *0x0
    566f:	48 89 c1             	mov    %rax,%rcx
{
	if (sizeof(pteval_t) > sizeof(long))
		/* 5 arg words */
		pv_mmu_ops.set_pte_at(mm, addr, ptep, pte);
	else
		PVOP_VCALL4(pv_mmu_ops.set_pte_at, mm, addr, ptep, pte.pte);
    5672:	4c 89 e7             	mov    %r12,%rdi
    5675:	4c 89 ee             	mov    %r13,%rsi

static inline pte_t pte_set_flags(pte_t pte, pteval_t set)
{
	pteval_t v = native_pte_val(pte);

	return native_make_pte(v | set);
    5678:	80 cd 02             	or     $0x2,%ch
    567b:	ff 14 25 00 00 00 00 	callq  *0x0
	/* Ok, finally just insert the thing.. */
	entry = pte_mkspecial(pfn_pte(pfn, prot));
	set_pte_at(mm, addr, pte, entry);
	update_mmu_cache(vma, addr, pte); /* XXX: why not for insert_page? */

	retval = 0;
    5682:	45 31 f6             	xor    %r14d,%r14d
    5685:	eb a8                	jmp    562f <insert_pfn.isra.65+0x3f>
	struct mm_struct *mm = vma->vm_mm;
	int retval;
	pte_t *pte, entry;
	spinlock_t *ptl;

	retval = -ENOMEM;
    5687:	41 be f4 ff ff ff    	mov    $0xfffffff4,%r14d
    568d:	eb a9                	jmp    5638 <insert_pfn.isra.65+0x48>
    568f:	90                   	nop

0000000000005690 <vm_insert_pfn>:
 * As this is called only for pages that do not currently exist, we
 * do not need to flush old virtual caches or the TLB.
 */
int vm_insert_pfn(struct vm_area_struct *vma, unsigned long addr,
			unsigned long pfn)
{
    5690:	e8 00 00 00 00       	callq  5695 <vm_insert_pfn+0x5>
    5695:	55                   	push   %rbp
    5696:	48 89 e5             	mov    %rsp,%rbp
    5699:	41 55                	push   %r13
    569b:	49 89 d5             	mov    %rdx,%r13
    569e:	41 54                	push   %r12
    56a0:	53                   	push   %rbx
    56a1:	48 89 fb             	mov    %rdi,%rbx
    56a4:	48 83 ec 08          	sub    $0x8,%rsp
	int ret;
	pgprot_t pgprot = vma->vm_page_prot;
    56a8:	48 8b 47 48          	mov    0x48(%rdi),%rax
    56ac:	48 89 45 e0          	mov    %rax,-0x20(%rbp)
	 * Technically, architectures with pte_special can avoid all these
	 * restrictions (same for remap_pfn_range).  However we would like
	 * consistency in testing and feature parity among all, so we should
	 * try to keep these invariants in place for everybody.
	 */
	BUG_ON(!(vma->vm_flags & (VM_PFNMAP|VM_MIXEDMAP)));
    56b0:	48 8b 47 50          	mov    0x50(%rdi),%rax
    56b4:	48 89 c2             	mov    %rax,%rdx
    56b7:	81 e2 00 04 00 10    	and    $0x10000400,%edx
    56bd:	74 68                	je     5727 <vm_insert_pfn+0x97>
	BUG_ON((vma->vm_flags & (VM_PFNMAP|VM_MIXEDMAP)) ==
    56bf:	48 81 fa 00 04 00 10 	cmp    $0x10000400,%rdx
    56c6:	0f 84 a5 00 00 00    	je     5771 <vm_insert_pfn+0xe1>
				(VM_PFNMAP|VM_MIXEDMAP));
	BUG_ON((vma->vm_flags & VM_PFNMAP) && is_cow_mapping(vma->vm_flags));
    56cc:	f6 c4 04             	test   $0x4,%ah
    56cf:	49 89 f4             	mov    %rsi,%r12
    56d2:	0f 85 87 00 00 00    	jne    575f <vm_insert_pfn+0xcf>
	BUG_ON((vma->vm_flags & VM_MIXEDMAP) && pfn_valid(pfn));
    56d8:	a9 00 00 00 10       	test   $0x10000000,%eax
    56dd:	75 4a                	jne    5729 <vm_insert_pfn+0x99>

	if (addr < vma->vm_start || addr >= vma->vm_end)
    56df:	4c 39 23             	cmp    %r12,(%rbx)
	  return -EFAULT;
    56e2:	b8 f2 ff ff ff       	mov    $0xfffffff2,%eax
	BUG_ON((vma->vm_flags & (VM_PFNMAP|VM_MIXEDMAP)) ==
				(VM_PFNMAP|VM_MIXEDMAP));
	BUG_ON((vma->vm_flags & VM_PFNMAP) && is_cow_mapping(vma->vm_flags));
	BUG_ON((vma->vm_flags & VM_MIXEDMAP) && pfn_valid(pfn));

	if (addr < vma->vm_start || addr >= vma->vm_end)
    56e7:	77 33                	ja     571c <vm_insert_pfn+0x8c>
    56e9:	4c 3b 63 08          	cmp    0x8(%rbx),%r12
    56ed:	73 2d                	jae    571c <vm_insert_pfn+0x8c>
	  return -EFAULT;
	if (track_pfn_insert(vma, &pgprot, pfn))
    56ef:	48 8d 75 e0          	lea    -0x20(%rbp),%rsi
    56f3:	4c 89 ea             	mov    %r13,%rdx
    56f6:	48 89 df             	mov    %rbx,%rdi
    56f9:	e8 00 00 00 00       	callq  56fe <vm_insert_pfn+0x6e>
    56fe:	89 c2                	mov    %eax,%edx
	  return -EINVAL;
    5700:	b8 ea ff ff ff       	mov    $0xffffffea,%eax
	BUG_ON((vma->vm_flags & VM_PFNMAP) && is_cow_mapping(vma->vm_flags));
	BUG_ON((vma->vm_flags & VM_MIXEDMAP) && pfn_valid(pfn));

	if (addr < vma->vm_start || addr >= vma->vm_end)
	  return -EFAULT;
	if (track_pfn_insert(vma, &pgprot, pfn))
    5705:	85 d2                	test   %edx,%edx
    5707:	75 13                	jne    571c <vm_insert_pfn+0x8c>
	  return -EINVAL;

	ret = insert_pfn(vma, addr, pfn, pgprot);
    5709:	48 8b 7b 40          	mov    0x40(%rbx),%rdi
    570d:	48 8b 4d e0          	mov    -0x20(%rbp),%rcx
    5711:	4c 89 ea             	mov    %r13,%rdx
    5714:	4c 89 e6             	mov    %r12,%rsi
    5717:	e8 d4 fe ff ff       	callq  55f0 <insert_pfn.isra.65>

	return ret;
}
    571c:	48 83 c4 08          	add    $0x8,%rsp
    5720:	5b                   	pop    %rbx
    5721:	41 5c                	pop    %r12
    5723:	41 5d                	pop    %r13
    5725:	5d                   	pop    %rbp
    5726:	c3                   	retq   
	 * Technically, architectures with pte_special can avoid all these
	 * restrictions (same for remap_pfn_range).  However we would like
	 * consistency in testing and feature parity among all, so we should
	 * try to keep these invariants in place for everybody.
	 */
	BUG_ON(!(vma->vm_flags & (VM_PFNMAP|VM_MIXEDMAP)));
    5727:	0f 0b                	ud2    
}

#ifndef CONFIG_HAVE_ARCH_PFN_VALID
static inline int pfn_valid(unsigned long pfn)
{
	if (pfn_to_section_nr(pfn) >= NR_MEM_SECTIONS)
    5729:	4c 89 e8             	mov    %r13,%rax
    572c:	48 c1 e8 0f          	shr    $0xf,%rax
    5730:	48 3d ff ff 07 00    	cmp    $0x7ffff,%rax
    5736:	77 a7                	ja     56df <vm_insert_pfn+0x4f>
extern struct mem_section mem_section[NR_SECTION_ROOTS][SECTIONS_PER_ROOT];
#endif

static inline struct mem_section *__nr_to_section(unsigned long nr)
{
	if (!mem_section[SECTION_NR_TO_ROOT(nr)])
    5738:	4c 89 ea             	mov    %r13,%rdx
    573b:	48 c1 ea 16          	shr    $0x16,%rdx
    573f:	48 8b 14 d5 00 00 00 	mov    0x0(,%rdx,8),%rdx
    5746:	00 
    5747:	48 85 d2             	test   %rdx,%rdx
    574a:	74 93                	je     56df <vm_insert_pfn+0x4f>
		return NULL;
	return &mem_section[SECTION_NR_TO_ROOT(nr)][nr & SECTION_ROOT_MASK];
    574c:	83 e0 7f             	and    $0x7f,%eax
    574f:	48 c1 e0 05          	shl    $0x5,%rax
	return present_section(__nr_to_section(nr));
}

static inline int valid_section(struct mem_section *section)
{
	return (section && (section->section_mem_map & SECTION_HAS_MEM_MAP));
    5753:	48 01 d0             	add    %rdx,%rax
    5756:	74 87                	je     56df <vm_insert_pfn+0x4f>
    5758:	f6 00 02             	testb  $0x2,(%rax)
    575b:	74 82                	je     56df <vm_insert_pfn+0x4f>
	BUG_ON((vma->vm_flags & (VM_PFNMAP|VM_MIXEDMAP)) ==
				(VM_PFNMAP|VM_MIXEDMAP));
	BUG_ON((vma->vm_flags & VM_PFNMAP) && is_cow_mapping(vma->vm_flags));
	BUG_ON((vma->vm_flags & VM_MIXEDMAP) && pfn_valid(pfn));
    575d:	0f 0b                	ud2    
	add_taint(TAINT_BAD_PAGE, LOCKDEP_NOW_UNRELIABLE);
}

static inline bool is_cow_mapping(vm_flags_t flags)
{
	return (flags & (VM_SHARED | VM_MAYWRITE)) == VM_MAYWRITE;
    575f:	48 89 c2             	mov    %rax,%rdx
    5762:	83 e2 28             	and    $0x28,%edx
	 * try to keep these invariants in place for everybody.
	 */
	BUG_ON(!(vma->vm_flags & (VM_PFNMAP|VM_MIXEDMAP)));
	BUG_ON((vma->vm_flags & (VM_PFNMAP|VM_MIXEDMAP)) ==
				(VM_PFNMAP|VM_MIXEDMAP));
	BUG_ON((vma->vm_flags & VM_PFNMAP) && is_cow_mapping(vma->vm_flags));
    5765:	48 83 fa 20          	cmp    $0x20,%rdx
    5769:	0f 85 69 ff ff ff    	jne    56d8 <vm_insert_pfn+0x48>
    576f:	0f 0b                	ud2    
	 * restrictions (same for remap_pfn_range).  However we would like
	 * consistency in testing and feature parity among all, so we should
	 * try to keep these invariants in place for everybody.
	 */
	BUG_ON(!(vma->vm_flags & (VM_PFNMAP|VM_MIXEDMAP)));
	BUG_ON((vma->vm_flags & (VM_PFNMAP|VM_MIXEDMAP)) ==
    5771:	0f 0b                	ud2    
    5773:	66 66 66 66 2e 0f 1f 	data32 data32 data32 nopw %cs:0x0(%rax,%rax,1)
    577a:	84 00 00 00 00 00 

0000000000005780 <vm_insert_mixed>:
}
EXPORT_SYMBOL(vm_insert_pfn);

int vm_insert_mixed(struct vm_area_struct *vma, unsigned long addr,
			unsigned long pfn)
{
    5780:	e8 00 00 00 00       	callq  5785 <vm_insert_mixed+0x5>
	BUG_ON(!(vma->vm_flags & VM_MIXEDMAP));
    5785:	f6 47 53 10          	testb  $0x10,0x53(%rdi)
    5789:	74 27                	je     57b2 <vm_insert_mixed+0x32>

	if (addr < vma->vm_start || addr >= vma->vm_end)
    578b:	48 39 37             	cmp    %rsi,(%rdi)
	  return -EFAULT;
    578e:	b8 f2 ff ff ff       	mov    $0xfffffff2,%eax
int vm_insert_mixed(struct vm_area_struct *vma, unsigned long addr,
			unsigned long pfn)
{
	BUG_ON(!(vma->vm_flags & VM_MIXEDMAP));

	if (addr < vma->vm_start || addr >= vma->vm_end)
    5793:	77 1b                	ja     57b0 <vm_insert_mixed+0x30>
    5795:	48 3b 77 08          	cmp    0x8(%rdi),%rsi
    5799:	73 12                	jae    57ad <vm_insert_mixed+0x2d>
}
EXPORT_SYMBOL(vm_insert_pfn);

int vm_insert_mixed(struct vm_area_struct *vma, unsigned long addr,
			unsigned long pfn)
{
    579b:	55                   	push   %rbp
		struct page *page;

		page = pfn_to_page(pfn);
		return insert_page(vma, addr, page, vma->vm_page_prot);
	}
	return insert_pfn(vma, addr, pfn, vma->vm_page_prot);
    579c:	48 8b 4f 48          	mov    0x48(%rdi),%rcx
    57a0:	48 8b 7f 40          	mov    0x40(%rdi),%rdi
}
EXPORT_SYMBOL(vm_insert_pfn);

int vm_insert_mixed(struct vm_area_struct *vma, unsigned long addr,
			unsigned long pfn)
{
    57a4:	48 89 e5             	mov    %rsp,%rbp
		struct page *page;

		page = pfn_to_page(pfn);
		return insert_page(vma, addr, page, vma->vm_page_prot);
	}
	return insert_pfn(vma, addr, pfn, vma->vm_page_prot);
    57a7:	e8 44 fe ff ff       	callq  55f0 <insert_pfn.isra.65>
}
    57ac:	5d                   	pop    %rbp
    57ad:	f3 c3                	repz retq 
    57af:	90                   	nop
    57b0:	f3 c3                	repz retq 
EXPORT_SYMBOL(vm_insert_pfn);

int vm_insert_mixed(struct vm_area_struct *vma, unsigned long addr,
			unsigned long pfn)
{
	BUG_ON(!(vma->vm_flags & VM_MIXEDMAP));
    57b2:	0f 0b                	ud2    
    57b4:	66 66 66 2e 0f 1f 84 	data32 data32 nopw %cs:0x0(%rax,%rax,1)
    57bb:	00 00 00 00 00 

00000000000057c0 <apply_to_page_range>:
 * Scan a region of virtual memory, filling in page tables as necessary
 * and calling a provided function on each leaf page table.
 */
int apply_to_page_range(struct mm_struct *mm, unsigned long addr,
			unsigned long size, pte_fn_t fn, void *data)
{
    57c0:	e8 00 00 00 00       	callq  57c5 <apply_to_page_range+0x5>
    57c5:	55                   	push   %rbp
	pgd_t *pgd;
	unsigned long next;
	unsigned long end = addr + size;
    57c6:	48 8d 04 16          	lea    (%rsi,%rdx,1),%rax
 * Scan a region of virtual memory, filling in page tables as necessary
 * and calling a provided function on each leaf page table.
 */
int apply_to_page_range(struct mm_struct *mm, unsigned long addr,
			unsigned long size, pte_fn_t fn, void *data)
{
    57ca:	48 89 e5             	mov    %rsp,%rbp
    57cd:	41 57                	push   %r15
    57cf:	41 56                	push   %r14
    57d1:	41 55                	push   %r13
    57d3:	41 54                	push   %r12
    57d5:	53                   	push   %rbx
    57d6:	48 83 ec 60          	sub    $0x60,%rsp
	pgd_t *pgd;
	unsigned long next;
	unsigned long end = addr + size;
	int err;

	BUG_ON(addr >= end);
    57da:	48 39 c6             	cmp    %rax,%rsi
 * Scan a region of virtual memory, filling in page tables as necessary
 * and calling a provided function on each leaf page table.
 */
int apply_to_page_range(struct mm_struct *mm, unsigned long addr,
			unsigned long size, pte_fn_t fn, void *data)
{
    57dd:	48 89 7d b8          	mov    %rdi,-0x48(%rbp)
    57e1:	4c 89 45 d0          	mov    %r8,-0x30(%rbp)
	pgd_t *pgd;
	unsigned long next;
	unsigned long end = addr + size;
    57e5:	48 89 45 80          	mov    %rax,-0x80(%rbp)
	int err;

	BUG_ON(addr >= end);
    57e9:	0f 83 e4 03 00 00    	jae    5bd3 <apply_to_page_range+0x413>
	pgd = pgd_offset(mm, addr);
    57ef:	48 89 f0             	mov    %rsi,%rax
	return pgd_flags(pgd) & _PAGE_PRESENT;
}

static inline unsigned long pgd_page_vaddr(pgd_t pgd)
{
	return (unsigned long)__va((unsigned long)pgd_val(pgd) & PTE_PFN_MASK);
    57f2:	49 89 f5             	mov    %rsi,%r13
    57f5:	49 89 cf             	mov    %rcx,%r15
    57f8:	48 c1 e8 24          	shr    $0x24,%rax
    57fc:	48 89 45 88          	mov    %rax,-0x78(%rbp)
    5800:	48 8b 45 b8          	mov    -0x48(%rbp),%rax
    5804:	48 81 65 88 f8 0f 00 	andq   $0xff8,-0x78(%rbp)
    580b:	00 
    580c:	48 8b 40 40          	mov    0x40(%rax),%rax
    5810:	48 01 45 88          	add    %rax,-0x78(%rbp)
    5814:	48 8b 45 80          	mov    -0x80(%rbp),%rax
    5818:	48 83 e8 01          	sub    $0x1,%rax
    581c:	48 89 85 78 ff ff ff 	mov    %rax,-0x88(%rbp)
	do {
		next = pgd_addr_end(addr, end);
    5823:	48 b8 00 00 00 00 80 	movabs $0x8000000000,%rax
    582a:	00 00 00 
    582d:	4c 01 e8             	add    %r13,%rax
    5830:	48 89 45 a0          	mov    %rax,-0x60(%rbp)
    5834:	48 b8 00 00 00 00 80 	movabs $0xffffff8000000000,%rax
    583b:	ff ff ff 
    583e:	48 21 45 a0          	and    %rax,-0x60(%rbp)
    5842:	48 8b 4d a0          	mov    -0x60(%rbp),%rcx
    5846:	48 89 c8             	mov    %rcx,%rax
    5849:	48 83 e8 01          	sub    $0x1,%rax
    584d:	48 3b 85 78 ff ff ff 	cmp    -0x88(%rbp),%rax
    5854:	48 89 c8             	mov    %rcx,%rax
    5857:	48 0f 43 45 80       	cmovae -0x80(%rbp),%rax
    585c:	48 89 45 a0          	mov    %rax,-0x60(%rbp)
    5860:	48 8b 45 88          	mov    -0x78(%rbp),%rax
    5864:	48 8b 00             	mov    (%rax),%rax
 */
#if defined(CONFIG_MMU) && !defined(__ARCH_HAS_4LEVEL_HACK)
static inline pud_t *pud_alloc(struct mm_struct *mm, pgd_t *pgd, unsigned long address)
{
	return (unlikely(pgd_none(*pgd)) && __pud_alloc(mm, pgd, address))?
		NULL: pud_offset(pgd, address);
    5867:	48 85 c0             	test   %rax,%rax
    586a:	0f 84 40 03 00 00    	je     5bb0 <apply_to_page_range+0x3f0>
    5870:	48 89 c7             	mov    %rax,%rdi

	if (sizeof(pgdval_t) > sizeof(long))
		ret =  PVOP_CALLEE2(pgdval_t, pv_mmu_ops.pgd_val,
				    pgd.pgd, (u64)pgd.pgd >> 32);
	else
		ret =  PVOP_CALLEE1(pgdval_t, pv_mmu_ops.pgd_val,
    5873:	ff 14 25 00 00 00 00 	callq  *0x0
#define pgd_page(pgd)		pfn_to_page(pgd_val(pgd) >> PAGE_SHIFT)

/* to find an entry in a page-table-directory. */
static inline unsigned long pud_index(unsigned long address)
{
	return (address >> PUD_SHIFT) & (PTRS_PER_PUD - 1);
    587a:	4c 89 ea             	mov    %r13,%rdx
	return pgd_flags(pgd) & _PAGE_PRESENT;
}

static inline unsigned long pgd_page_vaddr(pgd_t pgd)
{
	return (unsigned long)__va((unsigned long)pgd_val(pgd) & PTE_PFN_MASK);
    587d:	48 b9 00 00 00 00 00 	movabs $0xffff880000000000,%rcx
    5884:	88 ff ff 
    5887:	48 be 00 f0 ff ff ff 	movabs $0x3ffffffff000,%rsi
    588e:	3f 00 00 
#define pgd_page(pgd)		pfn_to_page(pgd_val(pgd) >> PAGE_SHIFT)

/* to find an entry in a page-table-directory. */
static inline unsigned long pud_index(unsigned long address)
{
	return (address >> PUD_SHIFT) & (PTRS_PER_PUD - 1);
    5891:	48 c1 ea 1b          	shr    $0x1b,%rdx
	return pgd_flags(pgd) & _PAGE_PRESENT;
}

static inline unsigned long pgd_page_vaddr(pgd_t pgd)
{
	return (unsigned long)__va((unsigned long)pgd_val(pgd) & PTE_PFN_MASK);
    5895:	48 21 f0             	and    %rsi,%rax
	return (address >> PUD_SHIFT) & (PTRS_PER_PUD - 1);
}

static inline pud_t *pud_offset(pgd_t *pgd, unsigned long address)
{
	return (pud_t *)pgd_page_vaddr(*pgd) + pud_index(address);
    5898:	81 e2 f8 0f 00 00    	and    $0xff8,%edx
	return pgd_flags(pgd) & _PAGE_PRESENT;
}

static inline unsigned long pgd_page_vaddr(pgd_t pgd)
{
	return (unsigned long)__va((unsigned long)pgd_val(pgd) & PTE_PFN_MASK);
    589e:	48 01 ca             	add    %rcx,%rdx
	pud_t *pud;
	unsigned long next;
	int err;

	pud = pud_alloc(mm, pgd, addr);
	if (!pud)
    58a1:	48 01 c2             	add    %rax,%rdx
    58a4:	48 89 55 98          	mov    %rdx,-0x68(%rbp)
    58a8:	0f 84 41 02 00 00    	je     5aef <apply_to_page_range+0x32f>
    58ae:	48 8b 45 a0          	mov    -0x60(%rbp),%rax
    58b2:	48 83 e8 01          	sub    $0x1,%rax
    58b6:	48 89 45 90          	mov    %rax,-0x70(%rbp)
	  return -ENOMEM;
	do {
		next = pud_addr_end(addr, end);
    58ba:	49 8d 85 00 00 00 40 	lea    0x40000000(%r13),%rax
    58c1:	48 89 45 c0          	mov    %rax,-0x40(%rbp)
    58c5:	48 81 65 c0 00 00 00 	andq   $0xffffffffc0000000,-0x40(%rbp)
    58cc:	c0 
    58cd:	48 8b 4d c0          	mov    -0x40(%rbp),%rcx
    58d1:	48 89 c8             	mov    %rcx,%rax
    58d4:	48 83 e8 01          	sub    $0x1,%rax
    58d8:	48 3b 45 90          	cmp    -0x70(%rbp),%rax
    58dc:	48 89 c8             	mov    %rcx,%rax
    58df:	48 0f 43 45 a0       	cmovae -0x60(%rbp),%rax
    58e4:	48 89 45 c0          	mov    %rax,-0x40(%rbp)
    58e8:	48 8b 45 98          	mov    -0x68(%rbp),%rax
    58ec:	48 8b 00             	mov    (%rax),%rax
}

static inline pmd_t *pmd_alloc(struct mm_struct *mm, pud_t *pud, unsigned long address)
{
	return (unlikely(pud_none(*pud)) && __pmd_alloc(mm, pud, address))?
		NULL: pmd_offset(pud, address);
    58ef:	48 85 c0             	test   %rax,%rax
    58f2:	0f 84 94 02 00 00    	je     5b8c <apply_to_page_range+0x3cc>
    58f8:	48 89 c7             	mov    %rax,%rdi

	if (sizeof(pudval_t) > sizeof(long))
		ret =  PVOP_CALLEE2(pudval_t, pv_mmu_ops.pud_val,
				    pud.pud, (u64)pud.pud >> 32);
	else
		ret =  PVOP_CALLEE1(pudval_t, pv_mmu_ops.pud_val,
    58fb:	ff 14 25 00 00 00 00 	callq  *0x0
 * this macro returns the index of the entry in the pmd page which would
 * control the given virtual address
 */
static inline unsigned long pmd_index(unsigned long address)
{
	return (address >> PMD_SHIFT) & (PTRS_PER_PMD - 1);
    5902:	4c 89 ea             	mov    %r13,%rdx
	return pud_flags(pud) & _PAGE_PRESENT;
}

static inline unsigned long pud_page_vaddr(pud_t pud)
{
	return (unsigned long)__va((unsigned long)pud_val(pud) & PTE_PFN_MASK);
    5905:	48 b9 00 00 00 00 00 	movabs $0xffff880000000000,%rcx
    590c:	88 ff ff 
 * this macro returns the index of the entry in the pmd page which would
 * control the given virtual address
 */
static inline unsigned long pmd_index(unsigned long address)
{
	return (address >> PMD_SHIFT) & (PTRS_PER_PMD - 1);
    590f:	48 c1 ea 12          	shr    $0x12,%rdx
#define pud_page(pud)		pfn_to_page(pud_val(pud) >> PAGE_SHIFT)

/* Find an entry in the second-level page table.. */
static inline pmd_t *pmd_offset(pud_t *pud, unsigned long address)
{
	return (pmd_t *)pud_page_vaddr(*pud) + pmd_index(address);
    5913:	81 e2 f8 0f 00 00    	and    $0xff8,%edx
	return pud_flags(pud) & _PAGE_PRESENT;
}

static inline unsigned long pud_page_vaddr(pud_t pud)
{
	return (unsigned long)__va((unsigned long)pud_val(pud) & PTE_PFN_MASK);
    5919:	48 01 ca             	add    %rcx,%rdx
    591c:	48 b9 00 f0 ff ff ff 	movabs $0x3ffffffff000,%rcx
    5923:	3f 00 00 
    5926:	48 21 c8             	and    %rcx,%rax
	int err;

	BUG_ON(pud_huge(*pud));

	pmd = pmd_alloc(mm, pud, addr);
	if (!pmd)
    5929:	48 01 c2             	add    %rax,%rdx
    592c:	48 89 55 c8          	mov    %rdx,-0x38(%rbp)
    5930:	0f 84 b9 01 00 00    	je     5aef <apply_to_page_range+0x32f>
    5936:	48 8b 45 c0          	mov    -0x40(%rbp),%rax
    593a:	48 83 e8 01          	sub    $0x1,%rax
    593e:	48 89 45 a8          	mov    %rax,-0x58(%rbp)
    5942:	66 0f 1f 44 00 00    	nopw   0x0(%rax,%rax,1)
	  return -ENOMEM;
	do {
		next = pmd_addr_end(addr, end);
    5948:	4d 8d a5 00 00 20 00 	lea    0x200000(%r13),%r12
    594f:	49 81 e4 00 00 e0 ff 	and    $0xffffffffffe00000,%r12
    5956:	49 8d 44 24 ff       	lea    -0x1(%r12),%rax
    595b:	48 39 45 a8          	cmp    %rax,-0x58(%rbp)
    595f:	48 8b 45 c8          	mov    -0x38(%rbp),%rax
    5963:	4c 0f 46 65 c0       	cmovbe -0x40(%rbp),%r12
	int err;
	pgtable_t token;
	spinlock_t *uninitialized_var(ptl);

	pte = (mm == &init_mm) ?
		pte_alloc_kernel(pmd, addr) :
    5968:	48 81 7d b8 00 00 00 	cmpq   $0x0,-0x48(%rbp)
    596f:	00 
    5970:	48 8b 00             	mov    (%rax),%rax
    5973:	0f 84 27 01 00 00    	je     5aa0 <apply_to_page_range+0x2e0>
		pte_alloc_map_lock(mm, pmd, addr, &ptl);
    5979:	48 85 c0             	test   %rax,%rax
    597c:	0f 84 e4 01 00 00    	je     5b66 <apply_to_page_range+0x3a6>
    5982:	48 89 c7             	mov    %rax,%rdi

	if (sizeof(pmdval_t) > sizeof(long))
		ret =  PVOP_CALLEE2(pmdval_t, pv_mmu_ops.pmd_val,
				    pmd.pmd, (u64)pmd.pmd >> 32);
	else
		ret =  PVOP_CALLEE1(pmdval_t, pv_mmu_ops.pmd_val,
    5985:	ff 14 25 00 00 00 00 	callq  *0x0
}
#endif /* ALLOC_SPLIT_PTLOCKS */

static inline spinlock_t *pte_lockptr(struct mm_struct *mm, pmd_t *pmd)
{
	return ptlock_ptr(pmd_page(*pmd));
    598c:	48 be 00 f0 ff ff ff 	movabs $0x3ffffffff000,%rsi
    5993:	3f 00 00 
    5996:	48 b9 00 00 00 00 00 	movabs $0xffffea0000000000,%rcx
    599d:	ea ff ff 
    59a0:	48 21 f0             	and    %rsi,%rax
    59a3:	48 c1 e8 06          	shr    $0x6,%rax
    59a7:	48 8b 44 08 30       	mov    0x30(%rax,%rcx,1),%rax
    59ac:	48 89 c1             	mov    %rax,%rcx
    59af:	48 89 45 b0          	mov    %rax,-0x50(%rbp)
    59b3:	48 8b 45 c8          	mov    -0x38(%rbp),%rax
    59b7:	48 8b 38             	mov    (%rax),%rdi
    59ba:	ff 14 25 00 00 00 00 	callq  *0x0
 * this function returns the index of the entry in the pte page which would
 * control the given virtual address
 */
static inline unsigned long pte_index(unsigned long address)
{
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
    59c1:	4c 89 eb             	mov    %r13,%rbx
	return (unsigned long)native_pmd_val(pmd) == 0;
}

static inline unsigned long pmd_page_vaddr(pmd_t pmd)
{
	return (unsigned long)__va(pmd_val(pmd) & PTE_PFN_MASK);
    59c4:	48 ba 00 00 00 00 00 	movabs $0xffff880000000000,%rdx
    59cb:	88 ff ff 
    59ce:	48 21 f0             	and    %rsi,%rax
 * this function returns the index of the entry in the pte page which would
 * control the given virtual address
 */
static inline unsigned long pte_index(unsigned long address)
{
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
    59d1:	48 c1 eb 09          	shr    $0x9,%rbx
	raw_spin_lock_init(&(_lock)->rlock);		\
} while (0)

static inline void spin_lock(spinlock_t *lock)
{
	raw_spin_lock(&lock->rlock);
    59d5:	48 89 cf             	mov    %rcx,%rdi
}

static inline pte_t *pte_offset_kernel(pmd_t *pmd, unsigned long address)
{
	return (pte_t *)pmd_page_vaddr(*pmd) + pte_index(address);
    59d8:	81 e3 f8 0f 00 00    	and    $0xff8,%ebx
	return (unsigned long)native_pmd_val(pmd) == 0;
}

static inline unsigned long pmd_page_vaddr(pmd_t pmd)
{
	return (unsigned long)__va(pmd_val(pmd) & PTE_PFN_MASK);
    59de:	48 01 d3             	add    %rdx,%rbx
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
}

static inline pte_t *pte_offset_kernel(pmd_t *pmd, unsigned long address)
{
	return (pte_t *)pmd_page_vaddr(*pmd) + pte_index(address);
    59e1:	48 01 c3             	add    %rax,%rbx
    59e4:	e8 00 00 00 00       	callq  59e9 <apply_to_page_range+0x229>
	if (!pte)
    59e9:	48 85 db             	test   %rbx,%rbx
    59ec:	0f 84 fd 00 00 00    	je     5aef <apply_to_page_range+0x32f>
}

#define  __HAVE_ARCH_ENTER_LAZY_MMU_MODE
static inline void arch_enter_lazy_mmu_mode(void)
{
	PVOP_VCALL0(pv_mmu_ops.lazy_mode.enter);
    59f2:	ff 14 25 00 00 00 00 	callq  *0x0

	if (sizeof(pmdval_t) > sizeof(long))
		ret =  PVOP_CALLEE2(pmdval_t, pv_mmu_ops.pmd_val,
				    pmd.pmd, (u64)pmd.pmd >> 32);
	else
		ret =  PVOP_CALLEE1(pmdval_t, pv_mmu_ops.pmd_val,
    59f9:	48 8b 45 c8          	mov    -0x38(%rbp),%rax
    59fd:	48 8b 38             	mov    (%rax),%rdi
    5a00:	ff 14 25 00 00 00 00 	callq  *0x0

	BUG_ON(pmd_huge(*pmd));

	arch_enter_lazy_mmu_mode();

	token = pmd_pgtable(*pmd);
    5a07:	48 be 00 f0 ff ff ff 	movabs $0x3ffffffff000,%rsi
    5a0e:	3f 00 00 
    5a11:	48 ba 00 00 00 00 00 	movabs $0xffffea0000000000,%rdx
    5a18:	ea ff ff 
    5a1b:	48 89 df             	mov    %rbx,%rdi
    5a1e:	48 21 f0             	and    %rsi,%rax
    5a21:	4c 89 e3             	mov    %r12,%rbx
    5a24:	4d 89 ec             	mov    %r13,%r12
    5a27:	48 c1 e8 06          	shr    $0x6,%rax
    5a2b:	48 01 d0             	add    %rdx,%rax
    5a2e:	49 89 c5             	mov    %rax,%r13
    5a31:	eb 14                	jmp    5a47 <apply_to_page_range+0x287>
    5a33:	0f 1f 44 00 00       	nopl   0x0(%rax,%rax,1)

	do {
		err = fn(pte++, token, addr, data);
		if (err)
		  break;
	} while (addr += PAGE_SIZE, addr != end);
    5a38:	49 81 c4 00 10 00 00 	add    $0x1000,%r12
    5a3f:	4c 39 e3             	cmp    %r12,%rbx
    5a42:	74 18                	je     5a5c <apply_to_page_range+0x29c>
	arch_enter_lazy_mmu_mode();

	token = pmd_pgtable(*pmd);

	do {
		err = fn(pte++, token, addr, data);
    5a44:	4c 89 f7             	mov    %r14,%rdi
    5a47:	4c 8d 77 08          	lea    0x8(%rdi),%r14
    5a4b:	48 8b 4d d0          	mov    -0x30(%rbp),%rcx
    5a4f:	4c 89 e2             	mov    %r12,%rdx
    5a52:	4c 89 ee             	mov    %r13,%rsi
    5a55:	41 ff d7             	callq  *%r15
		if (err)
    5a58:	85 c0                	test   %eax,%eax
    5a5a:	74 dc                	je     5a38 <apply_to_page_range+0x278>
    5a5c:	49 89 dc             	mov    %rbx,%r12
    5a5f:	89 c3                	mov    %eax,%ebx
	PVOP_VCALL0(pv_mmu_ops.lazy_mode.enter);
}

static inline void arch_leave_lazy_mmu_mode(void)
{
	PVOP_VCALL0(pv_mmu_ops.lazy_mode.leave);
    5a61:	ff 14 25 00 00 00 00 	callq  *0x0
		  break;
	} while (addr += PAGE_SIZE, addr != end);

	arch_leave_lazy_mmu_mode();

	if (mm != &init_mm)
    5a68:	48 81 7d b8 00 00 00 	cmpq   $0x0,-0x48(%rbp)
    5a6f:	00 
    5a70:	74 09                	je     5a7b <apply_to_page_range+0x2bb>
	raw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \
} while (0)

static inline void spin_unlock(spinlock_t *lock)
{
	raw_spin_unlock(&lock->rlock);
    5a72:	48 8b 7d b0          	mov    -0x50(%rbp),%rdi
    5a76:	e8 00 00 00 00       	callq  5a7b <apply_to_page_range+0x2bb>
	if (!pmd)
	  return -ENOMEM;
	do {
		next = pmd_addr_end(addr, end);
		err = apply_to_pte_range(mm, pmd, addr, next, fn, data);
		if (err)
    5a7b:	85 db                	test   %ebx,%ebx
    5a7d:	0f 85 97 00 00 00    	jne    5b1a <apply_to_page_range+0x35a>
		  break;
	} while (pmd++, addr = next, addr != end);
    5a83:	48 83 45 c8 08       	addq   $0x8,-0x38(%rbp)
    5a88:	4c 39 65 c0          	cmp    %r12,-0x40(%rbp)
    5a8c:	74 75                	je     5b03 <apply_to_page_range+0x343>
    5a8e:	4d 89 e5             	mov    %r12,%r13
    5a91:	e9 b2 fe ff ff       	jmpq   5948 <apply_to_page_range+0x188>
    5a96:	66 2e 0f 1f 84 00 00 	nopw   %cs:0x0(%rax,%rax,1)
    5a9d:	00 00 00 
	int err;
	pgtable_t token;
	spinlock_t *uninitialized_var(ptl);

	pte = (mm == &init_mm) ?
		pte_alloc_kernel(pmd, addr) :
    5aa0:	48 85 c0             	test   %rax,%rax
    5aa3:	0f 84 a1 00 00 00    	je     5b4a <apply_to_page_range+0x38a>
    5aa9:	48 89 c7             	mov    %rax,%rdi

	if (sizeof(pmdval_t) > sizeof(long))
		ret =  PVOP_CALLEE2(pmdval_t, pv_mmu_ops.pmd_val,
				    pmd.pmd, (u64)pmd.pmd >> 32);
	else
		ret =  PVOP_CALLEE1(pmdval_t, pv_mmu_ops.pmd_val,
    5aac:	ff 14 25 00 00 00 00 	callq  *0x0
 * this function returns the index of the entry in the pte page which would
 * control the given virtual address
 */
static inline unsigned long pte_index(unsigned long address)
{
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
    5ab3:	4c 89 ea             	mov    %r13,%rdx
	return (unsigned long)native_pmd_val(pmd) == 0;
}

static inline unsigned long pmd_page_vaddr(pmd_t pmd)
{
	return (unsigned long)__va(pmd_val(pmd) & PTE_PFN_MASK);
    5ab6:	48 b9 00 00 00 00 00 	movabs $0xffff880000000000,%rcx
    5abd:	88 ff ff 
			pte_fn_t fn, void *data)
{
	pte_t *pte;
	int err;
	pgtable_t token;
	spinlock_t *uninitialized_var(ptl);
    5ac0:	48 c7 45 b0 00 00 00 	movq   $0x0,-0x50(%rbp)
    5ac7:	00 
 * this function returns the index of the entry in the pte page which would
 * control the given virtual address
 */
static inline unsigned long pte_index(unsigned long address)
{
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
    5ac8:	48 c1 ea 09          	shr    $0x9,%rdx
}

static inline pte_t *pte_offset_kernel(pmd_t *pmd, unsigned long address)
{
	return (pte_t *)pmd_page_vaddr(*pmd) + pte_index(address);
    5acc:	81 e2 f8 0f 00 00    	and    $0xff8,%edx
	return (unsigned long)native_pmd_val(pmd) == 0;
}

static inline unsigned long pmd_page_vaddr(pmd_t pmd)
{
	return (unsigned long)__va(pmd_val(pmd) & PTE_PFN_MASK);
    5ad2:	48 01 ca             	add    %rcx,%rdx
    5ad5:	48 b9 00 f0 ff ff ff 	movabs $0x3ffffffff000,%rcx
    5adc:	3f 00 00 
    5adf:	48 21 c8             	and    %rcx,%rax
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
}

static inline pte_t *pte_offset_kernel(pmd_t *pmd, unsigned long address)
{
	return (pte_t *)pmd_page_vaddr(*pmd) + pte_index(address);
    5ae2:	48 8d 1c 02          	lea    (%rdx,%rax,1),%rbx

	pte = (mm == &init_mm) ?
		pte_alloc_kernel(pmd, addr) :
		pte_alloc_map_lock(mm, pmd, addr, &ptl);
	if (!pte)
    5ae6:	48 85 db             	test   %rbx,%rbx
    5ae9:	0f 85 03 ff ff ff    	jne    59f2 <apply_to_page_range+0x232>
		if (err)
		  break;
	} while (pgd++, addr = next, addr != end);

	return err;
}
    5aef:	48 83 c4 60          	add    $0x60,%rsp
	unsigned long next;
	int err;

	pud = pud_alloc(mm, pgd, addr);
	if (!pud)
	  return -ENOMEM;
    5af3:	b8 f4 ff ff ff       	mov    $0xfffffff4,%eax
		if (err)
		  break;
	} while (pgd++, addr = next, addr != end);

	return err;
}
    5af8:	5b                   	pop    %rbx
    5af9:	41 5c                	pop    %r12
    5afb:	41 5d                	pop    %r13
    5afd:	41 5e                	pop    %r14
    5aff:	41 5f                	pop    %r15
    5b01:	5d                   	pop    %rbp
    5b02:	c3                   	retq   
	do {
		next = pud_addr_end(addr, end);
		err = apply_to_pmd_range(mm, pud, addr, next, fn, data);
		if (err)
		  break;
	} while (pud++, addr = next, addr != end);
    5b03:	48 83 45 98 08       	addq   $0x8,-0x68(%rbp)
    5b08:	48 8b 45 c0          	mov    -0x40(%rbp),%rax
    5b0c:	48 39 45 a0          	cmp    %rax,-0x60(%rbp)
    5b10:	74 1e                	je     5b30 <apply_to_page_range+0x370>
    5b12:	49 89 c5             	mov    %rax,%r13
    5b15:	e9 a0 fd ff ff       	jmpq   58ba <apply_to_page_range+0xfa>
	if (!pmd)
	  return -ENOMEM;
	do {
		next = pmd_addr_end(addr, end);
		err = apply_to_pte_range(mm, pmd, addr, next, fn, data);
		if (err)
    5b1a:	89 d8                	mov    %ebx,%eax
		if (err)
		  break;
	} while (pgd++, addr = next, addr != end);

	return err;
}
    5b1c:	48 83 c4 60          	add    $0x60,%rsp
    5b20:	5b                   	pop    %rbx
    5b21:	41 5c                	pop    %r12
    5b23:	41 5d                	pop    %r13
    5b25:	41 5e                	pop    %r14
    5b27:	41 5f                	pop    %r15
    5b29:	5d                   	pop    %rbp
    5b2a:	c3                   	retq   
    5b2b:	0f 1f 44 00 00       	nopl   0x0(%rax,%rax,1)
	do {
		next = pgd_addr_end(addr, end);
		err = apply_to_pud_range(mm, pgd, addr, next, fn, data);
		if (err)
		  break;
	} while (pgd++, addr = next, addr != end);
    5b30:	48 8b 45 80          	mov    -0x80(%rbp),%rax
    5b34:	4c 8b 6d a0          	mov    -0x60(%rbp),%r13
    5b38:	48 83 45 88 08       	addq   $0x8,-0x78(%rbp)
    5b3d:	49 39 c5             	cmp    %rax,%r13
    5b40:	0f 85 dd fc ff ff    	jne    5823 <apply_to_page_range+0x63>
    5b46:	31 c0                	xor    %eax,%eax
    5b48:	eb d2                	jmp    5b1c <apply_to_page_range+0x35c>
	int err;
	pgtable_t token;
	spinlock_t *uninitialized_var(ptl);

	pte = (mm == &init_mm) ?
		pte_alloc_kernel(pmd, addr) :
    5b4a:	48 8b 7d c8          	mov    -0x38(%rbp),%rdi
    5b4e:	4c 89 ee             	mov    %r13,%rsi
    5b51:	e8 00 00 00 00       	callq  5b56 <apply_to_page_range+0x396>
    5b56:	85 c0                	test   %eax,%eax
    5b58:	75 95                	jne    5aef <apply_to_page_range+0x32f>
    5b5a:	48 8b 45 c8          	mov    -0x38(%rbp),%rax
    5b5e:	48 8b 38             	mov    (%rax),%rdi
    5b61:	e9 46 ff ff ff       	jmpq   5aac <apply_to_page_range+0x2ec>
		pte_alloc_map_lock(mm, pmd, addr, &ptl);
    5b66:	48 8b 55 c8          	mov    -0x38(%rbp),%rdx
    5b6a:	48 8b 7d b8          	mov    -0x48(%rbp),%rdi
    5b6e:	31 f6                	xor    %esi,%esi
    5b70:	4c 89 e9             	mov    %r13,%rcx
    5b73:	e8 00 00 00 00       	callq  5b78 <apply_to_page_range+0x3b8>
    5b78:	85 c0                	test   %eax,%eax
    5b7a:	0f 85 6f ff ff ff    	jne    5aef <apply_to_page_range+0x32f>
    5b80:	48 8b 45 c8          	mov    -0x38(%rbp),%rax
    5b84:	48 8b 38             	mov    (%rax),%rdi
    5b87:	e9 f9 fd ff ff       	jmpq   5985 <apply_to_page_range+0x1c5>
		NULL: pud_offset(pgd, address);
}

static inline pmd_t *pmd_alloc(struct mm_struct *mm, pud_t *pud, unsigned long address)
{
	return (unlikely(pud_none(*pud)) && __pmd_alloc(mm, pud, address))?
    5b8c:	48 8b 75 98          	mov    -0x68(%rbp),%rsi
    5b90:	48 8b 7d b8          	mov    -0x48(%rbp),%rdi
    5b94:	4c 89 ea             	mov    %r13,%rdx
    5b97:	e8 00 00 00 00       	callq  5b9c <apply_to_page_range+0x3dc>
    5b9c:	85 c0                	test   %eax,%eax
    5b9e:	0f 85 4b ff ff ff    	jne    5aef <apply_to_page_range+0x32f>
    5ba4:	48 8b 45 98          	mov    -0x68(%rbp),%rax
    5ba8:	48 8b 38             	mov    (%rax),%rdi
    5bab:	e9 4b fd ff ff       	jmpq   58fb <apply_to_page_range+0x13b>
 * Remove it when 4level-fixup.h has been removed.
 */
#if defined(CONFIG_MMU) && !defined(__ARCH_HAS_4LEVEL_HACK)
static inline pud_t *pud_alloc(struct mm_struct *mm, pgd_t *pgd, unsigned long address)
{
	return (unlikely(pgd_none(*pgd)) && __pud_alloc(mm, pgd, address))?
    5bb0:	48 8b 5d 88          	mov    -0x78(%rbp),%rbx
    5bb4:	48 8b 7d b8          	mov    -0x48(%rbp),%rdi
    5bb8:	4c 89 ea             	mov    %r13,%rdx
    5bbb:	48 89 de             	mov    %rbx,%rsi
    5bbe:	e8 00 00 00 00       	callq  5bc3 <apply_to_page_range+0x403>
    5bc3:	85 c0                	test   %eax,%eax
    5bc5:	0f 85 24 ff ff ff    	jne    5aef <apply_to_page_range+0x32f>
    5bcb:	48 8b 3b             	mov    (%rbx),%rdi
    5bce:	e9 a0 fc ff ff       	jmpq   5873 <apply_to_page_range+0xb3>
	pgd_t *pgd;
	unsigned long next;
	unsigned long end = addr + size;
	int err;

	BUG_ON(addr >= end);
    5bd3:	0f 0b                	ud2    
    5bd5:	66 66 2e 0f 1f 84 00 	data32 nopw %cs:0x0(%rax,%rax,1)
    5bdc:	00 00 00 00 

0000000000005be0 <copy_page_range>:
	return 0;
}

int copy_page_range(struct mm_struct *dst_mm, struct mm_struct *src_mm,
			struct vm_area_struct *vma)
{
    5be0:	e8 00 00 00 00       	callq  5be5 <copy_page_range+0x5>
    5be5:	55                   	push   %rbp
    5be6:	48 89 e5             	mov    %rsp,%rbp
    5be9:	41 57                	push   %r15
    5beb:	41 56                	push   %r14
    5bed:	41 55                	push   %r13
    5bef:	41 54                	push   %r12
    5bf1:	53                   	push   %rbx
    5bf2:	48 83 c4 80          	add    $0xffffffffffffff80,%rsp
    5bf6:	48 89 75 d0          	mov    %rsi,-0x30(%rbp)
	pgd_t *src_pgd, *dst_pgd;
	unsigned long next;
	unsigned long addr = vma->vm_start;
    5bfa:	48 8b 32             	mov    (%rdx),%rsi
	 * Don't copy ptes where a page fault will fill them correctly.
	 * Fork becomes much lighter when there are big shared or private
	 * readonly mappings. The tradeoff is that copy_page_range is more
	 * efficient than faulting.
	 */
	if (!(vma->vm_flags & (VM_HUGETLB | VM_NONLINEAR |
    5bfd:	48 8b 42 50          	mov    0x50(%rdx),%rax
	return 0;
}

int copy_page_range(struct mm_struct *dst_mm, struct mm_struct *src_mm,
			struct vm_area_struct *vma)
{
    5c01:	48 89 7d c8          	mov    %rdi,-0x38(%rbp)
    5c05:	48 89 55 c0          	mov    %rdx,-0x40(%rbp)
	pgd_t *src_pgd, *dst_pgd;
	unsigned long next;
	unsigned long addr = vma->vm_start;
    5c09:	48 89 b5 70 ff ff ff 	mov    %rsi,-0x90(%rbp)
	unsigned long end = vma->vm_end;
    5c10:	48 8b 72 08          	mov    0x8(%rdx),%rsi
	 * Don't copy ptes where a page fault will fill them correctly.
	 * Fork becomes much lighter when there are big shared or private
	 * readonly mappings. The tradeoff is that copy_page_range is more
	 * efficient than faulting.
	 */
	if (!(vma->vm_flags & (VM_HUGETLB | VM_NONLINEAR |
    5c14:	a9 00 04 c0 10       	test   $0x10c00400,%eax
			struct vm_area_struct *vma)
{
	pgd_t *src_pgd, *dst_pgd;
	unsigned long next;
	unsigned long addr = vma->vm_start;
	unsigned long end = vma->vm_end;
    5c19:	48 89 75 90          	mov    %rsi,-0x70(%rbp)
	 * Don't copy ptes where a page fault will fill them correctly.
	 * Fork becomes much lighter when there are big shared or private
	 * readonly mappings. The tradeoff is that copy_page_range is more
	 * efficient than faulting.
	 */
	if (!(vma->vm_flags & (VM_HUGETLB | VM_NONLINEAR |
    5c1d:	75 0e                	jne    5c2d <copy_page_range+0x4d>
						VM_PFNMAP | VM_MIXEDMAP))) {
		if (!vma->anon_vma)
    5c1f:	48 83 ba 90 00 00 00 	cmpq   $0x0,0x90(%rdx)
    5c26:	00 
    5c27:	0f 84 f7 02 00 00    	je     5f24 <copy_page_range+0x344>
	}

	if (is_vm_hugetlb_page(vma))
	  return copy_hugetlb_page_range(dst_mm, src_mm, vma);

	if (unlikely(vma->vm_flags & VM_PFNMAP)) {
    5c2d:	f6 c4 04             	test   $0x4,%ah
    5c30:	0f 85 1b 03 00 00    	jne    5f51 <copy_page_range+0x371>
	add_taint(TAINT_BAD_PAGE, LOCKDEP_NOW_UNRELIABLE);
}

static inline bool is_cow_mapping(vm_flags_t flags)
{
	return (flags & (VM_SHARED | VM_MAYWRITE)) == VM_MAYWRITE;
    5c36:	83 e0 28             	and    $0x28,%eax
    5c39:	48 83 f8 20          	cmp    $0x20,%rax
	 * is_cow_mapping() returns true.
	 */
	is_cow = is_cow_mapping(vma->vm_flags);
	mmun_start = addr;
	mmun_end   = end;
	if (is_cow)
    5c3d:	0f 94 85 6f ff ff ff 	sete   -0x91(%rbp)
    5c44:	75 12                	jne    5c58 <copy_page_range+0x78>
}

static inline void mmu_notifier_invalidate_range_start(struct mm_struct *mm,
				  unsigned long start, unsigned long end)
{
	if (mm_has_notifiers(mm))
    5c46:	48 8b 45 d0          	mov    -0x30(%rbp),%rax
    5c4a:	48 83 b8 a0 03 00 00 	cmpq   $0x0,0x3a0(%rax)
    5c51:	00 
    5c52:	0f 85 12 03 00 00    	jne    5f6a <copy_page_range+0x38a>
	  mmu_notifier_invalidate_range_start(src_mm, mmun_start,
				  mmun_end);

	ret = 0;
	dst_pgd = pgd_offset(dst_mm, addr);
    5c58:	4c 8b 8d 70 ff ff ff 	mov    -0x90(%rbp),%r9
    5c5f:	48 8b 7d c8          	mov    -0x38(%rbp),%rdi
    5c63:	4c 89 c8             	mov    %r9,%rax
    5c66:	48 c1 e8 24          	shr    $0x24,%rax
    5c6a:	25 f8 0f 00 00       	and    $0xff8,%eax
    5c6f:	48 89 c6             	mov    %rax,%rsi
    5c72:	48 03 77 40          	add    0x40(%rdi),%rsi
    5c76:	48 89 75 80          	mov    %rsi,-0x80(%rbp)
	src_pgd = pgd_offset(src_mm, addr);
    5c7a:	48 8b 75 d0          	mov    -0x30(%rbp),%rsi
    5c7e:	48 03 46 40          	add    0x40(%rsi),%rax
    5c82:	48 89 45 98          	mov    %rax,-0x68(%rbp)
    5c86:	48 8b 45 90          	mov    -0x70(%rbp),%rax
    5c8a:	48 83 e8 01          	sub    $0x1,%rax
    5c8e:	48 89 85 78 ff ff ff 	mov    %rax,-0x88(%rbp)
    5c95:	0f 1f 00             	nopl   (%rax)
	do {
		next = pgd_addr_end(addr, end);
    5c98:	48 b8 00 00 00 00 80 	movabs $0x8000000000,%rax
    5c9f:	00 00 00 
    5ca2:	4c 01 c8             	add    %r9,%rax
    5ca5:	48 89 45 b8          	mov    %rax,-0x48(%rbp)
    5ca9:	48 b8 00 00 00 00 80 	movabs $0xffffff8000000000,%rax
    5cb0:	ff ff ff 
    5cb3:	48 21 45 b8          	and    %rax,-0x48(%rbp)
    5cb7:	48 8b 7d b8          	mov    -0x48(%rbp),%rdi
    5cbb:	48 89 f8             	mov    %rdi,%rax
    5cbe:	48 83 e8 01          	sub    $0x1,%rax
			struct vm_area_struct *vma)
{
	pgd_t *src_pgd, *dst_pgd;
	unsigned long next;
	unsigned long addr = vma->vm_start;
	unsigned long end = vma->vm_end;
    5cc2:	48 3b 85 78 ff ff ff 	cmp    -0x88(%rbp),%rax
    5cc9:	48 89 f8             	mov    %rdi,%rax
    5ccc:	48 0f 43 45 90       	cmovae -0x70(%rbp),%rax
    5cd1:	48 89 45 b8          	mov    %rax,-0x48(%rbp)
    5cd5:	48 8b 45 98          	mov    -0x68(%rbp),%rax
    5cd9:	48 8b 00             	mov    (%rax),%rax
void pud_clear_bad(pud_t *);
void pmd_clear_bad(pmd_t *);

static inline int pgd_none_or_clear_bad(pgd_t *pgd)
{
	if (pgd_none(*pgd))
    5cdc:	48 85 c0             	test   %rax,%rax
    5cdf:	0f 84 db 01 00 00    	je     5ec0 <copy_page_range+0x2e0>
	return (pud_t *)pgd_page_vaddr(*pgd) + pud_index(address);
}

static inline int pgd_bad(pgd_t pgd)
{
	return (pgd_flags(pgd) & ~_PAGE_USER) != _KERNPG_TABLE;
    5ce5:	48 bf fb 0f 00 00 00 	movabs $0xffffc00000000ffb,%rdi
    5cec:	c0 ff ff 
    5cef:	48 21 f8             	and    %rdi,%rax
		return 1;
	if (unlikely(pgd_bad(*pgd))) {
    5cf2:	48 83 f8 63          	cmp    $0x63,%rax
    5cf6:	0f 85 b9 01 00 00    	jne    5eb5 <copy_page_range+0x2d5>
    5cfc:	48 8b 45 80          	mov    -0x80(%rbp),%rax
    5d00:	48 8b 38             	mov    (%rax),%rdi
		NULL: pud_offset(pgd, address);
    5d03:	48 85 ff             	test   %rdi,%rdi
    5d06:	0f 84 76 02 00 00    	je     5f82 <copy_page_range+0x3a2>

	if (sizeof(pgdval_t) > sizeof(long))
		ret =  PVOP_CALLEE2(pgdval_t, pv_mmu_ops.pgd_val,
				    pgd.pgd, (u64)pgd.pgd >> 32);
	else
		ret =  PVOP_CALLEE1(pgdval_t, pv_mmu_ops.pgd_val,
    5d0c:	ff 14 25 00 00 00 00 	callq  *0x0
#define pgd_page(pgd)		pfn_to_page(pgd_val(pgd) >> PAGE_SHIFT)

/* to find an entry in a page-table-directory. */
static inline unsigned long pud_index(unsigned long address)
{
	return (address >> PUD_SHIFT) & (PTRS_PER_PUD - 1);
    5d13:	4c 89 c9             	mov    %r9,%rcx
	return pgd_flags(pgd) & _PAGE_PRESENT;
}

static inline unsigned long pgd_page_vaddr(pgd_t pgd)
{
	return (unsigned long)__va((unsigned long)pgd_val(pgd) & PTE_PFN_MASK);
    5d16:	48 be 00 00 00 00 00 	movabs $0xffff880000000000,%rsi
    5d1d:	88 ff ff 
    5d20:	48 ba 00 f0 ff ff ff 	movabs $0x3ffffffff000,%rdx
    5d27:	3f 00 00 
#define pgd_page(pgd)		pfn_to_page(pgd_val(pgd) >> PAGE_SHIFT)

/* to find an entry in a page-table-directory. */
static inline unsigned long pud_index(unsigned long address)
{
	return (address >> PUD_SHIFT) & (PTRS_PER_PUD - 1);
    5d2a:	48 c1 e9 1b          	shr    $0x1b,%rcx
	return pgd_flags(pgd) & _PAGE_PRESENT;
}

static inline unsigned long pgd_page_vaddr(pgd_t pgd)
{
	return (unsigned long)__va((unsigned long)pgd_val(pgd) & PTE_PFN_MASK);
    5d2e:	48 21 d0             	and    %rdx,%rax
	return (address >> PUD_SHIFT) & (PTRS_PER_PUD - 1);
}

static inline pud_t *pud_offset(pgd_t *pgd, unsigned long address)
{
	return (pud_t *)pgd_page_vaddr(*pgd) + pud_index(address);
    5d31:	81 e1 f8 0f 00 00    	and    $0xff8,%ecx
	return pgd_flags(pgd) & _PAGE_PRESENT;
}

static inline unsigned long pgd_page_vaddr(pgd_t pgd)
{
	return (unsigned long)__va((unsigned long)pgd_val(pgd) & PTE_PFN_MASK);
    5d37:	48 01 f1             	add    %rsi,%rcx
{
	pud_t *src_pud, *dst_pud;
	unsigned long next;

	dst_pud = pud_alloc(dst_mm, dst_pgd, addr);
	if (!dst_pud)
    5d3a:	48 01 c8             	add    %rcx,%rax
    5d3d:	48 89 45 a8          	mov    %rax,-0x58(%rbp)
    5d41:	0f 84 c9 01 00 00    	je     5f10 <copy_page_range+0x330>
    5d47:	48 8b 45 98          	mov    -0x68(%rbp),%rax
    5d4b:	48 8b 38             	mov    (%rax),%rdi
    5d4e:	ff 14 25 00 00 00 00 	callq  *0x0
    5d55:	48 21 d0             	and    %rdx,%rax
	return pud_flags(pud) & _PAGE_PRESENT;
}

static inline unsigned long pud_page_vaddr(pud_t pud)
{
	return (unsigned long)__va((unsigned long)pud_val(pud) & PTE_PFN_MASK);
    5d58:	48 89 55 88          	mov    %rdx,-0x78(%rbp)
	return (address >> PUD_SHIFT) & (PTRS_PER_PUD - 1);
}

static inline pud_t *pud_offset(pgd_t *pgd, unsigned long address)
{
	return (pud_t *)pgd_page_vaddr(*pgd) + pud_index(address);
    5d5c:	48 01 c8             	add    %rcx,%rax
    5d5f:	48 89 45 b0          	mov    %rax,-0x50(%rbp)
    5d63:	48 8b 45 b8          	mov    -0x48(%rbp),%rax
    5d67:	48 83 e8 01          	sub    $0x1,%rax
    5d6b:	48 89 45 a0          	mov    %rax,-0x60(%rbp)
    5d6f:	90                   	nop
	  return -ENOMEM;
	src_pud = pud_offset(src_pgd, addr);
	do {
		next = pud_addr_end(addr, end);
    5d70:	4d 8d b9 00 00 00 40 	lea    0x40000000(%r9),%r15
    5d77:	49 81 e7 00 00 00 c0 	and    $0xffffffffc0000000,%r15
    5d7e:	49 8d 47 ff          	lea    -0x1(%r15),%rax
    5d82:	48 39 45 a0          	cmp    %rax,-0x60(%rbp)
    5d86:	48 8b 45 b0          	mov    -0x50(%rbp),%rax
    5d8a:	4c 0f 46 7d b8       	cmovbe -0x48(%rbp),%r15
    5d8f:	48 8b 00             	mov    (%rax),%rax
	return 0;
}

static inline int pud_none_or_clear_bad(pud_t *pud)
{
	if (pud_none(*pud))
    5d92:	48 85 c0             	test   %rax,%rax
    5d95:	0f 84 02 01 00 00    	je     5e9d <copy_page_range+0x2bd>
		return 1;
	if (unlikely(pud_bad(*pud))) {
    5d9b:	48 bf 98 0f 00 00 00 	movabs $0xffffc00000000f98,%rdi
    5da2:	c0 ff ff 
    5da5:	48 85 f8             	test   %rdi,%rax
    5da8:	0f 85 ff 01 00 00    	jne    5fad <copy_page_range+0x3cd>
    5dae:	48 8b 45 a8          	mov    -0x58(%rbp),%rax
    5db2:	48 8b 38             	mov    (%rax),%rdi
}

static inline pmd_t *pmd_alloc(struct mm_struct *mm, pud_t *pud, unsigned long address)
{
	return (unlikely(pud_none(*pud)) && __pmd_alloc(mm, pud, address))?
		NULL: pmd_offset(pud, address);
    5db5:	48 85 ff             	test   %rdi,%rdi
    5db8:	0f 84 fd 01 00 00    	je     5fbb <copy_page_range+0x3db>

	if (sizeof(pudval_t) > sizeof(long))
		ret =  PVOP_CALLEE2(pudval_t, pv_mmu_ops.pud_val,
				    pud.pud, (u64)pud.pud >> 32);
	else
		ret =  PVOP_CALLEE1(pudval_t, pv_mmu_ops.pud_val,
    5dbe:	ff 14 25 00 00 00 00 	callq  *0x0
 * this macro returns the index of the entry in the pmd page which would
 * control the given virtual address
 */
static inline unsigned long pmd_index(unsigned long address)
{
	return (address >> PMD_SHIFT) & (PTRS_PER_PMD - 1);
    5dc5:	4d 89 ca             	mov    %r9,%r10
	return pud_flags(pud) & _PAGE_PRESENT;
}

static inline unsigned long pud_page_vaddr(pud_t pud)
{
	return (unsigned long)__va((unsigned long)pud_val(pud) & PTE_PFN_MASK);
    5dc8:	48 be 00 00 00 00 00 	movabs $0xffff880000000000,%rsi
    5dcf:	88 ff ff 
 * this macro returns the index of the entry in the pmd page which would
 * control the given virtual address
 */
static inline unsigned long pmd_index(unsigned long address)
{
	return (address >> PMD_SHIFT) & (PTRS_PER_PMD - 1);
    5dd2:	49 c1 ea 12          	shr    $0x12,%r10
#define pud_page(pud)		pfn_to_page(pud_val(pud) >> PAGE_SHIFT)

/* Find an entry in the second-level page table.. */
static inline pmd_t *pmd_offset(pud_t *pud, unsigned long address)
{
	return (pmd_t *)pud_page_vaddr(*pud) + pmd_index(address);
    5dd6:	41 81 e2 f8 0f 00 00 	and    $0xff8,%r10d
	return pud_flags(pud) & _PAGE_PRESENT;
}

static inline unsigned long pud_page_vaddr(pud_t pud)
{
	return (unsigned long)__va((unsigned long)pud_val(pud) & PTE_PFN_MASK);
    5ddd:	49 01 f2             	add    %rsi,%r10
    5de0:	48 8b 75 88          	mov    -0x78(%rbp),%rsi
    5de4:	48 21 f0             	and    %rsi,%rax
{
	pmd_t *src_pmd, *dst_pmd;
	unsigned long next;

	dst_pmd = pmd_alloc(dst_mm, dst_pud, addr);
	if (!dst_pmd)
    5de7:	4c 01 d0             	add    %r10,%rax
    5dea:	49 89 c4             	mov    %rax,%r12
    5ded:	0f 84 1d 01 00 00    	je     5f10 <copy_page_range+0x330>
    5df3:	48 8b 45 b0          	mov    -0x50(%rbp),%rax
    5df7:	48 8b 38             	mov    (%rax),%rdi
    5dfa:	ff 14 25 00 00 00 00 	callq  *0x0
    5e01:	48 21 f0             	and    %rsi,%rax
    5e04:	4d 8d 6f ff          	lea    -0x1(%r15),%r13
#define pud_page(pud)		pfn_to_page(pud_val(pud) >> PAGE_SHIFT)

/* Find an entry in the second-level page table.. */
static inline pmd_t *pmd_offset(pud_t *pud, unsigned long address)
{
	return (pmd_t *)pud_page_vaddr(*pud) + pmd_index(address);
    5e08:	4e 8d 34 10          	lea    (%rax,%r10,1),%r14
    5e0c:	4c 89 f0             	mov    %r14,%rax
    5e0f:	4d 29 f4             	sub    %r14,%r12
    5e12:	4d 89 fe             	mov    %r15,%r14
    5e15:	49 89 c7             	mov    %rax,%r15
    5e18:	eb 09                	jmp    5e23 <copy_page_range+0x243>
    5e1a:	66 0f 1f 44 00 00    	nopw   0x0(%rax,%rax,1)
		if (pmd_none_or_clear_bad(src_pmd))
		  continue;
		if (copy_pte_range(dst_mm, src_mm, dst_pmd, src_pmd,
						vma, addr, next))
		  return -ENOMEM;
	} while (dst_pmd++, src_pmd++, addr = next, addr != end);
    5e20:	49 89 d9             	mov    %rbx,%r9
	dst_pmd = pmd_alloc(dst_mm, dst_pud, addr);
	if (!dst_pmd)
	  return -ENOMEM;
	src_pmd = pmd_offset(src_pud, addr);
	do {
		next = pmd_addr_end(addr, end);
    5e23:	49 8d 99 00 00 20 00 	lea    0x200000(%r9),%rbx
    5e2a:	4b 8d 14 27          	lea    (%r15,%r12,1),%rdx
    5e2e:	48 81 e3 00 00 e0 ff 	and    $0xffffffffffe00000,%rbx
    5e35:	48 8d 43 ff          	lea    -0x1(%rbx),%rax
    5e39:	49 39 c5             	cmp    %rax,%r13
    5e3c:	49 8b 07             	mov    (%r15),%rax
    5e3f:	49 0f 46 de          	cmovbe %r14,%rbx
	return 0;
}

static inline int pmd_none_or_clear_bad(pmd_t *pmd)
{
	if (pmd_none(*pmd))
    5e43:	48 85 c0             	test   %rax,%rax
    5e46:	74 49                	je     5e91 <copy_page_range+0x2b1>

static inline int pmd_bad(pmd_t pmd)
{
#ifdef CONFIG_NUMA_BALANCING
	/* pmd_numa check */
	if ((pmd_flags(pmd) & (_PAGE_NUMA|_PAGE_PRESENT)) == _PAGE_NUMA)
    5e48:	48 89 c1             	mov    %rax,%rcx
    5e4b:	81 e1 01 01 00 00    	and    $0x101,%ecx
    5e51:	48 81 f9 00 01 00 00 	cmp    $0x100,%rcx
    5e58:	74 17                	je     5e71 <copy_page_range+0x291>
		return 0;
#endif
	return (pmd_flags(pmd) & ~_PAGE_USER) != _KERNPG_TABLE;
    5e5a:	48 be fb 0f 00 00 00 	movabs $0xffffc00000000ffb,%rsi
    5e61:	c0 ff ff 
    5e64:	48 21 f0             	and    %rsi,%rax
		return 1;
	if (unlikely(pmd_bad(*pmd))) {
    5e67:	48 83 f8 63          	cmp    $0x63,%rax
    5e6b:	0f 85 a6 00 00 00    	jne    5f17 <copy_page_range+0x337>
			  continue;
			/* fall through */
		}
		if (pmd_none_or_clear_bad(src_pmd))
		  continue;
		if (copy_pte_range(dst_mm, src_mm, dst_pmd, src_pmd,
    5e71:	4c 8b 45 c0          	mov    -0x40(%rbp),%r8
    5e75:	48 8b 75 d0          	mov    -0x30(%rbp),%rsi
    5e79:	4c 89 f9             	mov    %r15,%rcx
    5e7c:	48 8b 7d c8          	mov    -0x38(%rbp),%rdi
    5e80:	48 89 1c 24          	mov    %rbx,(%rsp)
    5e84:	e8 00 00 00 00       	callq  5e89 <copy_page_range+0x2a9>
    5e89:	85 c0                	test   %eax,%eax
    5e8b:	0f 85 7f 00 00 00    	jne    5f10 <copy_page_range+0x330>
						vma, addr, next))
		  return -ENOMEM;
	} while (dst_pmd++, src_pmd++, addr = next, addr != end);
    5e91:	49 83 c7 08          	add    $0x8,%r15
    5e95:	49 39 de             	cmp    %rbx,%r14
    5e98:	75 86                	jne    5e20 <copy_page_range+0x240>
    5e9a:	4d 89 f7             	mov    %r14,%r15
		if (pud_none_or_clear_bad(src_pud))
		  continue;
		if (copy_pmd_range(dst_mm, src_mm, dst_pud, src_pud,
						vma, addr, next))
		  return -ENOMEM;
	} while (dst_pud++, src_pud++, addr = next, addr != end);
    5e9d:	48 83 45 a8 08       	addq   $0x8,-0x58(%rbp)
    5ea2:	48 83 45 b0 08       	addq   $0x8,-0x50(%rbp)
    5ea7:	4c 39 7d b8          	cmp    %r15,-0x48(%rbp)
    5eab:	74 13                	je     5ec0 <copy_page_range+0x2e0>
    5ead:	4d 89 f9             	mov    %r15,%r9
    5eb0:	e9 bb fe ff ff       	jmpq   5d70 <copy_page_range+0x190>
static inline int pgd_none_or_clear_bad(pgd_t *pgd)
{
	if (pgd_none(*pgd))
		return 1;
	if (unlikely(pgd_bad(*pgd))) {
		pgd_clear_bad(pgd);
    5eb5:	48 8b 7d 98          	mov    -0x68(%rbp),%rdi
    5eb9:	e8 00 00 00 00       	callq  5ebe <copy_page_range+0x2de>
    5ebe:	66 90                	xchg   %ax,%ax
		if (unlikely(copy_pud_range(dst_mm, src_mm, dst_pgd, src_pgd,
							vma, addr, next))) {
			ret = -ENOMEM;
			break;
		}
	} while (dst_pgd++, src_pgd++, addr = next, addr != end);
    5ec0:	48 8b 45 90          	mov    -0x70(%rbp),%rax
    5ec4:	4c 8b 4d b8          	mov    -0x48(%rbp),%r9
    5ec8:	48 83 45 80 08       	addq   $0x8,-0x80(%rbp)
    5ecd:	48 83 45 98 08       	addq   $0x8,-0x68(%rbp)
    5ed2:	49 39 c1             	cmp    %rax,%r9
    5ed5:	0f 85 bd fd ff ff    	jne    5c98 <copy_page_range+0xb8>
	mmun_end   = end;
	if (is_cow)
	  mmu_notifier_invalidate_range_start(src_mm, mmun_start,
				  mmun_end);

	ret = 0;
    5edb:	31 c0                	xor    %eax,%eax
    5edd:	0f 1f 00             	nopl   (%rax)
			ret = -ENOMEM;
			break;
		}
	} while (dst_pgd++, src_pgd++, addr = next, addr != end);

	if (is_cow)
    5ee0:	80 bd 6f ff ff ff 00 	cmpb   $0x0,-0x91(%rbp)
    5ee7:	74 0e                	je     5ef7 <copy_page_range+0x317>
}

static inline void mmu_notifier_invalidate_range_end(struct mm_struct *mm,
				  unsigned long start, unsigned long end)
{
	if (mm_has_notifiers(mm))
    5ee9:	48 8b 75 d0          	mov    -0x30(%rbp),%rsi
    5eed:	48 83 be a0 03 00 00 	cmpq   $0x0,0x3a0(%rsi)
    5ef4:	00 
    5ef5:	75 3e                	jne    5f35 <copy_page_range+0x355>
	  mmu_notifier_invalidate_range_end(src_mm, mmun_start, mmun_end);
	return ret;
}
    5ef7:	48 83 ec 80          	sub    $0xffffffffffffff80,%rsp
    5efb:	5b                   	pop    %rbx
    5efc:	41 5c                	pop    %r12
    5efe:	41 5d                	pop    %r13
    5f00:	41 5e                	pop    %r14
    5f02:	41 5f                	pop    %r15
    5f04:	5d                   	pop    %rbp
    5f05:	c3                   	retq   
    5f06:	66 2e 0f 1f 84 00 00 	nopw   %cs:0x0(%rax,%rax,1)
    5f0d:	00 00 00 
		next = pgd_addr_end(addr, end);
		if (pgd_none_or_clear_bad(src_pgd))
		  continue;
		if (unlikely(copy_pud_range(dst_mm, src_mm, dst_pgd, src_pgd,
							vma, addr, next))) {
			ret = -ENOMEM;
    5f10:	b8 f4 ff ff ff       	mov    $0xfffffff4,%eax
    5f15:	eb c9                	jmp    5ee0 <copy_page_range+0x300>
static inline int pmd_none_or_clear_bad(pmd_t *pmd)
{
	if (pmd_none(*pmd))
		return 1;
	if (unlikely(pmd_bad(*pmd))) {
		pmd_clear_bad(pmd);
    5f17:	4c 89 ff             	mov    %r15,%rdi
    5f1a:	e8 00 00 00 00       	callq  5f1f <copy_page_range+0x33f>
    5f1f:	e9 6d ff ff ff       	jmpq   5e91 <copy_page_range+0x2b1>
	} while (dst_pgd++, src_pgd++, addr = next, addr != end);

	if (is_cow)
	  mmu_notifier_invalidate_range_end(src_mm, mmun_start, mmun_end);
	return ret;
}
    5f24:	48 83 ec 80          	sub    $0xffffffffffffff80,%rsp
	 * efficient than faulting.
	 */
	if (!(vma->vm_flags & (VM_HUGETLB | VM_NONLINEAR |
						VM_PFNMAP | VM_MIXEDMAP))) {
		if (!vma->anon_vma)
		  return 0;
    5f28:	31 c0                	xor    %eax,%eax
	} while (dst_pgd++, src_pgd++, addr = next, addr != end);

	if (is_cow)
	  mmu_notifier_invalidate_range_end(src_mm, mmun_start, mmun_end);
	return ret;
}
    5f2a:	5b                   	pop    %rbx
    5f2b:	41 5c                	pop    %r12
    5f2d:	41 5d                	pop    %r13
    5f2f:	41 5e                	pop    %r14
    5f31:	41 5f                	pop    %r15
    5f33:	5d                   	pop    %rbp
    5f34:	c3                   	retq   
		__mmu_notifier_invalidate_range_end(mm, start, end);
    5f35:	48 8b 55 90          	mov    -0x70(%rbp),%rdx
    5f39:	48 8b b5 70 ff ff ff 	mov    -0x90(%rbp),%rsi
    5f40:	48 8b 7d d0          	mov    -0x30(%rbp),%rdi
    5f44:	89 45 c8             	mov    %eax,-0x38(%rbp)
    5f47:	e8 00 00 00 00       	callq  5f4c <copy_page_range+0x36c>
    5f4c:	8b 45 c8             	mov    -0x38(%rbp),%eax
    5f4f:	eb a6                	jmp    5ef7 <copy_page_range+0x317>
	if (unlikely(vma->vm_flags & VM_PFNMAP)) {
		/*
		 * We do not free on error cases below as remove_vma
		 * gets called on error from higher level routine
		 */
		ret = track_pfn_copy(vma);
    5f51:	48 8b 5d c0          	mov    -0x40(%rbp),%rbx
    5f55:	48 89 df             	mov    %rbx,%rdi
    5f58:	e8 00 00 00 00       	callq  5f5d <copy_page_range+0x37d>
		if (ret)
    5f5d:	85 c0                	test   %eax,%eax
    5f5f:	75 96                	jne    5ef7 <copy_page_range+0x317>
    5f61:	48 8b 43 50          	mov    0x50(%rbx),%rax
    5f65:	e9 cc fc ff ff       	jmpq   5c36 <copy_page_range+0x56>

static inline void mmu_notifier_invalidate_range_start(struct mm_struct *mm,
				  unsigned long start, unsigned long end)
{
	if (mm_has_notifiers(mm))
		__mmu_notifier_invalidate_range_start(mm, start, end);
    5f6a:	48 8b 55 90          	mov    -0x70(%rbp),%rdx
    5f6e:	48 8b b5 70 ff ff ff 	mov    -0x90(%rbp),%rsi
    5f75:	48 89 c7             	mov    %rax,%rdi
    5f78:	e8 00 00 00 00       	callq  5f7d <copy_page_range+0x39d>
    5f7d:	e9 d6 fc ff ff       	jmpq   5c58 <copy_page_range+0x78>
 * Remove it when 4level-fixup.h has been removed.
 */
#if defined(CONFIG_MMU) && !defined(__ARCH_HAS_4LEVEL_HACK)
static inline pud_t *pud_alloc(struct mm_struct *mm, pgd_t *pgd, unsigned long address)
{
	return (unlikely(pgd_none(*pgd)) && __pud_alloc(mm, pgd, address))?
    5f82:	48 8b 5d 80          	mov    -0x80(%rbp),%rbx
    5f86:	48 8b 7d c8          	mov    -0x38(%rbp),%rdi
    5f8a:	4c 89 ca             	mov    %r9,%rdx
    5f8d:	4c 89 4d b0          	mov    %r9,-0x50(%rbp)
    5f91:	48 89 de             	mov    %rbx,%rsi
    5f94:	e8 00 00 00 00       	callq  5f99 <copy_page_range+0x3b9>
    5f99:	85 c0                	test   %eax,%eax
    5f9b:	0f 85 6f ff ff ff    	jne    5f10 <copy_page_range+0x330>
    5fa1:	48 8b 3b             	mov    (%rbx),%rdi
    5fa4:	4c 8b 4d b0          	mov    -0x50(%rbp),%r9
    5fa8:	e9 5f fd ff ff       	jmpq   5d0c <copy_page_range+0x12c>
static inline int pud_none_or_clear_bad(pud_t *pud)
{
	if (pud_none(*pud))
		return 1;
	if (unlikely(pud_bad(*pud))) {
		pud_clear_bad(pud);
    5fad:	48 8b 7d b0          	mov    -0x50(%rbp),%rdi
    5fb1:	e8 00 00 00 00       	callq  5fb6 <copy_page_range+0x3d6>
    5fb6:	e9 e2 fe ff ff       	jmpq   5e9d <copy_page_range+0x2bd>
		NULL: pud_offset(pgd, address);
}

static inline pmd_t *pmd_alloc(struct mm_struct *mm, pud_t *pud, unsigned long address)
{
	return (unlikely(pud_none(*pud)) && __pmd_alloc(mm, pud, address))?
    5fbb:	48 8b 5d a8          	mov    -0x58(%rbp),%rbx
    5fbf:	48 8b 7d c8          	mov    -0x38(%rbp),%rdi
    5fc3:	4c 89 ca             	mov    %r9,%rdx
    5fc6:	4c 89 8d 60 ff ff ff 	mov    %r9,-0xa0(%rbp)
    5fcd:	48 89 de             	mov    %rbx,%rsi
    5fd0:	e8 00 00 00 00       	callq  5fd5 <copy_page_range+0x3f5>
    5fd5:	85 c0                	test   %eax,%eax
    5fd7:	0f 85 33 ff ff ff    	jne    5f10 <copy_page_range+0x330>
    5fdd:	48 8b 3b             	mov    (%rbx),%rdi
    5fe0:	4c 8b 8d 60 ff ff ff 	mov    -0xa0(%rbp),%r9
    5fe7:	e9 d2 fd ff ff       	jmpq   5dbe <copy_page_range+0x1de>
    5fec:	0f 1f 40 00          	nopl   0x0(%rax)

0000000000005ff0 <ptlock_free>:
	page->ptl = ptl;
	return true;
}

void ptlock_free(struct page *page)
{
    5ff0:	e8 00 00 00 00       	callq  5ff5 <ptlock_free+0x5>
    5ff5:	55                   	push   %rbp
	kfree(page->ptl);
    5ff6:	48 8b 7f 30          	mov    0x30(%rdi),%rdi
	page->ptl = ptl;
	return true;
}

void ptlock_free(struct page *page)
{
    5ffa:	48 89 e5             	mov    %rsp,%rbp
	kfree(page->ptl);
    5ffd:	e8 00 00 00 00       	callq  6002 <ptlock_free+0x12>
}
    6002:	5d                   	pop    %rbp
    6003:	c3                   	retq   
    6004:	66 66 66 2e 0f 1f 84 	data32 data32 nopw %cs:0x0(%rax,%rax,1)
    600b:	00 00 00 00 00 

0000000000006010 <pte_to_physical>:
#endif
long pte_to_physical(pte_t * pte,long address){
    6010:	e8 00 00 00 00       	callq  6015 <pte_to_physical+0x5>
    6015:	55                   	push   %rbp

	if (sizeof(pteval_t) > sizeof(long))
		ret = PVOP_CALLEE2(pteval_t, pv_mmu_ops.pte_val,
				   pte.pte, (u64)pte.pte >> 32);
	else
		ret = PVOP_CALLEE1(pteval_t, pv_mmu_ops.pte_val,
    6016:	48 8b 3f             	mov    (%rdi),%rdi
    6019:	48 89 e5             	mov    %rsp,%rbp
    601c:	ff 14 25 00 00 00 00 	callq  *0x0
	return ((pte_val(*pte)&PAGE_MASK)|(address &~PAGE_MASK));
    6023:	48 25 00 f0 ff ff    	and    $0xfffffffffffff000,%rax
    6029:	81 e6 ff 0f 00 00    	and    $0xfff,%esi
    602f:	48 09 f0             	or     %rsi,%rax
}
    6032:	5d                   	pop    %rbp
    6033:	c3                   	retq   
    6034:	66 66 66 2e 0f 1f 84 	data32 data32 nopw %cs:0x0(%rax,%rax,1)
    603b:	00 00 00 00 00 

0000000000006040 <coa_cache_init>:
static struct kmem_cache* sclock_coa_children_cachep;
static struct kmem_cache* sclock_coa_parent_cachep;
void  coa_cache_init(void){
    6040:	e8 00 00 00 00       	callq  6045 <coa_cache_init+0x5>
    6045:	55                   	push   %rbp
	sclock_coa_children_cachep=kmem_cache_create("sclock_coa_children_cachep", sizeof(struct sclock_coa_children), ARCH_MIN_TASKALIGN, SLAB_PANIC | SLAB_NOTRACK, NULL);
    6046:	45 31 c0             	xor    %r8d,%r8d
    6049:	b9 00 00 04 00       	mov    $0x40000,%ecx
    604e:	ba 10 00 00 00       	mov    $0x10,%edx
    6053:	be 28 00 00 00       	mov    $0x28,%esi
    6058:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
long pte_to_physical(pte_t * pte,long address){
	return ((pte_val(*pte)&PAGE_MASK)|(address &~PAGE_MASK));
}
static struct kmem_cache* sclock_coa_children_cachep;
static struct kmem_cache* sclock_coa_parent_cachep;
void  coa_cache_init(void){
    605f:	48 89 e5             	mov    %rsp,%rbp
	sclock_coa_children_cachep=kmem_cache_create("sclock_coa_children_cachep", sizeof(struct sclock_coa_children), ARCH_MIN_TASKALIGN, SLAB_PANIC | SLAB_NOTRACK, NULL);
    6062:	e8 00 00 00 00       	callq  6067 <coa_cache_init+0x27>
	sclock_coa_parent_cachep=kmem_cache_create("sclock_coa_parent_cachep", sizeof(struct sclock_coa_parent), ARCH_MIN_TASKALIGN, SLAB_PANIC | SLAB_NOTRACK, NULL);
    6067:	45 31 c0             	xor    %r8d,%r8d
    606a:	b9 00 00 04 00       	mov    $0x40000,%ecx
    606f:	ba 10 00 00 00       	mov    $0x10,%edx
    6074:	be 80 00 00 00       	mov    $0x80,%esi
    6079:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
	return ((pte_val(*pte)&PAGE_MASK)|(address &~PAGE_MASK));
}
static struct kmem_cache* sclock_coa_children_cachep;
static struct kmem_cache* sclock_coa_parent_cachep;
void  coa_cache_init(void){
	sclock_coa_children_cachep=kmem_cache_create("sclock_coa_children_cachep", sizeof(struct sclock_coa_children), ARCH_MIN_TASKALIGN, SLAB_PANIC | SLAB_NOTRACK, NULL);
    6080:	48 89 05 00 00 00 00 	mov    %rax,0x0(%rip)        # 6087 <coa_cache_init+0x47>
	sclock_coa_parent_cachep=kmem_cache_create("sclock_coa_parent_cachep", sizeof(struct sclock_coa_parent), ARCH_MIN_TASKALIGN, SLAB_PANIC | SLAB_NOTRACK, NULL);
    6087:	e8 00 00 00 00       	callq  608c <coa_cache_init+0x4c>
}
    608c:	5d                   	pop    %rbp
}
static struct kmem_cache* sclock_coa_children_cachep;
static struct kmem_cache* sclock_coa_parent_cachep;
void  coa_cache_init(void){
	sclock_coa_children_cachep=kmem_cache_create("sclock_coa_children_cachep", sizeof(struct sclock_coa_children), ARCH_MIN_TASKALIGN, SLAB_PANIC | SLAB_NOTRACK, NULL);
	sclock_coa_parent_cachep=kmem_cache_create("sclock_coa_parent_cachep", sizeof(struct sclock_coa_parent), ARCH_MIN_TASKALIGN, SLAB_PANIC | SLAB_NOTRACK, NULL);
    608d:	48 89 05 00 00 00 00 	mov    %rax,0x0(%rip)        # 6094 <coa_cache_init+0x54>
}
    6094:	c3                   	retq   
    6095:	66 66 2e 0f 1f 84 00 	data32 nopw %cs:0x0(%rax,%rax,1)
    609c:	00 00 00 00 

00000000000060a0 <init_coa_parent>:
struct sclock_coa_parent* init_coa_parent(struct page* page){
    60a0:	e8 00 00 00 00       	callq  60a5 <init_coa_parent+0x5>
    60a5:	55                   	push   %rbp
	struct sclock_coa_parent* coa_parent=kmem_cache_alloc(sclock_coa_parent_cachep,GFP_KERNEL);
    60a6:	be d0 00 00 00       	mov    $0xd0,%esi
static struct kmem_cache* sclock_coa_parent_cachep;
void  coa_cache_init(void){
	sclock_coa_children_cachep=kmem_cache_create("sclock_coa_children_cachep", sizeof(struct sclock_coa_children), ARCH_MIN_TASKALIGN, SLAB_PANIC | SLAB_NOTRACK, NULL);
	sclock_coa_parent_cachep=kmem_cache_create("sclock_coa_parent_cachep", sizeof(struct sclock_coa_parent), ARCH_MIN_TASKALIGN, SLAB_PANIC | SLAB_NOTRACK, NULL);
}
struct sclock_coa_parent* init_coa_parent(struct page* page){
    60ab:	48 89 e5             	mov    %rsp,%rbp
    60ae:	41 54                	push   %r12
    60b0:	49 89 fc             	mov    %rdi,%r12
	struct sclock_coa_parent* coa_parent=kmem_cache_alloc(sclock_coa_parent_cachep,GFP_KERNEL);
    60b3:	48 8b 3d 00 00 00 00 	mov    0x0(%rip),%rdi        # 60ba <init_coa_parent+0x1a>
static struct kmem_cache* sclock_coa_parent_cachep;
void  coa_cache_init(void){
	sclock_coa_children_cachep=kmem_cache_create("sclock_coa_children_cachep", sizeof(struct sclock_coa_children), ARCH_MIN_TASKALIGN, SLAB_PANIC | SLAB_NOTRACK, NULL);
	sclock_coa_parent_cachep=kmem_cache_create("sclock_coa_parent_cachep", sizeof(struct sclock_coa_parent), ARCH_MIN_TASKALIGN, SLAB_PANIC | SLAB_NOTRACK, NULL);
}
struct sclock_coa_parent* init_coa_parent(struct page* page){
    60ba:	53                   	push   %rbx
	struct sclock_coa_parent* coa_parent=kmem_cache_alloc(sclock_coa_parent_cachep,GFP_KERNEL);
    60bb:	e8 00 00 00 00       	callq  60c0 <init_coa_parent+0x20>
	if(coa_parent==NULL)
    60c0:	48 85 c0             	test   %rax,%rax
void  coa_cache_init(void){
	sclock_coa_children_cachep=kmem_cache_create("sclock_coa_children_cachep", sizeof(struct sclock_coa_children), ARCH_MIN_TASKALIGN, SLAB_PANIC | SLAB_NOTRACK, NULL);
	sclock_coa_parent_cachep=kmem_cache_create("sclock_coa_parent_cachep", sizeof(struct sclock_coa_parent), ARCH_MIN_TASKALIGN, SLAB_PANIC | SLAB_NOTRACK, NULL);
}
struct sclock_coa_parent* init_coa_parent(struct page* page){
	struct sclock_coa_parent* coa_parent=kmem_cache_alloc(sclock_coa_parent_cachep,GFP_KERNEL);
    60c3:	48 89 c3             	mov    %rax,%rbx
	if(coa_parent==NULL)
    60c6:	74 6d                	je     6135 <init_coa_parent+0x95>
	  return NULL;
	mutex_init(&coa_parent->lock);
    60c8:	48 8d 78 40          	lea    0x40(%rax),%rdi
    60cc:	48 c7 c2 00 00 00 00 	mov    $0x0,%rdx
    60d3:	48 c7 c6 00 00 00 00 	mov    $0x0,%rsi
    60da:	e8 00 00 00 00       	callq  60df <init_coa_parent+0x3f>

	coa_parent->pfn=page_to_pfn(page);
    60df:	48 b8 00 00 00 00 00 	movabs $0x160000000000,%rax
    60e6:	16 00 00 
#define LIST_HEAD(name) \
	struct list_head name = LIST_HEAD_INIT(name)

static inline void INIT_LIST_HEAD(struct list_head *list)
{
	list->next = list;
    60e9:	48 89 1b             	mov    %rbx,(%rbx)
 *
 * Atomically sets the value of @v to @i.
 */
static inline void atomic_set(atomic_t *v, int i)
{
	v->counter = i;
    60ec:	c7 43 28 00 00 00 00 	movl   $0x0,0x28(%rbx)
    60f3:	4c 01 e0             	add    %r12,%rax
	init_copy_num(coa_parent);
	coa_parent->parent_head=NULL;
    60f6:	48 c7 43 18 00 00 00 	movq   $0x0,0x18(%rbx)
    60fd:	00 
	coa_parent->owner=NULL;
    60fe:	48 c7 43 20 00 00 00 	movq   $0x0,0x20(%rbx)
    6105:	00 
	struct sclock_coa_parent* coa_parent=kmem_cache_alloc(sclock_coa_parent_cachep,GFP_KERNEL);
	if(coa_parent==NULL)
	  return NULL;
	mutex_init(&coa_parent->lock);

	coa_parent->pfn=page_to_pfn(page);
    6106:	48 c1 f8 06          	sar    $0x6,%rax
	list->prev = list;
    610a:	48 89 5b 08          	mov    %rbx,0x8(%rbx)
	coa_parent->parent_head=NULL;
	coa_parent->owner=NULL;
	INIT_LIST_HEAD(&coa_parent->head);
	page->coa_head=&coa_parent->head;
	//	printk("add one parent into list\n");
	list_add_tail(&coa_parent->node,parent_page_headp);
    610e:	48 8d 4b 30          	lea    0x30(%rbx),%rcx
	struct sclock_coa_parent* coa_parent=kmem_cache_alloc(sclock_coa_parent_cachep,GFP_KERNEL);
	if(coa_parent==NULL)
	  return NULL;
	mutex_init(&coa_parent->lock);

	coa_parent->pfn=page_to_pfn(page);
    6112:	48 89 43 10          	mov    %rax,0x10(%rbx)
	coa_parent->parent_head=NULL;
	coa_parent->owner=NULL;
	INIT_LIST_HEAD(&coa_parent->head);
	page->coa_head=&coa_parent->head;
	//	printk("add one parent into list\n");
	list_add_tail(&coa_parent->node,parent_page_headp);
    6116:	48 8b 05 00 00 00 00 	mov    0x0(%rip),%rax        # 611d <init_coa_parent+0x7d>
	coa_parent->pfn=page_to_pfn(page);
	init_copy_num(coa_parent);
	coa_parent->parent_head=NULL;
	coa_parent->owner=NULL;
	INIT_LIST_HEAD(&coa_parent->head);
	page->coa_head=&coa_parent->head;
    611d:	49 89 5c 24 38       	mov    %rbx,0x38(%r12)
 * Insert a new entry before the specified head.
 * This is useful for implementing queues.
 */
static inline void list_add_tail(struct list_head *new, struct list_head *head)
{
	__list_add(new, head->prev, head);
    6122:	48 8b 50 08          	mov    0x8(%rax),%rdx
#ifndef CONFIG_DEBUG_LIST
static inline void __list_add(struct list_head *new,
			      struct list_head *prev,
			      struct list_head *next)
{
	next->prev = new;
    6126:	48 89 48 08          	mov    %rcx,0x8(%rax)
	new->next = next;
    612a:	48 89 43 30          	mov    %rax,0x30(%rbx)
	new->prev = prev;
    612e:	48 89 53 38          	mov    %rdx,0x38(%rbx)
	prev->next = new;
    6132:	48 89 0a             	mov    %rcx,(%rdx)
	//	printk("add one parent into list\n");
	list_add_tail(&coa_parent->node,parent_page_headp);
	return coa_parent;
}
    6135:	48 89 d8             	mov    %rbx,%rax
    6138:	5b                   	pop    %rbx
    6139:	41 5c                	pop    %r12
    613b:	5d                   	pop    %rbp
    613c:	c3                   	retq   
    613d:	0f 1f 00             	nopl   (%rax)

0000000000006140 <__do_copy_on_read>:
	if(coa_parent->parent_head==NULL){
		return coa_parent;
	}
	return list_entry(coa_parent->parent_head,struct sclock_coa_parent,head);
}
static int __do_copy_on_read(struct mm_struct *mm, struct vm_area_struct *vma,unsigned long address, pte_t *page_table, pmd_t *pmd,spinlock_t *ptl, pte_t orig_pte)__releases(ptl){
    6140:	e8 00 00 00 00       	callq  6145 <__do_copy_on_read+0x5>
    6145:	55                   	push   %rbp
    6146:	48 89 e5             	mov    %rsp,%rbp
    6149:	41 57                	push   %r15
    614b:	49 89 ff             	mov    %rdi,%r15
    614e:	41 56                	push   %r14
    6150:	41 55                	push   %r13
    6152:	49 89 d5             	mov    %rdx,%r13
    6155:	41 54                	push   %r12
    6157:	4d 89 cc             	mov    %r9,%r12
    615a:	53                   	push   %rbx
    615b:	48 89 f3             	mov    %rsi,%rbx
    615e:	48 83 ec 60          	sub    $0x60,%rsp
    6162:	48 89 4d d0          	mov    %rcx,-0x30(%rbp)
    6166:	4c 89 45 b8          	mov    %r8,-0x48(%rbp)
	struct pid_namespace* pid_ns=NULL,*ns;
	struct sclock_coa_parent * coa_parent;
	unsigned long mmun_start = 0;	/* For mmu_notifiers */
	unsigned long mmun_end = 0;	/* For mmu_notifiers */
	unsigned long flags;
	unsigned long long time1=get_rdtsc();
    616a:	e8 00 00 00 00       	callq  616f <__do_copy_on_read+0x2f>
    616f:	48 89 45 c8          	mov    %rax,-0x38(%rbp)
	if(!(sclock_control->action&request_coa)){
    6173:	48 8b 05 00 00 00 00 	mov    0x0(%rip),%rax        # 617a <__do_copy_on_read+0x3a>
    617a:	f6 00 01             	testb  $0x1,(%rax)
    617d:	0f 85 9d 00 00 00    	jne    6220 <__do_copy_on_read+0xe0>
		mm->def_flags&=~VM_ISOLATION;
    6183:	48 b8 ff ff ff ff fe 	movabs $0xfffffffeffffffff,%rax
    618a:	ff ff ff 
    618d:	49 21 87 f8 00 00 00 	and    %rax,0xf8(%r15)
		vma->vm_flags&=~VM_ISOLATION;
    6194:	48 21 43 50          	and    %rax,0x50(%rbx)
    6198:	4d 89 e6             	mov    %r12,%r14
			//		set_pte_at_notify(mm, address, page_table, entry);
			if (old_page)
			  page_cpupid_xchg_last(old_page, (1 << LAST_CPUPID_SHIFT) - 1);
			flush_cache_page(vma, address, pte_pfn(orig_pte));
			entry = pte_mkyoung(orig_pte);
			entry.pte&=~_PAGE_ISOLATION;
    619b:	48 b9 ff ff ff ff ff 	movabs $0xfff7ffffffffffff,%rcx
    61a2:	ff f7 ff 
    61a5:	48 23 4d 10          	and    0x10(%rbp),%rcx
			if (ptep_set_access_flags(vma, address, page_table, entry,1))
    61a9:	4c 89 ee             	mov    %r13,%rsi
    61ac:	41 b8 01 00 00 00    	mov    $0x1,%r8d
    61b2:	48 89 df             	mov    %rbx,%rdi
				pte_unmap_unlock(page_table, ptl);
				goto early_out;
			}
not_coa:

			ret|=VM_FAULT_WRITE;
    61b5:	41 bd 08 00 00 00    	mov    $0x8,%r13d
			if (old_page)
			  page_cpupid_xchg_last(old_page, (1 << LAST_CPUPID_SHIFT) - 1);
			flush_cache_page(vma, address, pte_pfn(orig_pte));
			entry = pte_mkyoung(orig_pte);
			entry.pte&=~_PAGE_ISOLATION;
			if (ptep_set_access_flags(vma, address, page_table, entry,1))
    61bb:	48 8b 55 d0          	mov    -0x30(%rbp),%rdx
			//		set_pte_at_notify(mm, address, page_table, entry);
			if (old_page)
			  page_cpupid_xchg_last(old_page, (1 << LAST_CPUPID_SHIFT) - 1);
			flush_cache_page(vma, address, pte_pfn(orig_pte));
			entry = pte_mkyoung(orig_pte);
			entry.pte&=~_PAGE_ISOLATION;
    61bf:	48 83 c9 20          	or     $0x20,%rcx
			if (ptep_set_access_flags(vma, address, page_table, entry,1))
    61c3:	e8 00 00 00 00       	callq  61c8 <__do_copy_on_read+0x88>
    61c8:	4c 89 f7             	mov    %r14,%rdi
    61cb:	e8 00 00 00 00       	callq  61d0 <__do_copy_on_read+0x90>
	}
	__get_cpu_var(global_interval_coa_normal)+=get_rdtsc()-time1;
	__get_cpu_var(global_count_coa_normal)++;
	return ret;
early_out:
	__get_cpu_var(global_interval_coa_early)+=get_rdtsc()-time1; 
    61d0:	48 c7 c3 00 00 00 00 	mov    $0x0,%rbx
    61d7:	65 48 03 1c 25 00 00 	add    %gs:0x0,%rbx
    61de:	00 00 
    61e0:	4c 8b 23             	mov    (%rbx),%r12
    61e3:	e8 00 00 00 00       	callq  61e8 <__do_copy_on_read+0xa8>
    61e8:	48 2b 45 c8          	sub    -0x38(%rbp),%rax
	__get_cpu_var(global_count_coa_early)++;
    61ec:	48 c7 c2 00 00 00 00 	mov    $0x0,%rdx
	}
	__get_cpu_var(global_interval_coa_normal)+=get_rdtsc()-time1;
	__get_cpu_var(global_count_coa_normal)++;
	return ret;
early_out:
	__get_cpu_var(global_interval_coa_early)+=get_rdtsc()-time1; 
    61f3:	4c 01 e0             	add    %r12,%rax
    61f6:	48 89 03             	mov    %rax,(%rbx)
	__get_cpu_var(global_count_coa_early)++;
    61f9:	65 48 03 14 25 00 00 	add    %gs:0x0,%rdx
    6200:	00 00 
    6202:	48 83 02 01          	addq   $0x1,(%rdx)
	return ret;
    6206:	44 89 e8             	mov    %r13d,%eax
	__get_cpu_var(global_interval_coa_fail)+=get_rdtsc()-time1;
	__get_cpu_var(global_count_coa_fail)++;

	return VM_FAULT_OOM;

}
    6209:	48 83 c4 60          	add    $0x60,%rsp
    620d:	5b                   	pop    %rbx
    620e:	41 5c                	pop    %r12
    6210:	41 5d                	pop    %r13
    6212:	41 5e                	pop    %r14
    6214:	41 5f                	pop    %r15
    6216:	5d                   	pop    %rbp
    6217:	c3                   	retq   
    6218:	0f 1f 84 00 00 00 00 	nopl   0x0(%rax,%rax,1)
    621f:	00 
	if(!(sclock_control->action&request_coa)){
		mm->def_flags&=~VM_ISOLATION;
		vma->vm_flags&=~VM_ISOLATION;
		goto not_coa;
	}
	if(!mm->owner){
    6220:	49 8b 87 90 03 00 00 	mov    0x390(%r15),%rax
    6227:	48 85 c0             	test   %rax,%rax
    622a:	0f 84 68 ff ff ff    	je     6198 <__do_copy_on_read+0x58>
		goto not_coa;
	}
	if(!(mm->def_flags&VM_ISOLATION)){
    6230:	41 f6 87 fc 00 00 00 	testb  $0x1,0xfc(%r15)
    6237:	01 
    6238:	0f 84 5a ff ff ff    	je     6198 <__do_copy_on_read+0x58>
    623e:	48 8b 80 60 03 00 00 	mov    0x360(%rax),%rax
	if (pid)
    6245:	48 85 c0             	test   %rax,%rax
    6248:	0f 84 42 02 00 00    	je     6490 <__do_copy_on_read+0x350>
		ns = pid->numbers[pid->level].ns;
    624e:	8b 50 04             	mov    0x4(%rax),%edx
    6251:	48 c1 e2 05          	shl    $0x5,%rdx
    6255:	48 8b 44 10 38       	mov    0x38(%rax,%rdx,1),%rax
    625a:	48 89 45 b0          	mov    %rax,-0x50(%rbp)
		goto not_coa;
	}
	pid_ns=ns_of_pid(task_pid(mm->owner));
	old_page = vm_normal_page(vma, address, orig_pte);
    625e:	48 8b 55 10          	mov    0x10(%rbp),%rdx
    6262:	4c 89 ee             	mov    %r13,%rsi
    6265:	48 89 df             	mov    %rbx,%rdi
    6268:	e8 00 00 00 00       	callq  626d <__do_copy_on_read+0x12d>
	if (!old_page) {
    626d:	48 85 c0             	test   %rax,%rax
	}
	if(!(mm->def_flags&VM_ISOLATION)){
		goto not_coa;
	}
	pid_ns=ns_of_pid(task_pid(mm->owner));
	old_page = vm_normal_page(vma, address, orig_pte);
    6270:	48 89 45 c0          	mov    %rax,-0x40(%rbp)
	if (!old_page) {
    6274:	48 89 c6             	mov    %rax,%rsi
    6277:	0f 84 63 07 00 00    	je     69e0 <__do_copy_on_read+0x8a0>
	return page->mapping;
}

static inline int PageAnon(struct page *page)
{
	return ((unsigned long)page->mapping & PAGE_MAPPING_ANON) != 0;
    627d:	48 8b 40 08          	mov    0x8(%rax),%rax
		 * accounting on raw pfn maps.
		 */
		//	printk(KERN_DEBUG"-----------!old_page------------\n");
		goto gotten;
	}
	if(PageAnon(old_page)&&!PageKsm(old_page)){
    6281:	a8 01                	test   $0x1,%al
    6283:	74 4c                	je     62d1 <__do_copy_on_read+0x191>
    6285:	83 e0 03             	and    $0x3,%eax
    6288:	48 83 f8 03          	cmp    $0x3,%rax
    628c:	74 43                	je     62d1 <__do_copy_on_read+0x191>
 * This operation is atomic and cannot be reordered.
 * It also implies a memory barrier.
 */
static inline int test_and_set_bit(long nr, volatile unsigned long *addr)
{
	GEN_BINARY_RMWcc(LOCK_PREFIX "bts", *addr, "Ir", nr, "%0", "c");
    628e:	f0 0f ba 2e 00       	lock btsl $0x0,(%rsi)
    6293:	72 6b                	jb     6300 <__do_copy_on_read+0x1c0>
				unlock_page(old_page);
				goto unlock;
			}
			page_cache_release(old_page);
		}
		if (reuse_swap_page(old_page)) {
    6295:	48 8b 7d c0          	mov    -0x40(%rbp),%rdi
    6299:	e8 00 00 00 00       	callq  629e <__do_copy_on_read+0x15e>
    629e:	85 c0                	test   %eax,%eax
    62a0:	74 26                	je     62c8 <__do_copy_on_read+0x188>
			/*
			 * The page is all ours.  Move it to our anon_vma so
			 * the rmap code will not search our parent or siblings.
			 * Protected against the rmap code by the page lock.
			 */
			page_move_anon_rmap(old_page, vma, address);
    62a2:	4c 8b 7d c0          	mov    -0x40(%rbp),%r15
    62a6:	4c 89 ea             	mov    %r13,%rdx
    62a9:	48 89 de             	mov    %rbx,%rsi
    62ac:	4d 89 e6             	mov    %r12,%r14
    62af:	4c 89 ff             	mov    %r15,%rdi
    62b2:	e8 00 00 00 00       	callq  62b7 <__do_copy_on_read+0x177>
			unlock_page(old_page);
    62b7:	4c 89 ff             	mov    %r15,%rdi
    62ba:	e8 00 00 00 00       	callq  62bf <__do_copy_on_read+0x17f>
			goto not_coa;
    62bf:	eb 24                	jmp    62e5 <__do_copy_on_read+0x1a5>
    62c1:	0f 1f 80 00 00 00 00 	nopl   0x0(%rax)
		}
		unlock_page(old_page);
    62c8:	48 8b 7d c0          	mov    -0x40(%rbp),%rdi
    62cc:	e8 00 00 00 00       	callq  62d1 <__do_copy_on_read+0x191>
	}
	if(!isContainerSharedPage(old_page)){
    62d1:	48 8b 7d c0          	mov    -0x40(%rbp),%rdi
    62d5:	4d 89 e6             	mov    %r12,%r14
    62d8:	e8 00 00 00 00       	callq  62dd <__do_copy_on_read+0x19d>
    62dd:	85 c0                	test   %eax,%eax
    62df:	0f 85 bb 01 00 00    	jne    64a0 <__do_copy_on_read+0x360>

			ret|=VM_FAULT_WRITE;
			//	ptep_clear_flush(vma, address, page_table);
			//		set_pte_at_notify(mm, address, page_table, entry);
			if (old_page)
			  page_cpupid_xchg_last(old_page, (1 << LAST_CPUPID_SHIFT) - 1);
    62e5:	48 8b 7d c0          	mov    -0x40(%rbp),%rdi
    62e9:	be ff ff 00 00       	mov    $0xffff,%esi
    62ee:	e8 00 00 00 00       	callq  62f3 <__do_copy_on_read+0x1b3>
    62f3:	e9 a3 fe ff ff       	jmpq   619b <__do_copy_on_read+0x5b>
    62f8:	0f 1f 84 00 00 00 00 	nopl   0x0(%rax,%rax,1)
    62ff:	00 
		//	printk(KERN_DEBUG"-----------!old_page------------\n");
		goto gotten;
	}
	if(PageAnon(old_page)&&!PageKsm(old_page)){
		if (!trylock_page(old_page)) {
			page_cache_get(old_page);
    6300:	48 8b 7d c0          	mov    -0x40(%rbp),%rdi
    6304:	e8 f7 9d ff ff       	callq  100 <get_page>
    6309:	4c 89 e7             	mov    %r12,%rdi
    630c:	e8 00 00 00 00       	callq  6311 <__do_copy_on_read+0x1d1>
			pte_unmap_unlock(page_table, ptl);
			lock_page(old_page);
    6311:	48 8b 7d c0          	mov    -0x40(%rbp),%rdi
    6315:	e8 36 a4 ff ff       	callq  750 <lock_page>
			page_table = pte_offset_map_lock(mm, pmd, address,
    631a:	48 8b 7d b8          	mov    -0x48(%rbp),%rdi
    631e:	e8 0d 9e ff ff       	callq  130 <pte_lockptr.isra.16>
    6323:	48 8b 7d b8          	mov    -0x48(%rbp),%rdi
    6327:	49 89 c4             	mov    %rax,%r12
    632a:	4c 89 ee             	mov    %r13,%rsi
    632d:	e8 ce 9c ff ff       	callq  0 <pte_offset_kernel>
	raw_spin_lock_init(&(_lock)->rlock);		\
} while (0)

static inline void spin_lock(spinlock_t *lock)
{
	raw_spin_lock(&lock->rlock);
    6332:	4c 89 e7             	mov    %r12,%rdi
    6335:	48 89 45 d0          	mov    %rax,-0x30(%rbp)
    6339:	4c 89 65 80          	mov    %r12,-0x80(%rbp)
    633d:	e8 00 00 00 00       	callq  6342 <__do_copy_on_read+0x202>
						&ptl);
			if (!pte_same(*page_table, orig_pte)) {
    6342:	48 8b 45 d0          	mov    -0x30(%rbp),%rax
    6346:	48 8b 75 10          	mov    0x10(%rbp),%rsi
    634a:	48 39 30             	cmp    %rsi,(%rax)
    634d:	0f 84 92 08 00 00    	je     6be5 <__do_copy_on_read+0xaa5>
				unlock_page(old_page);
    6353:	48 8b 7d c0          	mov    -0x40(%rbp),%rdi
}
static int __do_copy_on_read(struct mm_struct *mm, struct vm_area_struct *vma,unsigned long address, pte_t *page_table, pmd_t *pmd,spinlock_t *ptl, pte_t orig_pte)__releases(ptl){
	struct page *old_page=NULL, *new_page = NULL,*real_new_page=NULL;
	pte_t entry=orig_pte;
	bool isFile=false,pte_lock=true;
	int ret = 0;
    6357:	45 31 ed             	xor    %r13d,%r13d
			pte_unmap_unlock(page_table, ptl);
			lock_page(old_page);
			page_table = pte_offset_map_lock(mm, pmd, address,
						&ptl);
			if (!pte_same(*page_table, orig_pte)) {
				unlock_page(old_page);
    635a:	e8 00 00 00 00       	callq  635f <__do_copy_on_read+0x21f>
	bool not_new=false;
	struct sclock_coa_children* coa_children;
	struct pid_namespace* pid_ns=NULL,*ns;
	struct sclock_coa_parent * coa_parent;
	unsigned long mmun_start = 0;	/* For mmu_notifiers */
	unsigned long mmun_end = 0;	/* For mmu_notifiers */
    635f:	48 c7 45 a0 00 00 00 	movq   $0x0,-0x60(%rbp)
    6366:	00 
	int reuse=0;
	bool not_new=false;
	struct sclock_coa_children* coa_children;
	struct pid_namespace* pid_ns=NULL,*ns;
	struct sclock_coa_parent * coa_parent;
	unsigned long mmun_start = 0;	/* For mmu_notifiers */
    6367:	48 c7 45 a8 00 00 00 	movq   $0x0,-0x58(%rbp)
    636e:	00 
		return coa_parent;
	}
	return list_entry(coa_parent->parent_head,struct sclock_coa_parent,head);
}
static int __do_copy_on_read(struct mm_struct *mm, struct vm_area_struct *vma,unsigned long address, pte_t *page_table, pmd_t *pmd,spinlock_t *ptl, pte_t orig_pte)__releases(ptl){
	struct page *old_page=NULL, *new_page = NULL,*real_new_page=NULL;
    636f:	48 c7 45 d0 00 00 00 	movq   $0x0,-0x30(%rbp)
    6376:	00 
	raw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \
} while (0)

static inline void spin_unlock(spinlock_t *lock)
{
	raw_spin_unlock(&lock->rlock);
    6377:	48 8b 7d 80          	mov    -0x80(%rbp),%rdi
    637b:	e8 00 00 00 00       	callq  6380 <__do_copy_on_read+0x240>
	if(new_page)
	  page_cache_release(new_page);
unlock:
	if(pte_lock)
	  pte_unmap_unlock(page_table, ptl);
	if (mmun_end > mmun_start)
    6380:	48 8b 45 a0          	mov    -0x60(%rbp),%rax
    6384:	48 39 45 a8          	cmp    %rax,-0x58(%rbp)
    6388:	73 0e                	jae    6398 <__do_copy_on_read+0x258>
}

static inline void mmu_notifier_invalidate_range_end(struct mm_struct *mm,
				  unsigned long start, unsigned long end)
{
	if (mm_has_notifiers(mm))
    638a:	49 83 bf a0 03 00 00 	cmpq   $0x0,0x3a0(%r15)
    6391:	00 
    6392:	0f 85 5d 08 00 00    	jne    6bf5 <__do_copy_on_read+0xab5>
	  mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);
	if (old_page) {
    6398:	48 83 7d c0 00       	cmpq   $0x0,-0x40(%rbp)
    639d:	0f 84 a5 00 00 00    	je     6448 <__do_copy_on_read+0x308>
		/*
		 * Don't let another task, with possibly unlocked vma,
		 * keep the mlocked page.
		 */
		if ( (ret&VM_FAULT_WRITE)&&(vma->vm_flags&VM_LOCKED)) {
    63a3:	45 85 ed             	test   %r13d,%r13d
    63a6:	74 0a                	je     63b2 <__do_copy_on_read+0x272>
    63a8:	f6 43 51 20          	testb  $0x20,0x51(%rbx)
    63ac:	0f 85 1e 07 00 00    	jne    6ad0 <__do_copy_on_read+0x990>
			lock_page(old_page);	/* LRU manipulation */
			munlock_vma_page(old_page);
			unlock_page(old_page);
		}
		page_cache_release(old_page);
    63b2:	48 8b 7d c0          	mov    -0x40(%rbp),%rdi
    63b6:	e8 00 00 00 00       	callq  63bb <__do_copy_on_read+0x27b>
		if(real_new_page){
    63bb:	4c 8b 7d d0          	mov    -0x30(%rbp),%r15
    63bf:	4d 85 ff             	test   %r15,%r15
    63c2:	0f 84 80 00 00 00    	je     6448 <__do_copy_on_read+0x308>
	return !memcmp_pages(page1, page2);
}


struct sclock_coa_parent *get_original(struct sclock_coa_parent *coa_parent){
	if(coa_parent->parent_head==NULL){
    63c8:	49 8b 46 18          	mov    0x18(%r14),%rax
	return 1;
}
static int	add_page_into_children_pages(struct sclock_coa_parent* coa_parent,struct page* page,struct pid_namespace *pid_ns){
	//	if(sclock_coa_children_cachep==NULL)
	//	struct sclock_coa_children* coa_children=kmalloc(sclock_coa_children_cachep,GFP_KERNEL);
	struct sclock_coa_children*	coa_children=kmem_cache_alloc(sclock_coa_children_cachep,GFP_KERNEL);
    63cc:	48 8b 3d 00 00 00 00 	mov    0x0(%rip),%rdi        # 63d3 <__do_copy_on_read+0x293>
    63d3:	be d0 00 00 00       	mov    $0xd0,%esi
	return !memcmp_pages(page1, page2);
}


struct sclock_coa_parent *get_original(struct sclock_coa_parent *coa_parent){
	if(coa_parent->parent_head==NULL){
    63d8:	48 85 c0             	test   %rax,%rax
    63db:	4c 0f 45 f0          	cmovne %rax,%r14
	return 1;
}
static int	add_page_into_children_pages(struct sclock_coa_parent* coa_parent,struct page* page,struct pid_namespace *pid_ns){
	//	if(sclock_coa_children_cachep==NULL)
	//	struct sclock_coa_children* coa_children=kmalloc(sclock_coa_children_cachep,GFP_KERNEL);
	struct sclock_coa_children*	coa_children=kmem_cache_alloc(sclock_coa_children_cachep,GFP_KERNEL);
    63df:	e8 00 00 00 00       	callq  63e4 <__do_copy_on_read+0x2a4>
	struct page* source;
	unsigned long flags;
	if(coa_children==NULL)
    63e4:	48 85 c0             	test   %rax,%rax
	return 1;
}
static int	add_page_into_children_pages(struct sclock_coa_parent* coa_parent,struct page* page,struct pid_namespace *pid_ns){
	//	if(sclock_coa_children_cachep==NULL)
	//	struct sclock_coa_children* coa_children=kmalloc(sclock_coa_children_cachep,GFP_KERNEL);
	struct sclock_coa_children*	coa_children=kmem_cache_alloc(sclock_coa_children_cachep,GFP_KERNEL);
    63e7:	48 89 c3             	mov    %rax,%rbx
	struct page* source;
	unsigned long flags;
	if(coa_children==NULL)
    63ea:	74 5c                	je     6448 <__do_copy_on_read+0x308>
	  return -1;
	//printk("add one copy, page=%lx,index=%lx\n",page,page->index);
	coa_children->pfn=page_to_pfn(page);
    63ec:	48 b8 00 00 00 00 00 	movabs $0x160000000000,%rax
    63f3:	16 00 00 
	atomic_t copy_counter;
	struct list_head node;
	struct mutex lock;
};
static inline void coa_list_lock(struct sclock_coa_parent* coa_parent){
	mutex_lock(&coa_parent->lock);
    63f6:	4d 8d 66 40          	lea    0x40(%r14),%r12
	coa_children->parent_head=&coa_parent->head;
    63fa:	4c 89 73 18          	mov    %r14,0x18(%rbx)
	struct page* source;
	unsigned long flags;
	if(coa_children==NULL)
	  return -1;
	//printk("add one copy, page=%lx,index=%lx\n",page,page->index);
	coa_children->pfn=page_to_pfn(page);
    63fe:	4c 01 f8             	add    %r15,%rax
    6401:	48 c1 f8 06          	sar    $0x6,%rax
    6405:	4c 89 e7             	mov    %r12,%rdi
    6408:	48 89 43 10          	mov    %rax,0x10(%rbx)
	coa_children->parent_head=&coa_parent->head;
	coa_children->owner=pid_ns;
    640c:	48 8b 45 b0          	mov    -0x50(%rbp),%rax
    6410:	48 89 43 20          	mov    %rax,0x20(%rbx)
	page->coa_head=&(coa_children->head);
    6414:	49 89 5f 38          	mov    %rbx,0x38(%r15)
    6418:	e8 00 00 00 00       	callq  641d <__do_copy_on_read+0x2dd>
 *
 * Atomically reads the value of @v.
 */
static inline int atomic_read(const atomic_t *v)
{
	return (*(volatile int *)&(v)->counter);
    641d:	41 8b 46 28          	mov    0x28(%r14),%eax
	source =pfn_to_page(coa_parent->pfn);
	//	printk("add one copy %lx: %d,%dfor %lx->count%d,mapcount%d in process %s pid=%d\n",coa_children->pfn,atomic_read(&page->_count),page_mapcount(page),page_to_pfn(source),atomic_read(&source->_count),page_mapcount(source),current->comm,current->pid);
	coa_list_lock(coa_parent);
	if(get_copy_num(coa_parent)>=0)	
    6421:	85 c0                	test   %eax,%eax
    6423:	0f 88 8d 06 00 00    	js     6ab6 <__do_copy_on_read+0x976>
 *
 * Atomically increments @v by 1.
 */
static inline void atomic_inc(atomic_t *v)
{
	asm volatile(LOCK_PREFIX "incl %0"
    6429:	f0 41 ff 46 28       	lock incl 0x28(%r14)
 * Insert a new entry before the specified head.
 * This is useful for implementing queues.
 */
static inline void list_add_tail(struct list_head *new, struct list_head *head)
{
	__list_add(new, head->prev, head);
    642e:	49 8b 46 08          	mov    0x8(%r14),%rax
static inline int coa_list_trylock(struct sclock_coa_parent* coa_parent){
	return mutex_trylock(&coa_parent->lock);
}
long RedoRequestProtectOthers(struct task_struct *p);
static inline void coa_list_unlock(struct sclock_coa_parent* coa_parent){
	mutex_unlock(&coa_parent->lock);
    6432:	4c 89 e7             	mov    %r12,%rdi
#ifndef CONFIG_DEBUG_LIST
static inline void __list_add(struct list_head *new,
			      struct list_head *prev,
			      struct list_head *next)
{
	next->prev = new;
    6435:	49 89 5e 08          	mov    %rbx,0x8(%r14)
	new->next = next;
    6439:	4c 89 33             	mov    %r14,(%rbx)
	new->prev = prev;
    643c:	48 89 43 08          	mov    %rax,0x8(%rbx)
	prev->next = new;
    6440:	48 89 18             	mov    %rbx,(%rax)
    6443:	e8 00 00 00 00       	callq  6448 <__do_copy_on_read+0x308>
			//	printk("new page by coa at %lx",address);
			add_page_into_children_pages(get_original(coa_parent),real_new_page,pid_ns);
			//dump_page(real_new_page);
		}
	}
	__get_cpu_var(global_interval_coa_normal)+=get_rdtsc()-time1;
    6448:	48 c7 c3 00 00 00 00 	mov    $0x0,%rbx
    644f:	65 48 03 1c 25 00 00 	add    %gs:0x0,%rbx
    6456:	00 00 
    6458:	4c 8b 23             	mov    (%rbx),%r12
    645b:	e8 00 00 00 00       	callq  6460 <__do_copy_on_read+0x320>
    6460:	48 2b 45 c8          	sub    -0x38(%rbp),%rax
    6464:	4c 01 e0             	add    %r12,%rax
    6467:	48 89 03             	mov    %rax,(%rbx)
	__get_cpu_var(global_count_coa_normal)++;
    646a:	48 c7 c0 00 00 00 00 	mov    $0x0,%rax
    6471:	65 48 03 04 25 00 00 	add    %gs:0x0,%rax
    6478:	00 00 
    647a:	48 83 00 01          	addq   $0x1,(%rax)
	return ret;
    647e:	44 89 e8             	mov    %r13d,%eax
    6481:	e9 83 fd ff ff       	jmpq   6209 <__do_copy_on_read+0xc9>
    6486:	66 2e 0f 1f 84 00 00 	nopw   %cs:0x0(%rax,%rax,1)
    648d:	00 00 00 
 * 	is expected to be non-NULL. If @pid is NULL, caller should handle
 * 	the resulting NULL pid-ns.
 */
static inline struct pid_namespace *ns_of_pid(struct pid *pid)
{
	struct pid_namespace *ns = NULL;
    6490:	48 c7 45 b0 00 00 00 	movq   $0x0,-0x50(%rbp)
    6497:	00 
    6498:	e9 c1 fd ff ff       	jmpq   625e <__do_copy_on_read+0x11e>
    649d:	0f 1f 00             	nopl   (%rax)
}

static __always_inline int constant_test_bit(long nr, const volatile unsigned long *addr)
{
	return ((1UL << (nr & (BITS_PER_LONG-1))) &
		(addr[nr >> _BITOPS_LONG_SHIFT])) != 0;
    64a0:	48 8b 45 c0          	mov    -0x40(%rbp),%rax
    64a4:	48 8b 00             	mov    (%rax),%rax
void get_huge_page_tail(struct page *page);/*ziqiao*/
extern bool __get_page_tail(struct page *page);

static inline void get_page(struct page *page)
{
	if (unlikely(PageTail(page)))
    64a7:	f6 c4 80             	test   $0x80,%ah
    64aa:	0f 85 a4 06 00 00    	jne    6b54 <__do_copy_on_read+0xa14>
    64b0:	48 8b 45 c0          	mov    -0x40(%rbp),%rax
    64b4:	f0 ff 40 1c          	lock incl 0x1c(%rax)
    64b8:	4c 89 e7             	mov    %r12,%rdi
    64bb:	e8 00 00 00 00       	callq  64c0 <__do_copy_on_read+0x380>
gotten:
	pte_unmap_unlock(page_table, ptl);
	/*check copy or parent and get coa_parent*/
	if(old_page){
check_coa_head:
		if(!old_page->coa_head){
    64c0:	48 8b 45 c0          	mov    -0x40(%rbp),%rax
    64c4:	4c 8b 70 38          	mov    0x38(%rax),%r14
    64c8:	4d 85 f6             	test   %r14,%r14
    64cb:	0f 84 23 06 00 00    	je     6af4 <__do_copy_on_read+0x9b4>
			init_coa_parent(old_page);
			//goto reuse;
		}
		coa_parent=list_entry(old_page->coa_head,struct sclock_coa_parent,head);

		if(coa_parent->owner==NULL){//first accessed shared page with COA alert
    64d1:	49 8b 46 20          	mov    0x20(%r14),%rax
    64d5:	48 85 c0             	test   %rax,%rax
    64d8:	0f 84 6a 04 00 00    	je     6948 <__do_copy_on_read+0x808>
			coa_parent->owner=pid_ns;
			goto reuse;
		}
		//	goto reuse;

		if(pid_ns==coa_parent->owner){
    64de:	48 3b 45 b0          	cmp    -0x50(%rbp),%rax
    64e2:	0f 84 68 04 00 00    	je     6950 <__do_copy_on_read+0x810>
	}

	//check whether the owner still exists
	//	printk("continue coa\n");

	if (unlikely(anon_vma_prepare(vma)))
    64e8:	48 89 df             	mov    %rbx,%rdi
    64eb:	e8 00 00 00 00       	callq  64f0 <__do_copy_on_read+0x3b0>
    64f0:	85 c0                	test   %eax,%eax
    64f2:	0f 85 34 05 00 00    	jne    6a2c <__do_copy_on_read+0x8ec>
	  goto oom;
	/*check whether we can use a exited exclusive page for this pid_ns*/
	if(coa_parent){
    64f8:	4d 85 f6             	test   %r14,%r14
    64fb:	0f 84 40 01 00 00    	je     6641 <__do_copy_on_read+0x501>
	return !memcmp_pages(page1, page2);
}


struct sclock_coa_parent *get_original(struct sclock_coa_parent *coa_parent){
	if(coa_parent->parent_head==NULL){
    6501:	4d 8b 66 18          	mov    0x18(%r14),%r12
    6505:	4d 85 e4             	test   %r12,%r12
    6508:	4d 0f 44 e6          	cmove  %r14,%r12
	atomic_t copy_counter;
	struct list_head node;
	struct mutex lock;
};
static inline void coa_list_lock(struct sclock_coa_parent* coa_parent){
	mutex_lock(&coa_parent->lock);
    650c:	4d 8d 4c 24 40       	lea    0x40(%r12),%r9
    6511:	4c 89 cf             	mov    %r9,%rdi
    6514:	4c 89 4d d0          	mov    %r9,-0x30(%rbp)
    6518:	e8 00 00 00 00       	callq  651d <__do_copy_on_read+0x3dd>
	  goto oom;
	/*check whether we can use a exited exclusive page for this pid_ns*/
	if(coa_parent){
		struct sclock_coa_parent * original=get_original(coa_parent);
		coa_list_lock(original);
		if(coa_parent->pfn!=page_to_pfn(old_page)){
    651d:	48 b8 00 00 00 00 00 	movabs $0x160000000000,%rax
    6524:	16 00 00 
    6527:	48 03 45 c0          	add    -0x40(%rbp),%rax
    652b:	4d 8b 46 10          	mov    0x10(%r14),%r8
    652f:	4c 8b 4d d0          	mov    -0x30(%rbp),%r9
    6533:	48 c1 f8 06          	sar    $0x6,%rax
    6537:	49 39 c0             	cmp    %rax,%r8
    653a:	0f 85 f9 00 00 00    	jne    6639 <__do_copy_on_read+0x4f9>
	return !memcmp_pages(page1, page2);
}


struct sclock_coa_parent *get_original(struct sclock_coa_parent *coa_parent){
	if(coa_parent->parent_head==NULL){
    6540:	49 8b 46 18          	mov    0x18(%r14),%rax
    6544:	4c 89 f2             	mov    %r14,%rdx
 */
#include <linux/vmstat.h>

static __always_inline void *lowmem_page_address(const struct page *page)
{
	return __va(PFN_PHYS(page_to_pfn(page)));
    6547:	48 b9 00 00 00 00 00 	movabs $0xffff880000000000,%rcx
    654e:	88 ff ff 
		}else{
			list_for_each_entry(coa_children,&get_original(coa_parent)->head,head){
				if(coa_children->owner==pid_ns){
					unsigned long coa_pfn=coa_children->pfn;
					if((coa_pfn!=COA_DEL)&&(coa_pfn!=COA_SKIP)){
						new_page=pfn_to_page(coa_pfn);
    6551:	49 ba 00 00 00 00 00 	movabs $0xffffea0000000000,%r10
    6558:	ea ff ff 
	return !memcmp_pages(page1, page2);
}


struct sclock_coa_parent *get_original(struct sclock_coa_parent *coa_parent){
	if(coa_parent->parent_head==NULL){
    655b:	48 85 c0             	test   %rax,%rax
    655e:	48 0f 45 d0          	cmovne %rax,%rdx
    6562:	49 c1 e0 0c          	shl    $0xc,%r8
		struct sclock_coa_parent * original=get_original(coa_parent);
		coa_list_lock(original);
		if(coa_parent->pfn!=page_to_pfn(old_page)){
			coa_list_unlock(original);
		}else{
			list_for_each_entry(coa_children,&get_original(coa_parent)->head,head){
    6566:	48 8b 32             	mov    (%rdx),%rsi
    6569:	49 01 c8             	add    %rcx,%r8
    656c:	4c 89 45 d0          	mov    %r8,-0x30(%rbp)
    6570:	4c 8b 45 b0          	mov    -0x50(%rbp),%r8
    6574:	49 89 f4             	mov    %rsi,%r12
    6577:	eb 0b                	jmp    6584 <__do_copy_on_read+0x444>
    6579:	0f 1f 80 00 00 00 00 	nopl   0x0(%rax)
    6580:	4d 8b 24 24          	mov    (%r12),%r12
	return !memcmp_pages(page1, page2);
}


struct sclock_coa_parent *get_original(struct sclock_coa_parent *coa_parent){
	if(coa_parent->parent_head==NULL){
    6584:	48 85 c0             	test   %rax,%rax
    6587:	4c 89 f2             	mov    %r14,%rdx
    658a:	48 0f 45 d0          	cmovne %rax,%rdx
		struct sclock_coa_parent * original=get_original(coa_parent);
		coa_list_lock(original);
		if(coa_parent->pfn!=page_to_pfn(old_page)){
			coa_list_unlock(original);
		}else{
			list_for_each_entry(coa_children,&get_original(coa_parent)->head,head){
    658e:	49 39 d4             	cmp    %rdx,%r12
    6591:	0f 84 9b 00 00 00    	je     6632 <__do_copy_on_read+0x4f2>
				if(coa_children->owner==pid_ns){
    6597:	4d 39 44 24 20       	cmp    %r8,0x20(%r12)
    659c:	75 e2                	jne    6580 <__do_copy_on_read+0x440>
					unsigned long coa_pfn=coa_children->pfn;
    659e:	49 8b 54 24 10       	mov    0x10(%r12),%rdx
					if((coa_pfn!=COA_DEL)&&(coa_pfn!=COA_SKIP)){
    65a3:	48 8d 72 ff          	lea    -0x1(%rdx),%rsi
    65a7:	48 83 fe fd          	cmp    $0xfffffffffffffffd,%rsi
    65ab:	77 d3                	ja     6580 <__do_copy_on_read+0x440>
						new_page=pfn_to_page(coa_pfn);
    65ad:	48 c1 e2 06          	shl    $0x6,%rdx
    65b1:	4e 8d 1c 12          	lea    (%rdx,%r10,1),%r11
						if((new_page->coa_head==&(coa_children->head))&&page_mapped(new_page)&&pages_identical(new_page,old_page)){
    65b5:	4d 39 63 38          	cmp    %r12,0x38(%r11)
    65b9:	75 c5                	jne    6580 <__do_copy_on_read+0x440>
 *
 * Atomically reads the value of @v.
 */
static inline int atomic_read(const atomic_t *v)
{
	return (*(volatile int *)&(v)->counter);
    65bb:	41 8b 73 18          	mov    0x18(%r11),%esi
    65bf:	4c 89 5d a8          	mov    %r11,-0x58(%rbp)
    65c3:	85 f6                	test   %esi,%esi
    65c5:	78 b9                	js     6580 <__do_copy_on_read+0x440>
    65c7:	4c 89 4d 88          	mov    %r9,-0x78(%rbp)
    65cb:	4c 89 45 90          	mov    %r8,-0x70(%rbp)
    65cf:	4c 89 55 98          	mov    %r10,-0x68(%rbp)
 * The various preempt_count add/sub methods
 */

static __always_inline void __preempt_count_add(int val)
{
	__this_cpu_add_4(__preempt_count, val);
    65d3:	65 ff 04 25 00 00 00 	incl   %gs:0x0
    65da:	00 
    65db:	65 ff 04 25 00 00 00 	incl   %gs:0x0
    65e2:	00 
    65e3:	48 c1 e2 06          	shl    $0x6,%rdx
	char *addr1, *addr2;
	int ret;

	addr1 = kmap_atomic(page1);
	addr2 = kmap_atomic(page2);
	ret = memcmp(addr1, addr2, PAGE_SIZE);
    65e7:	48 8b 75 d0          	mov    -0x30(%rbp),%rsi
    65eb:	48 89 4d a0          	mov    %rcx,-0x60(%rbp)
    65ef:	48 8d 3c 0a          	lea    (%rdx,%rcx,1),%rdi
    65f3:	ba 00 10 00 00       	mov    $0x1000,%edx
    65f8:	e8 00 00 00 00       	callq  65fd <__do_copy_on_read+0x4bd>
}

static __always_inline void __preempt_count_sub(int val)
{
	__this_cpu_add_4(__preempt_count, -val);
    65fd:	65 ff 0c 25 00 00 00 	decl   %gs:0x0
    6604:	00 
    6605:	65 ff 0c 25 00 00 00 	decl   %gs:0x0
    660c:	00 
			list_for_each_entry(coa_children,&get_original(coa_parent)->head,head){
				if(coa_children->owner==pid_ns){
					unsigned long coa_pfn=coa_children->pfn;
					if((coa_pfn!=COA_DEL)&&(coa_pfn!=COA_SKIP)){
						new_page=pfn_to_page(coa_pfn);
						if((new_page->coa_head==&(coa_children->head))&&page_mapped(new_page)&&pages_identical(new_page,old_page)){
    660d:	85 c0                	test   %eax,%eax
    660f:	4c 8b 4d 88          	mov    -0x78(%rbp),%r9
    6613:	4c 8b 5d a8          	mov    -0x58(%rbp),%r11
    6617:	0f 84 0e 05 00 00    	je     6b2b <__do_copy_on_read+0x9eb>
    661d:	49 8b 46 18          	mov    0x18(%r14),%rax
    6621:	48 8b 4d a0          	mov    -0x60(%rbp),%rcx
    6625:	4c 8b 55 98          	mov    -0x68(%rbp),%r10
    6629:	4c 8b 45 90          	mov    -0x70(%rbp),%r8
    662d:	e9 4e ff ff ff       	jmpq   6580 <__do_copy_on_read+0x440>
    6632:	4c 89 a5 78 ff ff ff 	mov    %r12,-0x88(%rbp)
static inline int coa_list_trylock(struct sclock_coa_parent* coa_parent){
	return mutex_trylock(&coa_parent->lock);
}
long RedoRequestProtectOthers(struct task_struct *p);
static inline void coa_list_unlock(struct sclock_coa_parent* coa_parent){
	mutex_unlock(&coa_parent->lock);
    6639:	4c 89 cf             	mov    %r9,%rdi
    663c:	e8 00 00 00 00       	callq  6641 <__do_copy_on_read+0x501>
    6641:	48 8b 7d 10          	mov    0x10(%rbp),%rdi
    6645:	ff 14 25 00 00 00 00 	callq  *0x0
	return pte_flags(pte) & _PAGE_SPECIAL;
}

static inline unsigned long pte_pfn(pte_t pte)
{
	return (pte_val(pte) & PTE_PFN_MASK) >> PAGE_SHIFT;
    664c:	48 c1 e0 12          	shl    $0x12,%rax
    6650:	31 f6                	xor    %esi,%esi
    6652:	4c 89 e9             	mov    %r13,%rcx
    6655:	48 c1 e8 1e          	shr    $0x1e,%rax
    6659:	48 89 da             	mov    %rbx,%rdx
    665c:	65 44 8b 04 25 00 00 	mov    %gs:0x0,%r8d
    6663:	00 00 
			}
			coa_list_unlock(original);
		}
	}
	/*check whether we can use a exited exclusive page for this pid_ns*/
	if (is_zero_pfn(pte_pfn(orig_pte))) {//assign a full zero page to the new page
    6665:	48 3b 05 00 00 00 00 	cmp    0x0(%rip),%rax        # 666c <__do_copy_on_read+0x52c>
    666c:	0f 84 96 03 00 00    	je     6a08 <__do_copy_on_read+0x8c8>
		{
			printk("!new page from coa\n");
			goto oom;
		}
	} else {//if it is not zero, copy old to new
		new_page = alloc_page_vma(GFP_HIGHUSER_MOVABLE, vma, address);
    6672:	bf da 00 02 00       	mov    $0x200da,%edi
    6677:	e8 00 00 00 00       	callq  667c <__do_copy_on_read+0x53c>
		if (!new_page)
    667c:	48 85 c0             	test   %rax,%rax
		{
			printk("!new page from coa\n");
			goto oom;
		}
	} else {//if it is not zero, copy old to new
		new_page = alloc_page_vma(GFP_HIGHUSER_MOVABLE, vma, address);
    667f:	49 89 c4             	mov    %rax,%r12
		if (!new_page)
    6682:	0f 84 96 03 00 00    	je     6a1e <__do_copy_on_read+0x8de>
	 * If the source page was a PFN mapping, we don't have
	 * a "struct page" for it. We do a best-effort copy by
	 * just copying from the original user address. If that
	 * fails, we just zero-fill it. Live with it.
	 */
	if (unlikely(!src)) {
    6688:	48 83 7d c0 00       	cmpq   $0x0,-0x40(%rbp)
    668d:	0f 84 40 05 00 00    	je     6bd3 <__do_copy_on_read+0xa93>
 * The various preempt_count add/sub methods
 */

static __always_inline void __preempt_count_add(int val)
{
	__this_cpu_add_4(__preempt_count, val);
    6693:	65 ff 04 25 00 00 00 	incl   %gs:0x0
    669a:	00 
    669b:	65 ff 04 25 00 00 00 	incl   %gs:0x0
    66a2:	00 
    66a3:	48 8b 75 c0          	mov    -0x40(%rbp),%rsi
    66a7:	48 b8 00 00 00 00 00 	movabs $0x160000000000,%rax
    66ae:	16 00 00 
    66b1:	48 bf 00 00 00 00 00 	movabs $0xffff880000000000,%rdi
    66b8:	88 ff ff 
    66bb:	48 01 c6             	add    %rax,%rsi
    66be:	4c 01 e0             	add    %r12,%rax
    66c1:	48 c1 fe 06          	sar    $0x6,%rsi
    66c5:	48 c1 f8 06          	sar    $0x6,%rax
    66c9:	48 c1 e0 0c          	shl    $0xc,%rax
    66cd:	48 c1 e6 0c          	shl    $0xc,%rsi
    66d1:	48 01 fe             	add    %rdi,%rsi
    66d4:	48 01 c7             	add    %rax,%rdi
    66d7:	e8 00 00 00 00       	callq  66dc <__do_copy_on_read+0x59c>
}

static __always_inline void __preempt_count_sub(int val)
{
	__this_cpu_add_4(__preempt_count, -val);
    66dc:	65 ff 0c 25 00 00 00 	decl   %gs:0x0
    66e3:	00 
    66e4:	65 ff 0c 25 00 00 00 	decl   %gs:0x0
    66eb:	00 
 * If it's called on the same region of memory simultaneously, the effect
 * may be that only one operation succeeds.
 */
static inline void __set_bit(long nr, volatile unsigned long *addr)
{
	asm volatile("bts %1,%0" : ADDR : "Ir" (nr) : "memory");
    66ec:	41 0f ba 2c 24 03    	btsl   $0x3,(%r12)
			goto oom;
		}
		cow_user_page(new_page, old_page, address, vma);
	}
	__SetPageUptodate(new_page);
	if (mem_cgroup_newpage_charge(new_page, mm, GFP_KERNEL)){
    66f2:	ba d0 00 00 00       	mov    $0xd0,%edx
    66f7:	4c 89 fe             	mov    %r15,%rsi
    66fa:	4c 89 e7             	mov    %r12,%rdi
    66fd:	e8 00 00 00 00       	callq  6702 <__do_copy_on_read+0x5c2>
    6702:	85 c0                	test   %eax,%eax
    6704:	0f 85 ae 04 00 00    	jne    6bb8 <__do_copy_on_read+0xa78>
	pte_t entry=orig_pte;
	bool isFile=false,pte_lock=true;
	int ret = 0;
	unsigned int cpu;
	int reuse=0;
	bool not_new=false;
    670a:	c6 45 98 00          	movb   $0x0,-0x68(%rbp)
		printk("oom");
		goto oom_free_new;
	}
new_page_got:
	//notify mmu about this new virtual page's update
	mmun_start  = address & PAGE_MASK;
    670e:	4c 89 e8             	mov    %r13,%rax
    6711:	48 25 00 f0 ff ff    	and    $0xfffffffffffff000,%rax
    6717:	48 89 45 a8          	mov    %rax,-0x58(%rbp)
	mmun_end    = mmun_start + PAGE_SIZE;
    671b:	48 05 00 10 00 00    	add    $0x1000,%rax
}

static inline void mmu_notifier_invalidate_range_start(struct mm_struct *mm,
				  unsigned long start, unsigned long end)
{
	if (mm_has_notifiers(mm))
    6721:	49 83 bf a0 03 00 00 	cmpq   $0x0,0x3a0(%r15)
    6728:	00 
    6729:	48 89 45 a0          	mov    %rax,-0x60(%rbp)
    672d:	0f 85 54 04 00 00    	jne    6b87 <__do_copy_on_read+0xa47>

	if (sizeof(pmdval_t) > sizeof(long))
		ret =  PVOP_CALLEE2(pmdval_t, pv_mmu_ops.pmd_val,
				    pmd.pmd, (u64)pmd.pmd >> 32);
	else
		ret =  PVOP_CALLEE1(pmdval_t, pv_mmu_ops.pmd_val,
    6733:	48 8b 75 b8          	mov    -0x48(%rbp),%rsi
    6737:	48 8b 3e             	mov    (%rsi),%rdi
    673a:	ff 14 25 00 00 00 00 	callq  *0x0
}
#endif /* ALLOC_SPLIT_PTLOCKS */

static inline spinlock_t *pte_lockptr(struct mm_struct *mm, pmd_t *pmd)
{
	return ptlock_ptr(pmd_page(*pmd));
    6741:	48 b9 00 f0 ff ff ff 	movabs $0x3ffffffff000,%rcx
    6748:	3f 00 00 
    674b:	48 ba 00 00 00 00 00 	movabs $0xffffea0000000000,%rdx
    6752:	ea ff ff 
    6755:	48 8b 3e             	mov    (%rsi),%rdi
    6758:	48 21 c8             	and    %rcx,%rax
    675b:	48 c1 e8 06          	shr    $0x6,%rax
    675f:	48 8b 44 10 30       	mov    0x30(%rax,%rdx,1),%rax
    6764:	49 89 c2             	mov    %rax,%r10
    6767:	48 89 45 90          	mov    %rax,-0x70(%rbp)
    676b:	ff 14 25 00 00 00 00 	callq  *0x0
 * this function returns the index of the entry in the pte page which would
 * control the given virtual address
 */
static inline unsigned long pte_index(unsigned long address)
{
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
    6772:	4c 89 ea             	mov    %r13,%rdx
	return (unsigned long)native_pmd_val(pmd) == 0;
}

static inline unsigned long pmd_page_vaddr(pmd_t pmd)
{
	return (unsigned long)__va(pmd_val(pmd) & PTE_PFN_MASK);
    6775:	48 be 00 00 00 00 00 	movabs $0xffff880000000000,%rsi
    677c:	88 ff ff 
    677f:	48 21 c8             	and    %rcx,%rax
 * this function returns the index of the entry in the pte page which would
 * control the given virtual address
 */
static inline unsigned long pte_index(unsigned long address)
{
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
    6782:	48 c1 ea 09          	shr    $0x9,%rdx
	raw_spin_lock_init(&(_lock)->rlock);		\
} while (0)

static inline void spin_lock(spinlock_t *lock)
{
	raw_spin_lock(&lock->rlock);
    6786:	4c 89 d7             	mov    %r10,%rdi
    6789:	4c 89 55 80          	mov    %r10,-0x80(%rbp)
}

static inline pte_t *pte_offset_kernel(pmd_t *pmd, unsigned long address)
{
	return (pte_t *)pmd_page_vaddr(*pmd) + pte_index(address);
    678d:	81 e2 f8 0f 00 00    	and    $0xff8,%edx
	return (unsigned long)native_pmd_val(pmd) == 0;
}

static inline unsigned long pmd_page_vaddr(pmd_t pmd)
{
	return (unsigned long)__va(pmd_val(pmd) & PTE_PFN_MASK);
    6793:	48 01 f2             	add    %rsi,%rdx
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
}

static inline pte_t *pte_offset_kernel(pmd_t *pmd, unsigned long address)
{
	return (pte_t *)pmd_page_vaddr(*pmd) + pte_index(address);
    6796:	48 01 d0             	add    %rdx,%rax
    6799:	48 89 45 d0          	mov    %rax,-0x30(%rbp)
    679d:	e8 00 00 00 00       	callq  67a2 <__do_copy_on_read+0x662>
	mmu_notifier_invalidate_range_start(mm,mmun_start,mmun_end);
	page_table = pte_offset_map_lock(mm, pmd, address, &ptl);
	if (likely((pte_same(*page_table, orig_pte)))) {
    67a2:	48 8b 45 d0          	mov    -0x30(%rbp),%rax
    67a6:	48 8b 75 10          	mov    0x10(%rbp),%rsi
    67aa:	48 39 30             	cmp    %rsi,(%rax)
    67ad:	0f 85 b7 03 00 00    	jne    6b6a <__do_copy_on_read+0xa2a>
		if(old_page){
    67b3:	48 8b 45 c0          	mov    -0x40(%rbp),%rax
    67b7:	48 85 c0             	test   %rax,%rax
    67ba:	0f 84 49 03 00 00    	je     6b09 <__do_copy_on_read+0x9c9>
			if (!PageAnon(old_page)) {
    67c0:	f6 40 08 01          	testb  $0x1,0x8(%rax)
    67c4:	0f 84 c6 02 00 00    	je     6a90 <__do_copy_on_read+0x950>
				inc_mm_counter_fast(mm, MM_ANONPAGES);
			}
		}else
		  inc_mm_counter_fast(mm,MM_ANONPAGES);
		flush_cache_page(vma, address, pte_pfn(orig_pte));
		entry =pte_mkyoung( mk_pte(new_page, vma->vm_page_prot));
    67ca:	48 8b 53 48          	mov    0x48(%rbx),%rdx
    67ce:	48 b8 00 00 00 00 00 	movabs $0x4000000000000,%rax
    67d5:	00 04 00 
    67d8:	48 21 d0             	and    %rdx,%rax
    67db:	48 89 c1             	mov    %rax,%rcx
    67de:	0f 84 44 01 00 00    	je     6928 <__do_copy_on_read+0x7e8>
}

static __always_inline int constant_test_bit(long nr, const volatile unsigned long *addr)
{
	return ((1UL << (nr & (BITS_PER_LONG-1))) &
		(addr[nr >> _BITOPS_LONG_SHIFT])) != 0;
    67e4:	49 8b 04 24          	mov    (%r12),%rax
    67e8:	48 b9 00 00 00 00 00 	movabs $0x8000000000000,%rcx
    67ef:	00 08 00 
    67f2:	a9 00 00 00 02       	test   $0x2000000,%eax
    67f7:	b8 00 00 00 00       	mov    $0x0,%eax
    67fc:	48 0f 44 c8          	cmove  %rax,%rcx
    6800:	48 89 d0             	mov    %rdx,%rax
    6803:	48 83 c8 10          	or     $0x10,%rax
    6807:	48 09 c8             	or     %rcx,%rax
    680a:	48 ba 00 00 00 00 00 	movabs $0x160000000000,%rdx
    6811:	16 00 00 
static inline pgprotval_t massage_pgprot(pgprot_t pgprot)
{
	pgprotval_t protval = pgprot_val(pgprot);

	if (protval & _PAGE_PRESENT)
	  protval &= __supported_pte_mask;
    6814:	48 89 c7             	mov    %rax,%rdi
    6817:	48 23 3d 00 00 00 00 	and    0x0(%rip),%rdi        # 681e <__do_copy_on_read+0x6de>
    681e:	49 8d 34 14          	lea    (%r12,%rdx,1),%rsi
    6822:	48 89 75 b8          	mov    %rsi,-0x48(%rbp)
    6826:	48 c1 7d b8 06       	sarq   $0x6,-0x48(%rbp)
	return protval;
}

static inline pte_t pfn_pte(unsigned long page_nr, pgprot_t pgprot)
{
	return __pte(((phys_addr_t)page_nr << PAGE_SHIFT) |
    682b:	48 8b 55 b8          	mov    -0x48(%rbp),%rdx
    682f:	48 c1 e2 0c          	shl    $0xc,%rdx
static inline pgprotval_t massage_pgprot(pgprot_t pgprot)
{
	pgprotval_t protval = pgprot_val(pgprot);

	if (protval & _PAGE_PRESENT)
	  protval &= __supported_pte_mask;
    6833:	a8 01                	test   $0x1,%al
    6835:	48 0f 44 f8          	cmove  %rax,%rdi
	return protval;
}

static inline pte_t pfn_pte(unsigned long page_nr, pgprot_t pgprot)
{
	return __pte(((phys_addr_t)page_nr << PAGE_SHIFT) |
    6839:	48 09 d7             	or     %rdx,%rdi
	if (sizeof(pteval_t) > sizeof(long))
		ret = PVOP_CALLEE2(pteval_t,
				   pv_mmu_ops.make_pte,
				   val, (u64)val >> 32);
	else
		ret = PVOP_CALLEE1(pteval_t,
    683c:	ff 14 25 00 00 00 00 	callq  *0x0
		entry.pte&=~(_PAGE_ISOLATION);
		ptep_clear_flush(vma,address,page_table);
    6843:	48 8b 55 d0          	mov    -0x30(%rbp),%rdx
    6847:	4c 89 ee             	mov    %r13,%rsi
    684a:	48 89 df             	mov    %rbx,%rdi
    684d:	48 89 45 88          	mov    %rax,-0x78(%rbp)
    6851:	e8 00 00 00 00       	callq  6856 <__do_copy_on_read+0x716>
			}
		}else
		  inc_mm_counter_fast(mm,MM_ANONPAGES);
		flush_cache_page(vma, address, pte_pfn(orig_pte));
		entry =pte_mkyoung( mk_pte(new_page, vma->vm_page_prot));
		entry.pte&=~(_PAGE_ISOLATION);
    6856:	48 b9 ff ff ff ff ff 	movabs $0xfff7ffffffffffff,%rcx
    685d:	ff f7 ff 
    6860:	48 23 4d 88          	and    -0x78(%rbp),%rcx
{
	if (sizeof(pteval_t) > sizeof(long))
		/* 5 arg words */
		pv_mmu_ops.set_pte_at(mm, addr, ptep, pte);
	else
		PVOP_VCALL4(pv_mmu_ops.set_pte_at, mm, addr, ptep, pte.pte);
    6864:	4c 89 ff             	mov    %r15,%rdi
    6867:	4c 89 ee             	mov    %r13,%rsi
    686a:	48 8b 55 d0          	mov    -0x30(%rbp),%rdx
    686e:	48 83 c9 20          	or     $0x20,%rcx
    6872:	ff 14 25 00 00 00 00 	callq  *0x0
	raw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \
} while (0)

static inline void spin_unlock(spinlock_t *lock)
{
	raw_spin_unlock(&lock->rlock);
    6879:	48 8b 7d 90          	mov    -0x70(%rbp),%rdi
    687d:	e8 00 00 00 00       	callq  6882 <__do_copy_on_read+0x742>
		ptep_clear_flush(vma,address,page_table);
		set_pte_at(mm, address, page_table, entry);
		update_mmu_cache(vma, address, page_table);
		pte_unmap_unlock(page_table, ptl);
		if(not_new){
    6882:	80 7d 98 00          	cmpb   $0x0,-0x68(%rbp)
    6886:	0f 84 84 00 00 00    	je     6910 <__do_copy_on_read+0x7d0>
 * This operation is atomic and cannot be reordered.
 * It also implies a memory barrier.
 */
static inline int test_and_set_bit(long nr, volatile unsigned long *addr)
{
	GEN_BINARY_RMWcc(LOCK_PREFIX "bts", *addr, "Ir", nr, "%0", "c");
    688c:	f0 41 0f ba 2c 24 00 	lock btsl $0x0,(%r12)
    6893:	0f 82 2a 02 00 00    	jb     6ac3 <__do_copy_on_read+0x983>
			lock_page(new_page);
			//	page_cache_get(new_page);
			page_add_anon_rmap(new_page, vma, address);
    6899:	48 89 de             	mov    %rbx,%rsi
    689c:	4c 89 ea             	mov    %r13,%rdx
    689f:	4c 89 e7             	mov    %r12,%rdi
    68a2:	e8 00 00 00 00       	callq  68a7 <__do_copy_on_read+0x767>
			unlock_page(new_page);
    68a7:	4c 89 e7             	mov    %r12,%rdi
    68aa:	e8 00 00 00 00       	callq  68af <__do_copy_on_read+0x76f>
		}
		else
		  page_add_new_anon_rmap(new_page, vma, address);
		real_new_page=not_new?NULL:new_page;
		if(not_new){
			coa_children->pfn=page_to_pfn(new_page);
    68af:	48 8b 85 78 ff ff ff 	mov    -0x88(%rbp),%rax
    68b6:	48 8b 75 b8          	mov    -0x48(%rbp),%rsi
			page_add_anon_rmap(new_page, vma, address);
			unlock_page(new_page);
		}
		else
		  page_add_new_anon_rmap(new_page, vma, address);
		real_new_page=not_new?NULL:new_page;
    68ba:	48 c7 45 d0 00 00 00 	movq   $0x0,-0x30(%rbp)
    68c1:	00 
		if(not_new){
			coa_children->pfn=page_to_pfn(new_page);
    68c2:	48 89 70 10          	mov    %rsi,0x10(%rax)
		}
		if (old_page) {
    68c6:	48 8b 45 c0          	mov    -0x40(%rbp),%rax
    68ca:	48 85 c0             	test   %rax,%rax
    68cd:	0f 84 1a 01 00 00    	je     69ed <__do_copy_on_read+0x8ad>
			page_remove_rmap(old_page);//old_page->'s _mapcount -1		
    68d3:	49 89 c4             	mov    %rax,%r12
    68d6:	48 89 c7             	mov    %rax,%rdi
			dec_page_counter_in_ns(old_page,vma);
			new_page=old_page;
			//	test_and_clear_bit(PG_isolation,&old_page->flags);
		}		/* Free the old page.. */
		ret|=VM_FAULT_WRITE;
    68d9:	41 bd 08 00 00 00    	mov    $0x8,%r13d
		real_new_page=not_new?NULL:new_page;
		if(not_new){
			coa_children->pfn=page_to_pfn(new_page);
		}
		if (old_page) {
			page_remove_rmap(old_page);//old_page->'s _mapcount -1		
    68df:	e8 00 00 00 00       	callq  68e4 <__do_copy_on_read+0x7a4>
			dec_page_counter_in_ns(old_page,vma);
    68e4:	48 89 de             	mov    %rbx,%rsi
    68e7:	4c 89 e7             	mov    %r12,%rdi
    68ea:	e8 00 00 00 00       	callq  68ef <__do_copy_on_read+0x7af>
			new_page=old_page;
			//	test_and_clear_bit(PG_isolation,&old_page->flags);
		}		/* Free the old page.. */
		ret|=VM_FAULT_WRITE;
		pte_lock=false;
    68ef:	31 c0                	xor    %eax,%eax
		if(!not_new){
			mem_cgroup_uncharge_page(new_page);
		}
	}
	if(new_page)
	  page_cache_release(new_page);
    68f1:	4c 89 e7             	mov    %r12,%rdi
    68f4:	89 45 b8             	mov    %eax,-0x48(%rbp)
    68f7:	e8 00 00 00 00       	callq  68fc <__do_copy_on_read+0x7bc>
    68fc:	8b 45 b8             	mov    -0x48(%rbp),%eax
unlock:
	if(pte_lock)
    68ff:	84 c0                	test   %al,%al
    6901:	0f 84 79 fa ff ff    	je     6380 <__do_copy_on_read+0x240>
    6907:	e9 6b fa ff ff       	jmpq   6377 <__do_copy_on_read+0x237>
    690c:	0f 1f 40 00          	nopl   0x0(%rax)
			//	page_cache_get(new_page);
			page_add_anon_rmap(new_page, vma, address);
			unlock_page(new_page);
		}
		else
		  page_add_new_anon_rmap(new_page, vma, address);
    6910:	4c 89 ea             	mov    %r13,%rdx
    6913:	48 89 de             	mov    %rbx,%rsi
    6916:	4c 89 e7             	mov    %r12,%rdi
    6919:	e8 00 00 00 00       	callq  691e <__do_copy_on_read+0x7de>
    691e:	4c 89 65 d0          	mov    %r12,-0x30(%rbp)
    6922:	eb a2                	jmp    68c6 <__do_copy_on_read+0x786>
    6924:	0f 1f 40 00          	nopl   0x0(%rax)
}

static __always_inline int constant_test_bit(long nr, const volatile unsigned long *addr)
{
	return ((1UL << (nr & (BITS_PER_LONG-1))) &
		(addr[nr >> _BITOPS_LONG_SHIFT])) != 0;
    6928:	49 8b 34 24          	mov    (%r12),%rsi
				inc_mm_counter_fast(mm, MM_ANONPAGES);
			}
		}else
		  inc_mm_counter_fast(mm,MM_ANONPAGES);
		flush_cache_page(vma, address, pte_pfn(orig_pte));
		entry =pte_mkyoung( mk_pte(new_page, vma->vm_page_prot));
    692c:	48 b8 00 00 00 00 00 	movabs $0x8000000000000,%rax
    6933:	00 08 00 
    6936:	f7 c6 00 00 00 02    	test   $0x2000000,%esi
    693c:	48 0f 44 c1          	cmove  %rcx,%rax
    6940:	48 09 d0             	or     %rdx,%rax
    6943:	e9 c2 fe ff ff       	jmpq   680a <__do_copy_on_read+0x6ca>
			//goto reuse;
		}
		coa_parent=list_entry(old_page->coa_head,struct sclock_coa_parent,head);

		if(coa_parent->owner==NULL){//first accessed shared page with COA alert
			coa_parent->owner=pid_ns;
    6948:	48 8b 45 b0          	mov    -0x50(%rbp),%rax
    694c:	49 89 46 20          	mov    %rax,0x20(%r14)
		//	goto reuse;

		if(pid_ns==coa_parent->owner){
reuse:
			if (old_page)
			  page_cache_release(old_page);
    6950:	48 8b 7d c0          	mov    -0x40(%rbp),%rdi
    6954:	e8 00 00 00 00       	callq  6959 <__do_copy_on_read+0x819>

	if (sizeof(pmdval_t) > sizeof(long))
		ret =  PVOP_CALLEE2(pmdval_t, pv_mmu_ops.pmd_val,
				    pmd.pmd, (u64)pmd.pmd >> 32);
	else
		ret =  PVOP_CALLEE1(pmdval_t, pv_mmu_ops.pmd_val,
    6959:	48 8b 75 b8          	mov    -0x48(%rbp),%rsi
    695d:	48 8b 3e             	mov    (%rsi),%rdi
    6960:	ff 14 25 00 00 00 00 	callq  *0x0
    6967:	48 b9 00 f0 ff ff ff 	movabs $0x3ffffffff000,%rcx
    696e:	3f 00 00 
    6971:	48 ba 00 00 00 00 00 	movabs $0xffffea0000000000,%rdx
    6978:	ea ff ff 
    697b:	48 8b 3e             	mov    (%rsi),%rdi
    697e:	48 21 c8             	and    %rcx,%rax
    6981:	48 c1 e8 06          	shr    $0x6,%rax
    6985:	4c 8b 64 10 30       	mov    0x30(%rax,%rdx,1),%r12
    698a:	ff 14 25 00 00 00 00 	callq  *0x0
 * this function returns the index of the entry in the pte page which would
 * control the given virtual address
 */
static inline unsigned long pte_index(unsigned long address)
{
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
    6991:	4c 89 ea             	mov    %r13,%rdx
	return (unsigned long)native_pmd_val(pmd) == 0;
}

static inline unsigned long pmd_page_vaddr(pmd_t pmd)
{
	return (unsigned long)__va(pmd_val(pmd) & PTE_PFN_MASK);
    6994:	48 be 00 00 00 00 00 	movabs $0xffff880000000000,%rsi
    699b:	88 ff ff 
    699e:	48 21 c8             	and    %rcx,%rax
 * this function returns the index of the entry in the pte page which would
 * control the given virtual address
 */
static inline unsigned long pte_index(unsigned long address)
{
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
    69a1:	48 c1 ea 09          	shr    $0x9,%rdx
	raw_spin_lock_init(&(_lock)->rlock);		\
} while (0)

static inline void spin_lock(spinlock_t *lock)
{
	raw_spin_lock(&lock->rlock);
    69a5:	4c 89 e7             	mov    %r12,%rdi
    69a8:	4d 89 e6             	mov    %r12,%r14
}

static inline pte_t *pte_offset_kernel(pmd_t *pmd, unsigned long address)
{
	return (pte_t *)pmd_page_vaddr(*pmd) + pte_index(address);
    69ab:	81 e2 f8 0f 00 00    	and    $0xff8,%edx
	return (unsigned long)native_pmd_val(pmd) == 0;
}

static inline unsigned long pmd_page_vaddr(pmd_t pmd)
{
	return (unsigned long)__va(pmd_val(pmd) & PTE_PFN_MASK);
    69b1:	48 01 f2             	add    %rsi,%rdx
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
}

static inline pte_t *pte_offset_kernel(pmd_t *pmd, unsigned long address)
{
	return (pte_t *)pmd_page_vaddr(*pmd) + pte_index(address);
    69b4:	48 01 d0             	add    %rdx,%rax
    69b7:	49 89 c7             	mov    %rax,%r15
    69ba:	48 89 45 d0          	mov    %rax,-0x30(%rbp)
    69be:	e8 00 00 00 00       	callq  69c3 <__do_copy_on_read+0x883>
			page_table = pte_offset_map_lock(mm, pmd, address,
						&ptl);
			if (!pte_same(*page_table, orig_pte)) {
    69c3:	48 8b 75 10          	mov    0x10(%rbp),%rsi
    69c7:	49 39 37             	cmp    %rsi,(%r15)
    69ca:	0f 84 15 f9 ff ff    	je     62e5 <__do_copy_on_read+0x1a5>
	raw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \
} while (0)

static inline void spin_unlock(spinlock_t *lock)
{
	raw_spin_unlock(&lock->rlock);
    69d0:	4c 89 e7             	mov    %r12,%rdi
}
static int __do_copy_on_read(struct mm_struct *mm, struct vm_area_struct *vma,unsigned long address, pte_t *page_table, pmd_t *pmd,spinlock_t *ptl, pte_t orig_pte)__releases(ptl){
	struct page *old_page=NULL, *new_page = NULL,*real_new_page=NULL;
	pte_t entry=orig_pte;
	bool isFile=false,pte_lock=true;
	int ret = 0;
    69d3:	45 31 ed             	xor    %r13d,%r13d
    69d6:	e8 00 00 00 00       	callq  69db <__do_copy_on_read+0x89b>
    69db:	e9 f0 f7 ff ff       	jmpq   61d0 <__do_copy_on_read+0x90>
    69e0:	4c 89 e7             	mov    %r12,%rdi
    69e3:	e8 00 00 00 00       	callq  69e8 <__do_copy_on_read+0x8a8>
    69e8:	e9 fb fa ff ff       	jmpq   64e8 <__do_copy_on_read+0x3a8>
			page_remove_rmap(old_page);//old_page->'s _mapcount -1		
			dec_page_counter_in_ns(old_page,vma);
			new_page=old_page;
			//	test_and_clear_bit(PG_isolation,&old_page->flags);
		}		/* Free the old page.. */
		ret|=VM_FAULT_WRITE;
    69ed:	41 bd 08 00 00 00    	mov    $0x8,%r13d
		pte_lock=false;
    69f3:	31 c0                	xor    %eax,%eax
	} else{
		if(!not_new){
			mem_cgroup_uncharge_page(new_page);
		}
	}
	if(new_page)
    69f5:	4d 85 e4             	test   %r12,%r12
    69f8:	0f 84 01 ff ff ff    	je     68ff <__do_copy_on_read+0x7bf>
    69fe:	e9 ee fe ff ff       	jmpq   68f1 <__do_copy_on_read+0x7b1>
    6a03:	0f 1f 44 00 00       	nopl   0x0(%rax,%rax,1)
    6a08:	bf da 80 02 00       	mov    $0x280da,%edi
    6a0d:	e8 00 00 00 00       	callq  6a12 <__do_copy_on_read+0x8d2>
	}
	/*check whether we can use a exited exclusive page for this pid_ns*/
	if (is_zero_pfn(pte_pfn(orig_pte))) {//assign a full zero page to the new page
		new_page = alloc_zeroed_user_highpage_movable(vma, address);
		//	printk(KERN_DEBUG"----------------gotten. assign  full zero---------------\n");
		if (!new_page)
    6a12:	48 85 c0             	test   %rax,%rax
    6a15:	49 89 c4             	mov    %rax,%r12
    6a18:	0f 85 ce fc ff ff    	jne    66ec <__do_copy_on_read+0x5ac>
		{
			printk("!new page from coa\n");
    6a1e:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
    6a25:	31 c0                	xor    %eax,%eax
    6a27:	e8 00 00 00 00       	callq  6a2c <__do_copy_on_read+0x8ec>
	return ret;
oom_free_new:
	page_cache_release(new_page);
oom:
	//	pte_unmap_unlock(page_table, ptl);
	if (old_page)
    6a2c:	48 8b 7d c0          	mov    -0x40(%rbp),%rdi
    6a30:	48 85 ff             	test   %rdi,%rdi
    6a33:	74 05                	je     6a3a <__do_copy_on_read+0x8fa>
	  page_cache_release(old_page);
    6a35:	e8 00 00 00 00       	callq  6a3a <__do_copy_on_read+0x8fa>
	printk("oom======from coa\n");
    6a3a:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
    6a41:	31 c0                	xor    %eax,%eax
	__get_cpu_var(global_interval_coa_fail)+=get_rdtsc()-time1;
    6a43:	48 c7 c3 00 00 00 00 	mov    $0x0,%rbx
	page_cache_release(new_page);
oom:
	//	pte_unmap_unlock(page_table, ptl);
	if (old_page)
	  page_cache_release(old_page);
	printk("oom======from coa\n");
    6a4a:	e8 00 00 00 00       	callq  6a4f <__do_copy_on_read+0x90f>
	__get_cpu_var(global_interval_coa_fail)+=get_rdtsc()-time1;
    6a4f:	65 48 03 1c 25 00 00 	add    %gs:0x0,%rbx
    6a56:	00 00 
    6a58:	4c 8b 23             	mov    (%rbx),%r12
    6a5b:	e8 00 00 00 00       	callq  6a60 <__do_copy_on_read+0x920>
    6a60:	48 2b 45 c8          	sub    -0x38(%rbp),%rax
    6a64:	4c 01 e0             	add    %r12,%rax
    6a67:	48 89 03             	mov    %rax,(%rbx)
	__get_cpu_var(global_count_coa_fail)++;
    6a6a:	48 c7 c0 00 00 00 00 	mov    $0x0,%rax
    6a71:	65 48 03 04 25 00 00 	add    %gs:0x0,%rax
    6a78:	00 00 
    6a7a:	48 83 00 01          	addq   $0x1,(%rax)

	return VM_FAULT_OOM;
    6a7e:	b8 01 00 00 00       	mov    $0x1,%eax
    6a83:	e9 81 f7 ff ff       	jmpq   6209 <__do_copy_on_read+0xc9>
    6a88:	0f 1f 84 00 00 00 00 	nopl   0x0(%rax,%rax,1)
    6a8f:	00 
	mmu_notifier_invalidate_range_start(mm,mmun_start,mmun_end);
	page_table = pte_offset_map_lock(mm, pmd, address, &ptl);
	if (likely((pte_same(*page_table, orig_pte)))) {
		if(old_page){
			if (!PageAnon(old_page)) {
				dec_mm_counter_fast(mm, MM_FILEPAGES);
    6a90:	31 f6                	xor    %esi,%esi
    6a92:	4c 89 ff             	mov    %r15,%rdi
    6a95:	ba ff ff ff ff       	mov    $0xffffffff,%edx
    6a9a:	e8 c1 96 ff ff       	callq  160 <add_mm_counter_fast>
				inc_mm_counter_fast(mm, MM_ANONPAGES);
    6a9f:	ba 01 00 00 00       	mov    $0x1,%edx
    6aa4:	be 01 00 00 00       	mov    $0x1,%esi
    6aa9:	4c 89 ff             	mov    %r15,%rdi
    6aac:	e8 af 96 ff ff       	callq  160 <add_mm_counter_fast>
    6ab1:	e9 14 fd ff ff       	jmpq   67ca <__do_copy_on_read+0x68a>
 *
 * Atomically sets the value of @v to @i.
 */
static inline void atomic_set(atomic_t *v, int i)
{
	v->counter = i;
    6ab6:	41 c7 46 28 01 00 00 	movl   $0x1,0x28(%r14)
    6abd:	00 
    6abe:	e9 6b f9 ff ff       	jmpq   642e <__do_copy_on_read+0x2ee>
 */
static inline void lock_page(struct page *page)
{
	might_sleep();
	if (!trylock_page(page))
		__lock_page(page);
    6ac3:	4c 89 e7             	mov    %r12,%rdi
    6ac6:	e8 00 00 00 00       	callq  6acb <__do_copy_on_read+0x98b>
    6acb:	e9 c9 fd ff ff       	jmpq   6899 <__do_copy_on_read+0x759>
 * This operation is atomic and cannot be reordered.
 * It also implies a memory barrier.
 */
static inline int test_and_set_bit(long nr, volatile unsigned long *addr)
{
	GEN_BINARY_RMWcc(LOCK_PREFIX "bts", *addr, "Ir", nr, "%0", "c");
    6ad0:	48 8b 45 c0          	mov    -0x40(%rbp),%rax
    6ad4:	f0 0f ba 28 00       	lock btsl $0x0,(%rax)
    6ad9:	72 45                	jb     6b20 <__do_copy_on_read+0x9e0>
		 * Don't let another task, with possibly unlocked vma,
		 * keep the mlocked page.
		 */
		if ( (ret&VM_FAULT_WRITE)&&(vma->vm_flags&VM_LOCKED)) {
			lock_page(old_page);	/* LRU manipulation */
			munlock_vma_page(old_page);
    6adb:	48 8b 5d c0          	mov    -0x40(%rbp),%rbx
    6adf:	48 89 df             	mov    %rbx,%rdi
    6ae2:	e8 00 00 00 00       	callq  6ae7 <__do_copy_on_read+0x9a7>
			unlock_page(old_page);
    6ae7:	48 89 df             	mov    %rbx,%rdi
    6aea:	e8 00 00 00 00       	callq  6aef <__do_copy_on_read+0x9af>
    6aef:	e9 be f8 ff ff       	jmpq   63b2 <__do_copy_on_read+0x272>
	pte_unmap_unlock(page_table, ptl);
	/*check copy or parent and get coa_parent*/
	if(old_page){
check_coa_head:
		if(!old_page->coa_head){
			init_coa_parent(old_page);
    6af4:	4c 8b 75 c0          	mov    -0x40(%rbp),%r14
    6af8:	4c 89 f7             	mov    %r14,%rdi
    6afb:	e8 00 00 00 00       	callq  6b00 <__do_copy_on_read+0x9c0>
    6b00:	4d 8b 76 38          	mov    0x38(%r14),%r14
    6b04:	e9 c8 f9 ff ff       	jmpq   64d1 <__do_copy_on_read+0x391>
			if (!PageAnon(old_page)) {
				dec_mm_counter_fast(mm, MM_FILEPAGES);
				inc_mm_counter_fast(mm, MM_ANONPAGES);
			}
		}else
		  inc_mm_counter_fast(mm,MM_ANONPAGES);
    6b09:	ba 01 00 00 00       	mov    $0x1,%edx
    6b0e:	be 01 00 00 00       	mov    $0x1,%esi
    6b13:	4c 89 ff             	mov    %r15,%rdi
    6b16:	e8 45 96 ff ff       	callq  160 <add_mm_counter_fast>
    6b1b:	e9 aa fc ff ff       	jmpq   67ca <__do_copy_on_read+0x68a>
    6b20:	48 8b 7d c0          	mov    -0x40(%rbp),%rdi
    6b24:	e8 00 00 00 00       	callq  6b29 <__do_copy_on_read+0x9e9>
    6b29:	eb b0                	jmp    6adb <__do_copy_on_read+0x99b>
    6b2b:	4c 89 cf             	mov    %r9,%rdi
    6b2e:	4c 89 a5 78 ff ff ff 	mov    %r12,-0x88(%rbp)
    6b35:	4d 89 dc             	mov    %r11,%r12
    6b38:	e8 00 00 00 00       	callq  6b3d <__do_copy_on_read+0x9fd>
 * If it's called on the same region of memory simultaneously, the effect
 * may be that only one operation succeeds.
 */
static inline void __set_bit(long nr, volatile unsigned long *addr)
{
	asm volatile("bts %1,%0" : ADDR : "Ir" (nr) : "memory");
    6b3d:	41 0f ba 2c 24 03    	btsl   $0x3,(%r12)
					if((coa_pfn!=COA_DEL)&&(coa_pfn!=COA_SKIP)){
						new_page=pfn_to_page(coa_pfn);
						if((new_page->coa_head==&(coa_children->head))&&page_mapped(new_page)&&pages_identical(new_page,old_page)){
							coa_list_unlock(original);
							__SetPageUptodate(new_page);
							page_cache_get(new_page);
    6b43:	4c 89 e7             	mov    %r12,%rdi
    6b46:	e8 b5 95 ff ff       	callq  100 <get_page>
							not_new=true;
    6b4b:	c6 45 98 01          	movb   $0x1,-0x68(%rbp)
							//	printk("reuse the container's exclusive copy at %lx ->page%lx, number of copies=%d\n",address,page_to_pfn(new_page),get_copy_num(original));
							goto new_page_got;
    6b4f:	e9 ba fb ff ff       	jmpq   670e <__do_copy_on_read+0x5ce>
extern bool __get_page_tail(struct page *page);

static inline void get_page(struct page *page)
{
	if (unlikely(PageTail(page)))
		if (likely(__get_page_tail(page)))
    6b54:	48 8b 7d c0          	mov    -0x40(%rbp),%rdi
    6b58:	e8 00 00 00 00       	callq  6b5d <__do_copy_on_read+0xa1d>
    6b5d:	84 c0                	test   %al,%al
    6b5f:	0f 85 53 f9 ff ff    	jne    64b8 <__do_copy_on_read+0x378>
    6b65:	e9 46 f9 ff ff       	jmpq   64b0 <__do_copy_on_read+0x370>
			//	test_and_clear_bit(PG_isolation,&old_page->flags);
		}		/* Free the old page.. */
		ret|=VM_FAULT_WRITE;
		pte_lock=false;
	} else{
		if(!not_new){
    6b6a:	80 7d 98 00          	cmpb   $0x0,-0x68(%rbp)
    6b6e:	66 90                	xchg   %ax,%ax
    6b70:	74 29                	je     6b9b <__do_copy_on_read+0xa5b>
}
static int __do_copy_on_read(struct mm_struct *mm, struct vm_area_struct *vma,unsigned long address, pte_t *page_table, pmd_t *pmd,spinlock_t *ptl, pte_t orig_pte)__releases(ptl){
	struct page *old_page=NULL, *new_page = NULL,*real_new_page=NULL;
	pte_t entry=orig_pte;
	bool isFile=false,pte_lock=true;
	int ret = 0;
    6b72:	45 31 ed             	xor    %r13d,%r13d
	return list_entry(coa_parent->parent_head,struct sclock_coa_parent,head);
}
static int __do_copy_on_read(struct mm_struct *mm, struct vm_area_struct *vma,unsigned long address, pte_t *page_table, pmd_t *pmd,spinlock_t *ptl, pte_t orig_pte)__releases(ptl){
	struct page *old_page=NULL, *new_page = NULL,*real_new_page=NULL;
	pte_t entry=orig_pte;
	bool isFile=false,pte_lock=true;
    6b75:	b8 01 00 00 00       	mov    $0x1,%eax
		return coa_parent;
	}
	return list_entry(coa_parent->parent_head,struct sclock_coa_parent,head);
}
static int __do_copy_on_read(struct mm_struct *mm, struct vm_area_struct *vma,unsigned long address, pte_t *page_table, pmd_t *pmd,spinlock_t *ptl, pte_t orig_pte)__releases(ptl){
	struct page *old_page=NULL, *new_page = NULL,*real_new_page=NULL;
    6b7a:	48 c7 45 d0 00 00 00 	movq   $0x0,-0x30(%rbp)
    6b81:	00 
    6b82:	e9 6e fe ff ff       	jmpq   69f5 <__do_copy_on_read+0x8b5>
		__mmu_notifier_invalidate_range_start(mm, start, end);
    6b87:	48 8b 75 a8          	mov    -0x58(%rbp),%rsi
    6b8b:	48 89 c2             	mov    %rax,%rdx
    6b8e:	4c 89 ff             	mov    %r15,%rdi
    6b91:	e8 00 00 00 00       	callq  6b96 <__do_copy_on_read+0xa56>
    6b96:	e9 98 fb ff ff       	jmpq   6733 <__do_copy_on_read+0x5f3>
		}		/* Free the old page.. */
		ret|=VM_FAULT_WRITE;
		pte_lock=false;
	} else{
		if(!not_new){
			mem_cgroup_uncharge_page(new_page);
    6b9b:	4c 89 e7             	mov    %r12,%rdi
}
static int __do_copy_on_read(struct mm_struct *mm, struct vm_area_struct *vma,unsigned long address, pte_t *page_table, pmd_t *pmd,spinlock_t *ptl, pte_t orig_pte)__releases(ptl){
	struct page *old_page=NULL, *new_page = NULL,*real_new_page=NULL;
	pte_t entry=orig_pte;
	bool isFile=false,pte_lock=true;
	int ret = 0;
    6b9e:	45 31 ed             	xor    %r13d,%r13d
		}		/* Free the old page.. */
		ret|=VM_FAULT_WRITE;
		pte_lock=false;
	} else{
		if(!not_new){
			mem_cgroup_uncharge_page(new_page);
    6ba1:	e8 00 00 00 00       	callq  6ba6 <__do_copy_on_read+0xa66>
	return list_entry(coa_parent->parent_head,struct sclock_coa_parent,head);
}
static int __do_copy_on_read(struct mm_struct *mm, struct vm_area_struct *vma,unsigned long address, pte_t *page_table, pmd_t *pmd,spinlock_t *ptl, pte_t orig_pte)__releases(ptl){
	struct page *old_page=NULL, *new_page = NULL,*real_new_page=NULL;
	pte_t entry=orig_pte;
	bool isFile=false,pte_lock=true;
    6ba6:	b8 01 00 00 00       	mov    $0x1,%eax
		return coa_parent;
	}
	return list_entry(coa_parent->parent_head,struct sclock_coa_parent,head);
}
static int __do_copy_on_read(struct mm_struct *mm, struct vm_area_struct *vma,unsigned long address, pte_t *page_table, pmd_t *pmd,spinlock_t *ptl, pte_t orig_pte)__releases(ptl){
	struct page *old_page=NULL, *new_page = NULL,*real_new_page=NULL;
    6bab:	48 c7 45 d0 00 00 00 	movq   $0x0,-0x30(%rbp)
    6bb2:	00 
    6bb3:	e9 3d fe ff ff       	jmpq   69f5 <__do_copy_on_read+0x8b5>
		}
		cow_user_page(new_page, old_page, address, vma);
	}
	__SetPageUptodate(new_page);
	if (mem_cgroup_newpage_charge(new_page, mm, GFP_KERNEL)){
		printk("oom");
    6bb8:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
    6bbf:	31 c0                	xor    %eax,%eax
    6bc1:	e8 00 00 00 00       	callq  6bc6 <__do_copy_on_read+0xa86>
early_out:
	__get_cpu_var(global_interval_coa_early)+=get_rdtsc()-time1; 
	__get_cpu_var(global_count_coa_early)++;
	return ret;
oom_free_new:
	page_cache_release(new_page);
    6bc6:	4c 89 e7             	mov    %r12,%rdi
    6bc9:	e8 00 00 00 00       	callq  6bce <__do_copy_on_read+0xa8e>
    6bce:	e9 59 fe ff ff       	jmpq   6a2c <__do_copy_on_read+0x8ec>
    6bd3:	4c 89 ee             	mov    %r13,%rsi
    6bd6:	48 89 c7             	mov    %rax,%rdi
    6bd9:	e8 00 00 00 00       	callq  6bde <__do_copy_on_read+0xa9e>
    6bde:	66 90                	xchg   %ax,%ax
    6be0:	e9 07 fb ff ff       	jmpq   66ec <__do_copy_on_read+0x5ac>
						&ptl);
			if (!pte_same(*page_table, orig_pte)) {
				unlock_page(old_page);
				goto unlock;
			}
			page_cache_release(old_page);
    6be5:	48 8b 7d c0          	mov    -0x40(%rbp),%rdi
    6be9:	e8 00 00 00 00       	callq  6bee <__do_copy_on_read+0xaae>
    6bee:	66 90                	xchg   %ax,%ax
    6bf0:	e9 a0 f6 ff ff       	jmpq   6295 <__do_copy_on_read+0x155>

static inline void mmu_notifier_invalidate_range_end(struct mm_struct *mm,
				  unsigned long start, unsigned long end)
{
	if (mm_has_notifiers(mm))
		__mmu_notifier_invalidate_range_end(mm, start, end);
    6bf5:	48 8b 75 a8          	mov    -0x58(%rbp),%rsi
    6bf9:	48 89 c2             	mov    %rax,%rdx
    6bfc:	4c 89 ff             	mov    %r15,%rdi
    6bff:	e8 00 00 00 00       	callq  6c04 <__do_copy_on_read+0xac4>
    6c04:	e9 8f f7 ff ff       	jmpq   6398 <__do_copy_on_read+0x258>
    6c09:	0f 1f 80 00 00 00 00 	nopl   0x0(%rax)

0000000000006c10 <coa_parent_free>:
	page->coa_head=&coa_parent->head;
	//	printk("add one parent into list\n");
	list_add_tail(&coa_parent->node,parent_page_headp);
	return coa_parent;
}
void coa_parent_free(struct sclock_coa_parent *coa_parent){
    6c10:	e8 00 00 00 00       	callq  6c15 <coa_parent_free+0x5>
    6c15:	55                   	push   %rbp
    6c16:	48 89 fe             	mov    %rdi,%rsi
	kmem_cache_free(sclock_coa_parent_cachep,coa_parent);
    6c19:	48 8b 3d 00 00 00 00 	mov    0x0(%rip),%rdi        # 6c20 <coa_parent_free+0x10>
	page->coa_head=&coa_parent->head;
	//	printk("add one parent into list\n");
	list_add_tail(&coa_parent->node,parent_page_headp);
	return coa_parent;
}
void coa_parent_free(struct sclock_coa_parent *coa_parent){
    6c20:	48 89 e5             	mov    %rsp,%rbp
	kmem_cache_free(sclock_coa_parent_cachep,coa_parent);
    6c23:	e8 00 00 00 00       	callq  6c28 <coa_parent_free+0x18>

}
    6c28:	5d                   	pop    %rbp
    6c29:	c3                   	retq   
    6c2a:	66 0f 1f 44 00 00    	nopw   0x0(%rax,%rax,1)

0000000000006c30 <coa_children_free>:
void coa_children_free(struct sclock_coa_children *coa_children){
    6c30:	e8 00 00 00 00       	callq  6c35 <coa_children_free+0x5>
    6c35:	55                   	push   %rbp
    6c36:	48 89 fe             	mov    %rdi,%rsi
	kmem_cache_free(sclock_coa_children_cachep,coa_children);
    6c39:	48 8b 3d 00 00 00 00 	mov    0x0(%rip),%rdi        # 6c40 <coa_children_free+0x10>
}
void coa_parent_free(struct sclock_coa_parent *coa_parent){
	kmem_cache_free(sclock_coa_parent_cachep,coa_parent);

}
void coa_children_free(struct sclock_coa_children *coa_children){
    6c40:	48 89 e5             	mov    %rsp,%rbp
	kmem_cache_free(sclock_coa_children_cachep,coa_children);
    6c43:	e8 00 00 00 00       	callq  6c48 <coa_children_free+0x18>
}
    6c48:	5d                   	pop    %rbp
    6c49:	c3                   	retq   
    6c4a:	66 0f 1f 44 00 00    	nopw   0x0(%rax,%rax,1)

0000000000006c50 <compare_sclock_owner>:


int compare_sclock_owner(void* priv,struct list_head* a,struct list_head* b){
    6c50:	e8 00 00 00 00       	callq  6c55 <compare_sclock_owner+0x5>
    6c55:	55                   	push   %rbp
	b_entry=list_entry(b,struct sclock_coa_children,head);
	a_count=a_entry->owner;
	b_count=b_entry->owner;
	if(a_count<b_count)
	  return -1;
	if(a_count==b_count)
    6c56:	31 c9                	xor    %ecx,%ecx
	unsigned long a_count,b_count;
	a_entry=list_entry(a,struct sclock_coa_children,head);
	b_entry=list_entry(b,struct sclock_coa_children,head);
	a_count=a_entry->owner;
	b_count=b_entry->owner;
	if(a_count<b_count)
    6c58:	48 8b 42 20          	mov    0x20(%rdx),%rax
	  return -1;
	if(a_count==b_count)
    6c5c:	48 39 46 20          	cmp    %rax,0x20(%rsi)
    6c60:	b8 ff ff ff ff       	mov    $0xffffffff,%eax
void coa_children_free(struct sclock_coa_children *coa_children){
	kmem_cache_free(sclock_coa_children_cachep,coa_children);
}


int compare_sclock_owner(void* priv,struct list_head* a,struct list_head* b){
    6c65:	48 89 e5             	mov    %rsp,%rbp
	if(a_count<b_count)
	  return -1;
	if(a_count==b_count)
	  return 0;
	return 1;
}
    6c68:	5d                   	pop    %rbp
	b_entry=list_entry(b,struct sclock_coa_children,head);
	a_count=a_entry->owner;
	b_count=b_entry->owner;
	if(a_count<b_count)
	  return -1;
	if(a_count==b_count)
    6c69:	0f 95 c1             	setne  %cl
    6c6c:	0f 43 c1             	cmovae %ecx,%eax
	  return 0;
	return 1;
}
    6c6f:	c3                   	retq   

0000000000006c70 <get_original>:
{
	return !memcmp_pages(page1, page2);
}


struct sclock_coa_parent *get_original(struct sclock_coa_parent *coa_parent){
    6c70:	e8 00 00 00 00       	callq  6c75 <get_original+0x5>
    6c75:	55                   	push   %rbp
	if(coa_parent->parent_head==NULL){
    6c76:	48 8b 47 18          	mov    0x18(%rdi),%rax
{
	return !memcmp_pages(page1, page2);
}


struct sclock_coa_parent *get_original(struct sclock_coa_parent *coa_parent){
    6c7a:	48 89 e5             	mov    %rsp,%rbp
	if(coa_parent->parent_head==NULL){
		return coa_parent;
    6c7d:	48 85 c0             	test   %rax,%rax
    6c80:	48 0f 44 c7          	cmove  %rdi,%rax
	}
	return list_entry(coa_parent->parent_head,struct sclock_coa_parent,head);
}
    6c84:	5d                   	pop    %rbp
    6c85:	c3                   	retq   
    6c86:	66 2e 0f 1f 84 00 00 	nopw   %cs:0x0(%rax,%rax,1)
    6c8d:	00 00 00 

0000000000006c90 <mkOneScolor>:
	return VM_FAULT_OOM;

}


void mkOneScolor(struct sclock_LRU* sclock_entry,unsigned long pfn){
    6c90:	e8 00 00 00 00       	callq  6c95 <mkOneScolor+0x5>
    6c95:	55                   	push   %rbp
#define LIST_HEAD(name) \
	struct list_head name = LIST_HEAD_INIT(name)

static inline void INIT_LIST_HEAD(struct list_head *list)
{
	list->next = list;
    6c96:	48 89 3f             	mov    %rdi,(%rdi)
	list->prev = list;
    6c99:	48 89 7f 08          	mov    %rdi,0x8(%rdi)
    6c9d:	c7 47 18 01 00 00 00 	movl   $0x1,0x18(%rdi)
    6ca4:	48 89 e5             	mov    %rsp,%rbp
	//	sclock_entry->sclock_lru.prev=&(sclock_entry->sclock_lru);
	//	ptep_clear_flush(vma,address,pte_table);
	atomic_set(&sclock_entry->access_times,1);
//	INIT_LIST_HEAD(&sclock_entry->pte_map);
//	atomic_set(&sclock_entry->pte_count,0);
	sclock_entry->pfn=pfn;
    6ca7:	48 89 77 10          	mov    %rsi,0x10(%rdi)
}
    6cab:	5d                   	pop    %rbp
    6cac:	c3                   	retq   
    6cad:	0f 1f 00             	nopl   (%rax)

0000000000006cb0 <mkOneSclockVirtual>:
atomic_set(&sclock_entry->access_times,1);
INIT_LIST_HEAD(&sclock_entry->pte_map);
atomic_set(&sclock_entry->pte_count,1);
sclock_entry->pfn=pfn;
}*/
void mkOneSclockVirtual(struct sclock_LRU_virtual* sclock_entry,unsigned long address,struct mm_struct * mm,pte_t* ptep,unsigned long pfn){
    6cb0:	e8 00 00 00 00       	callq  6cb5 <mkOneSclockVirtual+0x5>
    6cb5:	55                   	push   %rbp
 * However, if the list being initialized is visible to readers, you
 * need to keep the compiler from being too mischievous.
 */
static inline void INIT_LIST_HEAD_RCU(struct list_head *list)
{
	ACCESS_ONCE(list->next) = list;
    6cb6:	48 89 3f             	mov    %rdi,(%rdi)
    6cb9:	c7 47 30 01 00 00 00 	movl   $0x1,0x30(%rdi)
	ACCESS_ONCE(list->prev) = list;
    6cc0:	48 89 7f 08          	mov    %rdi,0x8(%rdi)
    6cc4:	48 89 e5             	mov    %rsp,%rbp
	INIT_LIST_HEAD_RCU(&(sclock_entry->sclock_lru));
	//	sclock_entry->sclock_lru.next=&(sclock_entry->sclock_lru);
	//	sclock_entry->sclock_lru.prev=&(sclock_entry->sclock_lru);
	//	ptep_clear_flush(vma,address,pte_table);
	atomic_set(&sclock_entry->access_times,1);
	sclock_entry->ptep=ptep;
    6cc7:	48 89 4f 10          	mov    %rcx,0x10(%rdi)
	sclock_entry->mm=mm;
    6ccb:	48 89 57 28          	mov    %rdx,0x28(%rdi)
	sclock_entry->pfn=pfn;
    6ccf:	4c 89 47 20          	mov    %r8,0x20(%rdi)
	sclock_entry->address=address;
    6cd3:	48 89 77 18          	mov    %rsi,0x18(%rdi)
}
    6cd7:	5d                   	pop    %rbp
    6cd8:	c3                   	retq   
    6cd9:	0f 1f 80 00 00 00 00 	nopl   0x0(%rax)

0000000000006ce0 <mkOneCacheableLRU>:
void mkOneCacheableLRU(struct sclock_LRU* sclock_entry,long address, pte_t * page_table,struct mm_struct *mm,struct page* page){
    6ce0:	e8 00 00 00 00       	callq  6ce5 <mkOneCacheableLRU+0x5>
    6ce5:	55                   	push   %rbp
	//	sclock_entry->sclock_lru.next=&(sclock_entry->sclock_lru);
	//	sclock_entry->sclock_lru.prev=&(sclock_entry->sclock_lru);
	//	ptep_clear_flush(vma,address,pte_table);
	atomic_set(&sclock_entry->access_times,1);
	sclock_entry->pfn=page_to_pfn(page);
	printk(KERN_DEBUG"after initial entry,pfn=%lx\n",sclock_entry->pfn);
    6ce6:	31 c0                	xor    %eax,%eax
	sclock_entry->ptep=ptep;
	sclock_entry->mm=mm;
	sclock_entry->pfn=pfn;
	sclock_entry->address=address;
}
void mkOneCacheableLRU(struct sclock_LRU* sclock_entry,long address, pte_t * page_table,struct mm_struct *mm,struct page* page){
    6ce8:	48 89 e5             	mov    %rsp,%rbp
    6ceb:	41 56                	push   %r14
    6ced:	49 89 f6             	mov    %rsi,%r14
	INIT_LIST_HEAD(&(sclock_entry->sclock_lru));
	//	sclock_entry->sclock_lru.next=&(sclock_entry->sclock_lru);
	//	sclock_entry->sclock_lru.prev=&(sclock_entry->sclock_lru);
	//	ptep_clear_flush(vma,address,pte_table);
	atomic_set(&sclock_entry->access_times,1);
	sclock_entry->pfn=page_to_pfn(page);
    6cf0:	48 be 00 00 00 00 00 	movabs $0x160000000000,%rsi
    6cf7:	16 00 00 
	sclock_entry->ptep=ptep;
	sclock_entry->mm=mm;
	sclock_entry->pfn=pfn;
	sclock_entry->address=address;
}
void mkOneCacheableLRU(struct sclock_LRU* sclock_entry,long address, pte_t * page_table,struct mm_struct *mm,struct page* page){
    6cfa:	41 55                	push   %r13
	INIT_LIST_HEAD(&(sclock_entry->sclock_lru));
	//	sclock_entry->sclock_lru.next=&(sclock_entry->sclock_lru);
	//	sclock_entry->sclock_lru.prev=&(sclock_entry->sclock_lru);
	//	ptep_clear_flush(vma,address,pte_table);
	atomic_set(&sclock_entry->access_times,1);
	sclock_entry->pfn=page_to_pfn(page);
    6cfc:	4c 01 c6             	add    %r8,%rsi
	sclock_entry->ptep=ptep;
	sclock_entry->mm=mm;
	sclock_entry->pfn=pfn;
	sclock_entry->address=address;
}
void mkOneCacheableLRU(struct sclock_LRU* sclock_entry,long address, pte_t * page_table,struct mm_struct *mm,struct page* page){
    6cff:	49 89 d5             	mov    %rdx,%r13
	INIT_LIST_HEAD(&(sclock_entry->sclock_lru));
	//	sclock_entry->sclock_lru.next=&(sclock_entry->sclock_lru);
	//	sclock_entry->sclock_lru.prev=&(sclock_entry->sclock_lru);
	//	ptep_clear_flush(vma,address,pte_table);
	atomic_set(&sclock_entry->access_times,1);
	sclock_entry->pfn=page_to_pfn(page);
    6d02:	48 c1 fe 06          	sar    $0x6,%rsi
	sclock_entry->ptep=ptep;
	sclock_entry->mm=mm;
	sclock_entry->pfn=pfn;
	sclock_entry->address=address;
}
void mkOneCacheableLRU(struct sclock_LRU* sclock_entry,long address, pte_t * page_table,struct mm_struct *mm,struct page* page){
    6d06:	41 54                	push   %r12
    6d08:	49 89 cc             	mov    %rcx,%r12
    6d0b:	53                   	push   %rbx
	pte_t pte_entry=*page_table;
    6d0c:	48 8b 1a             	mov    (%rdx),%rbx
#define LIST_HEAD(name) \
	struct list_head name = LIST_HEAD_INIT(name)

static inline void INIT_LIST_HEAD(struct list_head *list)
{
	list->next = list;
    6d0f:	48 89 3f             	mov    %rdi,(%rdi)
	list->prev = list;
    6d12:	48 89 7f 08          	mov    %rdi,0x8(%rdi)
    6d16:	c7 47 18 01 00 00 00 	movl   $0x1,0x18(%rdi)
	INIT_LIST_HEAD(&(sclock_entry->sclock_lru));
	//	sclock_entry->sclock_lru.next=&(sclock_entry->sclock_lru);
	//	sclock_entry->sclock_lru.prev=&(sclock_entry->sclock_lru);
	//	ptep_clear_flush(vma,address,pte_table);
	atomic_set(&sclock_entry->access_times,1);
	sclock_entry->pfn=page_to_pfn(page);
    6d1d:	48 89 77 10          	mov    %rsi,0x10(%rdi)
	printk(KERN_DEBUG"after initial entry,pfn=%lx\n",sclock_entry->pfn);
    6d21:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
    6d28:	e8 00 00 00 00       	callq  6d2d <mkOneCacheableLRU+0x4d>
	pte_entry.pte&=~_PAGE_CACHE_UC_MINUS;
	pte_entry.pte&=~_PAGE_NCACHE;
    6d2d:	48 b8 ef ff ff ff ff 	movabs $0xfffbffffffffffef,%rax
    6d34:	ff fb ff 
    6d37:	48 21 c3             	and    %rax,%rbx
}

static inline void mmu_notifier_change_pte(struct mm_struct *mm,
					   unsigned long address, pte_t pte)
{
	if (mm_has_notifiers(mm))
    6d3a:	49 83 bc 24 a0 03 00 	cmpq   $0x0,0x3a0(%r12)
    6d41:	00 00 
    6d43:	75 1c                	jne    6d61 <mkOneCacheableLRU+0x81>
{
	if (sizeof(pteval_t) > sizeof(long))
		/* 5 arg words */
		pv_mmu_ops.set_pte_at(mm, addr, ptep, pte);
	else
		PVOP_VCALL4(pv_mmu_ops.set_pte_at, mm, addr, ptep, pte.pte);
    6d45:	4c 89 e7             	mov    %r12,%rdi
    6d48:	4c 89 f6             	mov    %r14,%rsi
    6d4b:	4c 89 ea             	mov    %r13,%rdx
    6d4e:	48 89 d9             	mov    %rbx,%rcx
    6d51:	ff 14 25 00 00 00 00 	callq  *0x0
	//sclock_entry->pte=pte_entry;

	set_pte_at_notify(mm, address, page_table, pte_entry);
	//	update_mmu_cache(sclock_entry->vma, sclock_entry->address, sclock_entry->pte);
}
    6d58:	5b                   	pop    %rbx
    6d59:	41 5c                	pop    %r12
    6d5b:	41 5d                	pop    %r13
    6d5d:	41 5e                	pop    %r14
    6d5f:	5d                   	pop    %rbp
    6d60:	c3                   	retq   
		__mmu_notifier_change_pte(mm, address, pte);
    6d61:	48 89 da             	mov    %rbx,%rdx
    6d64:	4c 89 f6             	mov    %r14,%rsi
    6d67:	4c 89 e7             	mov    %r12,%rdi
    6d6a:	e8 00 00 00 00       	callq  6d6f <mkOneCacheableLRU+0x8f>
    6d6f:	eb d4                	jmp    6d45 <mkOneCacheableLRU+0x65>
    6d71:	66 66 66 66 66 66 2e 	data32 data32 data32 data32 data32 nopw %cs:0x0(%rax,%rax,1)
    6d78:	0f 1f 84 00 00 00 00 
    6d7f:	00 

0000000000006d80 <do_page_setirq>:
		}
	}
unlock:	page_unlock_anon_vma_read(anon_vma);
		return ret;
	}
	int do_page_setirq(struct page* page,struct pid_namespace* pid_ns, pteval_t flags,int count){	
    6d80:	e8 00 00 00 00       	callq  6d85 <do_page_setirq+0x5>
    6d85:	55                   	push   %rbp
    6d86:	48 89 e5             	mov    %rsp,%rbp
    6d89:	41 57                	push   %r15
    6d8b:	41 56                	push   %r14
    6d8d:	49 89 fe             	mov    %rdi,%r14
    6d90:	41 55                	push   %r13
    6d92:	41 54                	push   %r12
    6d94:	53                   	push   %rbx
    6d95:	48 83 ec 28          	sub    $0x28,%rsp
 *
 * Atomically reads the value of @v.
 */
static inline int atomic_read(const atomic_t *v)
{
	return (*(volatile int *)&(v)->counter);
    6d99:	8b 47 18             	mov    0x18(%rdi),%eax
	if(page_mapped(page)&&page_rmapping(page)){
    6d9c:	85 c0                	test   %eax,%eax
    6d9e:	0f 88 ad 01 00 00    	js     6f51 <do_page_setirq+0x1d1>
extern struct address_space *page_mapping(struct page *page);

/* Neutral page->mapping pointer to address_space or anon_vma or other */
static inline void *page_rmapping(struct page *page)
{
	return (void *)((unsigned long)page->mapping & ~PAGE_MAPPING_FLAGS);
    6da4:	48 8b 47 08          	mov    0x8(%rdi),%rax
    6da8:	48 a9 fc ff ff ff    	test   $0xfffffffffffffffc,%rax
    6dae:	0f 84 9d 01 00 00    	je     6f51 <do_page_setirq+0x1d1>
    6db4:	49 89 d7             	mov    %rdx,%r15
    6db7:	48 89 c2             	mov    %rax,%rdx
    6dba:	49 89 f5             	mov    %rsi,%r13
    6dbd:	83 e2 03             	and    $0x3,%edx
			//	if(page_mapcount(page)>0){
			//printk("mapcount==%d\n",page_mapcount(page));
			if(PageKsm(page)){
    6dc0:	48 83 fa 03          	cmp    $0x3,%rdx
    6dc4:	0f 84 7d 01 00 00    	je     6f47 <do_page_setirq+0x1c7>
				return do_ksm_page_set(page,pid_ns,flags,count);
				//printk("map=%d",	do_ksm_page_clear(page,pid_ns,_PAGE_CACHE_PROTECT));
			}else if(PageAnon(page)){
    6dca:	a8 01                	test   $0x1,%al
    6dcc:	0f 85 5e 01 00 00    	jne    6f30 <do_page_setirq+0x1b0>
				return do_anon_page_set(page,pid_ns,flags,count);
				//printk("map=%d",	do_anon_page_clear(page,pid_ns,_PAGE_CACHE_PROTECT));
			}
			else if(page_mapping(page)){
    6dd2:	e8 00 00 00 00       	callq  6dd7 <do_page_setirq+0x57>
    6dd7:	48 85 c0             	test   %rax,%rax
    6dda:	0f 84 71 01 00 00    	je     6f51 <do_page_setirq+0x1d1>
unlock:	
	anon_vma_unlock_read(anon_vma);
	return ret;
}
static int do_file_page_set_irq(struct page* page,struct pid_namespace* pid_ns_ref, pteval_t flags,int count){
	struct address_space *mapping = page->mapping;
    6de0:	4d 8b 66 08          	mov    0x8(%r14),%r12
	pgoff_t pgoff = page->index << (PAGE_CACHE_SHIFT - PAGE_SHIFT);
    6de4:	49 8b 5e 10          	mov    0x10(%r14),%rbx
}
static inline void i_mmap_lock_read(struct address_space *mapping){
 down_read(&mapping->i_mmap_rwsem);
    6de8:	49 8d 44 24 50       	lea    0x50(%r12),%rax
    6ded:	48 89 c7             	mov    %rax,%rdi
    6df0:	48 89 45 c8          	mov    %rax,-0x38(%rbp)
    6df4:	e8 00 00 00 00       	callq  6df9 <do_page_setirq+0x79>
	//spin_lock_irqsave(&mapping->i_mmap_spinlock,irqflags);
#else
	i_mmap_lock_read(mapping);

#endif
	vma_interval_tree_foreach(vma, &mapping->i_mmap, pgoff, pgoff) {
    6df9:	49 8d 7c 24 38       	lea    0x38(%r12),%rdi
    6dfe:	48 89 da             	mov    %rbx,%rdx
    6e01:	48 89 de             	mov    %rbx,%rsi
    6e04:	e8 00 00 00 00       	callq  6e09 <do_page_setirq+0x89>
    6e09:	48 85 c0             	test   %rax,%rax
    6e0c:	48 89 c1             	mov    %rax,%rcx
	struct vm_area_struct *vma;
	struct mm_struct * mm;
	struct task_struct * task;
	unsigned long address;
	pte_t* pte,pte_entry; 
	int ret=0;
    6e0f:	c7 45 d4 00 00 00 00 	movl   $0x0,-0x2c(%rbp)
	//spin_lock_irqsave(&mapping->i_mmap_spinlock,irqflags);
#else
	i_mmap_lock_read(mapping);

#endif
	vma_interval_tree_foreach(vma, &mapping->i_mmap, pgoff, pgoff) {
    6e16:	0f 84 f4 00 00 00    	je     6f10 <do_page_setirq+0x190>
		if((mm=vma->vm_mm))
		  if((mm->def_flags&VM_NCACHE)&&(task=mm->owner))
    6e1c:	49 bc 00 00 00 00 02 	movabs $0x200000000,%r12
    6e23:	00 00 00 
    6e26:	eb 33                	jmp    6e5b <do_page_setirq+0xdb>
    6e28:	0f 1f 84 00 00 00 00 	nopl   0x0(%rax,%rax,1)
    6e2f:	00 
	if (pid)
		ns = pid->numbers[pid->level].ns;
    6e30:	8b 70 04             	mov    0x4(%rax),%esi
    6e33:	48 c1 e6 05          	shl    $0x5,%rsi
    6e37:	48 8b 44 30 38       	mov    0x38(%rax,%rsi,1),%rax
			if(ns_of_pid(task_pid(task))==pid_ns_ref){
    6e3c:	49 39 c5             	cmp    %rax,%r13
    6e3f:	74 4f                	je     6e90 <do_page_setirq+0x110>
	//spin_lock_irqsave(&mapping->i_mmap_spinlock,irqflags);
#else
	i_mmap_lock_read(mapping);

#endif
	vma_interval_tree_foreach(vma, &mapping->i_mmap, pgoff, pgoff) {
    6e41:	48 89 cf             	mov    %rcx,%rdi
    6e44:	48 89 da             	mov    %rbx,%rdx
    6e47:	48 89 de             	mov    %rbx,%rsi
    6e4a:	e8 00 00 00 00       	callq  6e4f <do_page_setirq+0xcf>
    6e4f:	48 85 c0             	test   %rax,%rax
    6e52:	48 89 c1             	mov    %rax,%rcx
    6e55:	0f 84 b5 00 00 00    	je     6f10 <do_page_setirq+0x190>
		if((mm=vma->vm_mm))
    6e5b:	48 8b 51 40          	mov    0x40(%rcx),%rdx
    6e5f:	48 85 d2             	test   %rdx,%rdx
    6e62:	74 dd                	je     6e41 <do_page_setirq+0xc1>
		  if((mm->def_flags&VM_NCACHE)&&(task=mm->owner))
    6e64:	4c 85 a2 f8 00 00 00 	test   %r12,0xf8(%rdx)
    6e6b:	74 d4                	je     6e41 <do_page_setirq+0xc1>
    6e6d:	48 8b 82 90 03 00 00 	mov    0x390(%rdx),%rax
    6e74:	48 85 c0             	test   %rax,%rax
    6e77:	74 c8                	je     6e41 <do_page_setirq+0xc1>
    6e79:	48 8b 80 60 03 00 00 	mov    0x360(%rax),%rax
 * 	the resulting NULL pid-ns.
 */
static inline struct pid_namespace *ns_of_pid(struct pid *pid)
{
	struct pid_namespace *ns = NULL;
	if (pid)
    6e80:	48 85 c0             	test   %rax,%rax
    6e83:	75 ab                	jne    6e30 <do_page_setirq+0xb0>
 * 	is expected to be non-NULL. If @pid is NULL, caller should handle
 * 	the resulting NULL pid-ns.
 */
static inline struct pid_namespace *ns_of_pid(struct pid *pid)
{
	struct pid_namespace *ns = NULL;
    6e85:	31 c0                	xor    %eax,%eax
			if(ns_of_pid(task_pid(task))==pid_ns_ref){
    6e87:	49 39 c5             	cmp    %rax,%r13
    6e8a:	75 b5                	jne    6e41 <do_page_setirq+0xc1>
    6e8c:	0f 1f 40 00          	nopl   0x0(%rax)
				address = vma_to_address(page, vma);
    6e90:	48 89 ce             	mov    %rcx,%rsi
    6e93:	4c 89 f7             	mov    %r14,%rdi
    6e96:	48 89 4d c0          	mov    %rcx,-0x40(%rbp)
    6e9a:	48 89 55 b8          	mov    %rdx,-0x48(%rbp)
    6e9e:	e8 00 00 00 00       	callq  6ea3 <do_page_setirq+0x123>
				spinlock_t *ptl;
			//	if(!follow_pte(mm,address,&pte,&ptl)){
					pte=find_pte(mm,address);
    6ea3:	48 8b 55 b8          	mov    -0x48(%rbp),%rdx
    6ea7:	48 89 c6             	mov    %rax,%rsi
    6eaa:	48 89 45 b0          	mov    %rax,-0x50(%rbp)
    6eae:	48 89 d7             	mov    %rdx,%rdi
    6eb1:	e8 00 00 00 00       	callq  6eb6 <do_page_setirq+0x136>
					if(pte)
    6eb6:	48 85 c0             	test   %rax,%rax
		  if((mm->def_flags&VM_NCACHE)&&(task=mm->owner))
			if(ns_of_pid(task_pid(task))==pid_ns_ref){
				address = vma_to_address(page, vma);
				spinlock_t *ptl;
			//	if(!follow_pte(mm,address,&pte,&ptl)){
					pte=find_pte(mm,address);
    6eb9:	48 89 c2             	mov    %rax,%rdx
					if(pte)
    6ebc:	48 8b 4d c0          	mov    -0x40(%rbp),%rcx
    6ec0:	0f 84 7b ff ff ff    	je     6e41 <do_page_setirq+0xc1>
					  if(((pte->pte&flags)!=flags)){
    6ec6:	48 8b 00             	mov    (%rax),%rax
    6ec9:	4c 89 fe             	mov    %r15,%rsi
    6ecc:	48 21 c6             	and    %rax,%rsi
    6ecf:	49 39 f7             	cmp    %rsi,%r15
    6ed2:	0f 84 69 ff ff ff    	je     6e41 <do_page_setirq+0xc1>
						  ret++;
						  pte->pte|=flags;
						  flush_tlb_page(vma,address);
    6ed8:	4c 8b 45 b0          	mov    -0x50(%rbp),%r8
			//	if(!follow_pte(mm,address,&pte,&ptl)){
					pte=find_pte(mm,address);
					if(pte)
					  if(((pte->pte&flags)!=flags)){
						  ret++;
						  pte->pte|=flags;
    6edc:	4c 09 f8             	or     %r15,%rax
						  flush_tlb_page(vma,address);
    6edf:	48 89 cf             	mov    %rcx,%rdi
			//	if(!follow_pte(mm,address,&pte,&ptl)){
					pte=find_pte(mm,address);
					if(pte)
					  if(((pte->pte&flags)!=flags)){
						  ret++;
						  pte->pte|=flags;
    6ee2:	48 89 02             	mov    %rax,(%rdx)
				spinlock_t *ptl;
			//	if(!follow_pte(mm,address,&pte,&ptl)){
					pte=find_pte(mm,address);
					if(pte)
					  if(((pte->pte&flags)!=flags)){
						  ret++;
    6ee5:	83 45 d4 01          	addl   $0x1,-0x2c(%rbp)
						  pte->pte|=flags;
						  flush_tlb_page(vma,address);
    6ee9:	4c 89 c6             	mov    %r8,%rsi
    6eec:	e8 00 00 00 00       	callq  6ef1 <do_page_setirq+0x171>
						  if(ret>=get_page_counter_in_ns(page,pid_ns_ref))
    6ef1:	4c 89 ee             	mov    %r13,%rsi
    6ef4:	4c 89 f7             	mov    %r14,%rdi
    6ef7:	e8 00 00 00 00       	callq  6efc <do_page_setirq+0x17c>
    6efc:	39 45 d4             	cmp    %eax,-0x2c(%rbp)
    6eff:	48 8b 4d c0          	mov    -0x40(%rbp),%rcx
    6f03:	0f 8c 38 ff ff ff    	jl     6e41 <do_page_setirq+0xc1>
    6f09:	0f 1f 80 00 00 00 00 	nopl   0x0(%rax)
static inline int i_mmap_trylock_read(struct address_space *mapping){
 return down_read_trylock(&mapping->i_mmap_rwsem);
}

static inline void i_mmap_unlock_read(struct address_space *mapping){
 up_read(&mapping->i_mmap_rwsem);
    6f10:	48 8b 7d c8          	mov    -0x38(%rbp),%rdi
    6f14:	e8 00 00 00 00       	callq  6f19 <do_page_setirq+0x199>
			}else if(PageAnon(page)){
				return do_anon_page_set(page,pid_ns,flags,count);
				//printk("map=%d",	do_anon_page_clear(page,pid_ns,_PAGE_CACHE_PROTECT));
			}
			else if(page_mapping(page)){
				return do_file_page_set_irq(page,pid_ns,flags,count);
    6f19:	8b 45 d4             	mov    -0x2c(%rbp),%eax
				//printk("map=%d",	do_file_page_clear(page,pid_ns,_PAGE_CACHE_PROTECT));
			}
			//}
		}
		return -1;
	}
    6f1c:	48 83 c4 28          	add    $0x28,%rsp
    6f20:	5b                   	pop    %rbx
    6f21:	41 5c                	pop    %r12
    6f23:	41 5d                	pop    %r13
    6f25:	41 5e                	pop    %r14
    6f27:	41 5f                	pop    %r15
    6f29:	5d                   	pop    %rbp
    6f2a:	c3                   	retq   
    6f2b:	0f 1f 44 00 00       	nopl   0x0(%rax,%rax,1)
			//printk("mapcount==%d\n",page_mapcount(page));
			if(PageKsm(page)){
				return do_ksm_page_set(page,pid_ns,flags,count);
				//printk("map=%d",	do_ksm_page_clear(page,pid_ns,_PAGE_CACHE_PROTECT));
			}else if(PageAnon(page)){
				return do_anon_page_set(page,pid_ns,flags,count);
    6f30:	4c 89 fa             	mov    %r15,%rdx
    6f33:	e8 18 96 ff ff       	callq  550 <do_anon_page_set.isra.57>
				//printk("map=%d",	do_file_page_clear(page,pid_ns,_PAGE_CACHE_PROTECT));
			}
			//}
		}
		return -1;
	}
    6f38:	48 83 c4 28          	add    $0x28,%rsp
    6f3c:	5b                   	pop    %rbx
    6f3d:	41 5c                	pop    %r12
    6f3f:	41 5d                	pop    %r13
    6f41:	41 5e                	pop    %r14
    6f43:	41 5f                	pop    %r15
    6f45:	5d                   	pop    %rbp
    6f46:	c3                   	retq   
	int do_page_setirq(struct page* page,struct pid_namespace* pid_ns, pteval_t flags,int count){	
	if(page_mapped(page)&&page_rmapping(page)){
			//	if(page_mapcount(page)>0){
			//printk("mapcount==%d\n",page_mapcount(page));
			if(PageKsm(page)){
				return do_ksm_page_set(page,pid_ns,flags,count);
    6f47:	4c 89 fa             	mov    %r15,%rdx
    6f4a:	e8 21 94 ff ff       	callq  370 <do_ksm_page_set.isra.56>
    6f4f:	eb cb                	jmp    6f1c <do_page_setirq+0x19c>
				return do_file_page_set_irq(page,pid_ns,flags,count);
				//printk("map=%d",	do_file_page_clear(page,pid_ns,_PAGE_CACHE_PROTECT));
			}
			//}
		}
		return -1;
    6f51:	b8 ff ff ff ff       	mov    $0xffffffff,%eax
    6f56:	eb c4                	jmp    6f1c <do_page_setirq+0x19c>
    6f58:	0f 1f 84 00 00 00 00 	nopl   0x0(%rax,%rax,1)
    6f5f:	00 

0000000000006f60 <do_page_set_care>:
	}
	int do_page_set_care(struct page* page,struct pid_namespace* pid_ns, pteval_t flags,int count){
    6f60:	e8 00 00 00 00       	callq  6f65 <do_page_set_care+0x5>
    6f65:	55                   	push   %rbp
    6f66:	48 89 e5             	mov    %rsp,%rbp
    6f69:	41 57                	push   %r15
    6f6b:	41 56                	push   %r14
    6f6d:	41 55                	push   %r13
    6f6f:	49 89 fd             	mov    %rdi,%r13
    6f72:	41 54                	push   %r12
    6f74:	53                   	push   %rbx
    6f75:	48 83 ec 28          	sub    $0x28,%rsp
    6f79:	8b 47 18             	mov    0x18(%rdi),%eax
		if(page_mapped(page)&&page_rmapping(page)){
    6f7c:	85 c0                	test   %eax,%eax
    6f7e:	0f 88 b1 01 00 00    	js     7135 <do_page_set_care+0x1d5>
    6f84:	48 8b 47 08          	mov    0x8(%rdi),%rax
    6f88:	48 a9 fc ff ff ff    	test   $0xfffffffffffffffc,%rax
    6f8e:	0f 84 a1 01 00 00    	je     7135 <do_page_set_care+0x1d5>
    6f94:	49 89 d6             	mov    %rdx,%r14
    6f97:	48 89 c2             	mov    %rax,%rdx
    6f9a:	49 89 f4             	mov    %rsi,%r12
    6f9d:	83 e2 03             	and    $0x3,%edx
			//	if(page_mapcount(page)>0){
			//printk("mapcount==%d\n",page_mapcount(page));
			if(PageKsm(page)){
    6fa0:	48 83 fa 03          	cmp    $0x3,%rdx
    6fa4:	0f 84 7e 01 00 00    	je     7128 <do_page_set_care+0x1c8>
				return do_ksm_page_set(page,pid_ns,flags,count);
				//printk("map=%d",	do_ksm_page_clear(page,pid_ns,_PAGE_CACHE_PROTECT));
			}else if(PageAnon(page)){
    6faa:	a8 01                	test   $0x1,%al
    6fac:	75 32                	jne    6fe0 <do_page_set_care+0x80>
				return do_anon_page_set_care(page,pid_ns,flags,count);
				//printk("map=%d",	do_anon_page_clear(page,pid_ns,_PAGE_CACHE_PROTECT));
			}
			else if(page_mapping(page)){
    6fae:	e8 00 00 00 00       	callq  6fb3 <do_page_set_care+0x53>
    6fb3:	48 85 c0             	test   %rax,%rax
    6fb6:	0f 84 79 01 00 00    	je     7135 <do_page_set_care+0x1d5>
				return do_file_page_set(page,pid_ns,flags,count);
    6fbc:	4c 89 f2             	mov    %r14,%rdx
    6fbf:	4c 89 e6             	mov    %r12,%rsi
    6fc2:	4c 89 ef             	mov    %r13,%rdi
    6fc5:	e8 36 92 ff ff       	callq  200 <do_file_page_set.isra.55>
				//printk("map=%d",	do_file_page_clear(page,pid_ns,_PAGE_CACHE_PROTECT));
			}
			//}
		}
		return -1;
	}
    6fca:	48 83 c4 28          	add    $0x28,%rsp
    6fce:	5b                   	pop    %rbx
    6fcf:	41 5c                	pop    %r12
    6fd1:	41 5d                	pop    %r13
    6fd3:	41 5e                	pop    %r14
    6fd5:	41 5f                	pop    %r15
    6fd7:	5d                   	pop    %rbp
    6fd8:	c3                   	retq   
    6fd9:	0f 1f 80 00 00 00 00 	nopl   0x0(%rax)
	pgoff_t pgoff;
	struct vm_area_struct *vma;
	struct mm_struct* mm;
	struct task_struct * task;
	int ret=0;
	anon_vma=page_lock_anon_vma_read(page);
    6fe0:	e8 00 00 00 00       	callq  6fe5 <do_page_set_care+0x85>
	if (!anon_vma){
    6fe5:	48 85 c0             	test   %rax,%rax
	pgoff_t pgoff;
	struct vm_area_struct *vma;
	struct mm_struct* mm;
	struct task_struct * task;
	int ret=0;
	anon_vma=page_lock_anon_vma_read(page);
    6fe8:	49 89 c7             	mov    %rax,%r15
	if (!anon_vma){
		return ret;
    6feb:	c7 45 d4 00 00 00 00 	movl   $0x0,-0x2c(%rbp)
	struct vm_area_struct *vma;
	struct mm_struct* mm;
	struct task_struct * task;
	int ret=0;
	anon_vma=page_lock_anon_vma_read(page);
	if (!anon_vma){
    6ff2:	0f 84 18 01 00 00    	je     7110 <do_page_set_care+0x1b0>
		return ret;
	}
	pgoff=page->index<<(PAGE_CACHE_SHIFT-PAGE_SHIFT);
    6ff8:	49 8b 5d 10          	mov    0x10(%r13),%rbx
	anon_vma_interval_tree_foreach(avc,&anon_vma->rb_root,pgoff,pgoff){
    6ffc:	48 8d 78 40          	lea    0x40(%rax),%rdi
    7000:	48 89 da             	mov    %rbx,%rdx
    7003:	48 89 de             	mov    %rbx,%rsi
    7006:	e8 00 00 00 00       	callq  700b <do_page_set_care+0xab>
    700b:	48 85 c0             	test   %rax,%rax
    700e:	48 89 c1             	mov    %rax,%rcx
    7011:	75 38                	jne    704b <do_page_set_care+0xeb>
    7013:	e9 f0 00 00 00       	jmpq   7108 <do_page_set_care+0x1a8>
    7018:	0f 1f 84 00 00 00 00 	nopl   0x0(%rax,%rax,1)
    701f:	00 
	if (pid)
		ns = pid->numbers[pid->level].ns;
    7020:	8b 70 04             	mov    0x4(%rax),%esi
    7023:	48 c1 e6 05          	shl    $0x5,%rsi
    7027:	48 8b 44 30 38       	mov    0x38(%rax,%rsi,1),%rax
		vma=avc->vma;
		if(vma){
			if((mm=vma->vm_mm))
			  if((task=mm->owner))
				if(ns_of_pid(task_pid(task))==pid_ns_ref){
    702c:	49 39 c4             	cmp    %rax,%r12
    702f:	74 4f                	je     7080 <do_page_set_care+0x120>
	anon_vma=page_lock_anon_vma_read(page);
	if (!anon_vma){
		return ret;
	}
	pgoff=page->index<<(PAGE_CACHE_SHIFT-PAGE_SHIFT);
	anon_vma_interval_tree_foreach(avc,&anon_vma->rb_root,pgoff,pgoff){
    7031:	48 89 cf             	mov    %rcx,%rdi
    7034:	48 89 da             	mov    %rbx,%rdx
    7037:	48 89 de             	mov    %rbx,%rsi
    703a:	e8 00 00 00 00       	callq  703f <do_page_set_care+0xdf>
    703f:	48 85 c0             	test   %rax,%rax
    7042:	48 89 c1             	mov    %rax,%rcx
    7045:	0f 84 bd 00 00 00    	je     7108 <do_page_set_care+0x1a8>
		vma=avc->vma;
    704b:	48 8b 11             	mov    (%rcx),%rdx
		if(vma){
    704e:	48 85 d2             	test   %rdx,%rdx
    7051:	74 de                	je     7031 <do_page_set_care+0xd1>
			if((mm=vma->vm_mm))
    7053:	4c 8b 42 40          	mov    0x40(%rdx),%r8
    7057:	4d 85 c0             	test   %r8,%r8
    705a:	74 d5                	je     7031 <do_page_set_care+0xd1>
			  if((task=mm->owner))
    705c:	49 8b 80 90 03 00 00 	mov    0x390(%r8),%rax
    7063:	48 85 c0             	test   %rax,%rax
    7066:	74 c9                	je     7031 <do_page_set_care+0xd1>
    7068:	48 8b 80 60 03 00 00 	mov    0x360(%rax),%rax
 * 	the resulting NULL pid-ns.
 */
static inline struct pid_namespace *ns_of_pid(struct pid *pid)
{
	struct pid_namespace *ns = NULL;
	if (pid)
    706f:	48 85 c0             	test   %rax,%rax
    7072:	75 ac                	jne    7020 <do_page_set_care+0xc0>
 * 	is expected to be non-NULL. If @pid is NULL, caller should handle
 * 	the resulting NULL pid-ns.
 */
static inline struct pid_namespace *ns_of_pid(struct pid *pid)
{
	struct pid_namespace *ns = NULL;
    7074:	31 c0                	xor    %eax,%eax
				if(ns_of_pid(task_pid(task))==pid_ns_ref){
    7076:	49 39 c4             	cmp    %rax,%r12
    7079:	75 b6                	jne    7031 <do_page_set_care+0xd1>
    707b:	0f 1f 44 00 00       	nopl   0x0(%rax,%rax,1)
					address = vma_to_address(page, vma);
    7080:	48 89 d6             	mov    %rdx,%rsi
    7083:	4c 89 ef             	mov    %r13,%rdi
    7086:	48 89 4d c0          	mov    %rcx,-0x40(%rbp)
    708a:	4c 89 45 c8          	mov    %r8,-0x38(%rbp)
    708e:	48 89 55 b0          	mov    %rdx,-0x50(%rbp)
    7092:	e8 00 00 00 00       	callq  7097 <do_page_set_care+0x137>
					pte=find_pte(mm,address);
    7097:	4c 8b 45 c8          	mov    -0x38(%rbp),%r8
    709b:	48 89 c6             	mov    %rax,%rsi
    709e:	48 89 45 b8          	mov    %rax,-0x48(%rbp)
    70a2:	4c 89 c7             	mov    %r8,%rdi
    70a5:	e8 00 00 00 00       	callq  70aa <do_page_set_care+0x14a>
					if(pte)
    70aa:	48 85 c0             	test   %rax,%rax
		if(vma){
			if((mm=vma->vm_mm))
			  if((task=mm->owner))
				if(ns_of_pid(task_pid(task))==pid_ns_ref){
					address = vma_to_address(page, vma);
					pte=find_pte(mm,address);
    70ad:	48 89 c7             	mov    %rax,%rdi
					if(pte)
    70b0:	48 8b 4d c0          	mov    -0x40(%rbp),%rcx
    70b4:	0f 84 77 ff ff ff    	je     7031 <do_page_set_care+0xd1>
					  if(((pte->pte&flags)!=flags)){
    70ba:	48 8b 00             	mov    (%rax),%rax
    70bd:	4c 89 f6             	mov    %r14,%rsi
    70c0:	48 21 c6             	and    %rax,%rsi
    70c3:	49 39 f6             	cmp    %rsi,%r14
    70c6:	0f 84 65 ff ff ff    	je     7031 <do_page_set_care+0xd1>
						  ret++;
						  pte->pte|=flags;
						  flush_tlb_page(vma,address);
    70cc:	4c 8b 4d b8          	mov    -0x48(%rbp),%r9
    70d0:	48 8b 55 b0          	mov    -0x50(%rbp),%rdx
					address = vma_to_address(page, vma);
					pte=find_pte(mm,address);
					if(pte)
					  if(((pte->pte&flags)!=flags)){
						  ret++;
						  pte->pte|=flags;
    70d4:	4c 09 f0             	or     %r14,%rax
    70d7:	48 89 07             	mov    %rax,(%rdi)
    70da:	48 89 4d c8          	mov    %rcx,-0x38(%rbp)
				if(ns_of_pid(task_pid(task))==pid_ns_ref){
					address = vma_to_address(page, vma);
					pte=find_pte(mm,address);
					if(pte)
					  if(((pte->pte&flags)!=flags)){
						  ret++;
    70de:	83 45 d4 01          	addl   $0x1,-0x2c(%rbp)
						  pte->pte|=flags;
						  flush_tlb_page(vma,address);
    70e2:	4c 89 ce             	mov    %r9,%rsi
    70e5:	48 89 d7             	mov    %rdx,%rdi
    70e8:	e8 00 00 00 00       	callq  70ed <do_page_set_care+0x18d>
						  if(ret>= get_page_counter_in_ns(page,pid_ns_ref))
    70ed:	4c 89 e6             	mov    %r12,%rsi
    70f0:	4c 89 ef             	mov    %r13,%rdi
    70f3:	e8 00 00 00 00       	callq  70f8 <do_page_set_care+0x198>
    70f8:	39 45 d4             	cmp    %eax,-0x2c(%rbp)
    70fb:	48 8b 4d c8          	mov    -0x38(%rbp),%rcx
    70ff:	0f 8c 2c ff ff ff    	jl     7031 <do_page_set_care+0xd1>
    7105:	0f 1f 00             	nopl   (%rax)
						  //	  update_mmu_cache(vma, address,pte);
					  }
				}
		}
	}
unlock:	page_unlock_anon_vma_read(anon_vma);
    7108:	4c 89 ff             	mov    %r15,%rdi
    710b:	e8 00 00 00 00       	callq  7110 <do_page_set_care+0x1b0>
			//printk("mapcount==%d\n",page_mapcount(page));
			if(PageKsm(page)){
				return do_ksm_page_set(page,pid_ns,flags,count);
				//printk("map=%d",	do_ksm_page_clear(page,pid_ns,_PAGE_CACHE_PROTECT));
			}else if(PageAnon(page)){
				return do_anon_page_set_care(page,pid_ns,flags,count);
    7110:	8b 45 d4             	mov    -0x2c(%rbp),%eax
				//printk("map=%d",	do_file_page_clear(page,pid_ns,_PAGE_CACHE_PROTECT));
			}
			//}
		}
		return -1;
	}
    7113:	48 83 c4 28          	add    $0x28,%rsp
    7117:	5b                   	pop    %rbx
    7118:	41 5c                	pop    %r12
    711a:	41 5d                	pop    %r13
    711c:	41 5e                	pop    %r14
    711e:	41 5f                	pop    %r15
    7120:	5d                   	pop    %rbp
    7121:	c3                   	retq   
    7122:	66 0f 1f 44 00 00    	nopw   0x0(%rax,%rax,1)
	int do_page_set_care(struct page* page,struct pid_namespace* pid_ns, pteval_t flags,int count){
		if(page_mapped(page)&&page_rmapping(page)){
			//	if(page_mapcount(page)>0){
			//printk("mapcount==%d\n",page_mapcount(page));
			if(PageKsm(page)){
				return do_ksm_page_set(page,pid_ns,flags,count);
    7128:	4c 89 f2             	mov    %r14,%rdx
    712b:	e8 40 92 ff ff       	callq  370 <do_ksm_page_set.isra.56>
    7130:	e9 95 fe ff ff       	jmpq   6fca <do_page_set_care+0x6a>
				return do_file_page_set(page,pid_ns,flags,count);
				//printk("map=%d",	do_file_page_clear(page,pid_ns,_PAGE_CACHE_PROTECT));
			}
			//}
		}
		return -1;
    7135:	b8 ff ff ff ff       	mov    $0xffffffff,%eax
    713a:	e9 8b fe ff ff       	jmpq   6fca <do_page_set_care+0x6a>
    713f:	90                   	nop

0000000000007140 <do_page_set>:
	}
	int do_page_set(struct page* page,struct pid_namespace* pid_ns, pteval_t flags,int count){
    7140:	e8 00 00 00 00       	callq  7145 <do_page_set+0x5>
    7145:	55                   	push   %rbp
    7146:	48 89 e5             	mov    %rsp,%rbp
    7149:	53                   	push   %rbx
    714a:	48 89 fb             	mov    %rdi,%rbx
    714d:	48 83 ec 10          	sub    $0x10,%rsp
    7151:	8b 47 18             	mov    0x18(%rdi),%eax
		if(page_mapped(page)&&page_rmapping(page)){
    7154:	85 c0                	test   %eax,%eax
    7156:	78 68                	js     71c0 <do_page_set+0x80>
    7158:	48 8b 47 08          	mov    0x8(%rdi),%rax
    715c:	48 a9 fc ff ff ff    	test   $0xfffffffffffffffc,%rax
    7162:	74 5c                	je     71c0 <do_page_set+0x80>
    7164:	48 89 c1             	mov    %rax,%rcx
    7167:	83 e1 03             	and    $0x3,%ecx
			//	if(page_mapcount(page)>0){
			//printk("mapcount==%d\n",page_mapcount(page));
			if(PageKsm(page)){
    716a:	48 83 f9 03          	cmp    $0x3,%rcx
    716e:	74 40                	je     71b0 <do_page_set+0x70>
				return do_ksm_page_set(page,pid_ns,flags,count);
				//printk("map=%d",	do_ksm_page_clear(page,pid_ns,_PAGE_CACHE_PROTECT));
			}else if(PageAnon(page)){
    7170:	a8 01                	test   $0x1,%al
    7172:	75 2c                	jne    71a0 <do_page_set+0x60>
    7174:	48 89 55 e8          	mov    %rdx,-0x18(%rbp)
    7178:	48 89 75 f0          	mov    %rsi,-0x10(%rbp)
				return do_anon_page_set(page,pid_ns,flags,count);
				//printk("map=%d",	do_anon_page_clear(page,pid_ns,_PAGE_CACHE_PROTECT));
			}
			else if(page_mapping(page)){
    717c:	e8 00 00 00 00       	callq  7181 <do_page_set+0x41>
    7181:	48 85 c0             	test   %rax,%rax
    7184:	74 3a                	je     71c0 <do_page_set+0x80>
				return do_file_page_set(page,pid_ns,flags,count);
    7186:	48 8b 55 e8          	mov    -0x18(%rbp),%rdx
    718a:	48 8b 75 f0          	mov    -0x10(%rbp),%rsi
    718e:	48 89 df             	mov    %rbx,%rdi
    7191:	e8 6a 90 ff ff       	callq  200 <do_file_page_set.isra.55>
				//printk("map=%d",	do_file_page_clear(page,pid_ns,_PAGE_CACHE_PROTECT));
			}
			//}
		}
		return -1;
	}
    7196:	48 83 c4 10          	add    $0x10,%rsp
    719a:	5b                   	pop    %rbx
    719b:	5d                   	pop    %rbp
    719c:	c3                   	retq   
    719d:	0f 1f 00             	nopl   (%rax)
			//printk("mapcount==%d\n",page_mapcount(page));
			if(PageKsm(page)){
				return do_ksm_page_set(page,pid_ns,flags,count);
				//printk("map=%d",	do_ksm_page_clear(page,pid_ns,_PAGE_CACHE_PROTECT));
			}else if(PageAnon(page)){
				return do_anon_page_set(page,pid_ns,flags,count);
    71a0:	e8 ab 93 ff ff       	callq  550 <do_anon_page_set.isra.57>
				//printk("map=%d",	do_file_page_clear(page,pid_ns,_PAGE_CACHE_PROTECT));
			}
			//}
		}
		return -1;
	}
    71a5:	48 83 c4 10          	add    $0x10,%rsp
    71a9:	5b                   	pop    %rbx
    71aa:	5d                   	pop    %rbp
    71ab:	c3                   	retq   
    71ac:	0f 1f 40 00          	nopl   0x0(%rax)
	int do_page_set(struct page* page,struct pid_namespace* pid_ns, pteval_t flags,int count){
		if(page_mapped(page)&&page_rmapping(page)){
			//	if(page_mapcount(page)>0){
			//printk("mapcount==%d\n",page_mapcount(page));
			if(PageKsm(page)){
				return do_ksm_page_set(page,pid_ns,flags,count);
    71b0:	e8 bb 91 ff ff       	callq  370 <do_ksm_page_set.isra.56>
    71b5:	eb df                	jmp    7196 <do_page_set+0x56>
    71b7:	66 0f 1f 84 00 00 00 	nopw   0x0(%rax,%rax,1)
    71be:	00 00 
				return do_file_page_set(page,pid_ns,flags,count);
				//printk("map=%d",	do_file_page_clear(page,pid_ns,_PAGE_CACHE_PROTECT));
			}
			//}
		}
		return -1;
    71c0:	b8 ff ff ff ff       	mov    $0xffffffff,%eax
    71c5:	eb cf                	jmp    7196 <do_page_set+0x56>
    71c7:	66 0f 1f 84 00 00 00 	nopw   0x0(%rax,%rax,1)
    71ce:	00 00 

00000000000071d0 <_try_switch_NCache.isra.69>:
static void fault_queue_work(struct work_struct * work){
	struct fault_entry* fault_entry=container_of(work,struct fault_entry,work);
	try_switch_NCache(fault_entry->mm,fault_entry->vma,fault_entry->address,fault_entry->pfn,fault_entry->orig_pte,true);
	kmem_cache_free(sclock_fault_entry_cache,fault_entry);
}
static int _try_switch_NCache(struct mm_struct *mm, struct vm_area_struct *vma,unsigned long address, pte_t *page_table, pmd_t *pmd, spinlock_t *ptl, pte_t orig_pte,bool invalid){
    71d0:	e8 00 00 00 00       	callq  71d5 <_try_switch_NCache.isra.69+0x5>
    71d5:	55                   	push   %rbp
    71d6:	48 89 e5             	mov    %rsp,%rbp
    71d9:	41 57                	push   %r15
    71db:	49 89 ff             	mov    %rdi,%r15
    71de:	41 56                	push   %r14
    71e0:	49 89 f6             	mov    %rsi,%r14
    71e3:	48 89 d6             	mov    %rdx,%rsi
    71e6:	41 55                	push   %r13
    71e8:	4d 89 cd             	mov    %r9,%r13
    71eb:	41 54                	push   %r12
    71ed:	49 89 cc             	mov    %rcx,%r12
    71f0:	53                   	push   %rbx
    71f1:	48 83 ec 58          	sub    $0x58,%rsp
    71f5:	48 89 55 d0          	mov    %rdx,-0x30(%rbp)
    71f9:	48 8b 55 10          	mov    0x10(%rbp),%rdx
    71fd:	4c 89 45 c0          	mov    %r8,-0x40(%rbp)

	if (sizeof(pteval_t) > sizeof(long))
		ret = PVOP_CALLEE2(pteval_t, pv_mmu_ops.pte_val,
				   pte.pte, (u64)pte.pte >> 32);
	else
		ret = PVOP_CALLEE1(pteval_t, pv_mmu_ops.pte_val,
    7201:	48 89 d7             	mov    %rdx,%rdi
    7204:	ff 14 25 00 00 00 00 	callq  *0x0
    720b:	48 89 c3             	mov    %rax,%rbx
	return pte_flags(pte) & _PAGE_SPECIAL;
}

static inline unsigned long pte_pfn(pte_t pte)
{
	return (pte_val(pte) & PTE_PFN_MASK) >> PAGE_SHIFT;
    720e:	48 c1 e3 12          	shl    $0x12,%rbx
    7212:	48 c1 eb 1e          	shr    $0x1e,%rbx
		goto setearly;
	}
	if(mm->def_flags&VM_CACHE_PROTECT==0){
		goto setearly;
	}
	if(mm->owner==NULL)
    7216:	49 83 3f 00          	cmpq   $0x0,(%r15)
    721a:	0f 84 98 01 00 00    	je     73b8 <_try_switch_NCache.isra.69+0x1e8>
	  goto setearly;
	page= vm_normal_pfn_to_page(vma, address, orig_pte,pfn);
    7220:	48 89 d9             	mov    %rbx,%rcx
    7223:	4c 89 f7             	mov    %r14,%rdi
    7226:	48 89 75 d0          	mov    %rsi,-0x30(%rbp)
    722a:	e8 00 00 00 00       	callq  722f <_try_switch_NCache.isra.69+0x5f>
	if (!page||!page_mapped(page)) {
    722f:	48 85 c0             	test   %rax,%rax
	if(mm->def_flags&VM_CACHE_PROTECT==0){
		goto setearly;
	}
	if(mm->owner==NULL)
	  goto setearly;
	page= vm_normal_pfn_to_page(vma, address, orig_pte,pfn);
    7232:	49 89 c2             	mov    %rax,%r10
	if (!page||!page_mapped(page)) {
    7235:	0f 84 7d 01 00 00    	je     73b8 <_try_switch_NCache.isra.69+0x1e8>
    723b:	8b 40 18             	mov    0x18(%rax),%eax
    723e:	4c 89 55 b8          	mov    %r10,-0x48(%rbp)
    7242:	85 c0                	test   %eax,%eax
    7244:	0f 88 6e 01 00 00    	js     73b8 <_try_switch_NCache.isra.69+0x1e8>
		goto setearly;
	}
//	prefetch_range(kmap(page),PAGE_SIZE);
//	kunmap(page);
	page_table->pte&=~_PAGE_CACHE_PROTECT;
    724a:	48 b8 ef ff ff ff ff 	movabs $0xfffbffffffffffef,%rax
    7251:	ff fb ff 
    7254:	49 21 04 24          	and    %rax,(%r12)
	flush_tlb_page(vma,address);
    7258:	48 8b 75 d0          	mov    -0x30(%rbp),%rsi
    725c:	4c 89 f7             	mov    %r14,%rdi
    725f:	e8 00 00 00 00       	callq  7264 <_try_switch_NCache.isra.69+0x94>
    7264:	4c 89 ef             	mov    %r13,%rdi
    7267:	e8 00 00 00 00       	callq  726c <_try_switch_NCache.isra.69+0x9c>
	update_mmu_cache(vma, address, page_table);
	pte_unmap_unlock(page_table,ptl);
	set_number=pfn&(NPageColor-1);
    726c:	8b 05 00 00 00 00    	mov    0x0(%rip),%eax        # 7272 <_try_switch_NCache.isra.69+0xa2>
	if (pid)
    7272:	4c 8b 55 b8          	mov    -0x48(%rbp),%r10
    7276:	83 e8 01             	sub    $0x1,%eax
    7279:	89 45 cc             	mov    %eax,-0x34(%rbp)
    727c:	49 8b 07             	mov    (%r15),%rax
    727f:	21 5d cc             	and    %ebx,-0x34(%rbp)
    7282:	48 8b 80 60 03 00 00 	mov    0x360(%rax),%rax
    7289:	48 85 c0             	test   %rax,%rax
    728c:	0f 84 2e 04 00 00    	je     76c0 <_try_switch_NCache.isra.69+0x4f0>
		ns = pid->numbers[pid->level].ns;
    7292:	8b 50 04             	mov    0x4(%rax),%edx
    7295:	48 c1 e2 05          	shl    $0x5,%rdx
    7299:	4c 8b 6c 10 38       	mov    0x38(%rax,%rdx,1),%r13
	pid_ns=ns_of_pid(task_pid(mm->owner));
	if(pid_ns->sclock_lru==NULL){
    729e:	49 8b 95 90 08 00 00 	mov    0x890(%r13),%rdx
    72a5:	48 85 d2             	test   %rdx,%rdx
    72a8:	0f 84 1a 04 00 00    	je     76c8 <_try_switch_NCache.isra.69+0x4f8>
		initPidNsProtection(pid_ns);
	}
	counter=&(pid_ns->sclock_lru_counter[set_number]);
    72ae:	48 63 4d cc          	movslq -0x34(%rbp),%rcx
    72b2:	49 8b 85 b0 08 00 00 	mov    0x8b0(%r13),%rax
    72b9:	4c 89 55 a0          	mov    %r10,-0x60(%rbp)
    72bd:	48 89 45 98          	mov    %rax,-0x68(%rbp)
	lru_head=&(pid_ns->sclock_lru[set_number]);
    72c1:	48 89 c8             	mov    %rcx,%rax
	//	spin_lock(&(pid_ns->sclock_lock[set_number]));
	spin_lock(&(pid_ns->sclock_lock[set_number]));
    72c4:	48 89 4d b0          	mov    %rcx,-0x50(%rbp)
	pid_ns=ns_of_pid(task_pid(mm->owner));
	if(pid_ns->sclock_lru==NULL){
		initPidNsProtection(pid_ns);
	}
	counter=&(pid_ns->sclock_lru_counter[set_number]);
	lru_head=&(pid_ns->sclock_lru[set_number]);
    72c8:	48 c1 e0 04          	shl    $0x4,%rax
    72cc:	48 01 c2             	add    %rax,%rdx
    72cf:	48 89 45 90          	mov    %rax,-0x70(%rbp)
	//	spin_lock(&(pid_ns->sclock_lock[set_number]));
	spin_lock(&(pid_ns->sclock_lock[set_number]));
    72d3:	48 8d 04 49          	lea    (%rcx,%rcx,2),%rax
	pid_ns=ns_of_pid(task_pid(mm->owner));
	if(pid_ns->sclock_lru==NULL){
		initPidNsProtection(pid_ns);
	}
	counter=&(pid_ns->sclock_lru_counter[set_number]);
	lru_head=&(pid_ns->sclock_lru[set_number]);
    72d7:	48 89 55 a8          	mov    %rdx,-0x58(%rbp)
	//	spin_lock(&(pid_ns->sclock_lock[set_number]));
	spin_lock(&(pid_ns->sclock_lock[set_number]));
    72db:	48 c1 e0 03          	shl    $0x3,%rax
    72df:	48 89 c7             	mov    %rax,%rdi
    72e2:	49 03 bd c8 08 00 00 	add    0x8c8(%r13),%rdi
    72e9:	48 89 45 b8          	mov    %rax,-0x48(%rbp)
	raw_spin_lock_init(&(_lock)->rlock);		\
} while (0)

static inline void spin_lock(spinlock_t *lock)
{
	raw_spin_lock(&lock->rlock);
    72ed:	e8 00 00 00 00       	callq  72f2 <_try_switch_NCache.isra.69+0x122>
}

static __always_inline int constant_test_bit(long nr, const volatile unsigned long *addr)
{
	return ((1UL << (nr & (BITS_PER_LONG-1))) &
		(addr[nr >> _BITOPS_LONG_SHIFT])) != 0;
    72f2:	4c 8b 55 a0          	mov    -0x60(%rbp),%r10
	if(test_bit(PG_cacheable,&page->flags)){
    72f6:	48 8b 4d b0          	mov    -0x50(%rbp),%rcx
    72fa:	48 8b 55 a8          	mov    -0x58(%rbp),%rdx
    72fe:	49 8b 02             	mov    (%r10),%rax
    7301:	a9 00 00 00 08       	test   $0x8000000,%eax
    7306:	0f 84 e4 00 00 00    	je     73f0 <_try_switch_NCache.isra.69+0x220>
		list_for_each_entry(sclock_entry,lru_head,sclock_lru){
    730c:	48 8b 02             	mov    (%rdx),%rax
    730f:	48 39 c2             	cmp    %rax,%rdx
    7312:	75 18                	jne    732c <_try_switch_NCache.isra.69+0x15c>
    7314:	e9 d7 00 00 00       	jmpq   73f0 <_try_switch_NCache.isra.69+0x220>
    7319:	0f 1f 80 00 00 00 00 	nopl   0x0(%rax)
    7320:	48 8b 00             	mov    (%rax),%rax
    7323:	48 39 c2             	cmp    %rax,%rdx
    7326:	0f 84 c4 00 00 00    	je     73f0 <_try_switch_NCache.isra.69+0x220>
			if(sclock_entry->pfn==pfn){
    732c:	48 3b 58 10          	cmp    0x10(%rax),%rbx
    7330:	75 ee                	jne    7320 <_try_switch_NCache.isra.69+0x150>
				spin_unlock(&(pid_ns->sclock_lock[set_number]));
    7332:	48 8b 7d b8          	mov    -0x48(%rbp),%rdi
    7336:	49 03 bd c8 08 00 00 	add    0x8c8(%r13),%rdi
	raw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \
} while (0)

static inline void spin_unlock(spinlock_t *lock)
{
	raw_spin_unlock(&lock->rlock);
    733d:	e8 00 00 00 00       	callq  7342 <_try_switch_NCache.isra.69+0x172>
				if(sclock_control->debug==1){
    7342:	48 8b 05 00 00 00 00 	mov    0x0(%rip),%rax        # 7349 <_try_switch_NCache.isra.69+0x179>
    7349:	83 78 20 01          	cmpl   $0x1,0x20(%rax)
    734d:	0f 84 99 03 00 00    	je     76ec <_try_switch_NCache.isra.69+0x51c>

	if (sizeof(pmdval_t) > sizeof(long))
		ret =  PVOP_CALLEE2(pmdval_t, pv_mmu_ops.pmd_val,
				    pmd.pmd, (u64)pmd.pmd >> 32);
	else
		ret =  PVOP_CALLEE1(pmdval_t, pv_mmu_ops.pmd_val,
    7353:	48 8b 75 c0          	mov    -0x40(%rbp),%rsi
    7357:	48 8b 3e             	mov    (%rsi),%rdi
    735a:	ff 14 25 00 00 00 00 	callq  *0x0
}
#endif /* ALLOC_SPLIT_PTLOCKS */

static inline spinlock_t *pte_lockptr(struct mm_struct *mm, pmd_t *pmd)
{
	return ptlock_ptr(pmd_page(*pmd));
    7361:	48 b9 00 f0 ff ff ff 	movabs $0x3ffffffff000,%rcx
    7368:	3f 00 00 
    736b:	48 ba 00 00 00 00 00 	movabs $0xffffea0000000000,%rdx
    7372:	ea ff ff 
    7375:	48 8b 3e             	mov    (%rsi),%rdi
    7378:	48 21 c8             	and    %rcx,%rax
    737b:	48 c1 e8 06          	shr    $0x6,%rax
    737f:	4c 8b 6c 10 30       	mov    0x30(%rax,%rdx,1),%r13
    7384:	ff 14 25 00 00 00 00 	callq  *0x0
 * this function returns the index of the entry in the pte page which would
 * control the given virtual address
 */
static inline unsigned long pte_index(unsigned long address)
{
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
    738b:	48 8b 55 d0          	mov    -0x30(%rbp),%rdx
	return (unsigned long)native_pmd_val(pmd) == 0;
}

static inline unsigned long pmd_page_vaddr(pmd_t pmd)
{
	return (unsigned long)__va(pmd_val(pmd) & PTE_PFN_MASK);
    738f:	49 bc 00 00 00 00 00 	movabs $0xffff880000000000,%r12
    7396:	88 ff ff 
    7399:	48 21 c8             	and    %rcx,%rax
	raw_spin_lock_init(&(_lock)->rlock);		\
} while (0)

static inline void spin_lock(spinlock_t *lock)
{
	raw_spin_lock(&lock->rlock);
    739c:	4c 89 ef             	mov    %r13,%rdi
 * this function returns the index of the entry in the pte page which would
 * control the given virtual address
 */
static inline unsigned long pte_index(unsigned long address)
{
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
    739f:	48 c1 ea 09          	shr    $0x9,%rdx
}

static inline pte_t *pte_offset_kernel(pmd_t *pmd, unsigned long address)
{
	return (pte_t *)pmd_page_vaddr(*pmd) + pte_index(address);
    73a3:	81 e2 f8 0f 00 00    	and    $0xff8,%edx
	return (unsigned long)native_pmd_val(pmd) == 0;
}

static inline unsigned long pmd_page_vaddr(pmd_t pmd)
{
	return (unsigned long)__va(pmd_val(pmd) & PTE_PFN_MASK);
    73a9:	4c 01 e2             	add    %r12,%rdx
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
}

static inline pte_t *pte_offset_kernel(pmd_t *pmd, unsigned long address)
{
	return (pte_t *)pmd_page_vaddr(*pmd) + pte_index(address);
    73ac:	4c 8d 24 02          	lea    (%rdx,%rax,1),%r12
    73b0:	e8 00 00 00 00       	callq  73b5 <_try_switch_NCache.isra.69+0x1e5>
    73b5:	0f 1f 00             	nopl   (%rax)
normal_out:
	//	__get_cpu_var(global_count_normal)++;
	//__get_cpu_var(global_interval_normal)+=(get_rdtsc()-time1);
	return ret;
setearly:
	page_table->pte&=~_PAGE_CACHE_PROTECT;	
    73b8:	48 b8 ef ff ff ff ff 	movabs $0xfffbffffffffffef,%rax
    73bf:	ff fb ff 
    73c2:	49 21 04 24          	and    %rax,(%r12)
	flush_tlb_page(vma,address);
    73c6:	48 8b 75 d0          	mov    -0x30(%rbp),%rsi
    73ca:	4c 89 f7             	mov    %r14,%rdi
    73cd:	e8 00 00 00 00       	callq  73d2 <_try_switch_NCache.isra.69+0x202>
	raw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \
} while (0)

static inline void spin_unlock(spinlock_t *lock)
{
	raw_spin_unlock(&lock->rlock);
    73d2:	4c 89 ef             	mov    %r13,%rdi
    73d5:	e8 00 00 00 00       	callq  73da <_try_switch_NCache.isra.69+0x20a>
	pte_unmap_unlock(page_table, ptl);
out:
	//__get_cpu_var(global_count_early)++;   
	//__get_cpu_var(global_interval_early)+=(get_rdtsc()-time1);
	return ret;
}
    73da:	48 83 c4 58          	add    $0x58,%rsp
    73de:	31 c0                	xor    %eax,%eax
    73e0:	5b                   	pop    %rbx
    73e1:	41 5c                	pop    %r12
    73e3:	41 5d                	pop    %r13
    73e5:	41 5e                	pop    %r14
    73e7:	41 5f                	pop    %r15
    73e9:	5d                   	pop    %rbp
    73ea:	c3                   	retq   
    73eb:	0f 1f 44 00 00       	nopl   0x0(%rax,%rax,1)
	set_number=pfn&(NPageColor-1);
	pid_ns=ns_of_pid(task_pid(mm->owner));
	if(pid_ns->sclock_lru==NULL){
		initPidNsProtection(pid_ns);
	}
	counter=&(pid_ns->sclock_lru_counter[set_number]);
    73f0:	48 8d 04 8d 00 00 00 	lea    0x0(,%rcx,4),%rax
    73f7:	00 
    73f8:	48 8b 4d 98          	mov    -0x68(%rbp),%rcx
				page_table=pte_offset_map_lock(mm, pmd, address, &ptl);  
				goto setearly;
			}
		}
	}
	if(atomic_read(counter)<get_k_of_ns(pid_ns)){
    73fc:	4c 89 ef             	mov    %r13,%rdi
    73ff:	48 89 55 a8          	mov    %rdx,-0x58(%rbp)
    7403:	4c 89 55 b0          	mov    %r10,-0x50(%rbp)
	set_number=pfn&(NPageColor-1);
	pid_ns=ns_of_pid(task_pid(mm->owner));
	if(pid_ns->sclock_lru==NULL){
		initPidNsProtection(pid_ns);
	}
	counter=&(pid_ns->sclock_lru_counter[set_number]);
    7407:	48 89 45 c0          	mov    %rax,-0x40(%rbp)
    740b:	48 01 c1             	add    %rax,%rcx
    740e:	44 8b 31             	mov    (%rcx),%r14d
				page_table=pte_offset_map_lock(mm, pmd, address, &ptl);  
				goto setearly;
			}
		}
	}
	if(atomic_read(counter)<get_k_of_ns(pid_ns)){
    7411:	e8 00 00 00 00       	callq  7416 <_try_switch_NCache.isra.69+0x246>
    7416:	4c 8b 55 b0          	mov    -0x50(%rbp),%r10
    741a:	48 8b 55 a8          	mov    -0x58(%rbp),%rdx
    741e:	41 39 c6             	cmp    %eax,%r14d
    7421:	0f 8c f9 01 00 00    	jl     7620 <_try_switch_NCache.isra.69+0x450>
		findNToDel=0;
		addNewLRU=true;
		spin_unlock(&(pid_ns->sclock_lock[set_number]));
		goto early_add;
	}
	del_candidate=lru_head->next;
    7427:	4c 8b 32             	mov    (%rdx),%r14
	findNToDel=1;
find_del:
	sclock_entry=list_entry(del_candidate,struct sclock_LRU,sclock_lru);
	//if(pfn_valid(sclock_entry->pfn)){
	page_one=pfn_to_page(sclock_entry->pfn);
    742a:	49 bb 00 00 00 00 00 	movabs $0xffffea0000000000,%r11
    7431:	ea ff ff 
	//}else{
	//	page_one=NULL;
	//	}
	list_del_init(&sclock_entry->sclock_lru);

	spin_unlock(&(pid_ns->sclock_lock[set_number]));
    7434:	48 8b 7d b8          	mov    -0x48(%rbp),%rdi
    7438:	4c 89 55 a8          	mov    %r10,-0x58(%rbp)
	del_candidate=lru_head->next;
	findNToDel=1;
find_del:
	sclock_entry=list_entry(del_candidate,struct sclock_LRU,sclock_lru);
	//if(pfn_valid(sclock_entry->pfn)){
	page_one=pfn_to_page(sclock_entry->pfn);
    743c:	49 8b 46 10          	mov    0x10(%r14),%rax
 * in an undefined state.
 */
#ifndef CONFIG_DEBUG_LIST
static inline void __list_del_entry(struct list_head *entry)
{
	__list_del(entry->prev, entry->next);
    7440:	49 8b 16             	mov    (%r14),%rdx
    7443:	48 c1 e0 06          	shl    $0x6,%rax
    7447:	49 01 c3             	add    %rax,%r11
    744a:	49 8b 46 08          	mov    0x8(%r14),%rax
    744e:	4c 89 5d b0          	mov    %r11,-0x50(%rbp)
 * This is only for internal list manipulation where we know
 * the prev/next entries already!
 */
static inline void __list_del(struct list_head * prev, struct list_head * next)
{
	next->prev = prev;
    7452:	48 89 42 08          	mov    %rax,0x8(%rdx)
	prev->next = next;
    7456:	48 89 10             	mov    %rdx,(%rax)
#define LIST_HEAD(name) \
	struct list_head name = LIST_HEAD_INIT(name)

static inline void INIT_LIST_HEAD(struct list_head *list)
{
	list->next = list;
    7459:	4d 89 36             	mov    %r14,(%r14)
	list->prev = list;
    745c:	4d 89 76 08          	mov    %r14,0x8(%r14)
	//}else{
	//	page_one=NULL;
	//	}
	list_del_init(&sclock_entry->sclock_lru);

	spin_unlock(&(pid_ns->sclock_lock[set_number]));
    7460:	49 03 bd c8 08 00 00 	add    0x8c8(%r13),%rdi
    7467:	e8 00 00 00 00       	callq  746c <_try_switch_NCache.isra.69+0x29c>
		if(sclock_control->debug==1){
    746c:	48 8b 05 00 00 00 00 	mov    0x0(%rip),%rax        # 7473 <_try_switch_NCache.isra.69+0x2a3>
    7473:	4c 8b 5d b0          	mov    -0x50(%rbp),%r11
    7477:	4c 8b 55 a8          	mov    -0x58(%rbp),%r10
    747b:	83 78 20 01          	cmpl   $0x1,0x20(%rax)
    747f:	0f 84 95 02 00 00    	je     771a <_try_switch_NCache.isra.69+0x54a>
		printk("[replace]page %lx replace page %lx in queue in set %d for ns=%lx, address=%lx,task=%s,pte=%lx\n",pfn,sclock_entry->pfn,set_number,pid_ns,address,mm->owner->comm,page_table);
		}

		if(page_one)
    7485:	4d 85 db             	test   %r11,%r11
    7488:	0f 84 da 00 00 00    	je     7568 <_try_switch_NCache.isra.69+0x398>
    748e:	41 8b 43 18          	mov    0x18(%r11),%eax
		  if(page_one&&page_mapped(page_one)&& page_mapping(page_one)){
    7492:	85 c0                	test   %eax,%eax
    7494:	0f 88 ce 00 00 00    	js     7568 <_try_switch_NCache.isra.69+0x398>
    749a:	4c 89 df             	mov    %r11,%rdi
    749d:	4c 89 55 b0          	mov    %r10,-0x50(%rbp)
    74a1:	4c 89 5d a8          	mov    %r11,-0x58(%rbp)
    74a5:	e8 00 00 00 00       	callq  74aa <_try_switch_NCache.isra.69+0x2da>
    74aa:	48 85 c0             	test   %rax,%rax
    74ad:	4c 8b 55 b0          	mov    -0x50(%rbp),%r10
    74b1:	0f 84 b1 00 00 00    	je     7568 <_try_switch_NCache.isra.69+0x398>
 * This operation is atomic and cannot be reordered.
 * It also implies a memory barrier.
 */
static inline int test_and_set_bit(long nr, volatile unsigned long *addr)
{
	GEN_BINARY_RMWcc(LOCK_PREFIX "bts", *addr, "Ir", nr, "%0", "c");
    74b7:	4c 8b 5d a8          	mov    -0x58(%rbp),%r11
    74bb:	f0 41 0f ba 2b 00    	lock btsl $0x0,(%r11)
    74c1:	0f 82 1d 02 00 00    	jb     76e4 <_try_switch_NCache.isra.69+0x514>
			  int welock=true;
    74c7:	41 b8 01 00 00 00    	mov    $0x1,%r8d
    74cd:	41 8b 43 18          	mov    0x18(%r11),%eax
			  if(!trylock_page(page_one))
			  {
				  welock=false;  //	  atomic_inc(&(pid_ns->sclock_lru_counter[set_number]));
				  //lock_page(page_one);
			  }
			  if(page_mapped(page_one)&& page_mapping(page_one))
    74d1:	85 c0                	test   %eax,%eax
    74d3:	78 7a                	js     754f <_try_switch_NCache.isra.69+0x37f>
    74d5:	4c 89 df             	mov    %r11,%rdi
    74d8:	44 89 45 a0          	mov    %r8d,-0x60(%rbp)
    74dc:	4c 89 55 a8          	mov    %r10,-0x58(%rbp)
    74e0:	4c 89 5d b0          	mov    %r11,-0x50(%rbp)
    74e4:	e8 00 00 00 00       	callq  74e9 <_try_switch_NCache.isra.69+0x319>
    74e9:	48 85 c0             	test   %rax,%rax
    74ec:	4c 8b 5d b0          	mov    -0x50(%rbp),%r11
    74f0:	4c 8b 55 a8          	mov    -0x58(%rbp),%r10
    74f4:	44 8b 45 a0          	mov    -0x60(%rbp),%r8d
    74f8:	74 55                	je     754f <_try_switch_NCache.isra.69+0x37f>
}

static __always_inline int constant_test_bit(long nr, const volatile unsigned long *addr)
{
	return ((1UL << (nr & (BITS_PER_LONG-1))) &
		(addr[nr >> _BITOPS_LONG_SHIFT])) != 0;
    74fa:	49 8b 03             	mov    (%r11),%rax
void get_huge_page_tail(struct page *page);/*ziqiao*/
extern bool __get_page_tail(struct page *page);

static inline void get_page(struct page *page)
{
	if (unlikely(PageTail(page)))
    74fd:	f6 c4 80             	test   $0x80,%ah
    7500:	0f 85 5a 02 00 00    	jne    7760 <_try_switch_NCache.isra.69+0x590>
 *
 * Atomically increments @v by 1.
 */
static inline void atomic_inc(atomic_t *v)
{
	asm volatile(LOCK_PREFIX "incl %0"
    7506:	f0 41 ff 43 1c       	lock incl 0x1c(%r11)
    750b:	44 89 45 a0          	mov    %r8d,-0x60(%rbp)
    750f:	4c 89 55 a8          	mov    %r10,-0x58(%rbp)
 */
static __always_inline void
clear_bit(long nr, volatile unsigned long *addr)
{
	if (IS_IMMEDIATE(nr)) {
		asm volatile(LOCK_PREFIX "andb %1,%0"
    7513:	f0 41 80 63 03 f7    	lock andb $0xf7,0x3(%r11)
			  {
				  page_cache_get(page_one);
				  clear_bit(PG_cacheable,&page_one->flags);
				  if(do_page_set(page_one,pid_ns,_PAGE_CACHE_PROTECT,1000)){
    7519:	b9 e8 03 00 00       	mov    $0x3e8,%ecx
    751e:	4c 89 df             	mov    %r11,%rdi
    7521:	48 ba 10 00 00 00 00 	movabs $0x4000000000010,%rdx
    7528:	00 04 00 
    752b:	4c 89 ee             	mov    %r13,%rsi
    752e:	4c 89 5d b0          	mov    %r11,-0x50(%rbp)
    7532:	e8 00 00 00 00       	callq  7537 <_try_switch_NCache.isra.69+0x367>
							  clflush_one(kmap(page_one),address&PAGE_SIZE);
							  kunmap(page_one);
							  }
							  }*/
				  }
				  page_cache_release(page_one);
    7537:	4c 8b 5d b0          	mov    -0x50(%rbp),%r11
    753b:	4c 89 df             	mov    %r11,%rdi
    753e:	e8 00 00 00 00       	callq  7543 <_try_switch_NCache.isra.69+0x373>
    7543:	44 8b 45 a0          	mov    -0x60(%rbp),%r8d
    7547:	4c 8b 55 a8          	mov    -0x58(%rbp),%r10
    754b:	4c 8b 5d b0          	mov    -0x50(%rbp),%r11
				  }
				  if(welock)
    754f:	45 85 c0             	test   %r8d,%r8d
    7552:	74 14                	je     7568 <_try_switch_NCache.isra.69+0x398>
					unlock_page(page_one);
    7554:	4c 89 df             	mov    %r11,%rdi
    7557:	4c 89 55 b0          	mov    %r10,-0x50(%rbp)
    755b:	e8 00 00 00 00       	callq  7560 <_try_switch_NCache.isra.69+0x390>
    7560:	4c 8b 55 b0          	mov    -0x50(%rbp),%r10
    7564:	0f 1f 40 00          	nopl   0x0(%rax)
				  //	}
		//	queue_work(para->workqueue,&fault_entry->work);
	}
	atomic_dec(&(pid_ns->sclock_lru_counter[set_number]));
    7568:	48 8b 45 c0          	mov    -0x40(%rbp),%rax
    756c:	49 03 85 b0 08 00 00 	add    0x8b0(%r13),%rax
 *
 * Atomically decrements @v by 1.
 */
static inline void atomic_dec(atomic_t *v)
{
	asm volatile(LOCK_PREFIX "decl %0"
    7573:	f0 ff 08             	lock decl (%rax)
 */
static __always_inline void
set_bit(long nr, volatile unsigned long *addr)
{
	if (IS_IMMEDIATE(nr)) {
		asm volatile(LOCK_PREFIX "orb %1,%0"
    7576:	f0 41 80 4a 03 08    	lock orb $0x8,0x3(%r10)
#define LIST_HEAD(name) \
	struct list_head name = LIST_HEAD_INIT(name)

static inline void INIT_LIST_HEAD(struct list_head *list)
{
	list->next = list;
    757c:	4d 89 36             	mov    %r14,(%r14)
	list->prev = list;
    757f:	4d 89 76 08          	mov    %r14,0x8(%r14)
 *
 * Atomically sets the value of @v to @i.
 */
static inline void atomic_set(atomic_t *v, int i)
{
	v->counter = i;
    7583:	41 c7 46 18 01 00 00 	movl   $0x1,0x18(%r14)
    758a:	00 
	//	sclock_entry->sclock_lru.prev=&(sclock_entry->sclock_lru);
	//	ptep_clear_flush(vma,address,pte_table);
	atomic_set(&sclock_entry->access_times,1);
//	INIT_LIST_HEAD(&sclock_entry->pte_map);
//	atomic_set(&sclock_entry->pte_count,0);
	sclock_entry->pfn=pfn;
    758b:	49 89 5e 10          	mov    %rbx,0x10(%r14)
	}
	atomic_dec(&(pid_ns->sclock_lru_counter[set_number]));
	set_bit(PG_cacheable,&page->flags);
	mkOneScolor(sclock_entry,pfn);
		//list_pte_map_add(pte_map,sclock_entry);//reverse
	spin_lock(&(pid_ns->sclock_lock[set_number]));
    758f:	48 8b 7d b8          	mov    -0x48(%rbp),%rdi
    7593:	49 03 bd c8 08 00 00 	add    0x8c8(%r13),%rdi
	raw_spin_lock_init(&(_lock)->rlock);		\
} while (0)

static inline void spin_lock(spinlock_t *lock)
{
	raw_spin_lock(&lock->rlock);
    759a:	e8 00 00 00 00       	callq  759f <_try_switch_NCache.isra.69+0x3cf>
	list_add_tail(&sclock_entry->sclock_lru,&(pid_ns->sclock_lru[set_number]));
    759f:	48 8b 4d 90          	mov    -0x70(%rbp),%rcx
    75a3:	49 03 8d 90 08 00 00 	add    0x890(%r13),%rcx
	atomic_inc(&(pid_ns->sclock_lru_counter[set_number]));
    75aa:	48 8b 45 c0          	mov    -0x40(%rbp),%rax
 * Insert a new entry before the specified head.
 * This is useful for implementing queues.
 */
static inline void list_add_tail(struct list_head *new, struct list_head *head)
{
	__list_add(new, head->prev, head);
    75ae:	48 8b 51 08          	mov    0x8(%rcx),%rdx
#ifndef CONFIG_DEBUG_LIST
static inline void __list_add(struct list_head *new,
			      struct list_head *prev,
			      struct list_head *next)
{
	next->prev = new;
    75b2:	4c 89 71 08          	mov    %r14,0x8(%rcx)
	new->next = next;
    75b6:	49 89 0e             	mov    %rcx,(%r14)
	new->prev = prev;
    75b9:	49 89 56 08          	mov    %rdx,0x8(%r14)
	prev->next = new;
    75bd:	4c 89 32             	mov    %r14,(%rdx)
    75c0:	49 03 85 b0 08 00 00 	add    0x8b0(%r13),%rax
 *
 * Atomically increments @v by 1.
 */
static inline void atomic_inc(atomic_t *v)
{
	asm volatile(LOCK_PREFIX "incl %0"
    75c7:	f0 ff 00             	lock incl (%rax)
	spin_unlock(&(pid_ns->sclock_lock[set_number]));
    75ca:	48 8b 7d b8          	mov    -0x48(%rbp),%rdi
    75ce:	49 03 bd c8 08 00 00 	add    0x8c8(%r13),%rdi
	raw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \
} while (0)

static inline void spin_unlock(spinlock_t *lock)
{
	raw_spin_unlock(&lock->rlock);
    75d5:	e8 00 00 00 00       	callq  75da <_try_switch_NCache.isra.69+0x40a>
	if(sclock_control->debug==1){
    75da:	48 8b 05 00 00 00 00 	mov    0x0(%rip),%rax        # 75e1 <_try_switch_NCache.isra.69+0x411>
    75e1:	83 78 20 01          	cmpl   $0x1,0x20(%rax)
    75e5:	0f 85 ef fd ff ff    	jne    73da <_try_switch_NCache.isra.69+0x20a>
		printk("[add]page %lx enqueued in set %d for ns=%lx,address=%lx,task=%s,pte=%lx\n",pfn,set_number,pid_ns,address,mm->owner->comm,page_table);
    75eb:	49 8b 07             	mov    (%r15),%rax
    75ee:	4c 8b 45 d0          	mov    -0x30(%rbp),%r8
    75f2:	4c 89 e9             	mov    %r13,%rcx
    75f5:	8b 55 cc             	mov    -0x34(%rbp),%edx
    75f8:	4c 89 24 24          	mov    %r12,(%rsp)
    75fc:	48 89 de             	mov    %rbx,%rsi
    75ff:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
    7606:	4c 8d 88 d0 04 00 00 	lea    0x4d0(%rax),%r9
    760d:	31 c0                	xor    %eax,%eax
    760f:	e8 00 00 00 00       	callq  7614 <_try_switch_NCache.isra.69+0x444>
    7614:	e9 c1 fd ff ff       	jmpq   73da <_try_switch_NCache.isra.69+0x20a>
    7619:	0f 1f 80 00 00 00 00 	nopl   0x0(%rax)
		}
	}
	if(atomic_read(counter)<get_k_of_ns(pid_ns)){
		findNToDel=0;
		addNewLRU=true;
		spin_unlock(&(pid_ns->sclock_lock[set_number]));
    7620:	48 8b 7d b8          	mov    -0x48(%rbp),%rdi
    7624:	49 03 bd c8 08 00 00 	add    0x8c8(%r13),%rdi
    762b:	e8 00 00 00 00       	callq  7630 <_try_switch_NCache.isra.69+0x460>

early_add:
	//flush_tlb_page(vma,address);
	//	pte_map->ptep=page_table;//reverse
	//	pte_unmap_unlock(page_table,ptl);
	sclock_entry=kmem_cache_alloc(sclock_entry_cache,GFP_KERNEL);
    7630:	48 8b 3d 00 00 00 00 	mov    0x0(%rip),%rdi        # 7637 <_try_switch_NCache.isra.69+0x467>
    7637:	be d0 00 00 00       	mov    $0xd0,%esi
    763c:	e8 00 00 00 00       	callq  7641 <_try_switch_NCache.isra.69+0x471>
	if(!sclock_entry){
    7641:	48 85 c0             	test   %rax,%rax

early_add:
	//flush_tlb_page(vma,address);
	//	pte_map->ptep=page_table;//reverse
	//	pte_unmap_unlock(page_table,ptl);
	sclock_entry=kmem_cache_alloc(sclock_entry_cache,GFP_KERNEL);
    7644:	49 89 c4             	mov    %rax,%r12
	if(!sclock_entry){
    7647:	0f 84 40 01 00 00    	je     778d <_try_switch_NCache.isra.69+0x5bd>
#define LIST_HEAD(name) \
	struct list_head name = LIST_HEAD_INIT(name)

static inline void INIT_LIST_HEAD(struct list_head *list)
{
	list->next = list;
    764d:	49 89 04 24          	mov    %rax,(%r12)
	list->prev = list;
    7651:	49 89 44 24 08       	mov    %rax,0x8(%r12)
	//	sclock_entry->sclock_lru.prev=&(sclock_entry->sclock_lru);
	//	ptep_clear_flush(vma,address,pte_table);
	atomic_set(&sclock_entry->access_times,1);
//	INIT_LIST_HEAD(&sclock_entry->pte_map);
//	atomic_set(&sclock_entry->pte_count,0);
	sclock_entry->pfn=pfn;
    7656:	48 89 58 10          	mov    %rbx,0x10(%rax)
		goto out;
	}
	mkOneScolor(sclock_entry,pfn);
	//		pte_map=list_pte_map_create(page_table,vma,address);
	//	list_pte_map_add(pte_map,sclock_entry);//reverse
	spin_lock(&(pid_ns->sclock_lock[set_number]));
    765a:	48 8b 5d b8          	mov    -0x48(%rbp),%rbx
 *
 * Atomically sets the value of @v to @i.
 */
static inline void atomic_set(atomic_t *v, int i)
{
	v->counter = i;
    765e:	c7 40 18 01 00 00 00 	movl   $0x1,0x18(%rax)
    7665:	48 89 df             	mov    %rbx,%rdi
    7668:	49 03 bd c8 08 00 00 	add    0x8c8(%r13),%rdi
	raw_spin_lock_init(&(_lock)->rlock);		\
} while (0)

static inline void spin_lock(spinlock_t *lock)
{
	raw_spin_lock(&lock->rlock);
    766f:	e8 00 00 00 00       	callq  7674 <_try_switch_NCache.isra.69+0x4a4>
	atomic_inc(&(pid_ns->sclock_lru_counter[set_number]));
    7674:	4c 8b 7d c0          	mov    -0x40(%rbp),%r15
    7678:	4d 03 bd b0 08 00 00 	add    0x8b0(%r13),%r15
 *
 * Atomically increments @v by 1.
 */
static inline void atomic_inc(atomic_t *v)
{
	asm volatile(LOCK_PREFIX "incl %0"
    767f:	f0 41 ff 07          	lock incl (%r15)
	list_add_tail(&sclock_entry->sclock_lru,&(pid_ns->sclock_lru[set_number]));
    7683:	48 8b 45 90          	mov    -0x70(%rbp),%rax
    7687:	49 03 85 90 08 00 00 	add    0x890(%r13),%rax
	spin_unlock(&(pid_ns->sclock_lock[set_number]));
    768e:	48 89 df             	mov    %rbx,%rdi
 * Insert a new entry before the specified head.
 * This is useful for implementing queues.
 */
static inline void list_add_tail(struct list_head *new, struct list_head *head)
{
	__list_add(new, head->prev, head);
    7691:	48 8b 50 08          	mov    0x8(%rax),%rdx
#ifndef CONFIG_DEBUG_LIST
static inline void __list_add(struct list_head *new,
			      struct list_head *prev,
			      struct list_head *next)
{
	next->prev = new;
    7695:	4c 89 60 08          	mov    %r12,0x8(%rax)
	new->next = next;
    7699:	49 89 04 24          	mov    %rax,(%r12)
	new->prev = prev;
    769d:	49 89 54 24 08       	mov    %rdx,0x8(%r12)
	prev->next = new;
    76a2:	4c 89 22             	mov    %r12,(%rdx)
    76a5:	49 03 bd c8 08 00 00 	add    0x8c8(%r13),%rdi
	raw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \
} while (0)

static inline void spin_unlock(spinlock_t *lock)
{
	raw_spin_unlock(&lock->rlock);
    76ac:	e8 00 00 00 00       	callq  76b1 <_try_switch_NCache.isra.69+0x4e1>
    76b1:	e9 24 fd ff ff       	jmpq   73da <_try_switch_NCache.isra.69+0x20a>
    76b6:	66 2e 0f 1f 84 00 00 	nopw   %cs:0x0(%rax,%rax,1)
    76bd:	00 00 00 
 * 	is expected to be non-NULL. If @pid is NULL, caller should handle
 * 	the resulting NULL pid-ns.
 */
static inline struct pid_namespace *ns_of_pid(struct pid *pid)
{
	struct pid_namespace *ns = NULL;
    76c0:	45 31 ed             	xor    %r13d,%r13d
    76c3:	e9 d6 fb ff ff       	jmpq   729e <_try_switch_NCache.isra.69+0xce>
	update_mmu_cache(vma, address, page_table);
	pte_unmap_unlock(page_table,ptl);
	set_number=pfn&(NPageColor-1);
	pid_ns=ns_of_pid(task_pid(mm->owner));
	if(pid_ns->sclock_lru==NULL){
		initPidNsProtection(pid_ns);
    76c8:	4c 89 ef             	mov    %r13,%rdi
    76cb:	4c 89 55 b8          	mov    %r10,-0x48(%rbp)
    76cf:	e8 00 00 00 00       	callq  76d4 <_try_switch_NCache.isra.69+0x504>
    76d4:	49 8b 95 90 08 00 00 	mov    0x890(%r13),%rdx
    76db:	4c 8b 55 b8          	mov    -0x48(%rbp),%r10
    76df:	e9 ca fb ff ff       	jmpq   72ae <_try_switch_NCache.isra.69+0xde>
		if(page_one)
		  if(page_one&&page_mapped(page_one)&& page_mapping(page_one)){
			  int welock=true;
			  if(!trylock_page(page_one))
			  {
				  welock=false;  //	  atomic_inc(&(pid_ns->sclock_lru_counter[set_number]));
    76e4:	45 31 c0             	xor    %r8d,%r8d
    76e7:	e9 e1 fd ff ff       	jmpq   74cd <_try_switch_NCache.isra.69+0x2fd>
	if(test_bit(PG_cacheable,&page->flags)){
		list_for_each_entry(sclock_entry,lru_head,sclock_lru){
			if(sclock_entry->pfn==pfn){
				spin_unlock(&(pid_ns->sclock_lock[set_number]));
				if(sclock_control->debug==1){
					printk("[in queue]page %lx already in set %d for ns=%lx,address=%lx,task=%s,pte=%lx\n",pfn,set_number,pid_ns,address,mm->owner->comm,page_table);
    76ec:	49 8b 07             	mov    (%r15),%rax
    76ef:	4c 8b 45 d0          	mov    -0x30(%rbp),%r8
    76f3:	4c 89 e9             	mov    %r13,%rcx
    76f6:	8b 55 cc             	mov    -0x34(%rbp),%edx
    76f9:	4c 89 24 24          	mov    %r12,(%rsp)
    76fd:	48 89 de             	mov    %rbx,%rsi
    7700:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
    7707:	4c 8d 88 d0 04 00 00 	lea    0x4d0(%rax),%r9
    770e:	31 c0                	xor    %eax,%eax
    7710:	e8 00 00 00 00       	callq  7715 <_try_switch_NCache.isra.69+0x545>
    7715:	e9 39 fc ff ff       	jmpq   7353 <_try_switch_NCache.isra.69+0x183>
	//	}
	list_del_init(&sclock_entry->sclock_lru);

	spin_unlock(&(pid_ns->sclock_lock[set_number]));
		if(sclock_control->debug==1){
		printk("[replace]page %lx replace page %lx in queue in set %d for ns=%lx, address=%lx,task=%s,pte=%lx\n",pfn,sclock_entry->pfn,set_number,pid_ns,address,mm->owner->comm,page_table);
    771a:	49 8b 56 10          	mov    0x10(%r14),%rdx
    771e:	4c 89 64 24 08       	mov    %r12,0x8(%rsp)
    7723:	4d 89 e8             	mov    %r13,%r8
    7726:	49 8b 07             	mov    (%r15),%rax
    7729:	4c 8b 4d d0          	mov    -0x30(%rbp),%r9
    772d:	48 89 de             	mov    %rbx,%rsi
    7730:	8b 4d cc             	mov    -0x34(%rbp),%ecx
    7733:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
    773a:	4c 89 5d a8          	mov    %r11,-0x58(%rbp)
    773e:	4c 89 55 b0          	mov    %r10,-0x50(%rbp)
    7742:	48 05 d0 04 00 00    	add    $0x4d0,%rax
    7748:	48 89 04 24          	mov    %rax,(%rsp)
    774c:	31 c0                	xor    %eax,%eax
    774e:	e8 00 00 00 00       	callq  7753 <_try_switch_NCache.isra.69+0x583>
    7753:	4c 8b 5d a8          	mov    -0x58(%rbp),%r11
    7757:	4c 8b 55 b0          	mov    -0x50(%rbp),%r10
    775b:	e9 25 fd ff ff       	jmpq   7485 <_try_switch_NCache.isra.69+0x2b5>
		if (likely(__get_page_tail(page)))
    7760:	4c 89 df             	mov    %r11,%rdi
    7763:	44 89 45 a0          	mov    %r8d,-0x60(%rbp)
    7767:	4c 89 55 a8          	mov    %r10,-0x58(%rbp)
    776b:	4c 89 5d b0          	mov    %r11,-0x50(%rbp)
    776f:	e8 00 00 00 00       	callq  7774 <_try_switch_NCache.isra.69+0x5a4>
    7774:	84 c0                	test   %al,%al
    7776:	4c 8b 5d b0          	mov    -0x50(%rbp),%r11
    777a:	4c 8b 55 a8          	mov    -0x58(%rbp),%r10
    777e:	44 8b 45 a0          	mov    -0x60(%rbp),%r8d
    7782:	0f 85 83 fd ff ff    	jne    750b <_try_switch_NCache.isra.69+0x33b>
    7788:	e9 79 fd ff ff       	jmpq   7506 <_try_switch_NCache.isra.69+0x336>
	if(!sclock_entry){
		addNewLRU=0;
		//		pte_unmap_unlock(page_table, ptl);
		ret=0;
		//sclock_pte_map_free_quick(pte_map);//reverse
		printk("failed alloc new LRU");
    778d:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
    7794:	31 c0                	xor    %eax,%eax
    7796:	e8 00 00 00 00       	callq  779b <_try_switch_NCache.isra.69+0x5cb>
    779b:	e9 3a fc ff ff       	jmpq   73da <_try_switch_NCache.isra.69+0x20a>

00000000000077a0 <handle_mm_fault>:
	return handle_pte_fault(mm, vma, address, pte, pmd, flags);
}

int handle_mm_fault(struct mm_struct *mm, struct vm_area_struct *vma,
			unsigned long address, unsigned int flags)
{
    77a0:	e8 00 00 00 00       	callq  77a5 <handle_mm_fault+0x5>
    77a5:	55                   	push   %rbp
    77a6:	65 48 8b 04 25 00 00 	mov    %gs:0x0,%rax
    77ad:	00 00 
    77af:	48 89 e5             	mov    %rsp,%rbp
    77b2:	41 57                	push   %r15
    77b4:	41 56                	push   %r14
    77b6:	49 89 f6             	mov    %rsi,%r14
    77b9:	41 55                	push   %r13
    77bb:	49 89 fd             	mov    %rdi,%r13
    77be:	41 54                	push   %r12
    77c0:	41 89 cc             	mov    %ecx,%r12d
    77c3:	53                   	push   %rbx
    77c4:	48 89 d3             	mov    %rdx,%rbx
    77c7:	48 83 ec 48          	sub    $0x48,%rsp
	int ret;

	__set_current_state(TASK_RUNNING);
    77cb:	48 c7 00 00 00 00 00 	movq   $0x0,(%rax)
extern int do_swap_account;
#endif

static inline bool mem_cgroup_disabled(void)
{
	if (mem_cgroup_subsys.disabled)
    77d2:	8b 15 00 00 00 00    	mov    0x0(%rip),%edx        # 77d8 <handle_mm_fault+0x38>
    77d8:	65 48 ff 04 25 00 00 	incq   %gs:0x0
    77df:	00 00 
    77e1:	85 d2                	test   %edx,%edx
    77e3:	75 0a                	jne    77ef <handle_mm_fault+0x4f>
static inline void mem_cgroup_count_vm_event(struct mm_struct *mm,
					     enum vm_event_item idx)
{
	if (mem_cgroup_disabled())
		return;
	__mem_cgroup_count_vm_event(mm, idx);
    77e5:	be 0b 00 00 00       	mov    $0xb,%esi
    77ea:	e8 00 00 00 00       	callq  77ef <handle_mm_fault+0x4f>
    77ef:	65 4c 8b 3c 25 00 00 	mov    %gs:0x0,%r15
    77f6:	00 00 

	count_vm_event(PGFAULT);
	mem_cgroup_count_vm_event(mm, PGFAULT);

	/* do counter updates before entering really critical section. */
	check_sync_rss_stat(current);
    77f8:	4c 89 ff             	mov    %r15,%rdi
    77fb:	e8 20 9d ff ff       	callq  1520 <check_sync_rss_stat>

	/*
	 * Enable the memcg OOM handling for faults triggered in user
	 * space.  Kernel faults are handled more gracefully.
	 */
	if (flags & FAULT_FLAG_USER)
    7800:	44 89 e0             	mov    %r12d,%eax
    7803:	25 80 00 00 00       	and    $0x80,%eax
    7808:	89 45 d4             	mov    %eax,-0x2c(%rbp)
    780b:	0f 85 a7 02 00 00    	jne    7ab8 <handle_mm_fault+0x318>
	pte_t *pte;

	if (unlikely(is_vm_hugetlb_page(vma)))
	  return hugetlb_fault(mm, vma, address, flags);

	pgd = pgd_offset(mm, address);
    7811:	48 89 d9             	mov    %rbx,%rcx
    7814:	48 c1 e9 24          	shr    $0x24,%rcx
    7818:	81 e1 f8 0f 00 00    	and    $0xff8,%ecx
    781e:	49 03 4d 40          	add    0x40(%r13),%rcx
    7822:	48 8b 01             	mov    (%rcx),%rax
 */
#if defined(CONFIG_MMU) && !defined(__ARCH_HAS_4LEVEL_HACK)
static inline pud_t *pud_alloc(struct mm_struct *mm, pgd_t *pgd, unsigned long address)
{
	return (unlikely(pgd_none(*pgd)) && __pud_alloc(mm, pgd, address))?
		NULL: pud_offset(pgd, address);
    7825:	48 85 c0             	test   %rax,%rax
    7828:	0f 84 05 05 00 00    	je     7d33 <handle_mm_fault+0x593>
    782e:	48 89 c7             	mov    %rax,%rdi

	if (sizeof(pgdval_t) > sizeof(long))
		ret =  PVOP_CALLEE2(pgdval_t, pv_mmu_ops.pgd_val,
				    pgd.pgd, (u64)pgd.pgd >> 32);
	else
		ret =  PVOP_CALLEE1(pgdval_t, pv_mmu_ops.pgd_val,
    7831:	ff 14 25 00 00 00 00 	callq  *0x0
#define pgd_page(pgd)		pfn_to_page(pgd_val(pgd) >> PAGE_SHIFT)

/* to find an entry in a page-table-directory. */
static inline unsigned long pud_index(unsigned long address)
{
	return (address >> PUD_SHIFT) & (PTRS_PER_PUD - 1);
    7838:	48 89 da             	mov    %rbx,%rdx
	return pgd_flags(pgd) & _PAGE_PRESENT;
}

static inline unsigned long pgd_page_vaddr(pgd_t pgd)
{
	return (unsigned long)__va((unsigned long)pgd_val(pgd) & PTE_PFN_MASK);
    783b:	48 b9 00 00 00 00 00 	movabs $0xffff880000000000,%rcx
    7842:	88 ff ff 
#define pgd_page(pgd)		pfn_to_page(pgd_val(pgd) >> PAGE_SHIFT)

/* to find an entry in a page-table-directory. */
static inline unsigned long pud_index(unsigned long address)
{
	return (address >> PUD_SHIFT) & (PTRS_PER_PUD - 1);
    7845:	48 c1 ea 1b          	shr    $0x1b,%rdx
}

static inline pud_t *pud_offset(pgd_t *pgd, unsigned long address)
{
	return (pud_t *)pgd_page_vaddr(*pgd) + pud_index(address);
    7849:	81 e2 f8 0f 00 00    	and    $0xff8,%edx
	return pgd_flags(pgd) & _PAGE_PRESENT;
}

static inline unsigned long pgd_page_vaddr(pgd_t pgd)
{
	return (unsigned long)__va((unsigned long)pgd_val(pgd) & PTE_PFN_MASK);
    784f:	48 01 ca             	add    %rcx,%rdx
    7852:	48 b9 00 f0 ff ff ff 	movabs $0x3ffffffff000,%rcx
    7859:	3f 00 00 
    785c:	48 21 c8             	and    %rcx,%rax
	pud = pud_alloc(mm, pgd, address);
	if (!pud)
    785f:	48 01 c2             	add    %rax,%rdx
    7862:	48 89 d1             	mov    %rdx,%rcx
    7865:	0f 84 25 02 00 00    	je     7a90 <handle_mm_fault+0x2f0>
    786b:	48 8b 3a             	mov    (%rdx),%rdi
}

static inline pmd_t *pmd_alloc(struct mm_struct *mm, pud_t *pud, unsigned long address)
{
	return (unlikely(pud_none(*pud)) && __pmd_alloc(mm, pud, address))?
		NULL: pmd_offset(pud, address);
    786e:	48 85 ff             	test   %rdi,%rdi
    7871:	0f 84 e2 04 00 00    	je     7d59 <handle_mm_fault+0x5b9>

	if (sizeof(pudval_t) > sizeof(long))
		ret =  PVOP_CALLEE2(pudval_t, pv_mmu_ops.pud_val,
				    pud.pud, (u64)pud.pud >> 32);
	else
		ret =  PVOP_CALLEE1(pudval_t, pv_mmu_ops.pud_val,
    7877:	ff 14 25 00 00 00 00 	callq  *0x0
 * this macro returns the index of the entry in the pmd page which would
 * control the given virtual address
 */
static inline unsigned long pmd_index(unsigned long address)
{
	return (address >> PMD_SHIFT) & (PTRS_PER_PMD - 1);
    787e:	48 89 da             	mov    %rbx,%rdx
	return pud_flags(pud) & _PAGE_PRESENT;
}

static inline unsigned long pud_page_vaddr(pud_t pud)
{
	return (unsigned long)__va((unsigned long)pud_val(pud) & PTE_PFN_MASK);
    7881:	49 ba 00 00 00 00 00 	movabs $0xffff880000000000,%r10
    7888:	88 ff ff 
 * this macro returns the index of the entry in the pmd page which would
 * control the given virtual address
 */
static inline unsigned long pmd_index(unsigned long address)
{
	return (address >> PMD_SHIFT) & (PTRS_PER_PMD - 1);
    788b:	48 c1 ea 12          	shr    $0x12,%rdx
#define pud_page(pud)		pfn_to_page(pud_val(pud) >> PAGE_SHIFT)

/* Find an entry in the second-level page table.. */
static inline pmd_t *pmd_offset(pud_t *pud, unsigned long address)
{
	return (pmd_t *)pud_page_vaddr(*pud) + pmd_index(address);
    788f:	81 e2 f8 0f 00 00    	and    $0xff8,%edx
	return pud_flags(pud) & _PAGE_PRESENT;
}

static inline unsigned long pud_page_vaddr(pud_t pud)
{
	return (unsigned long)__va((unsigned long)pud_val(pud) & PTE_PFN_MASK);
    7895:	49 01 d2             	add    %rdx,%r10
    7898:	48 ba 00 f0 ff ff ff 	movabs $0x3ffffffff000,%rdx
    789f:	3f 00 00 
    78a2:	48 21 d0             	and    %rdx,%rax
	  return VM_FAULT_OOM;
	pmd = pmd_alloc(mm, pud, address);
	if (!pmd)
    78a5:	49 01 c2             	add    %rax,%r10
    78a8:	0f 84 e2 01 00 00    	je     7a90 <handle_mm_fault+0x2f0>
		  return ret;
	} else {
		pmd_t orig_pmd = *pmd;
		int ret;

		barrier();
    78ae:	49 8b 02             	mov    (%r10),%rax
	/*
	 * Use __pte_alloc instead of pte_alloc_map, because we can't
	 * run pte_offset_map on the pmd, if an huge pmd could
	 * materialize from under us from a different thread.
	 */
	if (unlikely(pmd_none(*pmd)) &&
    78b1:	48 85 c0             	test   %rax,%rax
    78b4:	0f 84 c5 04 00 00    	je     7d7f <handle_mm_fault+0x5df>
    78ba:	48 89 c7             	mov    %rax,%rdi

	if (sizeof(pmdval_t) > sizeof(long))
		ret =  PVOP_CALLEE2(pmdval_t, pv_mmu_ops.pmd_val,
				    pmd.pmd, (u64)pmd.pmd >> 32);
	else
		ret =  PVOP_CALLEE1(pmdval_t, pv_mmu_ops.pmd_val,
    78bd:	ff 14 25 00 00 00 00 	callq  *0x0
 * this function returns the index of the entry in the pte page which would
 * control the given virtual address
 */
static inline unsigned long pte_index(unsigned long address)
{
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
    78c4:	48 89 da             	mov    %rbx,%rdx
	return (unsigned long)native_pmd_val(pmd) == 0;
}

static inline unsigned long pmd_page_vaddr(pmd_t pmd)
{
	return (unsigned long)__va(pmd_val(pmd) & PTE_PFN_MASK);
    78c7:	49 bb 00 00 00 00 00 	movabs $0xffff880000000000,%r11
    78ce:	88 ff ff 
    78d1:	49 b8 00 f0 ff ff ff 	movabs $0x3ffffffff000,%r8
    78d8:	3f 00 00 
 * this function returns the index of the entry in the pte page which would
 * control the given virtual address
 */
static inline unsigned long pte_index(unsigned long address)
{
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
    78db:	48 c1 ea 09          	shr    $0x9,%rdx
	return (unsigned long)native_pmd_val(pmd) == 0;
}

static inline unsigned long pmd_page_vaddr(pmd_t pmd)
{
	return (unsigned long)__va(pmd_val(pmd) & PTE_PFN_MASK);
    78df:	4c 21 c0             	and    %r8,%rax
    78e2:	48 be ff 0f 00 00 00 	movabs $0xffffc00000000fff,%rsi
    78e9:	c0 ff ff 
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
}

static inline pte_t *pte_offset_kernel(pmd_t *pmd, unsigned long address)
{
	return (pte_t *)pmd_page_vaddr(*pmd) + pte_index(address);
    78ec:	81 e2 f8 0f 00 00    	and    $0xff8,%edx
	return (unsigned long)native_pmd_val(pmd) == 0;
}

static inline unsigned long pmd_page_vaddr(pmd_t pmd)
{
	return (unsigned long)__va(pmd_val(pmd) & PTE_PFN_MASK);
    78f2:	49 01 d3             	add    %rdx,%r11
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
}

static inline pte_t *pte_offset_kernel(pmd_t *pmd, unsigned long address)
{
	return (pte_t *)pmd_page_vaddr(*pmd) + pte_index(address);
    78f5:	4a 8d 14 18          	lea    (%rax,%r11,1),%rdx
			struct vm_area_struct *vma, unsigned long address,
			pte_t *pte, pmd_t *pmd, unsigned int flags)
{
	pte_t entry;
	spinlock_t *ptl;
	entry = *pte;
    78f9:	48 8b 0a             	mov    (%rdx),%rcx
    78fc:	48 21 ce             	and    %rcx,%rsi
	int ret=0;
	if (!pte_present(entry)) {
    78ff:	f7 c6 01 01 00 00    	test   $0x101,%esi
    7905:	0f 84 15 01 00 00    	je     7a20 <handle_mm_fault+0x280>
 * (because _PAGE_PRESENT is not set).
 */
#ifndef pte_numa
static inline int pte_numa(pte_t pte)
{
	return (pte_flags(pte) &
    790b:	48 89 c8             	mov    %rcx,%rax
    790e:	25 01 01 00 00       	and    $0x101,%eax
		{ printk("bad in do_swap_fault");

		}
		return ret;
	}
	if (pte_numa(entry))
    7913:	48 3d 00 01 00 00    	cmp    $0x100,%rax
    7919:	0f 84 51 02 00 00    	je     7b70 <handle_mm_fault+0x3d0>
    791f:	48 89 75 a0          	mov    %rsi,-0x60(%rbp)
    7923:	48 89 4d a8          	mov    %rcx,-0x58(%rbp)
    7927:	48 89 55 b0          	mov    %rdx,-0x50(%rbp)
    792b:	4c 89 5d b8          	mov    %r11,-0x48(%rbp)
    792f:	49 8b 3a             	mov    (%r10),%rdi
    7932:	4c 89 55 c0          	mov    %r10,-0x40(%rbp)
    7936:	ff 14 25 00 00 00 00 	callq  *0x0
}
#endif /* ALLOC_SPLIT_PTLOCKS */

static inline spinlock_t *pte_lockptr(struct mm_struct *mm, pmd_t *pmd)
{
	return ptlock_ptr(pmd_page(*pmd));
    793d:	4c 21 c0             	and    %r8,%rax
    7940:	48 bf 00 00 00 00 00 	movabs $0xffffea0000000000,%rdi
    7947:	ea ff ff 
    794a:	48 c1 e8 06          	shr    $0x6,%rax
    794e:	4c 8b 4c 38 30       	mov    0x30(%rax,%rdi,1),%r9
	raw_spin_lock_init(&(_lock)->rlock);		\
} while (0)

static inline void spin_lock(spinlock_t *lock)
{
	raw_spin_lock(&lock->rlock);
    7953:	4c 89 cf             	mov    %r9,%rdi
    7956:	4c 89 4d 98          	mov    %r9,-0x68(%rbp)
    795a:	4c 89 4d c8          	mov    %r9,-0x38(%rbp)
    795e:	e8 00 00 00 00       	callq  7963 <handle_mm_fault+0x1c3>
		  printk("bad in do_numa_fault");
		return ret;
	}
	ptl = pte_lockptr(mm, pmd);
	spin_lock(ptl);
	if (unlikely(!pte_same(*pte, entry)))
    7963:	48 8b 55 b0          	mov    -0x50(%rbp),%rdx
    7967:	48 8b 4d a8          	mov    -0x58(%rbp),%rcx
    796b:	4c 8b 4d c8          	mov    -0x38(%rbp),%r9
    796f:	48 3b 0a             	cmp    (%rdx),%rcx
    7972:	75 47                	jne    79bb <handle_mm_fault+0x21b>
	  goto unlock;
	if (flags & FAULT_FLAG_WRITE) {
    7974:	41 f6 c4 01          	test   $0x1,%r12b
    7978:	4c 8b 55 c0          	mov    -0x40(%rbp),%r10
    797c:	4c 8b 5d b8          	mov    -0x48(%rbp),%r11
    7980:	48 8b 75 a0          	mov    -0x60(%rbp),%rsi
    7984:	0f 85 96 01 00 00    	jne    7b20 <handle_mm_fault+0x380>
    798a:	49 89 cf             	mov    %rcx,%r15
			  printk("bad in do_wp_fault");
			return ret;
		}
		entry = pte_mkdirty(entry);
	}
	if(flags&FAULT_FLAG_RSVD){
    798d:	41 f7 c4 00 01 00 00 	test   $0x100,%r12d
    7994:	0f 85 36 02 00 00    	jne    7bd0 <handle_mm_fault+0x430>

static inline pte_t pte_set_flags(pte_t pte, pteval_t set)
{
	pteval_t v = native_pte_val(pte);

	return native_make_pte(v | set);
    799a:	4c 89 f9             	mov    %r15,%rcx
		//		//printk("FAULT_FLAG_RSVD,lock=%lx\n",ptl);
		return handle_double_cache_pte_fault(mm, vma, address,
					pte, pmd, ptl, entry,true);
	}
	entry = pte_mkyoung(entry);
	if (ptep_set_access_flags(vma, address, pte, entry, flags & FAULT_FLAG_WRITE)) {
    799d:	45 89 e0             	mov    %r12d,%r8d
    79a0:	48 89 de             	mov    %rbx,%rsi
    79a3:	48 83 c9 20          	or     $0x20,%rcx
    79a7:	41 83 e0 01          	and    $0x1,%r8d
    79ab:	4c 89 f7             	mov    %r14,%rdi
    79ae:	4c 89 4d c8          	mov    %r9,-0x38(%rbp)
    79b2:	e8 00 00 00 00       	callq  79b7 <handle_mm_fault+0x217>
    79b7:	4c 8b 4d c8          	mov    -0x38(%rbp),%r9
	raw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \
} while (0)

static inline void spin_unlock(spinlock_t *lock)
{
	raw_spin_unlock(&lock->rlock);
    79bb:	4c 89 cf             	mov    %r9,%rdi
		  flush_tlb_fix_spurious_fault(vma, address);
	}
unlock:
	pte_unmap_unlock(pte, ptl);
	//printk("unlock at handle_pte\n");
	return 0;
    79be:	31 db                	xor    %ebx,%ebx
    79c0:	e8 00 00 00 00       	callq  79c5 <handle_mm_fault+0x225>
	if (flags & FAULT_FLAG_USER)
	  mem_cgroup_oom_enable();

	ret = __handle_mm_fault(mm, vma, address, flags);

	if (flags & FAULT_FLAG_USER) {
    79c5:	8b 45 d4             	mov    -0x2c(%rbp),%eax
    79c8:	85 c0                	test   %eax,%eax
    79ca:	0f 84 d0 00 00 00    	je     7aa0 <handle_mm_fault+0x300>
    79d0:	65 48 8b 04 25 00 00 	mov    %gs:0x0,%rax
    79d7:	00 00 
	current->memcg_oom.may_oom = 1;
}

static inline void mem_cgroup_oom_disable(void)
{
	WARN_ON(!current->memcg_oom.may_oom);
    79d9:	f6 80 38 18 00 00 01 	testb  $0x1,0x1838(%rax)
    79e0:	0f 84 c2 03 00 00    	je     7da8 <handle_mm_fault+0x608>
    79e6:	65 48 8b 04 25 00 00 	mov    %gs:0x0,%rax
    79ed:	00 00 
	current->memcg_oom.may_oom = 0;
    79ef:	80 a0 38 18 00 00 fe 	andb   $0xfe,0x1838(%rax)
		 * The task may have entered a memcg OOM situation but
		 * if the allocation error was handled gracefully (no
		 * VM_FAULT_OOM), there is no need to kill anything.
		 * Just clean up the OOM state peacefully.
		 */
		if (task_in_memcg_oom(current) && !(ret & VM_FAULT_OOM))
    79f6:	48 83 b8 28 18 00 00 	cmpq   $0x0,0x1828(%rax)
    79fd:	00 
    79fe:	0f 84 9c 00 00 00    	je     7aa0 <handle_mm_fault+0x300>
    7a04:	f6 c3 01             	test   $0x1,%bl
    7a07:	0f 85 93 00 00 00    	jne    7aa0 <handle_mm_fault+0x300>
		  mem_cgroup_oom_synchronize(false);
    7a0d:	31 ff                	xor    %edi,%edi
    7a0f:	e8 00 00 00 00       	callq  7a14 <handle_mm_fault+0x274>
    7a14:	e9 87 00 00 00       	jmpq   7aa0 <handle_mm_fault+0x300>
    7a19:	0f 1f 80 00 00 00 00 	nopl   0x0(%rax)
	pte_t entry;
	spinlock_t *ptl;
	entry = *pte;
	int ret=0;
	if (!pte_present(entry)) {
		if (pte_none(entry)) {
    7a20:	48 85 c9             	test   %rcx,%rcx
    7a23:	0f 85 b7 00 00 00    	jne    7ae0 <handle_mm_fault+0x340>
			if (vma->vm_ops) {
    7a29:	49 8b 86 98 00 00 00 	mov    0x98(%r14),%rax
    7a30:	48 85 c0             	test   %rax,%rax
    7a33:	0f 84 a6 02 00 00    	je     7cdf <handle_mm_fault+0x53f>
				if (likely(vma->vm_ops->fault))
    7a39:	48 83 78 10 00       	cmpq   $0x0,0x10(%rax)
    7a3e:	0f 84 9b 02 00 00    	je     7cdf <handle_mm_fault+0x53f>

static int do_linear_fault(struct mm_struct *mm, struct vm_area_struct *vma,
			unsigned long address, pte_t *page_table, pmd_t *pmd,
			unsigned int flags, pte_t orig_pte)
{
	pgoff_t pgoff = (((address & PAGE_MASK)
    7a44:	49 89 d8             	mov    %rbx,%r8
					- vma->vm_start) >> PAGE_SHIFT) + vma->vm_pgoff;

	pte_unmap(page_table);
	return __do_fault(mm, vma, address, pmd, pgoff, flags, orig_pte);
    7a47:	48 89 da             	mov    %rbx,%rdx
    7a4a:	45 89 e1             	mov    %r12d,%r9d

static int do_linear_fault(struct mm_struct *mm, struct vm_area_struct *vma,
			unsigned long address, pte_t *page_table, pmd_t *pmd,
			unsigned int flags, pte_t orig_pte)
{
	pgoff_t pgoff = (((address & PAGE_MASK)
    7a4d:	49 81 e0 00 f0 ff ff 	and    $0xfffffffffffff000,%r8
					- vma->vm_start) >> PAGE_SHIFT) + vma->vm_pgoff;
    7a54:	4d 2b 06             	sub    (%r14),%r8

	pte_unmap(page_table);
	return __do_fault(mm, vma, address, pmd, pgoff, flags, orig_pte);
    7a57:	4c 89 d1             	mov    %r10,%rcx
    7a5a:	4c 89 f6             	mov    %r14,%rsi
    7a5d:	4c 89 ef             	mov    %r13,%rdi
static int do_linear_fault(struct mm_struct *mm, struct vm_area_struct *vma,
			unsigned long address, pte_t *page_table, pmd_t *pmd,
			unsigned int flags, pte_t orig_pte)
{
	pgoff_t pgoff = (((address & PAGE_MASK)
					- vma->vm_start) >> PAGE_SHIFT) + vma->vm_pgoff;
    7a60:	49 c1 e8 0c          	shr    $0xc,%r8

static int do_linear_fault(struct mm_struct *mm, struct vm_area_struct *vma,
			unsigned long address, pte_t *page_table, pmd_t *pmd,
			unsigned int flags, pte_t orig_pte)
{
	pgoff_t pgoff = (((address & PAGE_MASK)
    7a64:	4d 03 86 a0 00 00 00 	add    0xa0(%r14),%r8
					- vma->vm_start) >> PAGE_SHIFT) + vma->vm_pgoff;

	pte_unmap(page_table);
	return __do_fault(mm, vma, address, pmd, pgoff, flags, orig_pte);
    7a6b:	48 c7 04 24 00 00 00 	movq   $0x0,(%rsp)
    7a72:	00 
    7a73:	e8 88 94 ff ff       	callq  f00 <__do_fault>
		if (pte_none(entry)) {
			if (vma->vm_ops) {
				if (likely(vma->vm_ops->fault))
				{  ret= do_linear_fault(mm, vma, address,
							pte, pmd, flags, entry);
				if(ret&VM_FAULT_OOM)  
    7a78:	a8 01                	test   $0x1,%al
{
	pgoff_t pgoff = (((address & PAGE_MASK)
					- vma->vm_start) >> PAGE_SHIFT) + vma->vm_pgoff;

	pte_unmap(page_table);
	return __do_fault(mm, vma, address, pmd, pgoff, flags, orig_pte);
    7a7a:	89 c3                	mov    %eax,%ebx
		if (pte_none(entry)) {
			if (vma->vm_ops) {
				if (likely(vma->vm_ops->fault))
				{  ret= do_linear_fault(mm, vma, address,
							pte, pmd, flags, entry);
				if(ret&VM_FAULT_OOM)  
    7a7c:	74 17                	je     7a95 <handle_mm_fault+0x2f5>
				  printk("bad in do_linear_fault");
    7a7e:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
    7a85:	31 c0                	xor    %eax,%eax
    7a87:	e8 00 00 00 00       	callq  7a8c <handle_mm_fault+0x2ec>
    7a8c:	eb 07                	jmp    7a95 <handle_mm_fault+0x2f5>
    7a8e:	66 90                	xchg   %ax,%ax
	  return hugetlb_fault(mm, vma, address, flags);

	pgd = pgd_offset(mm, address);
	pud = pud_alloc(mm, pgd, address);
	if (!pud)
	  return VM_FAULT_OOM;
    7a90:	bb 01 00 00 00       	mov    $0x1,%ebx
	if (flags & FAULT_FLAG_USER)
	  mem_cgroup_oom_enable();

	ret = __handle_mm_fault(mm, vma, address, flags);

	if (flags & FAULT_FLAG_USER) {
    7a95:	8b 45 d4             	mov    -0x2c(%rbp),%eax
    7a98:	85 c0                	test   %eax,%eax
    7a9a:	0f 85 30 ff ff ff    	jne    79d0 <handle_mm_fault+0x230>
		if (task_in_memcg_oom(current) && !(ret & VM_FAULT_OOM))
		  mem_cgroup_oom_synchronize(false);
	}

	return ret;
}
    7aa0:	48 83 c4 48          	add    $0x48,%rsp
    7aa4:	89 d8                	mov    %ebx,%eax
    7aa6:	5b                   	pop    %rbx
    7aa7:	41 5c                	pop    %r12
    7aa9:	41 5d                	pop    %r13
    7aab:	41 5e                	pop    %r14
    7aad:	41 5f                	pop    %r15
    7aaf:	5d                   	pop    %rbp
    7ab0:	c3                   	retq   
    7ab1:	0f 1f 80 00 00 00 00 	nopl   0x0(%rax)
extern void mem_cgroup_replace_page_cache(struct page *oldpage,
					struct page *newpage);

static inline void mem_cgroup_oom_enable(void)
{
	WARN_ON(current->memcg_oom.may_oom);
    7ab8:	41 f6 87 38 18 00 00 	testb  $0x1,0x1838(%r15)
    7abf:	01 
    7ac0:	0f 85 f8 02 00 00    	jne    7dbe <handle_mm_fault+0x61e>
    7ac6:	65 48 8b 04 25 00 00 	mov    %gs:0x0,%rax
    7acd:	00 00 
	current->memcg_oom.may_oom = 1;
    7acf:	80 88 38 18 00 00 01 	orb    $0x1,0x1838(%rax)
    7ad6:	e9 36 fd ff ff       	jmpq   7811 <handle_mm_fault+0x71>
    7adb:	0f 1f 44 00 00       	nopl   0x0(%rax,%rax,1)
			if(ret&VM_FAULT_OOM)  
			  printk("bad in do_anon_fault");  
			return ret;	

		}
		if (pte_file(entry)){
    7ae0:	83 e6 40             	and    $0x40,%esi
			ret= do_nonlinear_fault(mm, vma, address,
    7ae3:	49 89 c9             	mov    %rcx,%r9
    7ae6:	45 89 e0             	mov    %r12d,%r8d
    7ae9:	4c 89 d1             	mov    %r10,%rcx
    7aec:	48 89 da             	mov    %rbx,%rdx
    7aef:	4c 89 f6             	mov    %r14,%rsi
    7af2:	4c 89 ef             	mov    %r13,%rdi
			if(ret&VM_FAULT_OOM)  
			  printk("bad in do_anon_fault");  
			return ret;	

		}
		if (pte_file(entry)){
    7af5:	0f 84 ad 00 00 00    	je     7ba8 <handle_mm_fault+0x408>
			ret= do_nonlinear_fault(mm, vma, address,
    7afb:	e8 60 99 ff ff       	callq  1460 <do_nonlinear_fault.isra.46>
						pte, pmd, flags, entry);
			if(ret&VM_FAULT_OOM)	
    7b00:	a8 01                	test   $0x1,%al
			  printk("bad in do_anon_fault");  
			return ret;	

		}
		if (pte_file(entry)){
			ret= do_nonlinear_fault(mm, vma, address,
    7b02:	89 c3                	mov    %eax,%ebx
						pte, pmd, flags, entry);
			if(ret&VM_FAULT_OOM)	
    7b04:	74 8f                	je     7a95 <handle_mm_fault+0x2f5>
			  printk("bad in do_non_linear_fault");
    7b06:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
    7b0d:	31 c0                	xor    %eax,%eax
    7b0f:	e8 00 00 00 00       	callq  7b14 <handle_mm_fault+0x374>
    7b14:	e9 7c ff ff ff       	jmpq   7a95 <handle_mm_fault+0x2f5>
    7b19:	0f 1f 80 00 00 00 00 	nopl   0x0(%rax)
    7b20:	48 89 c8             	mov    %rcx,%rax
    7b23:	48 0d 40 08 00 00    	or     $0x840,%rax
	ptl = pte_lockptr(mm, pmd);
	spin_lock(ptl);
	if (unlikely(!pte_same(*pte, entry)))
	  goto unlock;
	if (flags & FAULT_FLAG_WRITE) {
		if (!pte_write(entry))
    7b29:	83 e6 02             	and    $0x2,%esi
    7b2c:	49 89 c7             	mov    %rax,%r15
    7b2f:	0f 85 58 fe ff ff    	jne    798d <handle_mm_fault+0x1ed>
		{
			ret= do_wp_page(mm, vma, address,
    7b35:	48 89 0c 24          	mov    %rcx,(%rsp)
    7b39:	4d 89 d0             	mov    %r10,%r8
    7b3c:	48 89 d1             	mov    %rdx,%rcx
    7b3f:	4c 89 f6             	mov    %r14,%rsi
    7b42:	48 89 da             	mov    %rbx,%rdx
    7b45:	4c 89 ef             	mov    %r13,%rdi
    7b48:	e8 43 bb ff ff       	callq  3690 <do_wp_page>
						pte, pmd, ptl, entry);
			if(ret&VM_FAULT_OOM)	
    7b4d:	a8 01                	test   $0x1,%al
	if (unlikely(!pte_same(*pte, entry)))
	  goto unlock;
	if (flags & FAULT_FLAG_WRITE) {
		if (!pte_write(entry))
		{
			ret= do_wp_page(mm, vma, address,
    7b4f:	89 c3                	mov    %eax,%ebx
						pte, pmd, ptl, entry);
			if(ret&VM_FAULT_OOM)	
    7b51:	0f 84 3e ff ff ff    	je     7a95 <handle_mm_fault+0x2f5>
			  printk("bad in do_wp_fault");
    7b57:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
    7b5e:	31 c0                	xor    %eax,%eax
    7b60:	e8 00 00 00 00       	callq  7b65 <handle_mm_fault+0x3c5>
    7b65:	e9 2b ff ff ff       	jmpq   7a95 <handle_mm_fault+0x2f5>
    7b6a:	66 0f 1f 44 00 00    	nopw   0x0(%rax,%rax,1)

		}
		return ret;
	}
	if (pte_numa(entry))
	{ ret=do_numa_page(mm, vma, address, entry, pte, pmd);
    7b70:	49 89 d0             	mov    %rdx,%r8
    7b73:	4d 89 d1             	mov    %r10,%r9
    7b76:	48 89 da             	mov    %rbx,%rdx
    7b79:	4c 89 f6             	mov    %r14,%rsi
    7b7c:	4c 89 ef             	mov    %r13,%rdi
    7b7f:	e8 00 00 00 00       	callq  7b84 <handle_mm_fault+0x3e4>
		if(ret&VM_FAULT_OOM)  
    7b84:	a8 01                	test   $0x1,%al

		}
		return ret;
	}
	if (pte_numa(entry))
	{ ret=do_numa_page(mm, vma, address, entry, pte, pmd);
    7b86:	89 c3                	mov    %eax,%ebx
		if(ret&VM_FAULT_OOM)  
    7b88:	0f 84 07 ff ff ff    	je     7a95 <handle_mm_fault+0x2f5>
		  printk("bad in do_numa_fault");
    7b8e:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
    7b95:	31 c0                	xor    %eax,%eax
    7b97:	e8 00 00 00 00       	callq  7b9c <handle_mm_fault+0x3fc>
    7b9c:	e9 f4 fe ff ff       	jmpq   7a95 <handle_mm_fault+0x2f5>
    7ba1:	0f 1f 80 00 00 00 00 	nopl   0x0(%rax)
						pte, pmd, flags, entry);
			if(ret&VM_FAULT_OOM)	
			  printk("bad in do_non_linear_fault");
			return ret;
		}
		ret= do_swap_page(mm, vma, address,
    7ba8:	e8 13 c3 ff ff       	callq  3ec0 <do_swap_page.isra.68>
					pte, pmd, flags, entry);
		if(ret&VM_FAULT_OOM)  
    7bad:	a8 01                	test   $0x1,%al
						pte, pmd, flags, entry);
			if(ret&VM_FAULT_OOM)	
			  printk("bad in do_non_linear_fault");
			return ret;
		}
		ret= do_swap_page(mm, vma, address,
    7baf:	89 c3                	mov    %eax,%ebx
					pte, pmd, flags, entry);
		if(ret&VM_FAULT_OOM)  
    7bb1:	0f 84 de fe ff ff    	je     7a95 <handle_mm_fault+0x2f5>
		{ printk("bad in do_swap_fault");
    7bb7:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
    7bbe:	31 c0                	xor    %eax,%eax
    7bc0:	e8 00 00 00 00       	callq  7bc5 <handle_mm_fault+0x425>
    7bc5:	e9 cb fe ff ff       	jmpq   7a95 <handle_mm_fault+0x2f5>
    7bca:	66 0f 1f 44 00 00    	nopw   0x0(%rax,%rax,1)
			return _do_copy_on_read(mm,vma,address, page_table,pmd,ptl, orig_pte);
			}else if(mm->def_flags&VM_NCACHE){
			return _manage_cacheability(mm,vma,address, page_table,pmd,ptl, orig_pte);
			}
			*/
		if(trigger_by_isolation(page_table)==1)
    7bd0:	48 be 00 00 00 00 80 	movabs $0xbff8000000000,%rsi
    7bd7:	ff 0b 00 
    7bda:	48 b8 00 00 00 00 00 	movabs $0x8000000000000,%rax
    7be1:	00 08 00 
    7be4:	48 21 ce             	and    %rcx,%rsi
    7be7:	48 39 c6             	cmp    %rax,%rsi
    7bea:	74 34                	je     7c20 <handle_mm_fault+0x480>
			if(trigger_by_NCache(page_table)==1){
				return	ret|_try_switch_NCache(mm,vma,address, page_table,pmd,ptl, orig_pte,invalid);

			}
		}
		if(trigger_by_NCache(page_table)==1){
    7bec:	48 b8 00 00 00 00 80 	movabs $0xfff8000000000,%rax
    7bf3:	ff 0f 00 
    7bf6:	48 21 c1             	and    %rax,%rcx
    7bf9:	48 b8 00 00 00 00 00 	movabs $0x4000000000000,%rax
    7c00:	00 04 00 
    7c03:	48 39 c1             	cmp    %rax,%rcx
    7c06:	0f 84 04 01 00 00    	je     7d10 <handle_mm_fault+0x570>
    7c0c:	48 8b 7d 98          	mov    -0x68(%rbp),%rdi
		}
		entry = pte_mkdirty(entry);
	}
	if(flags&FAULT_FLAG_RSVD){
		//		//printk("FAULT_FLAG_RSVD,lock=%lx\n",ptl);
		return handle_double_cache_pte_fault(mm, vma, address,
    7c10:	31 db                	xor    %ebx,%ebx
    7c12:	e8 00 00 00 00       	callq  7c17 <handle_mm_fault+0x477>
    7c17:	e9 79 fe ff ff       	jmpq   7a95 <handle_mm_fault+0x2f5>
    7c1c:	0f 1f 40 00          	nopl   0x0(%rax)
			}
			*/
		if(trigger_by_isolation(page_table)==1)
		{
			//	printk(KERN_DEBUG"triggerd by isolation\n");
			ret=__do_copy_on_read(mm,vma,address, page_table,pmd,ptl, orig_pte);
    7c20:	4d 89 d0             	mov    %r10,%r8
    7c23:	48 89 d1             	mov    %rdx,%rcx
    7c26:	4c 89 ef             	mov    %r13,%rdi
    7c29:	4c 89 3c 24          	mov    %r15,(%rsp)
    7c2d:	48 89 da             	mov    %rbx,%rdx
    7c30:	4c 89 f6             	mov    %r14,%rsi
    7c33:	4c 89 55 c8          	mov    %r10,-0x38(%rbp)
    7c37:	4c 89 5d c0          	mov    %r11,-0x40(%rbp)
    7c3b:	e8 00 e5 ff ff       	callq  6140 <__do_copy_on_read>
    7c40:	4c 8b 55 c8          	mov    -0x38(%rbp),%r10
    7c44:	49 8b 3a             	mov    (%r10),%rdi
    7c47:	ff 14 25 00 00 00 00 	callq  *0x0
    7c4e:	48 ba 00 f0 ff ff ff 	movabs $0x3ffffffff000,%rdx
    7c55:	3f 00 00 
    7c58:	48 b9 00 00 00 00 00 	movabs $0xffffea0000000000,%rcx
    7c5f:	ea ff ff 
    7c62:	49 8b 3a             	mov    (%r10),%rdi
    7c65:	48 21 d0             	and    %rdx,%rax
    7c68:	4c 89 55 c8          	mov    %r10,-0x38(%rbp)
    7c6c:	48 c1 e8 06          	shr    $0x6,%rax
    7c70:	48 8b 4c 08 30       	mov    0x30(%rax,%rcx,1),%rcx
    7c75:	ff 14 25 00 00 00 00 	callq  *0x0
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
}

static inline pte_t *pte_offset_kernel(pmd_t *pmd, unsigned long address)
{
	return (pte_t *)pmd_page_vaddr(*pmd) + pte_index(address);
    7c7c:	4c 8b 5d c0          	mov    -0x40(%rbp),%r11
	return (unsigned long)native_pmd_val(pmd) == 0;
}

static inline unsigned long pmd_page_vaddr(pmd_t pmd)
{
	return (unsigned long)__va(pmd_val(pmd) & PTE_PFN_MASK);
    7c80:	48 21 d0             	and    %rdx,%rax
	raw_spin_lock_init(&(_lock)->rlock);		\
} while (0)

static inline void spin_lock(spinlock_t *lock)
{
	raw_spin_lock(&lock->rlock);
    7c83:	48 89 cf             	mov    %rcx,%rdi
    7c86:	48 89 4d 98          	mov    %rcx,-0x68(%rbp)
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
}

static inline pte_t *pte_offset_kernel(pmd_t *pmd, unsigned long address)
{
	return (pte_t *)pmd_page_vaddr(*pmd) + pte_index(address);
    7c8a:	4e 8d 24 18          	lea    (%rax,%r11,1),%r12
    7c8e:	e8 00 00 00 00       	callq  7c93 <handle_mm_fault+0x4f3>
			page_table = pte_offset_map_lock(mm, pmd, address,
						&ptl);
			if(trigger_by_NCache(page_table)==1){
    7c93:	48 b8 00 00 00 00 80 	movabs $0xfff8000000000,%rax
    7c9a:	ff 0f 00 
    7c9d:	49 23 04 24          	and    (%r12),%rax
    7ca1:	48 ba 00 00 00 00 00 	movabs $0x4000000000000,%rdx
    7ca8:	00 04 00 
    7cab:	4c 8b 55 c8          	mov    -0x38(%rbp),%r10
    7caf:	48 39 d0             	cmp    %rdx,%rax
    7cb2:	0f 85 54 ff ff ff    	jne    7c0c <handle_mm_fault+0x46c>
				return	ret|_try_switch_NCache(mm,vma,address, page_table,pmd,ptl, orig_pte,invalid);
    7cb8:	4c 8b 4d 98          	mov    -0x68(%rbp),%r9
    7cbc:	49 8d bd 90 03 00 00 	lea    0x390(%r13),%rdi
    7cc3:	48 89 da             	mov    %rbx,%rdx
    7cc6:	4c 89 3c 24          	mov    %r15,(%rsp)
    7cca:	4d 89 d0             	mov    %r10,%r8
    7ccd:	4c 89 e1             	mov    %r12,%rcx
    7cd0:	4c 89 f6             	mov    %r14,%rsi
		}
		entry = pte_mkdirty(entry);
	}
	if(flags&FAULT_FLAG_RSVD){
		//		//printk("FAULT_FLAG_RSVD,lock=%lx\n",ptl);
		return handle_double_cache_pte_fault(mm, vma, address,
    7cd3:	31 db                	xor    %ebx,%ebx
			//	printk(KERN_DEBUG"triggerd by isolation\n");
			ret=__do_copy_on_read(mm,vma,address, page_table,pmd,ptl, orig_pte);
			page_table = pte_offset_map_lock(mm, pmd, address,
						&ptl);
			if(trigger_by_NCache(page_table)==1){
				return	ret|_try_switch_NCache(mm,vma,address, page_table,pmd,ptl, orig_pte,invalid);
    7cd5:	e8 f6 f4 ff ff       	callq  71d0 <_try_switch_NCache.isra.69>
    7cda:	e9 b6 fd ff ff       	jmpq   7a95 <handle_mm_fault+0x2f5>
				if(ret&VM_FAULT_OOM)  
				  printk("bad in do_linear_fault");
				return ret;
				}
			}
			ret= do_anonymous_page(mm, vma, address,
    7cdf:	48 89 da             	mov    %rbx,%rdx
    7ce2:	45 89 e0             	mov    %r12d,%r8d
    7ce5:	4c 89 d1             	mov    %r10,%rcx
    7ce8:	4c 89 f6             	mov    %r14,%rsi
    7ceb:	4c 89 ef             	mov    %r13,%rdi
    7cee:	e8 bd 8e ff ff       	callq  bb0 <do_anonymous_page.isra.47>
						pte, pmd, flags);
			if(ret&VM_FAULT_OOM)  
    7cf3:	a8 01                	test   $0x1,%al
				if(ret&VM_FAULT_OOM)  
				  printk("bad in do_linear_fault");
				return ret;
				}
			}
			ret= do_anonymous_page(mm, vma, address,
    7cf5:	89 c3                	mov    %eax,%ebx
						pte, pmd, flags);
			if(ret&VM_FAULT_OOM)  
    7cf7:	0f 84 98 fd ff ff    	je     7a95 <handle_mm_fault+0x2f5>
			  printk("bad in do_anon_fault");  
    7cfd:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
    7d04:	31 c0                	xor    %eax,%eax
    7d06:	e8 00 00 00 00       	callq  7d0b <handle_mm_fault+0x56b>
    7d0b:	e9 85 fd ff ff       	jmpq   7a95 <handle_mm_fault+0x2f5>
    7d10:	49 8d bd 90 03 00 00 	lea    0x390(%r13),%rdi

			}
		}
		if(trigger_by_NCache(page_table)==1){
			////printk("triggerd by NCache\n");
			return	_try_switch_NCache(mm,vma,address, page_table,pmd,ptl, orig_pte,invalid);
    7d17:	48 89 d1             	mov    %rdx,%rcx
    7d1a:	4c 89 3c 24          	mov    %r15,(%rsp)
    7d1e:	48 89 da             	mov    %rbx,%rdx
    7d21:	4d 89 d0             	mov    %r10,%r8
    7d24:	4c 89 f6             	mov    %r14,%rsi
    7d27:	e8 a4 f4 ff ff       	callq  71d0 <_try_switch_NCache.isra.69>
		}
		entry = pte_mkdirty(entry);
	}
	if(flags&FAULT_FLAG_RSVD){
		//		//printk("FAULT_FLAG_RSVD,lock=%lx\n",ptl);
		return handle_double_cache_pte_fault(mm, vma, address,
    7d2c:	31 db                	xor    %ebx,%ebx
    7d2e:	e9 62 fd ff ff       	jmpq   7a95 <handle_mm_fault+0x2f5>
 * Remove it when 4level-fixup.h has been removed.
 */
#if defined(CONFIG_MMU) && !defined(__ARCH_HAS_4LEVEL_HACK)
static inline pud_t *pud_alloc(struct mm_struct *mm, pgd_t *pgd, unsigned long address)
{
	return (unlikely(pgd_none(*pgd)) && __pud_alloc(mm, pgd, address))?
    7d33:	48 89 ce             	mov    %rcx,%rsi
    7d36:	48 89 da             	mov    %rbx,%rdx
    7d39:	4c 89 ef             	mov    %r13,%rdi
    7d3c:	48 89 4d c8          	mov    %rcx,-0x38(%rbp)
    7d40:	e8 00 00 00 00       	callq  7d45 <handle_mm_fault+0x5a5>
    7d45:	85 c0                	test   %eax,%eax
    7d47:	48 8b 4d c8          	mov    -0x38(%rbp),%rcx
    7d4b:	0f 85 3f fd ff ff    	jne    7a90 <handle_mm_fault+0x2f0>
    7d51:	48 8b 39             	mov    (%rcx),%rdi
    7d54:	e9 d8 fa ff ff       	jmpq   7831 <handle_mm_fault+0x91>
		NULL: pud_offset(pgd, address);
}

static inline pmd_t *pmd_alloc(struct mm_struct *mm, pud_t *pud, unsigned long address)
{
	return (unlikely(pud_none(*pud)) && __pmd_alloc(mm, pud, address))?
    7d59:	48 89 da             	mov    %rbx,%rdx
    7d5c:	48 89 ce             	mov    %rcx,%rsi
    7d5f:	4c 89 ef             	mov    %r13,%rdi
    7d62:	48 89 4d c8          	mov    %rcx,-0x38(%rbp)
    7d66:	e8 00 00 00 00       	callq  7d6b <handle_mm_fault+0x5cb>
    7d6b:	85 c0                	test   %eax,%eax
    7d6d:	0f 85 1d fd ff ff    	jne    7a90 <handle_mm_fault+0x2f0>
    7d73:	48 8b 4d c8          	mov    -0x38(%rbp),%rcx
    7d77:	48 8b 39             	mov    (%rcx),%rdi
    7d7a:	e9 f8 fa ff ff       	jmpq   7877 <handle_mm_fault+0xd7>
	 * Use __pte_alloc instead of pte_alloc_map, because we can't
	 * run pte_offset_map on the pmd, if an huge pmd could
	 * materialize from under us from a different thread.
	 */
	if (unlikely(pmd_none(*pmd)) &&
				unlikely(__pte_alloc(mm, vma, pmd, address)))
    7d7f:	48 89 d9             	mov    %rbx,%rcx
    7d82:	4c 89 d2             	mov    %r10,%rdx
    7d85:	4c 89 f6             	mov    %r14,%rsi
    7d88:	4c 89 ef             	mov    %r13,%rdi
    7d8b:	4c 89 55 c8          	mov    %r10,-0x38(%rbp)
    7d8f:	e8 00 00 00 00       	callq  7d94 <handle_mm_fault+0x5f4>
	/*
	 * Use __pte_alloc instead of pte_alloc_map, because we can't
	 * run pte_offset_map on the pmd, if an huge pmd could
	 * materialize from under us from a different thread.
	 */
	if (unlikely(pmd_none(*pmd)) &&
    7d94:	85 c0                	test   %eax,%eax
    7d96:	0f 85 f4 fc ff ff    	jne    7a90 <handle_mm_fault+0x2f0>
    7d9c:	4c 8b 55 c8          	mov    -0x38(%rbp),%r10
    7da0:	49 8b 3a             	mov    (%r10),%rdi
    7da3:	e9 15 fb ff ff       	jmpq   78bd <handle_mm_fault+0x11d>
}

static inline void mem_cgroup_oom_disable(void)
{
	WARN_ON(!current->memcg_oom.may_oom);
    7da8:	be 94 00 00 00       	mov    $0x94,%esi
    7dad:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
    7db4:	e8 00 00 00 00       	callq  7db9 <handle_mm_fault+0x619>
    7db9:	e9 28 fc ff ff       	jmpq   79e6 <handle_mm_fault+0x246>
extern void mem_cgroup_replace_page_cache(struct page *oldpage,
					struct page *newpage);

static inline void mem_cgroup_oom_enable(void)
{
	WARN_ON(current->memcg_oom.may_oom);
    7dbe:	be 8e 00 00 00       	mov    $0x8e,%esi
    7dc3:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
    7dca:	e8 00 00 00 00       	callq  7dcf <handle_mm_fault+0x62f>
    7dcf:	e9 f2 fc ff ff       	jmpq   7ac6 <handle_mm_fault+0x326>
    7dd4:	66 66 66 2e 0f 1f 84 	data32 data32 nopw %cs:0x0(%rax,%rax,1)
    7ddb:	00 00 00 00 00 

0000000000007de0 <__get_user_pages.part.72>:
 *
 * In most cases, get_user_pages or get_user_pages_fast should be used
 * instead of __get_user_pages. __get_user_pages should be used only if
 * you need some special @gup_flags.
 */
long __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,
    7de0:	e8 00 00 00 00       	callq  7de5 <__get_user_pages.part.72+0x5>
    7de5:	55                   	push   %rbp

	/* 
	 * Require read or write permissions.
	 * If FOLL_FORCE is set, we only require the "MAY" flags.
	 */
	vm_flags  = (gup_flags & FOLL_WRITE) ?
    7de6:	44 89 c0             	mov    %r8d,%eax
    7de9:	83 e0 01             	and    $0x1,%eax
 *
 * In most cases, get_user_pages or get_user_pages_fast should be used
 * instead of __get_user_pages. __get_user_pages should be used only if
 * you need some special @gup_flags.
 */
long __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,
    7dec:	48 89 e5             	mov    %rsp,%rbp
    7def:	41 57                	push   %r15
    7df1:	41 56                	push   %r14
    7df3:	49 89 d6             	mov    %rdx,%r14
    7df6:	44 89 c2             	mov    %r8d,%edx
    7df9:	41 55                	push   %r13
    7dfb:	49 89 fd             	mov    %rdi,%r13
    7dfe:	41 54                	push   %r12
    7e00:	53                   	push   %rbx
    7e01:	48 83 ec 58          	sub    $0x58,%rsp

	/* 
	 * Require read or write permissions.
	 * If FOLL_FORCE is set, we only require the "MAY" flags.
	 */
	vm_flags  = (gup_flags & FOLL_WRITE) ?
    7e05:	83 f8 01             	cmp    $0x1,%eax
 *
 * In most cases, get_user_pages or get_user_pages_fast should be used
 * instead of __get_user_pages. __get_user_pages should be used only if
 * you need some special @gup_flags.
 */
long __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,
    7e08:	48 89 75 c8          	mov    %rsi,-0x38(%rbp)

	/* 
	 * Require read or write permissions.
	 * If FOLL_FORCE is set, we only require the "MAY" flags.
	 */
	vm_flags  = (gup_flags & FOLL_WRITE) ?
    7e0c:	48 19 c0             	sbb    %rax,%rax
 *
 * In most cases, get_user_pages or get_user_pages_fast should be used
 * instead of __get_user_pages. __get_user_pages should be used only if
 * you need some special @gup_flags.
 */
long __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,
    7e0f:	48 89 4d 98          	mov    %rcx,-0x68(%rbp)
    7e13:	44 89 45 c4          	mov    %r8d,-0x3c(%rbp)

	/* 
	 * Require read or write permissions.
	 * If FOLL_FORCE is set, we only require the "MAY" flags.
	 */
	vm_flags  = (gup_flags & FOLL_WRITE) ?
    7e17:	48 83 e0 ef          	and    $0xffffffffffffffef,%rax
 *
 * In most cases, get_user_pages or get_user_pages_fast should be used
 * instead of __get_user_pages. __get_user_pages should be used only if
 * you need some special @gup_flags.
 */
long __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,
    7e1b:	4c 89 4d a0          	mov    %r9,-0x60(%rbp)
    7e1f:	4c 8b 7d 18          	mov    0x18(%rbp),%r15

	/* 
	 * Require read or write permissions.
	 * If FOLL_FORCE is set, we only require the "MAY" flags.
	 */
	vm_flags  = (gup_flags & FOLL_WRITE) ?
    7e23:	48 83 c0 22          	add    $0x22,%rax
		(VM_WRITE | VM_MAYWRITE) : (VM_READ | VM_MAYREAD);
	vm_flags &= (gup_flags & FOLL_FORCE) ?
    7e27:	48 89 c6             	mov    %rax,%rsi
    7e2a:	83 e6 30             	and    $0x30,%esi
    7e2d:	83 e2 10             	and    $0x10,%edx
    7e30:	48 89 75 b8          	mov    %rsi,-0x48(%rbp)
    7e34:	75 0e                	jne    7e44 <__get_user_pages.part.72+0x64>
	 * _PAGE_NUMA and _PAGE_PROTNONE are sharing the same pte/pmd
	 * bitflag. So to avoid that, don't set FOLL_NUMA if
	 * FOLL_FORCE is set.
	 */
	if (!(gup_flags & FOLL_FORCE))
	  gup_flags |= FOLL_NUMA;
    7e36:	81 4d c4 00 02 00 00 	orl    $0x200,-0x3c(%rbp)
	 * Require read or write permissions.
	 * If FOLL_FORCE is set, we only require the "MAY" flags.
	 */
	vm_flags  = (gup_flags & FOLL_WRITE) ?
		(VM_WRITE | VM_MAYWRITE) : (VM_READ | VM_MAYREAD);
	vm_flags &= (gup_flags & FOLL_FORCE) ?
    7e3d:	83 e0 03             	and    $0x3,%eax
    7e40:	48 89 45 b8          	mov    %rax,-0x48(%rbp)
    7e44:	65 48 8b 04 25 00 00 	mov    %gs:0x0,%rax
    7e4b:	00 00 
    7e4d:	65 48 8b 3c 25 00 00 	mov    %gs:0x0,%rdi
    7e54:	00 00 
	 * FOLL_FORCE is set.
	 */
	if (!(gup_flags & FOLL_FORCE))
	  gup_flags |= FOLL_NUMA;

	i = 0;
    7e56:	48 c7 45 b0 00 00 00 	movq   $0x0,-0x50(%rbp)
    7e5d:	00 
    7e5e:	48 89 45 a8          	mov    %rax,-0x58(%rbp)
    7e62:	48 89 7d 90          	mov    %rdi,-0x70(%rbp)
    7e66:	48 89 45 88          	mov    %rax,-0x78(%rbp)

	do {
		struct vm_area_struct *vma;

		vma = find_extend_vma(mm, start);
    7e6a:	48 8b 7d c8          	mov    -0x38(%rbp),%rdi
    7e6e:	4c 89 f6             	mov    %r14,%rsi
    7e71:	e8 00 00 00 00       	callq  7e76 <__get_user_pages.part.72+0x96>
		if (!vma && in_gate_area(mm, start)) {
    7e76:	48 85 c0             	test   %rax,%rax
	i = 0;

	do {
		struct vm_area_struct *vma;

		vma = find_extend_vma(mm, start);
    7e79:	49 89 c4             	mov    %rax,%r12
		if (!vma && in_gate_area(mm, start)) {
    7e7c:	0f 84 20 02 00 00    	je     80a2 <__get_user_pages.part.72+0x2c2>
			page_mask = 0;
			goto next_page;
		}

		if (!vma ||
					(vma->vm_flags & (VM_IO | VM_PFNMAP)) ||
    7e82:	48 8b 40 50          	mov    0x50(%rax),%rax
			pte_unmap(pte);
			page_mask = 0;
			goto next_page;
		}

		if (!vma ||
    7e86:	f6 c4 44             	test   $0x44,%ah
    7e89:	0f 85 fa 01 00 00    	jne    8089 <__get_user_pages.part.72+0x2a9>
					(vma->vm_flags & (VM_IO | VM_PFNMAP)) ||
    7e8f:	48 85 45 b8          	test   %rax,-0x48(%rbp)
    7e93:	0f 84 f0 01 00 00    	je     8089 <__get_user_pages.part.72+0x2a9>
	return test_and_clear_ti_thread_flag(task_thread_info(tsk), flag);
}

static inline int test_tsk_thread_flag(struct task_struct *tsk, int flag)
{
	return test_ti_thread_flag(task_thread_info(tsk), flag);
    7e99:	48 8b 45 a8          	mov    -0x58(%rbp),%rax
    7e9d:	48 8b 40 08          	mov    0x8(%rax),%rax
}

static __always_inline int constant_test_bit(long nr, const volatile unsigned long *addr)
{
	return ((1UL << (nr & (BITS_PER_LONG-1))) &
		(addr[nr >> _BITOPS_LONG_SHIFT])) != 0;
    7ea1:	48 8b 40 10          	mov    0x10(%rax),%rax
	return unlikely(sigismember(&p->pending.signal, SIGKILL));
}

static inline int fatal_signal_pending(struct task_struct *p)
{
	return signal_pending(p) && __fatal_signal_pending(p);
    7ea5:	a8 04                	test   $0x4,%al
    7ea7:	0f 85 f1 03 00 00    	jne    829e <__get_user_pages.part.72+0x4be>
			 * pages and potentially allocating memory.
			 */
			if (unlikely(fatal_signal_pending(current)))
			  return i ? i : -ERESTARTSYS;

			cond_resched();
    7ead:	e8 00 00 00 00       	callq  7eb2 <__get_user_pages.part.72+0xd2>
    7eb2:	8b 5d c4             	mov    -0x3c(%rbp),%ebx
    7eb5:	eb 31                	jmp    7ee8 <__get_user_pages.part.72+0x108>
    7eb7:	66 0f 1f 84 00 00 00 	nopw   0x0(%rax,%rax,1)
    7ebe:	00 00 
					BUG();
				}

				if (tsk) {
					if (ret & VM_FAULT_MAJOR)
					  tsk->maj_flt++;
    7ec0:	49 83 85 70 04 00 00 	addq   $0x1,0x470(%r13)
    7ec7:	01 
					else
					  tsk->min_flt++;
				}

				if (ret & VM_FAULT_RETRY) {
    7ec8:	f6 c4 04             	test   $0x4,%ah
    7ecb:	0f 85 9e 00 00 00    	jne    7f6f <__get_user_pages.part.72+0x18f>
				 * in some cases userspace may also be wanting
				 * to write to the gotten user page, which a
				 * read fault here might prevent (a readonly
				 * page might get reCOWed by userspace write).
				 */
				if ((ret & VM_FAULT_WRITE) &&
    7ed1:	a8 08                	test   $0x8,%al
    7ed3:	74 0e                	je     7ee3 <__get_user_pages.part.72+0x103>
							!(vma->vm_flags & VM_WRITE))
				  foll_flags &= ~FOLL_WRITE;
    7ed5:	89 d8                	mov    %ebx,%eax
    7ed7:	83 e0 fe             	and    $0xfffffffe,%eax
    7eda:	41 f6 44 24 50 02    	testb  $0x2,0x50(%r12)
    7ee0:	0f 44 d8             	cmove  %eax,%ebx

				cond_resched();
    7ee3:	e8 00 00 00 00       	callq  7ee8 <__get_user_pages.part.72+0x108>
			 */
			if (unlikely(fatal_signal_pending(current)))
			  return i ? i : -ERESTARTSYS;

			cond_resched();
			while (!(page = follow_page_mask(vma, start,
    7ee8:	48 8d 4d d4          	lea    -0x2c(%rbp),%rcx
    7eec:	89 da                	mov    %ebx,%edx
    7eee:	4c 89 f6             	mov    %r14,%rsi
    7ef1:	4c 89 e7             	mov    %r12,%rdi
    7ef4:	e8 00 00 00 00       	callq  7ef9 <__get_user_pages.part.72+0x119>
    7ef9:	48 85 c0             	test   %rax,%rax
    7efc:	0f 85 e6 00 00 00    	jne    7fe8 <__get_user_pages.part.72+0x208>
								foll_flags, &page_mask))) {
				int ret;
				unsigned int fault_flags = 0;

				/* For mlock, just skip the stack guard page. */
				if (foll_flags & FOLL_MLOCK) {
    7f02:	f6 c3 40             	test   $0x40,%bl
    7f05:	74 11                	je     7f18 <__get_user_pages.part.72+0x138>

static inline int stack_guard_page_start(struct vm_area_struct *vma,
					     unsigned long addr)
{
	return (vma->vm_flags & VM_GROWSDOWN) &&
		(vma->vm_start == addr) &&
    7f07:	41 f6 44 24 51 01    	testb  $0x1,0x51(%r12)
    7f0d:	74 09                	je     7f18 <__get_user_pages.part.72+0x138>
}

static inline int stack_guard_page_start(struct vm_area_struct *vma,
					     unsigned long addr)
{
	return (vma->vm_flags & VM_GROWSDOWN) &&
    7f0f:	4d 3b 34 24          	cmp    (%r12),%r14
    7f13:	74 7b                	je     7f90 <__get_user_pages.part.72+0x1b0>
    7f15:	0f 1f 00             	nopl   (%rax)
					if (stack_guard_page(vma, start))
					  goto next_page;
				}
				if (foll_flags & FOLL_WRITE)
    7f18:	89 d9                	mov    %ebx,%ecx
				if (nonblocking)
				  fault_flags |= FAULT_FLAG_ALLOW_RETRY;
				if (foll_flags & FOLL_NOWAIT)
				  fault_flags |= (FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_RETRY_NOWAIT);

				ret = handle_mm_fault(mm, vma, start,
    7f1a:	48 8b 7d c8          	mov    -0x38(%rbp),%rdi
    7f1e:	4c 89 f2             	mov    %r14,%rdx
				/* For mlock, just skip the stack guard page. */
				if (foll_flags & FOLL_MLOCK) {
					if (stack_guard_page(vma, start))
					  goto next_page;
				}
				if (foll_flags & FOLL_WRITE)
    7f21:	83 e1 01             	and    $0x1,%ecx
				if (nonblocking)
				  fault_flags |= FAULT_FLAG_ALLOW_RETRY;
				if (foll_flags & FOLL_NOWAIT)
				  fault_flags |= (FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_RETRY_NOWAIT);

				ret = handle_mm_fault(mm, vma, start,
    7f24:	4c 89 e6             	mov    %r12,%rsi
					  goto next_page;
				}
				if (foll_flags & FOLL_WRITE)
				  fault_flags |= FAULT_FLAG_WRITE;
				if (nonblocking)
				  fault_flags |= FAULT_FLAG_ALLOW_RETRY;
    7f27:	89 c8                	mov    %ecx,%eax
    7f29:	83 c8 08             	or     $0x8,%eax
    7f2c:	4d 85 ff             	test   %r15,%r15
    7f2f:	0f 45 c8             	cmovne %eax,%ecx
				if (foll_flags & FOLL_NOWAIT)
				  fault_flags |= (FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_RETRY_NOWAIT);
    7f32:	89 c8                	mov    %ecx,%eax
    7f34:	83 c8 18             	or     $0x18,%eax
    7f37:	f6 c3 20             	test   $0x20,%bl
    7f3a:	0f 45 c8             	cmovne %eax,%ecx

				ret = handle_mm_fault(mm, vma, start,
    7f3d:	e8 00 00 00 00       	callq  7f42 <__get_user_pages.part.72+0x162>
							fault_flags);

				if (ret & VM_FAULT_ERROR) {
    7f42:	a9 33 08 00 00       	test   $0x833,%eax
    7f47:	0f 85 c3 00 00 00    	jne    8010 <__get_user_pages.part.72+0x230>
					if (ret & VM_FAULT_SIGBUS)
					  return i ? i : -EFAULT;
					BUG();
				}

				if (tsk) {
    7f4d:	4d 85 ed             	test   %r13,%r13
    7f50:	0f 84 72 ff ff ff    	je     7ec8 <__get_user_pages.part.72+0xe8>
					if (ret & VM_FAULT_MAJOR)
    7f56:	a8 04                	test   $0x4,%al
    7f58:	0f 85 62 ff ff ff    	jne    7ec0 <__get_user_pages.part.72+0xe0>
					  tsk->maj_flt++;
					else
					  tsk->min_flt++;
    7f5e:	49 83 85 68 04 00 00 	addq   $0x1,0x468(%r13)
    7f65:	01 
				}

				if (ret & VM_FAULT_RETRY) {
    7f66:	f6 c4 04             	test   $0x4,%ah
    7f69:	0f 84 62 ff ff ff    	je     7ed1 <__get_user_pages.part.72+0xf1>
					if (nonblocking)
    7f6f:	4d 85 ff             	test   %r15,%r15
    7f72:	74 69                	je     7fdd <__get_user_pages.part.72+0x1fd>
					  *nonblocking = 0;
    7f74:	48 8b 45 b0          	mov    -0x50(%rbp),%rax
    7f78:	41 c7 07 00 00 00 00 	movl   $0x0,(%r15)
			start += page_increm * PAGE_SIZE;
			nr_pages -= page_increm;
		} while (nr_pages && start < vma->vm_end);
	} while (nr_pages);
	return i;
}
    7f7f:	48 83 c4 58          	add    $0x58,%rsp
    7f83:	5b                   	pop    %rbx
    7f84:	41 5c                	pop    %r12
    7f86:	41 5d                	pop    %r13
    7f88:	41 5e                	pop    %r14
    7f8a:	41 5f                	pop    %r15
    7f8c:	5d                   	pop    %rbp
    7f8d:	c3                   	retq   
    7f8e:	66 90                	xchg   %ax,%ax
		(vma->vm_start == addr) &&
		!vma_growsdown(vma->vm_prev, addr);
    7f90:	49 8b 44 24 18       	mov    0x18(%r12),%rax
int clear_page_dirty_for_io(struct page *page);

/* Is the vma a continuation of the stack vma above it? */
static inline int vma_growsdown(struct vm_area_struct *vma, unsigned long addr)
{
	return vma && (vma->vm_end == addr) && (vma->vm_flags & VM_GROWSDOWN);
    7f95:	48 85 c0             	test   %rax,%rax
    7f98:	74 0a                	je     7fa4 <__get_user_pages.part.72+0x1c4>
    7f9a:	4c 3b 70 08          	cmp    0x8(%rax),%r14
    7f9e:	0f 84 a4 00 00 00    	je     8048 <__get_user_pages.part.72+0x268>
				flush_anon_page(vma, page, start);
				flush_dcache_page(page);
				page_mask = 0;
			}
next_page:
			if (vmas) {
    7fa4:	48 83 7d 10 00       	cmpq   $0x0,0x10(%rbp)
    7fa9:	0f 84 51 02 00 00    	je     8200 <__get_user_pages.part.72+0x420>
				vmas[i] = vma;
    7faf:	48 8b 45 10          	mov    0x10(%rbp),%rax
    7fb3:	48 8b 7d b0          	mov    -0x50(%rbp),%rdi
				page_mask = 0;
    7fb7:	ba 01 00 00 00       	mov    $0x1,%edx
    7fbc:	c7 45 d4 00 00 00 00 	movl   $0x0,-0x2c(%rbp)
				flush_dcache_page(page);
				page_mask = 0;
			}
next_page:
			if (vmas) {
				vmas[i] = vma;
    7fc3:	4c 89 24 f8          	mov    %r12,(%rax,%rdi,8)
				page_mask = 0;
    7fc7:	b8 01 00 00 00       	mov    $0x1,%eax
			}
			page_increm = 1 + (~(start >> PAGE_SHIFT) & page_mask);
			if (page_increm > nr_pages)
    7fcc:	48 39 45 98          	cmp    %rax,-0x68(%rbp)
    7fd0:	0f 83 58 02 00 00    	jae    822e <__get_user_pages.part.72+0x44e>
    7fd6:	8b 45 98             	mov    -0x68(%rbp),%eax
			  page_increm = nr_pages;
			i += page_increm;
    7fd9:	48 01 45 b0          	add    %rax,-0x50(%rbp)
					else
					  tsk->min_flt++;
				}

				if (ret & VM_FAULT_RETRY) {
					if (nonblocking)
    7fdd:	48 8b 45 b0          	mov    -0x50(%rbp),%rax
    7fe1:	eb 9c                	jmp    7f7f <__get_user_pages.part.72+0x19f>
    7fe3:	0f 1f 44 00 00       	nopl   0x0(%rax,%rax,1)
							!(vma->vm_flags & VM_WRITE))
				  foll_flags &= ~FOLL_WRITE;

				cond_resched();
			}
			if (IS_ERR(page))
    7fe8:	48 3d 00 f0 ff ff    	cmp    $0xfffffffffffff000,%rax
    7fee:	0f 87 d4 02 00 00    	ja     82c8 <__get_user_pages.part.72+0x4e8>
			  return i ? i : PTR_ERR(page);
			if (pages) {
    7ff4:	48 8b 7d a0          	mov    -0x60(%rbp),%rdi
    7ff8:	48 85 ff             	test   %rdi,%rdi
    7ffb:	74 a7                	je     7fa4 <__get_user_pages.part.72+0x1c4>
				pages[i] = page;
    7ffd:	48 8b 75 b0          	mov    -0x50(%rbp),%rsi
    8001:	48 89 04 f7          	mov    %rax,(%rdi,%rsi,8)

				flush_anon_page(vma, page, start);
				flush_dcache_page(page);
				page_mask = 0;
    8005:	c7 45 d4 00 00 00 00 	movl   $0x0,-0x2c(%rbp)
    800c:	eb 96                	jmp    7fa4 <__get_user_pages.part.72+0x1c4>
    800e:	66 90                	xchg   %ax,%ax

				ret = handle_mm_fault(mm, vma, start,
							fault_flags);

				if (ret & VM_FAULT_ERROR) {
					if (ret & VM_FAULT_OOM)
    8010:	a8 01                	test   $0x1,%al
    8012:	75 4c                	jne    8060 <__get_user_pages.part.72+0x280>
					  return i ? i : -ENOMEM;
					if (ret & (VM_FAULT_HWPOISON |
    8014:	a8 30                	test   $0x30,%al
    8016:	74 69                	je     8081 <__get_user_pages.part.72+0x2a1>
									VM_FAULT_HWPOISON_LARGE)) {
						if (i)
    8018:	48 8b 45 b0          	mov    -0x50(%rbp),%rax
    801c:	48 85 c0             	test   %rax,%rax
    801f:	0f 85 5a ff ff ff    	jne    7f7f <__get_user_pages.part.72+0x19f>
						  return i;
						else if (gup_flags & FOLL_HWPOISON)
    8025:	8b 45 c4             	mov    -0x3c(%rbp),%eax
    8028:	25 00 01 00 00       	and    $0x100,%eax
						  return -EHWPOISON;
    802d:	83 f8 01             	cmp    $0x1,%eax
    8030:	48 19 c0             	sbb    %rax,%rax
    8033:	83 e0 77             	and    $0x77,%eax
    8036:	48 2d 85 00 00 00    	sub    $0x85,%rax
    803c:	e9 3e ff ff ff       	jmpq   7f7f <__get_user_pages.part.72+0x19f>
    8041:	0f 1f 80 00 00 00 00 	nopl   0x0(%rax)
    8048:	f6 40 51 01          	testb  $0x1,0x51(%rax)
    804c:	0f 84 52 ff ff ff    	je     7fa4 <__get_user_pages.part.72+0x1c4>
    8052:	e9 c1 fe ff ff       	jmpq   7f18 <__get_user_pages.part.72+0x138>
    8057:	66 0f 1f 84 00 00 00 	nopw   0x0(%rax,%rax,1)
    805e:	00 00 
				ret = handle_mm_fault(mm, vma, start,
							fault_flags);

				if (ret & VM_FAULT_ERROR) {
					if (ret & VM_FAULT_OOM)
					  return i ? i : -ENOMEM;
    8060:	48 8b 75 b0          	mov    -0x50(%rbp),%rsi
    8064:	48 c7 c0 f4 ff ff ff 	mov    $0xfffffffffffffff4,%rax
    806b:	48 85 f6             	test   %rsi,%rsi
    806e:	48 0f 45 c6          	cmovne %rsi,%rax
			start += page_increm * PAGE_SIZE;
			nr_pages -= page_increm;
		} while (nr_pages && start < vma->vm_end);
	} while (nr_pages);
	return i;
}
    8072:	48 83 c4 58          	add    $0x58,%rsp
    8076:	5b                   	pop    %rbx
    8077:	41 5c                	pop    %r12
    8079:	41 5d                	pop    %r13
    807b:	41 5e                	pop    %r14
    807d:	41 5f                	pop    %r15
    807f:	5d                   	pop    %rbp
    8080:	c3                   	retq   
						else if (gup_flags & FOLL_HWPOISON)
						  return -EHWPOISON;
						else
						  return -EFAULT;
					}
					if (ret & VM_FAULT_SIGBUS)
    8081:	a8 02                	test   $0x2,%al
    8083:	0f 84 3d 02 00 00    	je     82c6 <__get_user_pages.part.72+0x4e6>
			BUG_ON(pgd_none(*pgd));
			pud = pud_offset(pgd, pg);
			BUG_ON(pud_none(*pud));
			pmd = pmd_offset(pud, pg);
			if (pmd_none(*pmd))
			  return i ? : -EFAULT;
    8089:	48 8b 45 b0          	mov    -0x50(%rbp),%rax
    808d:	48 85 c0             	test   %rax,%rax
    8090:	0f 85 e9 fe ff ff    	jne    7f7f <__get_user_pages.part.72+0x19f>
    8096:	48 c7 c0 f2 ff ff ff 	mov    $0xfffffffffffffff2,%rax
    809d:	e9 dd fe ff ff       	jmpq   7f7f <__get_user_pages.part.72+0x19f>

	do {
		struct vm_area_struct *vma;

		vma = find_extend_vma(mm, start);
		if (!vma && in_gate_area(mm, start)) {
    80a2:	48 8b 7d c8          	mov    -0x38(%rbp),%rdi
    80a6:	4c 89 f6             	mov    %r14,%rsi
    80a9:	e8 00 00 00 00       	callq  80ae <__get_user_pages.part.72+0x2ce>
    80ae:	85 c0                	test   %eax,%eax
    80b0:	74 d7                	je     8089 <__get_user_pages.part.72+0x2a9>
			unsigned long pg = start & PAGE_MASK;
    80b2:	4c 89 f2             	mov    %r14,%rdx
    80b5:	48 81 e2 00 f0 ff ff 	and    $0xfffffffffffff000,%rdx
			pud_t *pud;
			pmd_t *pmd;
			pte_t *pte;

			/* user gate pages are read-only */
			if (gup_flags & FOLL_WRITE)
    80bc:	f6 45 c4 01          	testb  $0x1,-0x3c(%rbp)
    80c0:	75 c7                	jne    8089 <__get_user_pages.part.72+0x2a9>
    80c2:	48 8b 45 90          	mov    -0x70(%rbp),%rax
    80c6:	48 8b 88 38 e0 ff ff 	mov    -0x1fc8(%rax),%rcx
			  return i ? : -EFAULT;
			if (pg > TASK_SIZE)
    80cd:	48 b8 00 f0 ff ff ff 	movabs $0x7ffffffff000,%rax
    80d4:	7f 00 00 
    80d7:	f7 c1 00 00 00 20    	test   $0x20000000,%ecx
    80dd:	74 19                	je     80f8 <__get_user_pages.part.72+0x318>
    80df:	48 8b 7d 88          	mov    -0x78(%rbp),%rdi
    80e3:	b8 00 00 00 c0       	mov    $0xc0000000,%eax
    80e8:	b9 00 e0 ff ff       	mov    $0xffffe000,%ecx
    80ed:	f6 87 df 02 00 00 08 	testb  $0x8,0x2df(%rdi)
    80f4:	48 0f 44 c1          	cmove  %rcx,%rax
    80f8:	48 39 c2             	cmp    %rax,%rdx
    80fb:	0f 86 15 01 00 00    	jbe    8216 <__get_user_pages.part.72+0x436>
			  pgd = pgd_offset_k(pg);
    8101:	48 89 d0             	mov    %rdx,%rax
    8104:	48 c1 e8 24          	shr    $0x24,%rax
    8108:	25 f8 0f 00 00       	and    $0xff8,%eax
    810d:	48 03 05 00 00 00 00 	add    0x0(%rip),%rax        # 8114 <__get_user_pages.part.72+0x334>
    8114:	48 8b 38             	mov    (%rax),%rdi
			else
			  pgd = pgd_offset_gate(mm, pg);
			BUG_ON(pgd_none(*pgd));
    8117:	48 85 ff             	test   %rdi,%rdi
    811a:	0f 84 d7 01 00 00    	je     82f7 <__get_user_pages.part.72+0x517>

	if (sizeof(pgdval_t) > sizeof(long))
		ret =  PVOP_CALLEE2(pgdval_t, pv_mmu_ops.pgd_val,
				    pgd.pgd, (u64)pgd.pgd >> 32);
	else
		ret =  PVOP_CALLEE1(pgdval_t, pv_mmu_ops.pgd_val,
    8120:	ff 14 25 00 00 00 00 	callq  *0x0
#define pgd_page(pgd)		pfn_to_page(pgd_val(pgd) >> PAGE_SHIFT)

/* to find an entry in a page-table-directory. */
static inline unsigned long pud_index(unsigned long address)
{
	return (address >> PUD_SHIFT) & (PTRS_PER_PUD - 1);
    8127:	48 89 d1             	mov    %rdx,%rcx
	return pgd_flags(pgd) & _PAGE_PRESENT;
}

static inline unsigned long pgd_page_vaddr(pgd_t pgd)
{
	return (unsigned long)__va((unsigned long)pgd_val(pgd) & PTE_PFN_MASK);
    812a:	48 bb 00 f0 ff ff ff 	movabs $0x3ffffffff000,%rbx
    8131:	3f 00 00 
#define pgd_page(pgd)		pfn_to_page(pgd_val(pgd) >> PAGE_SHIFT)

/* to find an entry in a page-table-directory. */
static inline unsigned long pud_index(unsigned long address)
{
	return (address >> PUD_SHIFT) & (PTRS_PER_PUD - 1);
    8134:	48 c1 e9 1b          	shr    $0x1b,%rcx
	return pgd_flags(pgd) & _PAGE_PRESENT;
}

static inline unsigned long pgd_page_vaddr(pgd_t pgd)
{
	return (unsigned long)__va((unsigned long)pgd_val(pgd) & PTE_PFN_MASK);
    8138:	48 21 d8             	and    %rbx,%rax
    813b:	81 e1 f8 0f 00 00    	and    $0xff8,%ecx
    8141:	48 01 c8             	add    %rcx,%rax
    8144:	48 b9 00 00 00 00 00 	movabs $0xffff880000000000,%rcx
    814b:	88 ff ff 
    814e:	48 8b 3c 08          	mov    (%rax,%rcx,1),%rdi
			pud = pud_offset(pgd, pg);
			BUG_ON(pud_none(*pud));
    8152:	48 85 ff             	test   %rdi,%rdi
    8155:	0f 84 7d 01 00 00    	je     82d8 <__get_user_pages.part.72+0x4f8>

	if (sizeof(pudval_t) > sizeof(long))
		ret =  PVOP_CALLEE2(pudval_t, pv_mmu_ops.pud_val,
				    pud.pud, (u64)pud.pud >> 32);
	else
		ret =  PVOP_CALLEE1(pudval_t, pv_mmu_ops.pud_val,
    815b:	ff 14 25 00 00 00 00 	callq  *0x0
 * this macro returns the index of the entry in the pmd page which would
 * control the given virtual address
 */
static inline unsigned long pmd_index(unsigned long address)
{
	return (address >> PMD_SHIFT) & (PTRS_PER_PMD - 1);
    8162:	48 89 d6             	mov    %rdx,%rsi
	return pud_flags(pud) & _PAGE_PRESENT;
}

static inline unsigned long pud_page_vaddr(pud_t pud)
{
	return (unsigned long)__va((unsigned long)pud_val(pud) & PTE_PFN_MASK);
    8165:	48 21 d8             	and    %rbx,%rax
 * this macro returns the index of the entry in the pmd page which would
 * control the given virtual address
 */
static inline unsigned long pmd_index(unsigned long address)
{
	return (address >> PMD_SHIFT) & (PTRS_PER_PMD - 1);
    8168:	48 c1 ee 12          	shr    $0x12,%rsi
    816c:	81 e6 f8 0f 00 00    	and    $0xff8,%esi
    8172:	48 01 f0             	add    %rsi,%rax
    8175:	48 8b 3c 08          	mov    (%rax,%rcx,1),%rdi
			pmd = pmd_offset(pud, pg);
			if (pmd_none(*pmd))
    8179:	48 85 ff             	test   %rdi,%rdi
    817c:	0f 84 07 ff ff ff    	je     8089 <__get_user_pages.part.72+0x2a9>

	if (sizeof(pmdval_t) > sizeof(long))
		ret =  PVOP_CALLEE2(pmdval_t, pv_mmu_ops.pmd_val,
				    pmd.pmd, (u64)pmd.pmd >> 32);
	else
		ret =  PVOP_CALLEE1(pmdval_t, pv_mmu_ops.pmd_val,
    8182:	ff 14 25 00 00 00 00 	callq  *0x0
 * this function returns the index of the entry in the pte page which would
 * control the given virtual address
 */
static inline unsigned long pte_index(unsigned long address)
{
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
    8189:	48 c1 ea 09          	shr    $0x9,%rdx
	return (unsigned long)native_pmd_val(pmd) == 0;
}

static inline unsigned long pmd_page_vaddr(pmd_t pmd)
{
	return (unsigned long)__va(pmd_val(pmd) & PTE_PFN_MASK);
    818d:	48 21 d8             	and    %rbx,%rax
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
}

static inline pte_t *pte_offset_kernel(pmd_t *pmd, unsigned long address)
{
	return (pte_t *)pmd_page_vaddr(*pmd) + pte_index(address);
    8190:	81 e2 f8 0f 00 00    	and    $0xff8,%edx
	return (unsigned long)native_pmd_val(pmd) == 0;
}

static inline unsigned long pmd_page_vaddr(pmd_t pmd)
{
	return (unsigned long)__va(pmd_val(pmd) & PTE_PFN_MASK);
    8196:	48 01 d1             	add    %rdx,%rcx
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
}

static inline pte_t *pte_offset_kernel(pmd_t *pmd, unsigned long address)
{
	return (pte_t *)pmd_page_vaddr(*pmd) + pte_index(address);
    8199:	48 01 c1             	add    %rax,%rcx
			  return i ? : -EFAULT;
			VM_BUG_ON(pmd_trans_huge(*pmd));
			pte = pte_offset_map(pmd, pg);
			if (pte_none(*pte)) {
    819c:	48 83 39 00          	cmpq   $0x0,(%rcx)
    81a0:	0f 84 e3 fe ff ff    	je     8089 <__get_user_pages.part.72+0x2a9>
				pte_unmap(pte);
				return i ? : -EFAULT;
			}
			vma = get_gate_vma(mm);
    81a6:	48 8b 7d c8          	mov    -0x38(%rbp),%rdi
    81aa:	48 89 4d 80          	mov    %rcx,-0x80(%rbp)
    81ae:	e8 00 00 00 00       	callq  81b3 <__get_user_pages.part.72+0x3d3>
			if (pages) {
    81b3:	48 83 7d a0 00       	cmpq   $0x0,-0x60(%rbp)
			pte = pte_offset_map(pmd, pg);
			if (pte_none(*pte)) {
				pte_unmap(pte);
				return i ? : -EFAULT;
			}
			vma = get_gate_vma(mm);
    81b8:	49 89 c4             	mov    %rax,%r12
			if (pages) {
    81bb:	0f 84 44 fe ff ff    	je     8005 <__get_user_pages.part.72+0x225>
				struct page *page;

				page = vm_normal_page(vma, start, *pte);
    81c1:	48 8b 4d 80          	mov    -0x80(%rbp),%rcx
    81c5:	4c 89 f6             	mov    %r14,%rsi
    81c8:	48 89 c7             	mov    %rax,%rdi
    81cb:	48 8b 11             	mov    (%rcx),%rdx
    81ce:	e8 00 00 00 00       	callq  81d3 <__get_user_pages.part.72+0x3f3>
				if (!page) {
    81d3:	48 85 c0             	test   %rax,%rax
			}
			vma = get_gate_vma(mm);
			if (pages) {
				struct page *page;

				page = vm_normal_page(vma, start, *pte);
    81d6:	48 89 c2             	mov    %rax,%rdx
				if (!page) {
    81d9:	48 8b 4d 80          	mov    -0x80(%rbp),%rcx
    81dd:	74 74                	je     8253 <__get_user_pages.part.72+0x473>
					else {
						pte_unmap(pte);
						return i ? : -EFAULT;
					}
				}
				pages[i] = page;
    81df:	48 8b 45 a0          	mov    -0x60(%rbp),%rax
    81e3:	48 8b 7d b0          	mov    -0x50(%rbp),%rdi
    81e7:	48 89 14 f8          	mov    %rdx,(%rax,%rdi,8)
    81eb:	48 8b 02             	mov    (%rdx),%rax
void get_huge_page_tail(struct page *page);/*ziqiao*/
extern bool __get_page_tail(struct page *page);

static inline void get_page(struct page *page)
{
	if (unlikely(PageTail(page)))
    81ee:	f6 c4 80             	test   $0x80,%ah
    81f1:	0f 85 e3 00 00 00    	jne    82da <__get_user_pages.part.72+0x4fa>
    81f7:	f0 ff 42 1c          	lock incl 0x1c(%rdx)
    81fb:	e9 05 fe ff ff       	jmpq   8005 <__get_user_pages.part.72+0x225>
    8200:	4c 89 f2             	mov    %r14,%rdx
    8203:	48 c1 ea 0c          	shr    $0xc,%rdx
    8207:	f7 d2                	not    %edx
    8209:	23 55 d4             	and    -0x2c(%rbp),%edx
    820c:	83 c2 01             	add    $0x1,%edx
    820f:	89 d0                	mov    %edx,%eax
    8211:	e9 b6 fd ff ff       	jmpq   7fcc <__get_user_pages.part.72+0x1ec>
			if (gup_flags & FOLL_WRITE)
			  return i ? : -EFAULT;
			if (pg > TASK_SIZE)
			  pgd = pgd_offset_k(pg);
			else
			  pgd = pgd_offset_gate(mm, pg);
    8216:	48 8b 45 c8          	mov    -0x38(%rbp),%rax
    821a:	48 89 d1             	mov    %rdx,%rcx
    821d:	48 c1 e9 27          	shr    $0x27,%rcx
    8221:	48 8b 40 40          	mov    0x40(%rax),%rax
    8225:	48 8d 04 c8          	lea    (%rax,%rcx,8),%rax
    8229:	e9 e6 fe ff ff       	jmpq   8114 <__get_user_pages.part.72+0x334>
				page_mask = 0;
			}
			page_increm = 1 + (~(start >> PAGE_SHIFT) & page_mask);
			if (page_increm > nr_pages)
			  page_increm = nr_pages;
			i += page_increm;
    822e:	48 01 55 b0          	add    %rdx,-0x50(%rbp)
			start += page_increm * PAGE_SIZE;
			nr_pages -= page_increm;
		} while (nr_pages && start < vma->vm_end);
    8232:	48 29 45 98          	sub    %rax,-0x68(%rbp)
    8236:	0f 84 a1 fd ff ff    	je     7fdd <__get_user_pages.part.72+0x1fd>
			}
			page_increm = 1 + (~(start >> PAGE_SHIFT) & page_mask);
			if (page_increm > nr_pages)
			  page_increm = nr_pages;
			i += page_increm;
			start += page_increm * PAGE_SIZE;
    823c:	48 c1 e0 0c          	shl    $0xc,%rax
    8240:	49 01 c6             	add    %rax,%r14
			nr_pages -= page_increm;
		} while (nr_pages && start < vma->vm_end);
    8243:	4d 3b 74 24 08       	cmp    0x8(%r12),%r14
    8248:	0f 82 4b fc ff ff    	jb     7e99 <__get_user_pages.part.72+0xb9>
    824e:	e9 17 fc ff ff       	jmpq   7e6a <__get_user_pages.part.72+0x8a>
			if (pages) {
				struct page *page;

				page = vm_normal_page(vma, start, *pte);
				if (!page) {
					if (!(gup_flags & FOLL_DUMP) &&
    8253:	f6 45 c4 08          	testb  $0x8,-0x3c(%rbp)
    8257:	0f 85 2c fe ff ff    	jne    8089 <__get_user_pages.part.72+0x2a9>

	if (sizeof(pteval_t) > sizeof(long))
		ret = PVOP_CALLEE2(pteval_t, pv_mmu_ops.pte_val,
				   pte.pte, (u64)pte.pte >> 32);
	else
		ret = PVOP_CALLEE1(pteval_t, pv_mmu_ops.pte_val,
    825d:	48 8b 39             	mov    (%rcx),%rdi
    8260:	ff 14 25 00 00 00 00 	callq  *0x0
	return pte_flags(pte) & _PAGE_SPECIAL;
}

static inline unsigned long pte_pfn(pte_t pte)
{
	return (pte_val(pte) & PTE_PFN_MASK) >> PAGE_SHIFT;
    8267:	48 21 d8             	and    %rbx,%rax
    826a:	48 c1 e8 0c          	shr    $0xc,%rax
    826e:	48 3b 05 00 00 00 00 	cmp    0x0(%rip),%rax        # 8275 <__get_user_pages.part.72+0x495>
    8275:	0f 85 0e fe ff ff    	jne    8089 <__get_user_pages.part.72+0x2a9>
    827b:	48 8b 39             	mov    (%rcx),%rdi
    827e:	ff 14 25 00 00 00 00 	callq  *0x0
    8285:	48 21 d8             	and    %rbx,%rax
								is_zero_pfn(pte_pfn(*pte)))
					  page = pte_page(*pte);
    8288:	48 ba 00 00 00 00 00 	movabs $0xffffea0000000000,%rdx
    828f:	ea ff ff 
    8292:	48 c1 e8 06          	shr    $0x6,%rax
    8296:	48 01 c2             	add    %rax,%rdx
    8299:	e9 41 ff ff ff       	jmpq   81df <__get_user_pages.part.72+0x3ff>
    829e:	48 8b 45 a8          	mov    -0x58(%rbp),%rax
    82a2:	f6 80 01 06 00 00 01 	testb  $0x1,0x601(%rax)
    82a9:	0f 84 fe fb ff ff    	je     7ead <__get_user_pages.part.72+0xcd>
			/*
			 * If we have a pending SIGKILL, don't keep faulting
			 * pages and potentially allocating memory.
			 */
			if (unlikely(fatal_signal_pending(current)))
			  return i ? i : -ERESTARTSYS;
    82af:	48 8b 7d b0          	mov    -0x50(%rbp),%rdi
    82b3:	48 c7 c0 00 fe ff ff 	mov    $0xfffffffffffffe00,%rax
    82ba:	48 85 ff             	test   %rdi,%rdi
    82bd:	48 0f 45 c7          	cmovne %rdi,%rax
    82c1:	e9 b9 fc ff ff       	jmpq   7f7f <__get_user_pages.part.72+0x19f>
						else
						  return -EFAULT;
					}
					if (ret & VM_FAULT_SIGBUS)
					  return i ? i : -EFAULT;
					BUG();
    82c6:	0f 0b                	ud2    
	return (void *) error;
}

static inline long __must_check PTR_ERR(__force const void *ptr)
{
	return (long) ptr;
    82c8:	48 8b 7d b0          	mov    -0x50(%rbp),%rdi
    82cc:	48 85 ff             	test   %rdi,%rdi
    82cf:	48 0f 45 c7          	cmovne %rdi,%rax
    82d3:	e9 a7 fc ff ff       	jmpq   7f7f <__get_user_pages.part.72+0x19f>
			  pgd = pgd_offset_k(pg);
			else
			  pgd = pgd_offset_gate(mm, pg);
			BUG_ON(pgd_none(*pgd));
			pud = pud_offset(pgd, pg);
			BUG_ON(pud_none(*pud));
    82d8:	0f 0b                	ud2    
		if (likely(__get_page_tail(page)))
    82da:	48 89 d7             	mov    %rdx,%rdi
    82dd:	48 89 55 80          	mov    %rdx,-0x80(%rbp)
    82e1:	e8 00 00 00 00       	callq  82e6 <__get_user_pages.part.72+0x506>
    82e6:	84 c0                	test   %al,%al
    82e8:	48 8b 55 80          	mov    -0x80(%rbp),%rdx
    82ec:	0f 85 13 fd ff ff    	jne    8005 <__get_user_pages.part.72+0x225>
    82f2:	e9 00 ff ff ff       	jmpq   81f7 <__get_user_pages.part.72+0x417>
			  return i ? : -EFAULT;
			if (pg > TASK_SIZE)
			  pgd = pgd_offset_k(pg);
			else
			  pgd = pgd_offset_gate(mm, pg);
			BUG_ON(pgd_none(*pgd));
    82f7:	0f 0b                	ud2    
    82f9:	0f 1f 80 00 00 00 00 	nopl   0x0(%rax)

0000000000008300 <__get_user_pages>:
 */
long __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,
			unsigned long start, unsigned long nr_pages,
			unsigned int gup_flags, struct page **pages,
			struct vm_area_struct **vmas, int *nonblocking)
{
    8300:	e8 00 00 00 00       	callq  8305 <__get_user_pages+0x5>
	long i;
	unsigned long vm_flags;
	unsigned int page_mask;

	if (!nr_pages)
	  return 0;
    8305:	31 c0                	xor    %eax,%eax
{
	long i;
	unsigned long vm_flags;
	unsigned int page_mask;

	if (!nr_pages)
    8307:	48 85 c9             	test   %rcx,%rcx
    830a:	74 1f                	je     832b <__get_user_pages+0x2b>
 */
long __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,
			unsigned long start, unsigned long nr_pages,
			unsigned int gup_flags, struct page **pages,
			struct vm_area_struct **vmas, int *nonblocking)
{
    830c:	55                   	push   %rbp
    830d:	48 89 e5             	mov    %rsp,%rbp
    8310:	48 83 ec 10          	sub    $0x10,%rsp
    8314:	48 8b 45 18          	mov    0x18(%rbp),%rax
    8318:	48 89 44 24 08       	mov    %rax,0x8(%rsp)
    831d:	48 8b 45 10          	mov    0x10(%rbp),%rax
    8321:	48 89 04 24          	mov    %rax,(%rsp)
    8325:	e8 b6 fa ff ff       	callq  7de0 <__get_user_pages.part.72>
			start += page_increm * PAGE_SIZE;
			nr_pages -= page_increm;
		} while (nr_pages && start < vma->vm_end);
	} while (nr_pages);
	return i;
}
    832a:	c9                   	leaveq 
    832b:	f3 c3                	repz retq 
    832d:	0f 1f 00             	nopl   (%rax)

0000000000008330 <get_user_pages>:
 * See also get_user_pages_fast, for performance critical applications.
 */
long get_user_pages(struct task_struct *tsk, struct mm_struct *mm,
			unsigned long start, unsigned long nr_pages, int write,
			int force, struct page **pages, struct vm_area_struct **vmas)
{
    8330:	e8 00 00 00 00       	callq  8335 <get_user_pages+0x5>
    8335:	55                   	push   %rbp
    8336:	48 89 e5             	mov    %rsp,%rbp
    8339:	48 83 ec 10          	sub    $0x10,%rsp
    833d:	4c 8b 5d 10          	mov    0x10(%rbp),%r11
	int flags = FOLL_TOUCH;
    8341:	49 83 fb 01          	cmp    $0x1,%r11
    8345:	45 19 d2             	sbb    %r10d,%r10d
    8348:	41 83 e2 fc          	and    $0xfffffffc,%r10d
    834c:	41 83 c2 06          	add    $0x6,%r10d

	if (pages)
	  flags |= FOLL_GET;
	if (write)
	  flags |= FOLL_WRITE;
    8350:	44 89 d0             	mov    %r10d,%eax
    8353:	83 c8 01             	or     $0x1,%eax
    8356:	45 85 c0             	test   %r8d,%r8d
    8359:	44 0f 45 d0          	cmovne %eax,%r10d
	if (force)
	  flags |= FOLL_FORCE;
    835d:	44 89 d0             	mov    %r10d,%eax
    8360:	83 c8 10             	or     $0x10,%eax
    8363:	45 85 c9             	test   %r9d,%r9d
    8366:	44 0f 45 d0          	cmovne %eax,%r10d
	long i;
	unsigned long vm_flags;
	unsigned int page_mask;

	if (!nr_pages)
	  return 0;
    836a:	31 c0                	xor    %eax,%eax
{
	long i;
	unsigned long vm_flags;
	unsigned int page_mask;

	if (!nr_pages)
    836c:	48 85 c9             	test   %rcx,%rcx
    836f:	75 07                	jne    8378 <get_user_pages+0x48>
	if (force)
	  flags |= FOLL_FORCE;

	return __get_user_pages(tsk, mm, start, nr_pages, flags, pages, vmas,
				NULL);
}
    8371:	c9                   	leaveq 
    8372:	c3                   	retq   
    8373:	0f 1f 44 00 00       	nopl   0x0(%rax,%rax,1)
    8378:	48 8b 45 18          	mov    0x18(%rbp),%rax
    837c:	48 c7 44 24 08 00 00 	movq   $0x0,0x8(%rsp)
    8383:	00 00 
    8385:	4d 89 d9             	mov    %r11,%r9
    8388:	45 89 d0             	mov    %r10d,%r8d
    838b:	48 89 04 24          	mov    %rax,(%rsp)
    838f:	e8 4c fa ff ff       	callq  7de0 <__get_user_pages.part.72>
    8394:	c9                   	leaveq 
    8395:	c3                   	retq   
    8396:	66 2e 0f 1f 84 00 00 	nopw   %cs:0x0(%rax,%rax,1)
    839d:	00 00 00 

00000000000083a0 <__access_remote_vm>:
 * Access another process' address space as given in mm.  If non-NULL, use the
 * given task for page fault accounting.
 */
static int __access_remote_vm(struct task_struct *tsk, struct mm_struct *mm,
			unsigned long addr, void *buf, int len, int write)
{
    83a0:	e8 00 00 00 00       	callq  83a5 <__access_remote_vm+0x5>
    83a5:	55                   	push   %rbp
    83a6:	48 89 f0             	mov    %rsi,%rax
	struct vm_area_struct *vma;
	void *old_buf = buf;

	down_read(&mm->mmap_sem);
    83a9:	48 83 c0 78          	add    $0x78,%rax
 * Access another process' address space as given in mm.  If non-NULL, use the
 * given task for page fault accounting.
 */
static int __access_remote_vm(struct task_struct *tsk, struct mm_struct *mm,
			unsigned long addr, void *buf, int len, int write)
{
    83ad:	48 89 e5             	mov    %rsp,%rbp
    83b0:	41 57                	push   %r15
    83b2:	45 89 c7             	mov    %r8d,%r15d
    83b5:	41 56                	push   %r14
    83b7:	41 55                	push   %r13
    83b9:	49 89 cd             	mov    %rcx,%r13
    83bc:	41 54                	push   %r12
    83be:	53                   	push   %rbx
    83bf:	48 89 d3             	mov    %rdx,%rbx
    83c2:	48 83 ec 48          	sub    $0x48,%rsp
    83c6:	48 89 7d b0          	mov    %rdi,-0x50(%rbp)
	struct vm_area_struct *vma;
	void *old_buf = buf;

	down_read(&mm->mmap_sem);
    83ca:	48 89 c7             	mov    %rax,%rdi
 * Access another process' address space as given in mm.  If non-NULL, use the
 * given task for page fault accounting.
 */
static int __access_remote_vm(struct task_struct *tsk, struct mm_struct *mm,
			unsigned long addr, void *buf, int len, int write)
{
    83cd:	48 89 75 b8          	mov    %rsi,-0x48(%rbp)
    83d1:	48 89 4d a8          	mov    %rcx,-0x58(%rbp)
    83d5:	44 89 4d c4          	mov    %r9d,-0x3c(%rbp)
	struct vm_area_struct *vma;
	void *old_buf = buf;

	down_read(&mm->mmap_sem);
    83d9:	48 89 45 a0          	mov    %rax,-0x60(%rbp)
    83dd:	e8 00 00 00 00       	callq  83e2 <__access_remote_vm+0x42>
	/* ignore errors, just check how much was successfully transferred */
	while (len) {
    83e2:	45 85 ff             	test   %r15d,%r15d
    83e5:	75 3c                	jne    8423 <__access_remote_vm+0x83>
    83e7:	e9 6b 01 00 00       	jmpq   8557 <__access_remote_vm+0x1b7>
    83ec:	0f 1f 40 00          	nopl   0x0(%rax)
			if (bytes > PAGE_SIZE-offset)
			  bytes = PAGE_SIZE-offset;

			maddr = kmap(page);
			if (write) {
				copy_to_user_page(vma, page, addr,
    83f0:	48 8d 3c 10          	lea    (%rax,%rdx,1),%rdi
    83f4:	4c 89 ee             	mov    %r13,%rsi
    83f7:	4c 89 e2             	mov    %r12,%rdx
    83fa:	e8 00 00 00 00       	callq  83ff <__access_remote_vm+0x5f>
							maddr + offset, buf, bytes);
				set_page_dirty_lock(page);
    83ff:	48 8b 7d d0          	mov    -0x30(%rbp),%rdi
    8403:	e8 00 00 00 00       	callq  8408 <__access_remote_vm+0x68>
    8408:	48 8b 7d d0          	mov    -0x30(%rbp),%rdi
			} else {
				copy_from_user_page(vma, page, addr,
							buf, maddr + offset, bytes);
			}
			kunmap(page);
			page_cache_release(page);
    840c:	45 89 f7             	mov    %r14d,%r15d
		}
		len -= bytes;
		buf += bytes;
    840f:	4d 01 e5             	add    %r12,%r13
		addr += bytes;
    8412:	4c 01 e3             	add    %r12,%rbx
			} else {
				copy_from_user_page(vma, page, addr,
							buf, maddr + offset, bytes);
			}
			kunmap(page);
			page_cache_release(page);
    8415:	e8 00 00 00 00       	callq  841a <__access_remote_vm+0x7a>
	struct vm_area_struct *vma;
	void *old_buf = buf;

	down_read(&mm->mmap_sem);
	/* ignore errors, just check how much was successfully transferred */
	while (len) {
    841a:	45 85 ff             	test   %r15d,%r15d
    841d:	0f 84 15 01 00 00    	je     8538 <__access_remote_vm+0x198>
		int bytes, ret, offset;
		void *maddr;
		struct page *page = NULL;

		ret = get_user_pages(tsk, mm, addr, 1,
    8423:	48 8d 45 c8          	lea    -0x38(%rbp),%rax
    8427:	44 8b 45 c4          	mov    -0x3c(%rbp),%r8d
    842b:	48 8b 75 b8          	mov    -0x48(%rbp),%rsi
    842f:	48 8b 7d b0          	mov    -0x50(%rbp),%rdi
    8433:	41 b9 01 00 00 00    	mov    $0x1,%r9d
    8439:	b9 01 00 00 00       	mov    $0x1,%ecx
    843e:	48 89 44 24 08       	mov    %rax,0x8(%rsp)
    8443:	48 8d 45 d0          	lea    -0x30(%rbp),%rax
    8447:	48 89 da             	mov    %rbx,%rdx
	down_read(&mm->mmap_sem);
	/* ignore errors, just check how much was successfully transferred */
	while (len) {
		int bytes, ret, offset;
		void *maddr;
		struct page *page = NULL;
    844a:	48 c7 45 d0 00 00 00 	movq   $0x0,-0x30(%rbp)
    8451:	00 

		ret = get_user_pages(tsk, mm, addr, 1,
    8452:	48 89 04 24          	mov    %rax,(%rsp)
    8456:	e8 00 00 00 00       	callq  845b <__access_remote_vm+0xbb>
					write, 1, &page, &vma);
		if (ret <= 0) {
    845b:	85 c0                	test   %eax,%eax
    845d:	7e 79                	jle    84d8 <__access_remote_vm+0x138>
#endif
			  break;
			bytes = ret;
		} else {
			bytes = len;
			offset = addr & (PAGE_SIZE-1);
    845f:	89 d8                	mov    %ebx,%eax
			if (bytes > PAGE_SIZE-offset)
    8461:	b9 00 10 00 00       	mov    $0x1000,%ecx
    8466:	4d 63 e7             	movslq %r15d,%r12
#endif
			  break;
			bytes = ret;
		} else {
			bytes = len;
			offset = addr & (PAGE_SIZE-1);
    8469:	25 ff 0f 00 00       	and    $0xfff,%eax
    846e:	45 31 f6             	xor    %r14d,%r14d
			if (bytes > PAGE_SIZE-offset)
    8471:	48 63 d0             	movslq %eax,%rdx
    8474:	48 29 d1             	sub    %rdx,%rcx
    8477:	49 39 cc             	cmp    %rcx,%r12
    847a:	76 10                	jbe    848c <__access_remote_vm+0xec>
			  bytes = PAGE_SIZE-offset;
    847c:	be 00 10 00 00       	mov    $0x1000,%esi
    8481:	45 89 fe             	mov    %r15d,%r14d
    8484:	29 c6                	sub    %eax,%esi
    8486:	41 89 f4             	mov    %esi,%r12d
    8489:	41 29 f6             	sub    %esi,%r14d

			maddr = kmap(page);
    848c:	4c 8b 7d d0          	mov    -0x30(%rbp),%r15
 */
#include <linux/vmstat.h>

static __always_inline void *lowmem_page_address(const struct page *page)
{
	return __va(PFN_PHYS(page_to_pfn(page)));
    8490:	48 b8 00 00 00 00 00 	movabs $0x160000000000,%rax
    8497:	16 00 00 
			if (write) {
    849a:	8b 4d c4             	mov    -0x3c(%rbp),%ecx
    849d:	48 be 00 00 00 00 00 	movabs $0xffff880000000000,%rsi
    84a4:	88 ff ff 
    84a7:	4c 01 f8             	add    %r15,%rax
    84aa:	48 c1 f8 06          	sar    $0x6,%rax
    84ae:	48 c1 e0 0c          	shl    $0xc,%rax
    84b2:	48 01 f0             	add    %rsi,%rax
    84b5:	85 c9                	test   %ecx,%ecx
    84b7:	0f 85 33 ff ff ff    	jne    83f0 <__access_remote_vm+0x50>
				copy_to_user_page(vma, page, addr,
							maddr + offset, buf, bytes);
				set_page_dirty_lock(page);
			} else {
				copy_from_user_page(vma, page, addr,
    84bd:	48 8d 34 10          	lea    (%rax,%rdx,1),%rsi
    84c1:	4c 89 ef             	mov    %r13,%rdi
    84c4:	4c 89 e2             	mov    %r12,%rdx
    84c7:	e8 00 00 00 00       	callq  84cc <__access_remote_vm+0x12c>
    84cc:	4c 89 ff             	mov    %r15,%rdi
    84cf:	e9 38 ff ff ff       	jmpq   840c <__access_remote_vm+0x6c>
    84d4:	0f 1f 40 00          	nopl   0x0(%rax)
			/*
			 * Check if this is a VM_IO | VM_PFNMAP VMA, which
			 * we can access using slightly different code.
			 */
#ifdef CONFIG_HAVE_IOREMAP_PROT
			vma = find_vma(mm, addr);
    84d8:	48 8b 7d b8          	mov    -0x48(%rbp),%rdi
    84dc:	48 89 de             	mov    %rbx,%rsi
    84df:	e8 00 00 00 00       	callq  84e4 <__access_remote_vm+0x144>
			if (!vma || vma->vm_start > addr)
    84e4:	48 85 c0             	test   %rax,%rax
			/*
			 * Check if this is a VM_IO | VM_PFNMAP VMA, which
			 * we can access using slightly different code.
			 */
#ifdef CONFIG_HAVE_IOREMAP_PROT
			vma = find_vma(mm, addr);
    84e7:	48 89 45 c8          	mov    %rax,-0x38(%rbp)
			if (!vma || vma->vm_start > addr)
    84eb:	74 4b                	je     8538 <__access_remote_vm+0x198>
    84ed:	48 39 18             	cmp    %rbx,(%rax)
    84f0:	77 46                	ja     8538 <__access_remote_vm+0x198>
			  break;
			if (vma->vm_ops && vma->vm_ops->access)
    84f2:	48 8b 90 98 00 00 00 	mov    0x98(%rax),%rdx
    84f9:	48 85 d2             	test   %rdx,%rdx
    84fc:	74 3a                	je     8538 <__access_remote_vm+0x198>
    84fe:	4c 8b 4a 20          	mov    0x20(%rdx),%r9
    8502:	4d 85 c9             	test   %r9,%r9
    8505:	74 31                	je     8538 <__access_remote_vm+0x198>
			  ret = vma->vm_ops->access(vma, addr, buf,
    8507:	44 8b 45 c4          	mov    -0x3c(%rbp),%r8d
    850b:	44 89 f9             	mov    %r15d,%ecx
    850e:	4c 89 ea             	mov    %r13,%rdx
    8511:	48 89 de             	mov    %rbx,%rsi
    8514:	48 89 c7             	mov    %rax,%rdi
    8517:	41 ff d1             	callq  *%r9
						  len, write);
			if (ret <= 0)
    851a:	85 c0                	test   %eax,%eax
    851c:	7e 1a                	jle    8538 <__access_remote_vm+0x198>
    851e:	4c 63 e0             	movslq %eax,%r12
    8521:	41 29 c7             	sub    %eax,%r15d
			}
			kunmap(page);
			page_cache_release(page);
		}
		len -= bytes;
		buf += bytes;
    8524:	4d 01 e5             	add    %r12,%r13
		addr += bytes;
    8527:	4c 01 e3             	add    %r12,%rbx
	struct vm_area_struct *vma;
	void *old_buf = buf;

	down_read(&mm->mmap_sem);
	/* ignore errors, just check how much was successfully transferred */
	while (len) {
    852a:	45 85 ff             	test   %r15d,%r15d
    852d:	0f 85 f0 fe ff ff    	jne    8423 <__access_remote_vm+0x83>
    8533:	0f 1f 44 00 00       	nopl   0x0(%rax,%rax,1)
    8538:	44 2b 6d a8          	sub    -0x58(%rbp),%r13d
		}
		len -= bytes;
		buf += bytes;
		addr += bytes;
	}
	up_read(&mm->mmap_sem);
    853c:	48 8b 7d a0          	mov    -0x60(%rbp),%rdi
    8540:	e8 00 00 00 00       	callq  8545 <__access_remote_vm+0x1a5>

	return buf - old_buf;
}
    8545:	48 83 c4 48          	add    $0x48,%rsp
    8549:	44 89 e8             	mov    %r13d,%eax
    854c:	5b                   	pop    %rbx
    854d:	41 5c                	pop    %r12
    854f:	41 5d                	pop    %r13
    8551:	41 5e                	pop    %r14
    8553:	41 5f                	pop    %r15
    8555:	5d                   	pop    %rbp
    8556:	c3                   	retq   
	struct vm_area_struct *vma;
	void *old_buf = buf;

	down_read(&mm->mmap_sem);
	/* ignore errors, just check how much was successfully transferred */
	while (len) {
    8557:	45 31 ed             	xor    %r13d,%r13d
    855a:	eb e0                	jmp    853c <__access_remote_vm+0x19c>
    855c:	0f 1f 40 00          	nopl   0x0(%rax)

0000000000008560 <access_remote_vm>:
 *
 * The caller must hold a reference on @mm.
 */
int access_remote_vm(struct mm_struct *mm, unsigned long addr,
			void *buf, int len, int write)
{
    8560:	e8 00 00 00 00       	callq  8565 <access_remote_vm+0x5>
    8565:	55                   	push   %rbp
	return __access_remote_vm(NULL, mm, addr, buf, len, write);
    8566:	45 89 c1             	mov    %r8d,%r9d
    8569:	41 89 c8             	mov    %ecx,%r8d
    856c:	48 89 d1             	mov    %rdx,%rcx
    856f:	48 89 f2             	mov    %rsi,%rdx
    8572:	48 89 fe             	mov    %rdi,%rsi
 *
 * The caller must hold a reference on @mm.
 */
int access_remote_vm(struct mm_struct *mm, unsigned long addr,
			void *buf, int len, int write)
{
    8575:	48 89 e5             	mov    %rsp,%rbp
	return __access_remote_vm(NULL, mm, addr, buf, len, write);
    8578:	31 ff                	xor    %edi,%edi
    857a:	e8 21 fe ff ff       	callq  83a0 <__access_remote_vm>
}
    857f:	5d                   	pop    %rbp
    8580:	c3                   	retq   
    8581:	66 66 66 66 66 66 2e 	data32 data32 data32 data32 data32 nopw %cs:0x0(%rax,%rax,1)
    8588:	0f 1f 84 00 00 00 00 
    858f:	00 

0000000000008590 <access_process_vm>:
 * Source/target buffer must be kernel space,
 * Do not walk the page table directly, use get_user_pages
 */
int access_process_vm(struct task_struct *tsk, unsigned long addr,
			void *buf, int len, int write)
{
    8590:	e8 00 00 00 00       	callq  8595 <access_process_vm+0x5>
    8595:	55                   	push   %rbp
    8596:	48 89 e5             	mov    %rsp,%rbp
    8599:	41 57                	push   %r15
    859b:	41 89 cf             	mov    %ecx,%r15d
    859e:	41 56                	push   %r14
    85a0:	49 89 d6             	mov    %rdx,%r14
    85a3:	41 55                	push   %r13
    85a5:	49 89 f5             	mov    %rsi,%r13
    85a8:	41 54                	push   %r12
    85aa:	49 89 fc             	mov    %rdi,%r12
    85ad:	53                   	push   %rbx
    85ae:	48 83 ec 08          	sub    $0x8,%rsp
    85b2:	44 89 45 d4          	mov    %r8d,-0x2c(%rbp)
	struct mm_struct *mm;
	int ret;

	mm = get_task_mm(tsk);
    85b6:	e8 00 00 00 00       	callq  85bb <access_process_vm+0x2b>
    85bb:	48 89 c3             	mov    %rax,%rbx
	if (!mm)
	  return 0;
    85be:	31 c0                	xor    %eax,%eax
{
	struct mm_struct *mm;
	int ret;

	mm = get_task_mm(tsk);
	if (!mm)
    85c0:	48 85 db             	test   %rbx,%rbx
    85c3:	74 29                	je     85ee <access_process_vm+0x5e>
	  return 0;

	ret = __access_remote_vm(tsk, mm, addr, buf, len, write);
    85c5:	44 8b 45 d4          	mov    -0x2c(%rbp),%r8d
    85c9:	4c 89 e7             	mov    %r12,%rdi
    85cc:	4c 89 f1             	mov    %r14,%rcx
    85cf:	4c 89 ea             	mov    %r13,%rdx
    85d2:	48 89 de             	mov    %rbx,%rsi
    85d5:	45 89 c1             	mov    %r8d,%r9d
    85d8:	45 89 f8             	mov    %r15d,%r8d
    85db:	e8 c0 fd ff ff       	callq  83a0 <__access_remote_vm>
	mmput(mm);
    85e0:	48 89 df             	mov    %rbx,%rdi

	mm = get_task_mm(tsk);
	if (!mm)
	  return 0;

	ret = __access_remote_vm(tsk, mm, addr, buf, len, write);
    85e3:	41 89 c4             	mov    %eax,%r12d
	mmput(mm);
    85e6:	e8 00 00 00 00       	callq  85eb <access_process_vm+0x5b>

	return ret;
    85eb:	44 89 e0             	mov    %r12d,%eax
}
    85ee:	48 83 c4 08          	add    $0x8,%rsp
    85f2:	5b                   	pop    %rbx
    85f3:	41 5c                	pop    %r12
    85f5:	41 5d                	pop    %r13
    85f7:	41 5e                	pop    %r14
    85f9:	41 5f                	pop    %r15
    85fb:	5d                   	pop    %rbp
    85fc:	c3                   	retq   
    85fd:	0f 1f 00             	nopl   (%rax)

0000000000008600 <get_dump_page>:
 *
 * Called without mmap_sem, but after all other threads have been killed.
 */
#ifdef CONFIG_ELF_CORE
struct page *get_dump_page(unsigned long addr)
{
    8600:	e8 00 00 00 00       	callq  8605 <get_dump_page+0x5>
    8605:	55                   	push   %rbp
    8606:	48 89 fa             	mov    %rdi,%rdx
    8609:	41 b8 1c 00 00 00    	mov    $0x1c,%r8d
    860f:	65 48 8b 3c 25 00 00 	mov    %gs:0x0,%rdi
    8616:	00 00 
    8618:	48 89 e5             	mov    %rsp,%rbp
    861b:	48 83 ec 20          	sub    $0x20,%rsp
    861f:	48 8b b7 a8 02 00 00 	mov    0x2a8(%rdi),%rsi
    8626:	48 8d 45 f0          	lea    -0x10(%rbp),%rax
    862a:	4c 8d 4d f8          	lea    -0x8(%rbp),%r9
    862e:	48 c7 44 24 08 00 00 	movq   $0x0,0x8(%rsp)
    8635:	00 00 
    8637:	b9 01 00 00 00       	mov    $0x1,%ecx
    863c:	48 89 04 24          	mov    %rax,(%rsp)
    8640:	e8 9b f7 ff ff       	callq  7de0 <__get_user_pages.part.72>
	struct vm_area_struct *vma;
	struct page *page;

	if (__get_user_pages(current, current->mm, addr, 1,
    8645:	48 85 c0             	test   %rax,%rax
    8648:	7e 06                	jle    8650 <get_dump_page+0x50>
					FOLL_FORCE | FOLL_DUMP | FOLL_GET, &page, &vma,
					NULL) < 1)
	  return NULL;
	flush_cache_page(vma, addr, page_to_pfn(page));
	return page;
    864a:	48 8b 45 f8          	mov    -0x8(%rbp),%rax
}
    864e:	c9                   	leaveq 
    864f:	c3                   	retq   
	struct page *page;

	if (__get_user_pages(current, current->mm, addr, 1,
					FOLL_FORCE | FOLL_DUMP | FOLL_GET, &page, &vma,
					NULL) < 1)
	  return NULL;
    8650:	31 c0                	xor    %eax,%eax
	flush_cache_page(vma, addr, page_to_pfn(page));
	return page;
}
    8652:	c9                   	leaveq 
    8653:	c3                   	retq   
    8654:	66 66 66 2e 0f 1f 84 	data32 data32 nopw %cs:0x0(%rax,%rax,1)
    865b:	00 00 00 00 00 

0000000000008660 <fixup_user_fault>:
 *
 * This should be called with the mm_sem held for read.
 */
int fixup_user_fault(struct task_struct *tsk, struct mm_struct *mm,
			unsigned long address, unsigned int fault_flags)
{
    8660:	e8 00 00 00 00       	callq  8665 <fixup_user_fault+0x5>
    8665:	55                   	push   %rbp
    8666:	48 89 e5             	mov    %rsp,%rbp
    8669:	41 56                	push   %r14
    866b:	41 89 ce             	mov    %ecx,%r14d
    866e:	41 55                	push   %r13
    8670:	49 89 f5             	mov    %rsi,%r13
	struct vm_area_struct *vma;
	vm_flags_t vm_flags;
	int ret;

	vma = find_extend_vma(mm, address);
    8673:	48 89 d6             	mov    %rdx,%rsi
 *
 * This should be called with the mm_sem held for read.
 */
int fixup_user_fault(struct task_struct *tsk, struct mm_struct *mm,
			unsigned long address, unsigned int fault_flags)
{
    8676:	41 54                	push   %r12
    8678:	49 89 fc             	mov    %rdi,%r12
	struct vm_area_struct *vma;
	vm_flags_t vm_flags;
	int ret;

	vma = find_extend_vma(mm, address);
    867b:	4c 89 ef             	mov    %r13,%rdi
 *
 * This should be called with the mm_sem held for read.
 */
int fixup_user_fault(struct task_struct *tsk, struct mm_struct *mm,
			unsigned long address, unsigned int fault_flags)
{
    867e:	53                   	push   %rbx
    867f:	48 89 d3             	mov    %rdx,%rbx
	struct vm_area_struct *vma;
	vm_flags_t vm_flags;
	int ret;

	vma = find_extend_vma(mm, address);
    8682:	e8 00 00 00 00       	callq  8687 <fixup_user_fault+0x27>
	if (!vma || address < vma->vm_start)
    8687:	48 85 c0             	test   %rax,%rax
    868a:	74 46                	je     86d2 <fixup_user_fault+0x72>
    868c:	48 3b 18             	cmp    (%rax),%rbx
    868f:	72 41                	jb     86d2 <fixup_user_fault+0x72>
	  return -EFAULT;

	vm_flags = (fault_flags & FAULT_FLAG_WRITE) ? VM_WRITE : VM_READ;
    8691:	44 89 f2             	mov    %r14d,%edx
    8694:	83 e2 01             	and    $0x1,%edx
    8697:	83 fa 01             	cmp    $0x1,%edx
    869a:	48 19 d2             	sbb    %rdx,%rdx
    869d:	48 83 c2 02          	add    $0x2,%rdx
	if (!(vm_flags & vma->vm_flags))
    86a1:	48 85 50 50          	test   %rdx,0x50(%rax)
    86a5:	74 2b                	je     86d2 <fixup_user_fault+0x72>
	  return -EFAULT;

	ret = handle_mm_fault(mm, vma, address, fault_flags);
    86a7:	48 89 da             	mov    %rbx,%rdx
    86aa:	44 89 f1             	mov    %r14d,%ecx
    86ad:	48 89 c6             	mov    %rax,%rsi
    86b0:	4c 89 ef             	mov    %r13,%rdi
    86b3:	e8 00 00 00 00       	callq  86b8 <fixup_user_fault+0x58>
    86b8:	89 c2                	mov    %eax,%edx
	if (ret & VM_FAULT_ERROR) {
    86ba:	25 33 08 00 00       	and    $0x833,%eax
    86bf:	74 1f                	je     86e0 <fixup_user_fault+0x80>
		if (ret & VM_FAULT_OOM)
    86c1:	f6 c2 01             	test   $0x1,%dl
    86c4:	75 52                	jne    8718 <fixup_user_fault+0xb8>
		  return -ENOMEM;
		if (ret & (VM_FAULT_HWPOISON | VM_FAULT_HWPOISON_LARGE))
    86c6:	f6 c2 30             	test   $0x30,%dl
    86c9:	75 55                	jne    8720 <fixup_user_fault+0xc0>
		  return -EHWPOISON;
		if (ret & VM_FAULT_SIGBUS)
    86cb:	83 e2 02             	and    $0x2,%edx
    86ce:	66 90                	xchg   %ax,%ax
    86d0:	74 40                	je     8712 <fixup_user_fault+0xb2>
	vm_flags_t vm_flags;
	int ret;

	vma = find_extend_vma(mm, address);
	if (!vma || address < vma->vm_start)
	  return -EFAULT;
    86d2:	b8 f2 ff ff ff       	mov    $0xfffffff2,%eax
		  tsk->maj_flt++;
		else
		  tsk->min_flt++;
	}
	return 0;
}
    86d7:	5b                   	pop    %rbx
    86d8:	41 5c                	pop    %r12
    86da:	41 5d                	pop    %r13
    86dc:	41 5e                	pop    %r14
    86de:	5d                   	pop    %rbp
    86df:	c3                   	retq   
		  return -EHWPOISON;
		if (ret & VM_FAULT_SIGBUS)
		  return -EFAULT;
		BUG();
	}
	if (tsk) {
    86e0:	4d 85 e4             	test   %r12,%r12
    86e3:	74 f2                	je     86d7 <fixup_user_fault+0x77>
		if (ret & VM_FAULT_MAJOR)
    86e5:	83 e2 04             	and    $0x4,%edx
    86e8:	75 16                	jne    8700 <fixup_user_fault+0xa0>
		  tsk->maj_flt++;
		else
		  tsk->min_flt++;
    86ea:	49 83 84 24 68 04 00 	addq   $0x1,0x468(%r12)
    86f1:	00 01 
	}
	return 0;
}
    86f3:	5b                   	pop    %rbx
    86f4:	41 5c                	pop    %r12
    86f6:	41 5d                	pop    %r13
    86f8:	41 5e                	pop    %r14
    86fa:	5d                   	pop    %rbp
    86fb:	c3                   	retq   
    86fc:	0f 1f 40 00          	nopl   0x0(%rax)
		  return -EFAULT;
		BUG();
	}
	if (tsk) {
		if (ret & VM_FAULT_MAJOR)
		  tsk->maj_flt++;
    8700:	49 83 84 24 70 04 00 	addq   $0x1,0x470(%r12)
    8707:	00 01 
		else
		  tsk->min_flt++;
	}
	return 0;
}
    8709:	5b                   	pop    %rbx
    870a:	41 5c                	pop    %r12
    870c:	41 5d                	pop    %r13
    870e:	41 5e                	pop    %r14
    8710:	5d                   	pop    %rbp
    8711:	c3                   	retq   
		  return -ENOMEM;
		if (ret & (VM_FAULT_HWPOISON | VM_FAULT_HWPOISON_LARGE))
		  return -EHWPOISON;
		if (ret & VM_FAULT_SIGBUS)
		  return -EFAULT;
		BUG();
    8712:	0f 0b                	ud2    
    8714:	0f 1f 40 00          	nopl   0x0(%rax)
	  return -EFAULT;

	ret = handle_mm_fault(mm, vma, address, fault_flags);
	if (ret & VM_FAULT_ERROR) {
		if (ret & VM_FAULT_OOM)
		  return -ENOMEM;
    8718:	b8 f4 ff ff ff       	mov    $0xfffffff4,%eax
    871d:	eb b8                	jmp    86d7 <fixup_user_fault+0x77>
    871f:	90                   	nop
		if (ret & (VM_FAULT_HWPOISON | VM_FAULT_HWPOISON_LARGE))
		  return -EHWPOISON;
    8720:	b8 7b ff ff ff       	mov    $0xffffff7b,%eax
    8725:	eb b0                	jmp    86d7 <fixup_user_fault+0x77>
    8727:	66 0f 1f 84 00 00 00 	nopw   0x0(%rax,%rax,1)
    872e:	00 00 

0000000000008730 <getElementInList>:
			}
			//}
		}
		return -1;
	}
	struct list_head * getElementInList(struct list_head* head,int n){
    8730:	e8 00 00 00 00       	callq  8735 <getElementInList+0x5>
		if(n==1)
    8735:	83 fe 01             	cmp    $0x1,%esi
    8738:	74 16                	je     8750 <getElementInList+0x20>
			}
			//}
		}
		return -1;
	}
	struct list_head * getElementInList(struct list_head* head,int n){
    873a:	55                   	push   %rbp
		if(n==1)
		  return head->next;
		return getElementInList(head->next,n-1); 
    873b:	48 8b 3f             	mov    (%rdi),%rdi
    873e:	83 ee 01             	sub    $0x1,%esi
			}
			//}
		}
		return -1;
	}
	struct list_head * getElementInList(struct list_head* head,int n){
    8741:	48 89 e5             	mov    %rsp,%rbp
		if(n==1)
		  return head->next;
		return getElementInList(head->next,n-1); 
    8744:	e8 00 00 00 00       	callq  8749 <getElementInList+0x19>
	}
    8749:	5d                   	pop    %rbp
    874a:	c3                   	retq   
    874b:	0f 1f 44 00 00       	nopl   0x0(%rax,%rax,1)
		}
		return -1;
	}
	struct list_head * getElementInList(struct list_head* head,int n){
		if(n==1)
		  return head->next;
    8750:	48 8b 07             	mov    (%rdi),%rax
		return getElementInList(head->next,n-1); 
	}
    8753:	c3                   	retq   
    8754:	66 66 66 2e 0f 1f 84 	data32 data32 nopw %cs:0x0(%rax,%rax,1)
    875b:	00 00 00 00 00 

0000000000008760 <sclock_pte_map_alloc_quick>:
	  return ret;
	  }
	  atomic_inc(&quick_pte_map_index);
	  return NULL;
	  }*/
	struct sclock_page_pte_map * sclock_pte_map_alloc_quick(void){
    8760:	e8 00 00 00 00       	callq  8765 <sclock_pte_map_alloc_quick+0x5>
    8765:	55                   	push   %rbp
    8766:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
    876d:	48 89 e5             	mov    %rsp,%rbp
    8770:	53                   	push   %rbx
    8771:	e8 00 00 00 00       	callq  8776 <sclock_pte_map_alloc_quick+0x16>
 * list_empty - tests whether a list is empty
 * @head: the list to test.
 */
static inline int list_empty(const struct list_head *head)
{
	return head->next == head;
    8776:	48 8b 1d 00 00 00 00 	mov    0x0(%rip),%rbx        # 877d <sclock_pte_map_alloc_quick+0x1d>

		struct sclock_page_pte_map* ret=NULL;
		struct list_head* entry;

		spin_lock(&quick_pte_map_mutex);
		if(list_empty(&quick_pte_map_head)){
    877d:	48 81 fb 00 00 00 00 	cmp    $0x0,%rbx
    8784:	74 32                	je     87b8 <sclock_pte_map_alloc_quick+0x58>
 *
 * Atomically decrements @v by 1.
 */
static inline void atomic_dec(atomic_t *v)
{
	asm volatile(LOCK_PREFIX "decl %0"
    8786:	f0 ff 0d 00 00 00 00 	lock decl 0x0(%rip)        # 878d <sclock_pte_map_alloc_quick+0x2d>
 * in an undefined state.
 */
#ifndef CONFIG_DEBUG_LIST
static inline void __list_del_entry(struct list_head *entry)
{
	__list_del(entry->prev, entry->next);
    878d:	48 8b 43 08          	mov    0x8(%rbx),%rax
    8791:	48 8b 13             	mov    (%rbx),%rdx
	raw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \
} while (0)

static inline void spin_unlock(spinlock_t *lock)
{
	raw_spin_unlock(&lock->rlock);
    8794:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
 * This is only for internal list manipulation where we know
 * the prev/next entries already!
 */
static inline void __list_del(struct list_head * prev, struct list_head * next)
{
	next->prev = prev;
    879b:	48 89 42 08          	mov    %rax,0x8(%rdx)
	prev->next = next;
    879f:	48 89 10             	mov    %rdx,(%rax)
#define LIST_HEAD(name) \
	struct list_head name = LIST_HEAD_INIT(name)

static inline void INIT_LIST_HEAD(struct list_head *list)
{
	list->next = list;
    87a2:	48 89 1b             	mov    %rbx,(%rbx)
	list->prev = list;
    87a5:	48 89 5b 08          	mov    %rbx,0x8(%rbx)
    87a9:	e8 00 00 00 00       	callq  87ae <sclock_pte_map_alloc_quick+0x4e>
		entry=quick_pte_map_head.next;
		atomic_dec(&quick_pte_map_index);
		list_del_init(entry);
		spin_unlock(&quick_pte_map_mutex);
		ret=list_entry(entry,struct sclock_page_pte_map,head);
		return ret;
    87ae:	48 8d 43 e8          	lea    -0x18(%rbx),%rax
	};
    87b2:	5b                   	pop    %rbx
    87b3:	5d                   	pop    %rbp
    87b4:	c3                   	retq   
    87b5:	0f 1f 00             	nopl   (%rax)
    87b8:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
    87bf:	e8 00 00 00 00       	callq  87c4 <sclock_pte_map_alloc_quick+0x64>
		struct list_head* entry;

		spin_lock(&quick_pte_map_mutex);
		if(list_empty(&quick_pte_map_head)){
			spin_unlock(&quick_pte_map_mutex);
			return NULL;
    87c4:	31 c0                	xor    %eax,%eax
    87c6:	eb ea                	jmp    87b2 <sclock_pte_map_alloc_quick+0x52>
    87c8:	0f 1f 84 00 00 00 00 	nopl   0x0(%rax,%rax,1)
    87cf:	00 

00000000000087d0 <sclock_pte_map_free_quick>:
		list_del_init(entry);
		spin_unlock(&quick_pte_map_mutex);
		ret=list_entry(entry,struct sclock_page_pte_map,head);
		return ret;
	};
bool sclock_pte_map_free_quick(struct sclock_page_pte_map* pte_map){
    87d0:	e8 00 00 00 00       	callq  87d5 <sclock_pte_map_free_quick+0x5>
 *
 * Atomically reads the value of @v.
 */
static inline int atomic_read(const atomic_t *v)
{
	return (*(volatile int *)&(v)->counter);
    87d5:	8b 05 00 00 00 00    	mov    0x0(%rip),%eax        # 87db <sclock_pte_map_free_quick+0xb>
    87db:	55                   	push   %rbp
    87dc:	48 89 e5             	mov    %rsp,%rbp
    87df:	53                   	push   %rbx
    87e0:	48 89 fb             	mov    %rdi,%rbx
	struct list_head* entry;
	bool ret=true;
	if(atomic_read(&quick_pte_map_index)>=MAX_QUICK_MAP){
    87e3:	3d ff 03 00 00       	cmp    $0x3ff,%eax
    87e8:	7e 2e                	jle    8818 <sclock_pte_map_free_quick+0x48>

		kmem_cache_free(sclock_page_pte_map_cache,pte_map);
    87ea:	48 89 fe             	mov    %rdi,%rsi
    87ed:	48 8b 3d 00 00 00 00 	mov    0x0(%rip),%rdi        # 87f4 <sclock_pte_map_free_quick+0x24>
    87f4:	e8 00 00 00 00       	callq  87f9 <sclock_pte_map_free_quick+0x29>
	spin_lock(&quick_pte_map_mutex);
	atomic_inc(&quick_pte_map_index);
	list_add_tail(&pte_map->head,&quick_pte_map_head);
	spin_unlock(&quick_pte_map_mutex);
out:
	if(atomic_read(&(pte_map->sclock_pte_map_vma_counter->counter))==0&&pte_map->sclock_pte_map_vma_counter->legal==false)
    87f9:	48 8b 7b 28          	mov    0x28(%rbx),%rdi
    87fd:	8b 07                	mov    (%rdi),%eax
    87ff:	85 c0                	test   %eax,%eax
    8801:	75 06                	jne    8809 <sclock_pte_map_free_quick+0x39>
    8803:	80 7f 04 00          	cmpb   $0x0,0x4(%rdi)
    8807:	74 57                	je     8860 <sclock_pte_map_free_quick+0x90>
 *
 * Atomically decrements @v by 1.
 */
static inline void atomic_dec(atomic_t *v)
{
	asm volatile(LOCK_PREFIX "decl %0"
    8809:	f0 ff 0f             	lock decl (%rdi)
	  destroy_sclock_pte_map_vma_counter(pte_map->sclock_pte_map_vma_counter);
	else
	  atomic_dec(&(pte_map->sclock_pte_map_vma_counter->counter));
	return true;
};/*
    880c:	5b                   	pop    %rbx
    880d:	b8 01 00 00 00       	mov    $0x1,%eax
    8812:	5d                   	pop    %rbp
    8813:	c3                   	retq   
    8814:	0f 1f 40 00          	nopl   0x0(%rax)
	raw_spin_lock_init(&(_lock)->rlock);		\
} while (0)

static inline void spin_lock(spinlock_t *lock)
{
	raw_spin_lock(&lock->rlock);
    8818:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
    881f:	e8 00 00 00 00       	callq  8824 <sclock_pte_map_free_quick+0x54>
 *
 * Atomically increments @v by 1.
 */
static inline void atomic_inc(atomic_t *v)
{
	asm volatile(LOCK_PREFIX "incl %0"
    8824:	f0 ff 05 00 00 00 00 	lock incl 0x0(%rip)        # 882b <sclock_pte_map_free_quick+0x5b>
 * Insert a new entry before the specified head.
 * This is useful for implementing queues.
 */
static inline void list_add_tail(struct list_head *new, struct list_head *head)
{
	__list_add(new, head->prev, head);
    882b:	48 8b 05 00 00 00 00 	mov    0x0(%rip),%rax        # 8832 <sclock_pte_map_free_quick+0x62>
		ret=false;
		goto out;
	}
	spin_lock(&quick_pte_map_mutex);
	atomic_inc(&quick_pte_map_index);
	list_add_tail(&pte_map->head,&quick_pte_map_head);
    8832:	48 8d 53 18          	lea    0x18(%rbx),%rdx
static inline void __list_add(struct list_head *new,
			      struct list_head *prev,
			      struct list_head *next)
{
	next->prev = new;
	new->next = next;
    8836:	48 c7 43 18 00 00 00 	movq   $0x0,0x18(%rbx)
    883d:	00 
	raw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \
} while (0)

static inline void spin_unlock(spinlock_t *lock)
{
	raw_spin_unlock(&lock->rlock);
    883e:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
#ifndef CONFIG_DEBUG_LIST
static inline void __list_add(struct list_head *new,
			      struct list_head *prev,
			      struct list_head *next)
{
	next->prev = new;
    8845:	48 89 15 00 00 00 00 	mov    %rdx,0x0(%rip)        # 884c <sclock_pte_map_free_quick+0x7c>
	new->next = next;
	new->prev = prev;
    884c:	48 89 43 20          	mov    %rax,0x20(%rbx)
	prev->next = new;
    8850:	48 89 10             	mov    %rdx,(%rax)
    8853:	e8 00 00 00 00       	callq  8858 <sclock_pte_map_free_quick+0x88>
    8858:	eb 9f                	jmp    87f9 <sclock_pte_map_free_quick+0x29>
    885a:	66 0f 1f 44 00 00    	nopw   0x0(%rax,%rax,1)
	spin_unlock(&quick_pte_map_mutex);
out:
	if(atomic_read(&(pte_map->sclock_pte_map_vma_counter->counter))==0&&pte_map->sclock_pte_map_vma_counter->legal==false)
	  destroy_sclock_pte_map_vma_counter(pte_map->sclock_pte_map_vma_counter);
    8860:	e8 00 00 00 00       	callq  8865 <sclock_pte_map_free_quick+0x95>
	else
	  atomic_dec(&(pte_map->sclock_pte_map_vma_counter->counter));
	return true;
};/*
    8865:	5b                   	pop    %rbx
    8866:	b8 01 00 00 00       	mov    $0x1,%eax
    886b:	5d                   	pop    %rbp
    886c:	c3                   	retq   
    886d:	0f 1f 00             	nopl   (%rax)

0000000000008870 <clean_sclock_pte_map>:
update_mmu_cache(sclock_entry->vma, sclock_entry->address,sclock_entry->ptep);

}
return 	atomic_read(&sclock_entry->pte_count);
}*/
void clean_sclock_pte_map(struct sclock_LRU * sclock_entry){
    8870:	e8 00 00 00 00       	callq  8875 <clean_sclock_pte_map+0x5>
    8875:	55                   	push   %rbp
    8876:	48 89 e5             	mov    %rsp,%rbp

		list_del(&pte_map->head);
		sclock_pte_map_free_quick(pte_map);
	}
*/
	};
    8879:	5d                   	pop    %rbp
    887a:	c3                   	retq   
    887b:	0f 1f 44 00 00       	nopl   0x0(%rax,%rax,1)

0000000000008880 <find_pte_check_pfn>:
}




pte_t * find_pte_check_pfn(struct mm_struct * mm, unsigned long address,unsigned long pfn){
    8880:	e8 00 00 00 00       	callq  8885 <find_pte_check_pfn+0x5>
    8885:	55                   	push   %rbp
    8886:	48 89 e5             	mov    %rsp,%rbp
    8889:	53                   	push   %rbx
    888a:	48 89 d3             	mov    %rdx,%rbx
	pte_t * pte=find_pte(mm,address);
    888d:	e8 00 00 00 00       	callq  8892 <find_pte_check_pfn+0x12>
	if(pte)
    8892:	48 85 c0             	test   %rax,%rax




pte_t * find_pte_check_pfn(struct mm_struct * mm, unsigned long address,unsigned long pfn){
	pte_t * pte=find_pte(mm,address);
    8895:	48 89 c1             	mov    %rax,%rcx
	if(pte)
    8898:	74 26                	je     88c0 <find_pte_check_pfn+0x40>
    889a:	48 8b 38             	mov    (%rax),%rdi
    889d:	ff 14 25 00 00 00 00 	callq  *0x0
    88a4:	48 c1 e0 12          	shl    $0x12,%rax
    88a8:	48 c1 e8 1e          	shr    $0x1e,%rax
	  if(pte_pfn(*pte)!=pfn)
    88ac:	48 39 c3             	cmp    %rax,%rbx
    88af:	75 0f                	jne    88c0 <find_pte_check_pfn+0x40>
		return NULL;
	return pte;
}
    88b1:	5b                   	pop    %rbx
    88b2:	48 89 c8             	mov    %rcx,%rax
    88b5:	5d                   	pop    %rbp
    88b6:	c3                   	retq   
    88b7:	66 0f 1f 84 00 00 00 	nopw   0x0(%rax,%rax,1)
    88be:	00 00 
    88c0:	5b                   	pop    %rbx
}




pte_t * find_pte_check_pfn(struct mm_struct * mm, unsigned long address,unsigned long pfn){
    88c1:	31 c0                	xor    %eax,%eax
	pte_t * pte=find_pte(mm,address);
	if(pte)
	  if(pte_pfn(*pte)!=pfn)
		return NULL;
	return pte;
}
    88c3:	5d                   	pop    %rbp
    88c4:	c3                   	retq   
    88c5:	66 66 2e 0f 1f 84 00 	data32 nopw %cs:0x0(%rax,%rax,1)
    88cc:	00 00 00 00 

00000000000088d0 <check_other_rsv>:

#define trigger_by_isolation( page_table)\
	((((page_table->pte)&_PAGE_RSV)==(_PAGE_ISOLATION|_PAGE_NCACHE))||(((page_table->pte)&_PAGE_RSV)==_PAGE_ISOLATION))?true:false
#define trigger_by_NCache( page_table)\
	((page_table->pte&_PAGE_RSV)==_PAGE_NCACHE)?true:false
	int check_other_rsv(pte_t* pte){
    88d0:	e8 00 00 00 00       	callq  88d5 <check_other_rsv+0x5>
		if(!trigger_by_NCache(pte)&&!trigger_by_isolation(pte)){
    88d5:	48 b8 00 00 00 00 80 	movabs $0xfff8000000000,%rax
    88dc:	ff 0f 00 
    88df:	48 23 07             	and    (%rdi),%rax
    88e2:	48 ba 00 00 00 00 00 	movabs $0x4000000000000,%rdx
    88e9:	00 04 00 
    88ec:	48 39 d0             	cmp    %rdx,%rax
    88ef:	75 03                	jne    88f4 <check_other_rsv+0x24>
			printk(KERN_DEBUG"Other reserved bit detected");
			return -1;// other reserved bit used! pgtable_bad();
		}
		return 0;
    88f1:	31 c0                	xor    %eax,%eax
    88f3:	c3                   	retq   

#define trigger_by_isolation( page_table)\
	((((page_table->pte)&_PAGE_RSV)==(_PAGE_ISOLATION|_PAGE_NCACHE))||(((page_table->pte)&_PAGE_RSV)==_PAGE_ISOLATION))?true:false
#define trigger_by_NCache( page_table)\
	((page_table->pte&_PAGE_RSV)==_PAGE_NCACHE)?true:false
	int check_other_rsv(pte_t* pte){
    88f4:	55                   	push   %rbp
		if(!trigger_by_NCache(pte)&&!trigger_by_isolation(pte)){
			printk(KERN_DEBUG"Other reserved bit detected");
    88f5:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
    88fc:	31 c0                	xor    %eax,%eax

#define trigger_by_isolation( page_table)\
	((((page_table->pte)&_PAGE_RSV)==(_PAGE_ISOLATION|_PAGE_NCACHE))||(((page_table->pte)&_PAGE_RSV)==_PAGE_ISOLATION))?true:false
#define trigger_by_NCache( page_table)\
	((page_table->pte&_PAGE_RSV)==_PAGE_NCACHE)?true:false
	int check_other_rsv(pte_t* pte){
    88fe:	48 89 e5             	mov    %rsp,%rbp
		if(!trigger_by_NCache(pte)&&!trigger_by_isolation(pte)){
			printk(KERN_DEBUG"Other reserved bit detected");
    8901:	e8 00 00 00 00       	callq  8906 <check_other_rsv+0x36>
			return -1;// other reserved bit used! pgtable_bad();
    8906:	83 c8 ff             	or     $0xffffffff,%eax
		}
		return 0;
	}
    8909:	5d                   	pop    %rbp
    890a:	c3                   	retq   
    890b:	0f 1f 44 00 00       	nopl   0x0(%rax,%rax,1)

0000000000008910 <do_copy_on_read>:

	int do_copy_on_read(struct mm_struct *mm, struct vm_area_struct *vma,
				unsigned long address, unsigned int flags)
	{
    8910:	e8 00 00 00 00       	callq  8915 <do_copy_on_read+0x5>
    8915:	55                   	push   %rbp
    8916:	48 89 e5             	mov    %rsp,%rbp
    8919:	41 57                	push   %r15
    891b:	41 89 cf             	mov    %ecx,%r15d
    891e:	41 56                	push   %r14
    8920:	49 89 f6             	mov    %rsi,%r14
    8923:	41 55                	push   %r13
    8925:	49 89 fd             	mov    %rdi,%r13
    8928:	41 54                	push   %r12
    892a:	49 89 d4             	mov    %rdx,%r12
    892d:	53                   	push   %rbx
		pte_t *pte;
		pte_t entry;
		spinlock_t *ptl;
		if (unlikely(is_vm_hugetlb_page(vma)))
		  return hugetlb_fault(mm, vma, address, flags);
		pgd = pgd_offset(mm, address);
    892e:	48 89 d3             	mov    %rdx,%rbx
    8931:	48 c1 eb 24          	shr    $0x24,%rbx
		return 0;
	}

	int do_copy_on_read(struct mm_struct *mm, struct vm_area_struct *vma,
				unsigned long address, unsigned int flags)
	{
    8935:	48 83 ec 18          	sub    $0x18,%rsp
		pte_t *pte;
		pte_t entry;
		spinlock_t *ptl;
		if (unlikely(is_vm_hugetlb_page(vma)))
		  return hugetlb_fault(mm, vma, address, flags);
		pgd = pgd_offset(mm, address);
    8939:	81 e3 f8 0f 00 00    	and    $0xff8,%ebx
    893f:	48 03 5f 40          	add    0x40(%rdi),%rbx
    8943:	48 8b 03             	mov    (%rbx),%rax
 */
#if defined(CONFIG_MMU) && !defined(__ARCH_HAS_4LEVEL_HACK)
static inline pud_t *pud_alloc(struct mm_struct *mm, pgd_t *pgd, unsigned long address)
{
	return (unlikely(pgd_none(*pgd)) && __pud_alloc(mm, pgd, address))?
		NULL: pud_offset(pgd, address);
    8946:	48 85 c0             	test   %rax,%rax
    8949:	0f 84 aa 02 00 00    	je     8bf9 <do_copy_on_read+0x2e9>
    894f:	48 89 c7             	mov    %rax,%rdi

	if (sizeof(pgdval_t) > sizeof(long))
		ret =  PVOP_CALLEE2(pgdval_t, pv_mmu_ops.pgd_val,
				    pgd.pgd, (u64)pgd.pgd >> 32);
	else
		ret =  PVOP_CALLEE1(pgdval_t, pv_mmu_ops.pgd_val,
    8952:	ff 14 25 00 00 00 00 	callq  *0x0
#define pgd_page(pgd)		pfn_to_page(pgd_val(pgd) >> PAGE_SHIFT)

/* to find an entry in a page-table-directory. */
static inline unsigned long pud_index(unsigned long address)
{
	return (address >> PUD_SHIFT) & (PTRS_PER_PUD - 1);
    8959:	4c 89 e2             	mov    %r12,%rdx
	return pgd_flags(pgd) & _PAGE_PRESENT;
}

static inline unsigned long pgd_page_vaddr(pgd_t pgd)
{
	return (unsigned long)__va((unsigned long)pgd_val(pgd) & PTE_PFN_MASK);
    895c:	48 b9 00 00 00 00 00 	movabs $0xffff880000000000,%rcx
    8963:	88 ff ff 
#define pgd_page(pgd)		pfn_to_page(pgd_val(pgd) >> PAGE_SHIFT)

/* to find an entry in a page-table-directory. */
static inline unsigned long pud_index(unsigned long address)
{
	return (address >> PUD_SHIFT) & (PTRS_PER_PUD - 1);
    8966:	48 c1 ea 1b          	shr    $0x1b,%rdx
}

static inline pud_t *pud_offset(pgd_t *pgd, unsigned long address)
{
	return (pud_t *)pgd_page_vaddr(*pgd) + pud_index(address);
    896a:	81 e2 f8 0f 00 00    	and    $0xff8,%edx
	return pgd_flags(pgd) & _PAGE_PRESENT;
}

static inline unsigned long pgd_page_vaddr(pgd_t pgd)
{
	return (unsigned long)__va((unsigned long)pgd_val(pgd) & PTE_PFN_MASK);
    8970:	48 01 ca             	add    %rcx,%rdx
    8973:	48 b9 00 f0 ff ff ff 	movabs $0x3ffffffff000,%rcx
    897a:	3f 00 00 
    897d:	48 21 c8             	and    %rcx,%rax
		pud = pud_alloc(mm, pgd, address);
		if (!pud)
    8980:	48 89 d3             	mov    %rdx,%rbx
    8983:	48 01 c3             	add    %rax,%rbx
    8986:	0f 84 ac 01 00 00    	je     8b38 <do_copy_on_read+0x228>
    898c:	48 8b 3b             	mov    (%rbx),%rdi
}

static inline pmd_t *pmd_alloc(struct mm_struct *mm, pud_t *pud, unsigned long address)
{
	return (unlikely(pud_none(*pud)) && __pmd_alloc(mm, pud, address))?
		NULL: pmd_offset(pud, address);
    898f:	48 85 ff             	test   %rdi,%rdi
    8992:	0f 84 9f 02 00 00    	je     8c37 <do_copy_on_read+0x327>

	if (sizeof(pudval_t) > sizeof(long))
		ret =  PVOP_CALLEE2(pudval_t, pv_mmu_ops.pud_val,
				    pud.pud, (u64)pud.pud >> 32);
	else
		ret =  PVOP_CALLEE1(pudval_t, pv_mmu_ops.pud_val,
    8998:	ff 14 25 00 00 00 00 	callq  *0x0
 * this macro returns the index of the entry in the pmd page which would
 * control the given virtual address
 */
static inline unsigned long pmd_index(unsigned long address)
{
	return (address >> PMD_SHIFT) & (PTRS_PER_PMD - 1);
    899f:	4c 89 e2             	mov    %r12,%rdx
	return pud_flags(pud) & _PAGE_PRESENT;
}

static inline unsigned long pud_page_vaddr(pud_t pud)
{
	return (unsigned long)__va((unsigned long)pud_val(pud) & PTE_PFN_MASK);
    89a2:	49 ba 00 00 00 00 00 	movabs $0xffff880000000000,%r10
    89a9:	88 ff ff 
    89ac:	48 b9 00 f0 ff ff ff 	movabs $0x3ffffffff000,%rcx
    89b3:	3f 00 00 
 * this macro returns the index of the entry in the pmd page which would
 * control the given virtual address
 */
static inline unsigned long pmd_index(unsigned long address)
{
	return (address >> PMD_SHIFT) & (PTRS_PER_PMD - 1);
    89b6:	48 c1 ea 12          	shr    $0x12,%rdx
	return pud_flags(pud) & _PAGE_PRESENT;
}

static inline unsigned long pud_page_vaddr(pud_t pud)
{
	return (unsigned long)__va((unsigned long)pud_val(pud) & PTE_PFN_MASK);
    89ba:	48 21 c8             	and    %rcx,%rax
#define pud_page(pud)		pfn_to_page(pud_val(pud) >> PAGE_SHIFT)

/* Find an entry in the second-level page table.. */
static inline pmd_t *pmd_offset(pud_t *pud, unsigned long address)
{
	return (pmd_t *)pud_page_vaddr(*pud) + pmd_index(address);
    89bd:	81 e2 f8 0f 00 00    	and    $0xff8,%edx
	return pud_flags(pud) & _PAGE_PRESENT;
}

static inline unsigned long pud_page_vaddr(pud_t pud)
{
	return (unsigned long)__va((unsigned long)pud_val(pud) & PTE_PFN_MASK);
    89c3:	4c 01 d2             	add    %r10,%rdx
		  return VM_FAULT_OOM;
		pmd = pmd_alloc(mm, pud, address);
		if (!pmd)
    89c6:	48 01 c2             	add    %rax,%rdx
    89c9:	49 89 d2             	mov    %rdx,%r10
    89cc:	0f 84 66 01 00 00    	je     8b38 <do_copy_on_read+0x228>
			if (!(ret& VM_FAULT_FALLBACK))
			  return ret;
		} else {
			pmd_t orig_pmd = *pmd;
			int ret;
			barrier();
    89d2:	48 8b 02             	mov    (%rdx),%rax
		/*
		 * Use __pte_alloc instead of pte_alloc_map, because we can't
		 * run pte_offset_map on the pmd, if an huge pmd could
		 * materialize from under us from a different thread.
		 */
		if (unlikely(pmd_none(*pmd)) &&
    89d5:	48 85 c0             	test   %rax,%rax
    89d8:	0f 84 33 02 00 00    	je     8c11 <do_copy_on_read+0x301>
    89de:	48 89 c7             	mov    %rax,%rdi

	if (sizeof(pmdval_t) > sizeof(long))
		ret =  PVOP_CALLEE2(pmdval_t, pv_mmu_ops.pmd_val,
				    pmd.pmd, (u64)pmd.pmd >> 32);
	else
		ret =  PVOP_CALLEE1(pmdval_t, pv_mmu_ops.pmd_val,
    89e1:	ff 14 25 00 00 00 00 	callq  *0x0
 * this function returns the index of the entry in the pte page which would
 * control the given virtual address
 */
static inline unsigned long pte_index(unsigned long address)
{
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
    89e8:	4c 89 e2             	mov    %r12,%rdx
	return (unsigned long)native_pmd_val(pmd) == 0;
}

static inline unsigned long pmd_page_vaddr(pmd_t pmd)
{
	return (unsigned long)__va(pmd_val(pmd) & PTE_PFN_MASK);
    89eb:	48 b9 00 00 00 00 00 	movabs $0xffff880000000000,%rcx
    89f2:	88 ff ff 
    89f5:	48 be 00 f0 ff ff ff 	movabs $0x3ffffffff000,%rsi
    89fc:	3f 00 00 
 * this function returns the index of the entry in the pte page which would
 * control the given virtual address
 */
static inline unsigned long pte_index(unsigned long address)
{
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
    89ff:	48 c1 ea 09          	shr    $0x9,%rdx
	return (unsigned long)native_pmd_val(pmd) == 0;
}

static inline unsigned long pmd_page_vaddr(pmd_t pmd)
{
	return (unsigned long)__va(pmd_val(pmd) & PTE_PFN_MASK);
    8a03:	48 21 f0             	and    %rsi,%rax
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
}

static inline pte_t *pte_offset_kernel(pmd_t *pmd, unsigned long address)
{
	return (pte_t *)pmd_page_vaddr(*pmd) + pte_index(address);
    8a06:	81 e2 f8 0f 00 00    	and    $0xff8,%edx
	return (unsigned long)native_pmd_val(pmd) == 0;
}

static inline unsigned long pmd_page_vaddr(pmd_t pmd)
{
	return (unsigned long)__va(pmd_val(pmd) & PTE_PFN_MASK);
    8a0c:	48 01 ca             	add    %rcx,%rdx
	return (address >> PAGE_SHIFT) & (PTRS_PER_PTE - 1);
}

static inline pte_t *pte_offset_kernel(pmd_t *pmd, unsigned long address)
{
	return (pte_t *)pmd_page_vaddr(*pmd) + pte_index(address);
    8a0f:	48 8d 0c 02          	lea    (%rdx,%rax,1),%rcx
    8a13:	48 b8 ff 0f 00 00 00 	movabs $0xffffc00000000fff,%rax
    8a1a:	c0 ff ff 
		 * from under us anymore at this point because we hold the mmap_sem
		 =* read mode and khugepaged takes it in write mode. So now it's
		 * safe to run pte_offset_map().
		 */
		pte = pte_offset_map(pmd, address);
		entry = *pte;
    8a1d:	48 8b 19             	mov    (%rcx),%rbx
    8a20:	48 21 d8             	and    %rbx,%rax
		if (!pte_present(entry)) {
    8a23:	a9 01 01 00 00       	test   $0x101,%eax
    8a28:	75 6e                	jne    8a98 <do_copy_on_read+0x188>
			if (pte_none(entry)) {
    8a2a:	48 85 db             	test   %rbx,%rbx
    8a2d:	0f 85 1d 01 00 00    	jne    8b50 <do_copy_on_read+0x240>
				if (vma->vm_ops) {
    8a33:	49 8b 86 98 00 00 00 	mov    0x98(%r14),%rax
    8a3a:	48 85 c0             	test   %rax,%rax
    8a3d:	0f 84 9d 01 00 00    	je     8be0 <do_copy_on_read+0x2d0>
					if (likely(vma->vm_ops->fault))
    8a43:	48 83 78 10 00       	cmpq   $0x0,0x10(%rax)
    8a48:	0f 84 92 01 00 00    	je     8be0 <do_copy_on_read+0x2d0>

static int do_linear_fault(struct mm_struct *mm, struct vm_area_struct *vma,
			unsigned long address, pte_t *page_table, pmd_t *pmd,
			unsigned int flags, pte_t orig_pte)
{
	pgoff_t pgoff = (((address & PAGE_MASK)
    8a4e:	4d 89 e0             	mov    %r12,%r8
					- vma->vm_start) >> PAGE_SHIFT) + vma->vm_pgoff;

	pte_unmap(page_table);
	return __do_fault(mm, vma, address, pmd, pgoff, flags, orig_pte);
    8a51:	45 89 f9             	mov    %r15d,%r9d
    8a54:	4c 89 d1             	mov    %r10,%rcx

static int do_linear_fault(struct mm_struct *mm, struct vm_area_struct *vma,
			unsigned long address, pte_t *page_table, pmd_t *pmd,
			unsigned int flags, pte_t orig_pte)
{
	pgoff_t pgoff = (((address & PAGE_MASK)
    8a57:	49 81 e0 00 f0 ff ff 	and    $0xfffffffffffff000,%r8
					- vma->vm_start) >> PAGE_SHIFT) + vma->vm_pgoff;
    8a5e:	4d 2b 06             	sub    (%r14),%r8

	pte_unmap(page_table);
	return __do_fault(mm, vma, address, pmd, pgoff, flags, orig_pte);
    8a61:	4c 89 e2             	mov    %r12,%rdx
    8a64:	4c 89 f6             	mov    %r14,%rsi
    8a67:	4c 89 ef             	mov    %r13,%rdi
static int do_linear_fault(struct mm_struct *mm, struct vm_area_struct *vma,
			unsigned long address, pte_t *page_table, pmd_t *pmd,
			unsigned int flags, pte_t orig_pte)
{
	pgoff_t pgoff = (((address & PAGE_MASK)
					- vma->vm_start) >> PAGE_SHIFT) + vma->vm_pgoff;
    8a6a:	49 c1 e8 0c          	shr    $0xc,%r8

static int do_linear_fault(struct mm_struct *mm, struct vm_area_struct *vma,
			unsigned long address, pte_t *page_table, pmd_t *pmd,
			unsigned int flags, pte_t orig_pte)
{
	pgoff_t pgoff = (((address & PAGE_MASK)
    8a6e:	4d 03 86 a0 00 00 00 	add    0xa0(%r14),%r8
					- vma->vm_start) >> PAGE_SHIFT) + vma->vm_pgoff;

	pte_unmap(page_table);
	return __do_fault(mm, vma, address, pmd, pgoff, flags, orig_pte);
    8a75:	48 c7 04 24 00 00 00 	movq   $0x0,(%rsp)
    8a7c:	00 
    8a7d:	e8 7e 84 ff ff       	callq  f00 <__do_fault>
		}
unlock:
		pte_unmap_unlock(pte, ptl);
		//	//printk("unlock not trigger");
		return 0;
	}
    8a82:	48 83 c4 18          	add    $0x18,%rsp
    8a86:	5b                   	pop    %rbx
    8a87:	41 5c                	pop    %r12
    8a89:	41 5d                	pop    %r13
    8a8b:	41 5e                	pop    %r14
    8a8d:	41 5f                	pop    %r15
    8a8f:	5d                   	pop    %rbp
    8a90:	c3                   	retq   
    8a91:	0f 1f 80 00 00 00 00 	nopl   0x0(%rax)
    8a98:	48 89 4d c8          	mov    %rcx,-0x38(%rbp)
    8a9c:	49 8b 3a             	mov    (%r10),%rdi
    8a9f:	4c 89 55 d0          	mov    %r10,-0x30(%rbp)
    8aa3:	ff 14 25 00 00 00 00 	callq  *0x0
}
#endif /* ALLOC_SPLIT_PTLOCKS */

static inline spinlock_t *pte_lockptr(struct mm_struct *mm, pmd_t *pmd)
{
	return ptlock_ptr(pmd_page(*pmd));
    8aaa:	48 21 f0             	and    %rsi,%rax
    8aad:	48 ba 00 00 00 00 00 	movabs $0xffffea0000000000,%rdx
    8ab4:	ea ff ff 
    8ab7:	48 c1 e8 06          	shr    $0x6,%rax
    8abb:	4c 8b 7c 10 30       	mov    0x30(%rax,%rdx,1),%r15
	raw_spin_lock_init(&(_lock)->rlock);		\
} while (0)

static inline void spin_lock(spinlock_t *lock)
{
	raw_spin_lock(&lock->rlock);
    8ac0:	4c 89 ff             	mov    %r15,%rdi
    8ac3:	e8 00 00 00 00       	callq  8ac8 <do_copy_on_read+0x1b8>
		}
		ptl = pte_lockptr(mm, pmd);
		//	//printk("before locking");
		spin_lock(ptl);
		//	//printk("lock");
		if (unlikely(!pte_same(*pte, entry)))
    8ac8:	48 8b 4d c8          	mov    -0x38(%rbp),%rcx
    8acc:	48 3b 19             	cmp    (%rcx),%rbx
    8acf:	75 47                	jne    8b18 <do_copy_on_read+0x208>
		  goto unlock;
		if(trigger_by_isolation(pte)==1){
    8ad1:	48 b8 00 00 00 00 80 	movabs $0xfff8000000000,%rax
    8ad8:	ff 0f 00 
    8adb:	48 ba 00 00 00 00 80 	movabs $0xbff8000000000,%rdx
    8ae2:	ff 0b 00 
    8ae5:	4c 8b 55 d0          	mov    -0x30(%rbp),%r10
    8ae9:	48 21 d8             	and    %rbx,%rax
    8aec:	48 21 da             	and    %rbx,%rdx
    8aef:	48 89 c6             	mov    %rax,%rsi
    8af2:	48 b8 00 00 00 00 00 	movabs $0x8000000000000,%rax
    8af9:	00 08 00 
    8afc:	48 39 c2             	cmp    %rax,%rdx
    8aff:	0f 84 93 00 00 00    	je     8b98 <do_copy_on_read+0x288>
			//	printk(KERN_DEBUG"triggerd by isolation\n");
			return	__do_copy_on_read(mm,vma,address,pte,pmd,ptl,entry);
		}
		else if(trigger_by_NCache(pte)==1){
    8b05:	48 b8 00 00 00 00 00 	movabs $0x4000000000000,%rax
    8b0c:	00 04 00 
    8b0f:	48 39 c6             	cmp    %rax,%rsi
    8b12:	0f 84 a0 00 00 00    	je     8bb8 <do_copy_on_read+0x2a8>
	raw_spin_lock_irqsave_nested(spinlock_check(lock), flags, subclass); \
} while (0)

static inline void spin_unlock(spinlock_t *lock)
{
	raw_spin_unlock(&lock->rlock);
    8b18:	4c 89 ff             	mov    %r15,%rdi
    8b1b:	e8 00 00 00 00       	callq  8b20 <do_copy_on_read+0x210>
		}
unlock:
		pte_unmap_unlock(pte, ptl);
		//	//printk("unlock not trigger");
		return 0;
	}
    8b20:	48 83 c4 18          	add    $0x18,%rsp
			return	_try_switch_NCache(mm,vma,address,pte,pmd,ptl,entry,true);
		}
unlock:
		pte_unmap_unlock(pte, ptl);
		//	//printk("unlock not trigger");
		return 0;
    8b24:	31 c0                	xor    %eax,%eax
	}
    8b26:	5b                   	pop    %rbx
    8b27:	41 5c                	pop    %r12
    8b29:	41 5d                	pop    %r13
    8b2b:	41 5e                	pop    %r14
    8b2d:	41 5f                	pop    %r15
    8b2f:	5d                   	pop    %rbp
    8b30:	c3                   	retq   
    8b31:	0f 1f 80 00 00 00 00 	nopl   0x0(%rax)
    8b38:	48 83 c4 18          	add    $0x18,%rsp
		if (unlikely(is_vm_hugetlb_page(vma)))
		  return hugetlb_fault(mm, vma, address, flags);
		pgd = pgd_offset(mm, address);
		pud = pud_alloc(mm, pgd, address);
		if (!pud)
		  return VM_FAULT_OOM;
    8b3c:	b8 01 00 00 00       	mov    $0x1,%eax
		}
unlock:
		pte_unmap_unlock(pte, ptl);
		//	//printk("unlock not trigger");
		return 0;
	}
    8b41:	5b                   	pop    %rbx
    8b42:	41 5c                	pop    %r12
    8b44:	41 5d                	pop    %r13
    8b46:	41 5e                	pop    %r14
    8b48:	41 5f                	pop    %r15
    8b4a:	5d                   	pop    %rbp
    8b4b:	c3                   	retq   
    8b4c:	0f 1f 40 00          	nopl   0x0(%rax)
								  pte, pmd, flags, entry);
				}
				return do_anonymous_page(mm, vma, address,
							pte, pmd, flags);
			}
			if (pte_file(entry))
    8b50:	a8 40                	test   $0x40,%al
			  return do_nonlinear_fault(mm, vma, address,
    8b52:	49 89 d9             	mov    %rbx,%r9
    8b55:	45 89 f8             	mov    %r15d,%r8d
    8b58:	4c 89 d1             	mov    %r10,%rcx
    8b5b:	4c 89 e2             	mov    %r12,%rdx
    8b5e:	4c 89 f6             	mov    %r14,%rsi
    8b61:	4c 89 ef             	mov    %r13,%rdi
								  pte, pmd, flags, entry);
				}
				return do_anonymous_page(mm, vma, address,
							pte, pmd, flags);
			}
			if (pte_file(entry))
    8b64:	74 1a                	je     8b80 <do_copy_on_read+0x270>
			  return do_nonlinear_fault(mm, vma, address,
    8b66:	e8 f5 88 ff ff       	callq  1460 <do_nonlinear_fault.isra.46>
		}
unlock:
		pte_unmap_unlock(pte, ptl);
		//	//printk("unlock not trigger");
		return 0;
	}
    8b6b:	48 83 c4 18          	add    $0x18,%rsp
    8b6f:	5b                   	pop    %rbx
    8b70:	41 5c                	pop    %r12
    8b72:	41 5d                	pop    %r13
    8b74:	41 5e                	pop    %r14
    8b76:	41 5f                	pop    %r15
    8b78:	5d                   	pop    %rbp
    8b79:	c3                   	retq   
    8b7a:	66 0f 1f 44 00 00    	nopw   0x0(%rax,%rax,1)
							pte, pmd, flags);
			}
			if (pte_file(entry))
			  return do_nonlinear_fault(mm, vma, address,
						  pte, pmd, flags, entry);
			return do_swap_page(mm, vma, address,
    8b80:	e8 3b b3 ff ff       	callq  3ec0 <do_swap_page.isra.68>
		}
unlock:
		pte_unmap_unlock(pte, ptl);
		//	//printk("unlock not trigger");
		return 0;
	}
    8b85:	48 83 c4 18          	add    $0x18,%rsp
    8b89:	5b                   	pop    %rbx
    8b8a:	41 5c                	pop    %r12
    8b8c:	41 5d                	pop    %r13
    8b8e:	41 5e                	pop    %r14
    8b90:	41 5f                	pop    %r15
    8b92:	5d                   	pop    %rbp
    8b93:	c3                   	retq   
    8b94:	0f 1f 40 00          	nopl   0x0(%rax)
		//	//printk("lock");
		if (unlikely(!pte_same(*pte, entry)))
		  goto unlock;
		if(trigger_by_isolation(pte)==1){
			//	printk(KERN_DEBUG"triggerd by isolation\n");
			return	__do_copy_on_read(mm,vma,address,pte,pmd,ptl,entry);
    8b98:	48 89 1c 24          	mov    %rbx,(%rsp)
    8b9c:	4d 89 f9             	mov    %r15,%r9
    8b9f:	4d 89 d0             	mov    %r10,%r8
    8ba2:	4c 89 e2             	mov    %r12,%rdx
    8ba5:	4c 89 f6             	mov    %r14,%rsi
    8ba8:	4c 89 ef             	mov    %r13,%rdi
    8bab:	e8 90 d5 ff ff       	callq  6140 <__do_copy_on_read>
    8bb0:	e9 cd fe ff ff       	jmpq   8a82 <do_copy_on_read+0x172>
    8bb5:	0f 1f 00             	nopl   (%rax)
    8bb8:	49 8d bd 90 03 00 00 	lea    0x390(%r13),%rdi
		}
		else if(trigger_by_NCache(pte)==1){
			//	printk(KERN_DEBUG"triggerd by NCache\n");
			return	_try_switch_NCache(mm,vma,address,pte,pmd,ptl,entry,true);
    8bbf:	48 89 1c 24          	mov    %rbx,(%rsp)
    8bc3:	4d 89 f9             	mov    %r15,%r9
    8bc6:	4d 89 d0             	mov    %r10,%r8
    8bc9:	4c 89 e2             	mov    %r12,%rdx
    8bcc:	4c 89 f6             	mov    %r14,%rsi
    8bcf:	e8 fc e5 ff ff       	callq  71d0 <_try_switch_NCache.isra.69>
    8bd4:	e9 a9 fe ff ff       	jmpq   8a82 <do_copy_on_read+0x172>
    8bd9:	0f 1f 80 00 00 00 00 	nopl   0x0(%rax)
				if (vma->vm_ops) {
					if (likely(vma->vm_ops->fault))
					  return do_linear_fault(mm, vma, address,
								  pte, pmd, flags, entry);
				}
				return do_anonymous_page(mm, vma, address,
    8be0:	45 89 f8             	mov    %r15d,%r8d
    8be3:	4c 89 d1             	mov    %r10,%rcx
    8be6:	4c 89 e2             	mov    %r12,%rdx
    8be9:	4c 89 f6             	mov    %r14,%rsi
    8bec:	4c 89 ef             	mov    %r13,%rdi
    8bef:	e8 bc 7f ff ff       	callq  bb0 <do_anonymous_page.isra.47>
    8bf4:	e9 89 fe ff ff       	jmpq   8a82 <do_copy_on_read+0x172>
 * Remove it when 4level-fixup.h has been removed.
 */
#if defined(CONFIG_MMU) && !defined(__ARCH_HAS_4LEVEL_HACK)
static inline pud_t *pud_alloc(struct mm_struct *mm, pgd_t *pgd, unsigned long address)
{
	return (unlikely(pgd_none(*pgd)) && __pud_alloc(mm, pgd, address))?
    8bf9:	48 89 de             	mov    %rbx,%rsi
    8bfc:	e8 00 00 00 00       	callq  8c01 <do_copy_on_read+0x2f1>
    8c01:	85 c0                	test   %eax,%eax
    8c03:	0f 85 2f ff ff ff    	jne    8b38 <do_copy_on_read+0x228>
    8c09:	48 8b 3b             	mov    (%rbx),%rdi
    8c0c:	e9 41 fd ff ff       	jmpq   8952 <do_copy_on_read+0x42>
		 * Use __pte_alloc instead of pte_alloc_map, because we can't
		 * run pte_offset_map on the pmd, if an huge pmd could
		 * materialize from under us from a different thread.
		 */
		if (unlikely(pmd_none(*pmd)) &&
					unlikely(__pte_alloc(mm, vma, pmd, address)))
    8c11:	4c 89 e1             	mov    %r12,%rcx
    8c14:	4c 89 f6             	mov    %r14,%rsi
    8c17:	4c 89 ef             	mov    %r13,%rdi
    8c1a:	48 89 55 d0          	mov    %rdx,-0x30(%rbp)
    8c1e:	e8 00 00 00 00       	callq  8c23 <do_copy_on_read+0x313>
		/*
		 * Use __pte_alloc instead of pte_alloc_map, because we can't
		 * run pte_offset_map on the pmd, if an huge pmd could
		 * materialize from under us from a different thread.
		 */
		if (unlikely(pmd_none(*pmd)) &&
    8c23:	85 c0                	test   %eax,%eax
    8c25:	0f 85 0d ff ff ff    	jne    8b38 <do_copy_on_read+0x228>
    8c2b:	4c 8b 55 d0          	mov    -0x30(%rbp),%r10
    8c2f:	49 8b 3a             	mov    (%r10),%rdi
    8c32:	e9 aa fd ff ff       	jmpq   89e1 <do_copy_on_read+0xd1>
		NULL: pud_offset(pgd, address);
}

static inline pmd_t *pmd_alloc(struct mm_struct *mm, pud_t *pud, unsigned long address)
{
	return (unlikely(pud_none(*pud)) && __pmd_alloc(mm, pud, address))?
    8c37:	4c 89 e2             	mov    %r12,%rdx
    8c3a:	48 89 de             	mov    %rbx,%rsi
    8c3d:	4c 89 ef             	mov    %r13,%rdi
    8c40:	e8 00 00 00 00       	callq  8c45 <do_copy_on_read+0x335>
    8c45:	85 c0                	test   %eax,%eax
    8c47:	0f 85 eb fe ff ff    	jne    8b38 <do_copy_on_read+0x228>
    8c4d:	48 8b 3b             	mov    (%rbx),%rdi
    8c50:	e9 43 fd ff ff       	jmpq   8998 <do_copy_on_read+0x88>
    8c55:	66 66 2e 0f 1f 84 00 	data32 nopw %cs:0x0(%rax,%rax,1)
    8c5c:	00 00 00 00 

0000000000008c60 <find_vma_by_vfn>:
		pte_unmap_unlock(page_table,ptl);
		//printk("unlock at handle_double_cache_pte\n");
		return ret;
	}
struct vm_area_struct * find_vma_by_vfn(struct mm_struct *mm, unsigned long pfn)
{
    8c60:	e8 00 00 00 00       	callq  8c65 <find_vma_by_vfn+0x5>
    8c65:	55                   	push   %rbp
	struct vm_area_struct *vma = NULL;

	if (mm) {
    8c66:	48 85 ff             	test   %rdi,%rdi
		pte_unmap_unlock(page_table,ptl);
		//printk("unlock at handle_double_cache_pte\n");
		return ret;
	}
struct vm_area_struct * find_vma_by_vfn(struct mm_struct *mm, unsigned long pfn)
{
    8c69:	48 89 e5             	mov    %rsp,%rbp
	struct vm_area_struct *vma = NULL;

	if (mm) {
    8c6c:	74 78                	je     8ce6 <find_vma_by_vfn+0x86>
		vma = ACCESS_ONCE(mm->mmap_cache);
    8c6e:	48 8b 47 10          	mov    0x10(%rdi),%rax
		if (!(vma &&(( vma->vm_end)>>PAGE_SHIFT) > pfn && ((vma->vm_start)>>PAGE_SHIFT) <= pfn)) {
    8c72:	48 85 c0             	test   %rax,%rax
    8c75:	74 0d                	je     8c84 <find_vma_by_vfn+0x24>
    8c77:	48 8b 50 08          	mov    0x8(%rax),%rdx
    8c7b:	48 c1 ea 0c          	shr    $0xc,%rdx
    8c7f:	48 39 f2             	cmp    %rsi,%rdx
    8c82:	77 54                	ja     8cd8 <find_vma_by_vfn+0x78>
			struct rb_node *rb_node;

			rb_node = mm->mm_rb.rb_node;
    8c84:	48 8b 57 08          	mov    0x8(%rdi),%rdx
			vma = NULL;
    8c88:	31 c0                	xor    %eax,%eax
			while (rb_node) {
    8c8a:	48 85 d2             	test   %rdx,%rdx
    8c8d:	75 23                	jne    8cb2 <find_vma_by_vfn+0x52>
    8c8f:	eb 40                	jmp    8cd1 <find_vma_by_vfn+0x71>
    8c91:	0f 1f 80 00 00 00 00 	nopl   0x0(%rax)

				vma_tmp = rb_entry(rb_node,
							struct vm_area_struct, vm_rb); 
				if ((( vma_tmp->vm_end)>>PAGE_SHIFT) > pfn) {
					vma = vma_tmp;
					if (((vma_tmp->vm_start)>>PAGE_SHIFT) <= pfn)
    8c98:	48 8b 4a e0          	mov    -0x20(%rdx),%rcx
			rb_node = mm->mm_rb.rb_node;
			vma = NULL;
			while (rb_node) {
				struct vm_area_struct * vma_tmp;

				vma_tmp = rb_entry(rb_node,
    8c9c:	48 8d 42 e0          	lea    -0x20(%rdx),%rax
							struct vm_area_struct, vm_rb); 
				if ((( vma_tmp->vm_end)>>PAGE_SHIFT) > pfn) {
					vma = vma_tmp;
					if (((vma_tmp->vm_start)>>PAGE_SHIFT) <= pfn)
    8ca0:	48 c1 e9 0c          	shr    $0xc,%rcx
    8ca4:	48 39 ce             	cmp    %rcx,%rsi
    8ca7:	73 24                	jae    8ccd <find_vma_by_vfn+0x6d>
					  break;
					rb_node = rb_node->rb_left;
    8ca9:	48 8b 52 10          	mov    0x10(%rdx),%rdx
		if (!(vma &&(( vma->vm_end)>>PAGE_SHIFT) > pfn && ((vma->vm_start)>>PAGE_SHIFT) <= pfn)) {
			struct rb_node *rb_node;

			rb_node = mm->mm_rb.rb_node;
			vma = NULL;
			while (rb_node) {
    8cad:	48 85 d2             	test   %rdx,%rdx
    8cb0:	74 16                	je     8cc8 <find_vma_by_vfn+0x68>
				struct vm_area_struct * vma_tmp;

				vma_tmp = rb_entry(rb_node,
							struct vm_area_struct, vm_rb); 
				if ((( vma_tmp->vm_end)>>PAGE_SHIFT) > pfn) {
    8cb2:	48 8b 4a e8          	mov    -0x18(%rdx),%rcx
    8cb6:	48 c1 e9 0c          	shr    $0xc,%rcx
    8cba:	48 39 ce             	cmp    %rcx,%rsi
    8cbd:	72 d9                	jb     8c98 <find_vma_by_vfn+0x38>
					vma = vma_tmp;
					if (((vma_tmp->vm_start)>>PAGE_SHIFT) <= pfn)
					  break;
					rb_node = rb_node->rb_left;
				} else
				  rb_node = rb_node->rb_right;
    8cbf:	48 8b 52 08          	mov    0x8(%rdx),%rdx
		if (!(vma &&(( vma->vm_end)>>PAGE_SHIFT) > pfn && ((vma->vm_start)>>PAGE_SHIFT) <= pfn)) {
			struct rb_node *rb_node;

			rb_node = mm->mm_rb.rb_node;
			vma = NULL;
			while (rb_node) {
    8cc3:	48 85 d2             	test   %rdx,%rdx
    8cc6:	75 ea                	jne    8cb2 <find_vma_by_vfn+0x52>
					  break;
					rb_node = rb_node->rb_left;
				} else
				  rb_node = rb_node->rb_right;
			}
			if (vma)
    8cc8:	48 85 c0             	test   %rax,%rax
    8ccb:	74 04                	je     8cd1 <find_vma_by_vfn+0x71>
			  mm->mmap_cache = vma;
    8ccd:	48 89 47 10          	mov    %rax,0x10(%rdi)
		}
	}

	return vma;
}
    8cd1:	5d                   	pop    %rbp
    8cd2:	c3                   	retq   
    8cd3:	0f 1f 44 00 00       	nopl   0x0(%rax,%rax,1)
{
	struct vm_area_struct *vma = NULL;

	if (mm) {
		vma = ACCESS_ONCE(mm->mmap_cache);
		if (!(vma &&(( vma->vm_end)>>PAGE_SHIFT) > pfn && ((vma->vm_start)>>PAGE_SHIFT) <= pfn)) {
    8cd8:	48 8b 10             	mov    (%rax),%rdx
    8cdb:	48 c1 ea 0c          	shr    $0xc,%rdx
    8cdf:	48 39 d6             	cmp    %rdx,%rsi
    8ce2:	72 a0                	jb     8c84 <find_vma_by_vfn+0x24>
			  mm->mmap_cache = vma;
		}
	}

	return vma;
}
    8ce4:	5d                   	pop    %rbp
    8ce5:	c3                   	retq   
		//printk("unlock at handle_double_cache_pte\n");
		return ret;
	}
struct vm_area_struct * find_vma_by_vfn(struct mm_struct *mm, unsigned long pfn)
{
	struct vm_area_struct *vma = NULL;
    8ce6:	31 c0                	xor    %eax,%eax
			  mm->mmap_cache = vma;
		}
	}

	return vma;
}
    8ce8:	5d                   	pop    %rbp
    8ce9:	c3                   	retq   
    8cea:	66 0f 1f 44 00 00    	nopw   0x0(%rax,%rax,1)

0000000000008cf0 <handle_double_cache_mm_fault>:
			}
		}
	}
	return 0;
}
int handle_double_cache_mm_fault(struct mm_struct *mm,struct vm_area_struct* vma,unsigned long address,unsigned int flags){
    8cf0:	e8 00 00 00 00       	callq  8cf5 <handle_double_cache_mm_fault+0x5>
    8cf5:	55                   	push   %rbp
    8cf6:	65 48 8b 04 25 00 00 	mov    %gs:0x0,%rax
    8cfd:	00 00 
    8cff:	48 89 e5             	mov    %rsp,%rbp
    8d02:	41 57                	push   %r15
    8d04:	41 56                	push   %r14
    8d06:	49 89 fe             	mov    %rdi,%r14
    8d09:	41 55                	push   %r13
    8d0b:	49 89 f5             	mov    %rsi,%r13
    8d0e:	41 54                	push   %r12
    8d10:	53                   	push   %rbx
    8d11:	89 cb                	mov    %ecx,%ebx
    8d13:	48 83 ec 10          	sub    $0x10,%rsp
	int ret;
	__set_current_state(TASK_RUNNING);
    8d17:	48 c7 00 00 00 00 00 	movq   $0x0,(%rax)
extern int do_swap_account;
#endif

static inline bool mem_cgroup_disabled(void)
{
	if (mem_cgroup_subsys.disabled)
    8d1e:	8b 05 00 00 00 00    	mov    0x0(%rip),%eax        # 8d24 <handle_double_cache_mm_fault+0x34>
    8d24:	65 48 ff 04 25 00 00 	incq   %gs:0x0
    8d2b:	00 00 
			}
		}
	}
	return 0;
}
int handle_double_cache_mm_fault(struct mm_struct *mm,struct vm_area_struct* vma,unsigned long address,unsigned int flags){
    8d2d:	48 89 55 d0          	mov    %rdx,-0x30(%rbp)
    8d31:	85 c0                	test   %eax,%eax
    8d33:	75 0a                	jne    8d3f <handle_double_cache_mm_fault+0x4f>
static inline void mem_cgroup_count_vm_event(struct mm_struct *mm,
					     enum vm_event_item idx)
{
	if (mem_cgroup_disabled())
		return;
	__mem_cgroup_count_vm_event(mm, idx);
    8d35:	be 0b 00 00 00       	mov    $0xb,%esi
    8d3a:	e8 00 00 00 00       	callq  8d3f <handle_double_cache_mm_fault+0x4f>
    8d3f:	65 4c 8b 3c 25 00 00 	mov    %gs:0x0,%r15
    8d46:	00 00 
	int ret;
	__set_current_state(TASK_RUNNING);
	count_vm_event(PGFAULT);
	mem_cgroup_count_vm_event(mm, PGFAULT);
	/* do counter updates before entering really critical section. */
	check_sync_rss_stat(current);
    8d48:	4c 89 ff             	mov    %r15,%rdi
    8d4b:	e8 d0 87 ff ff       	callq  1520 <check_sync_rss_stat>
	if (flags & FAULT_FLAG_USER)
    8d50:	41 89 d8             	mov    %ebx,%r8d
    8d53:	41 81 e0 80 00 00 00 	and    $0x80,%r8d
    8d5a:	0f 85 80 00 00 00    	jne    8de0 <handle_double_cache_mm_fault+0xf0>
		  mem_cgroup_oom_enable();
		if(vma->vm_flags&VM_ISOLATION){
    8d60:	41 f6 45 54 01       	testb  $0x1,0x54(%r13)
    8d65:	75 59                	jne    8dc0 <handle_double_cache_mm_fault+0xd0>
			ret=do_copy_on_read(mm,vma,address,flags);
		}else{
		//	ret=manage_cacheability(mm,vma,address,flags);
		}
		if (flags & FAULT_FLAG_USER) {
    8d67:	45 85 c0             	test   %r8d,%r8d
    8d6a:	75 14                	jne    8d80 <handle_double_cache_mm_fault+0x90>
			 */
			if (task_in_memcg_oom(current) && !(ret & VM_FAULT_OOM))
			  mem_cgroup_oom_synchronize(false);
		}
		return ret;
	}
    8d6c:	48 83 c4 10          	add    $0x10,%rsp
    8d70:	44 89 e0             	mov    %r12d,%eax
    8d73:	5b                   	pop    %rbx
    8d74:	41 5c                	pop    %r12
    8d76:	41 5d                	pop    %r13
    8d78:	41 5e                	pop    %r14
    8d7a:	41 5f                	pop    %r15
    8d7c:	5d                   	pop    %rbp
    8d7d:	c3                   	retq   
    8d7e:	66 90                	xchg   %ax,%ax
    8d80:	65 48 8b 04 25 00 00 	mov    %gs:0x0,%rax
    8d87:	00 00 
	current->memcg_oom.may_oom = 1;
}

static inline void mem_cgroup_oom_disable(void)
{
	WARN_ON(!current->memcg_oom.may_oom);
    8d89:	f6 80 38 18 00 00 01 	testb  $0x1,0x1838(%rax)
    8d90:	74 6d                	je     8dff <handle_double_cache_mm_fault+0x10f>
    8d92:	65 48 8b 04 25 00 00 	mov    %gs:0x0,%rax
    8d99:	00 00 
	current->memcg_oom.may_oom = 0;
    8d9b:	80 a0 38 18 00 00 fe 	andb   $0xfe,0x1838(%rax)
			 * The task may have entered a memcg OOM situation but
			 * if the allocation error was handled gracefully (no
			 * VM_FAULT_OOM), there is no need to kill anything.
			 * Just clean up the OOM state peacefully.
			 */
			if (task_in_memcg_oom(current) && !(ret & VM_FAULT_OOM))
    8da2:	48 83 b8 28 18 00 00 	cmpq   $0x0,0x1828(%rax)
    8da9:	00 
    8daa:	74 c0                	je     8d6c <handle_double_cache_mm_fault+0x7c>
    8dac:	41 f6 c4 01          	test   $0x1,%r12b
    8db0:	75 ba                	jne    8d6c <handle_double_cache_mm_fault+0x7c>
			  mem_cgroup_oom_synchronize(false);
    8db2:	31 ff                	xor    %edi,%edi
    8db4:	e8 00 00 00 00       	callq  8db9 <handle_double_cache_mm_fault+0xc9>
    8db9:	eb b1                	jmp    8d6c <handle_double_cache_mm_fault+0x7c>
    8dbb:	0f 1f 44 00 00       	nopl   0x0(%rax,%rax,1)
	/* do counter updates before entering really critical section. */
	check_sync_rss_stat(current);
	if (flags & FAULT_FLAG_USER)
		  mem_cgroup_oom_enable();
		if(vma->vm_flags&VM_ISOLATION){
			ret=do_copy_on_read(mm,vma,address,flags);
    8dc0:	48 8b 55 d0          	mov    -0x30(%rbp),%rdx
    8dc4:	89 d9                	mov    %ebx,%ecx
    8dc6:	4c 89 ee             	mov    %r13,%rsi
    8dc9:	4c 89 f7             	mov    %r14,%rdi
    8dcc:	44 89 45 cc          	mov    %r8d,-0x34(%rbp)
    8dd0:	e8 00 00 00 00       	callq  8dd5 <handle_double_cache_mm_fault+0xe5>
    8dd5:	44 8b 45 cc          	mov    -0x34(%rbp),%r8d
    8dd9:	41 89 c4             	mov    %eax,%r12d
    8ddc:	eb 89                	jmp    8d67 <handle_double_cache_mm_fault+0x77>
    8dde:	66 90                	xchg   %ax,%ax
extern void mem_cgroup_replace_page_cache(struct page *oldpage,
					struct page *newpage);

static inline void mem_cgroup_oom_enable(void)
{
	WARN_ON(current->memcg_oom.may_oom);
    8de0:	41 f6 87 38 18 00 00 	testb  $0x1,0x1838(%r15)
    8de7:	01 
    8de8:	75 28                	jne    8e12 <handle_double_cache_mm_fault+0x122>
    8dea:	65 48 8b 04 25 00 00 	mov    %gs:0x0,%rax
    8df1:	00 00 
	current->memcg_oom.may_oom = 1;
    8df3:	80 88 38 18 00 00 01 	orb    $0x1,0x1838(%rax)
    8dfa:	e9 61 ff ff ff       	jmpq   8d60 <handle_double_cache_mm_fault+0x70>
}

static inline void mem_cgroup_oom_disable(void)
{
	WARN_ON(!current->memcg_oom.may_oom);
    8dff:	be 94 00 00 00       	mov    $0x94,%esi
    8e04:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
    8e0b:	e8 00 00 00 00       	callq  8e10 <handle_double_cache_mm_fault+0x120>
    8e10:	eb 80                	jmp    8d92 <handle_double_cache_mm_fault+0xa2>
extern void mem_cgroup_replace_page_cache(struct page *oldpage,
					struct page *newpage);

static inline void mem_cgroup_oom_enable(void)
{
	WARN_ON(current->memcg_oom.may_oom);
    8e12:	be 8e 00 00 00       	mov    $0x8e,%esi
    8e17:	48 c7 c7 00 00 00 00 	mov    $0x0,%rdi
    8e1e:	44 89 45 cc          	mov    %r8d,-0x34(%rbp)
    8e22:	e8 00 00 00 00       	callq  8e27 <handle_double_cache_mm_fault+0x137>
    8e27:	44 8b 45 cc          	mov    -0x34(%rbp),%r8d
    8e2b:	eb bd                	jmp    8dea <handle_double_cache_mm_fault+0xfa>

Disassembly of section .init.text:

0000000000000000 <disable_randmaps>:
#else
2;
#endif

static int __init disable_randmaps(char *s)
{
   0:	55                   	push   %rbp
	randomize_va_space = 0;
	return 1;
}
   1:	b8 01 00 00 00       	mov    $0x1,%eax
2;
#endif

static int __init disable_randmaps(char *s)
{
	randomize_va_space = 0;
   6:	c7 05 00 00 00 00 00 	movl   $0x0,0x0(%rip)        # 10 <disable_randmaps+0x10>
   d:	00 00 00 
#else
2;
#endif

static int __init disable_randmaps(char *s)
{
  10:	48 89 e5             	mov    %rsp,%rbp
	randomize_va_space = 0;
	return 1;
}
  13:	5d                   	pop    %rbp
  14:	c3                   	retq   

0000000000000015 <init_zero_pfn>:
extern unsigned long max_pfn;
extern unsigned long phys_base;

static inline unsigned long __phys_addr_nodebug(unsigned long x)
{
	unsigned long y = x - __START_KERNEL_map;
  15:	48 ba 00 00 00 00 00 	movabs $0x0,%rdx
  1c:	00 00 00 

	/* use the carry flag to determine if x was < __START_KERNEL_map */
	x = y + ((x > y) ? phys_base : (__START_KERNEL_map - PAGE_OFFSET));
  1f:	48 b8 00 00 00 80 ff 	movabs $0x77ff80000000,%rax
  26:	77 00 00 

/*
 * CONFIG_MMU architectures set up ZERO_PAGE in their paging_init()
 */
static int __init init_zero_pfn(void)
{
  29:	55                   	push   %rbp
  2a:	48 81 fa 00 00 00 00 	cmp    $0x0,%rdx
  31:	48 0f 42 05 00 00 00 	cmovb  0x0(%rip),%rax        # 39 <init_zero_pfn+0x24>
  38:	00 
  39:	48 89 e5             	mov    %rsp,%rbp
	zero_pfn = page_to_pfn(ZERO_PAGE(0));
	return 0;
}
  3c:	5d                   	pop    %rbp
  3d:	48 01 d0             	add    %rdx,%rax
/*
 * CONFIG_MMU architectures set up ZERO_PAGE in their paging_init()
 */
static int __init init_zero_pfn(void)
{
	zero_pfn = page_to_pfn(ZERO_PAGE(0));
  40:	48 c1 e8 0c          	shr    $0xc,%rax
  44:	48 89 05 00 00 00 00 	mov    %rax,0x0(%rip)        # 4b <init_zero_pfn+0x36>
	return 0;
}
  4b:	31 c0                	xor    %eax,%eax
  4d:	c3                   	retq   

Disassembly of section .text.unlikely:

0000000000000000 <pte_to_swp_entry.part.35>:

	ret.val = (type << SWP_TYPE_SHIFT(ret)) |
			(offset & SWP_OFFSET_MASK(ret));
	return ret;
}
static inline swp_entry_t pte_to_swp_entry(pte_t pte)
   0:	55                   	push   %rbp
   1:	48 89 e5             	mov    %rsp,%rbp
{
	swp_entry_t arch_entry;
	BUG_ON(pte_file(pte));
   4:	0f 0b                	ud2    

0000000000000006 <migration_entry_to_page.part.36>:
static inline int is_write_migration_entry(swp_entry_t entry)
{
	return unlikely(swp_type(entry) == SWP_MIGRATION_WRITE);
}

static inline struct page *migration_entry_to_page(swp_entry_t entry)
   6:	55                   	push   %rbp
   7:	48 89 e5             	mov    %rsp,%rbp
	struct page *p = pfn_to_page(swp_offset(entry));
	/*
	 * Any use of migration entries may only occur while the
	 * corresponding page is locked
	 */
	BUG_ON(!PageLocked(p));
   a:	0f 0b                	ud2    

000000000000000c <cow_user_page.isra.49.part.50>:
#endif
	pte_unmap(page_table);
	return same;
}

static inline void cow_user_page(struct page *dst, struct page *src, unsigned long va, struct vm_area_struct *vma)
   c:	55                   	push   %rbp
 * The various preempt_count add/sub methods
 */

static __always_inline void __preempt_count_add(int val)
{
	__this_cpu_add_4(__preempt_count, val);
   d:	65 ff 04 25 00 00 00 	incl   %gs:0x0
  14:	00 
  15:	48 89 e5             	mov    %rsp,%rbp
  18:	53                   	push   %rbx
 */
#include <linux/vmstat.h>

static __always_inline void *lowmem_page_address(const struct page *page)
{
	return __va(PFN_PHYS(page_to_pfn(page)));
  19:	48 b8 00 00 00 00 00 	movabs $0x160000000000,%rax
  20:	16 00 00 
  23:	48 bb 00 00 00 00 00 	movabs $0xffff880000000000,%rbx
  2a:	88 ff ff 
	 * just copying from the original user address. If that
	 * fails, we just zero-fill it. Live with it.
	 */
	if (unlikely(!src)) {
		void *kaddr = kmap_atomic(dst);
		void __user *uaddr = (void __user *)(va & PAGE_MASK);
  2d:	48 81 e6 00 f0 ff ff 	and    $0xfffffffffffff000,%rsi
  34:	48 01 c7             	add    %rax,%rdi
	/*
	 * If CPU has ERMS feature, use copy_user_enhanced_fast_string.
	 * Otherwise, if CPU has rep_good feature, use copy_user_generic_string.
	 * Otherwise, use copy_user_generic_unrolled.
	 */
	alternative_call_2(copy_user_generic_unrolled,
  37:	ba 00 10 00 00       	mov    $0x1000,%edx
  3c:	48 c1 ff 06          	sar    $0x6,%rdi
  40:	48 c1 e7 0c          	shl    $0xc,%rdi
  44:	48 01 fb             	add    %rdi,%rbx
  47:	48 89 df             	mov    %rbx,%rdi
  4a:	e8 00 00 00 00       	callq  4f <cow_user_page.isra.49.part.50+0x43>
		 * This really shouldn't fail, because the page is there
		 * in the page tables. But it might just be unreadable,
		 * in which case we just give up and fill the result with
		 * zeroes.
		 */
		if (__copy_from_user_inatomic(kaddr, uaddr, PAGE_SIZE))
  4f:	85 c0                	test   %eax,%eax
  51:	74 08                	je     5b <cow_user_page.isra.49.part.50+0x4f>
		  clear_page(kaddr);
  53:	48 89 df             	mov    %rbx,%rdi
  56:	e8 00 00 00 00       	callq  5b <cow_user_page.isra.49.part.50+0x4f>
		kunmap_atomic(kaddr);
		flush_dcache_page(dst);
	} else{
		copy_user_highpage(dst, src, va, vma);
	}
}
  5b:	5b                   	pop    %rbx
}

static __always_inline void __preempt_count_sub(int val)
{
	__this_cpu_add_4(__preempt_count, -val);
  5c:	65 ff 0c 25 00 00 00 	decl   %gs:0x0
  63:	00 
  64:	5d                   	pop    %rbp
  65:	c3                   	retq   

Disassembly of section .altinstr_replacement:

0000000000000000 <.altinstr_replacement>:
   0:	e8 00 00 00 00       	callq  5 <.altinstr_replacement+0x5>
   5:	e8 00 00 00 00       	callq  a <zero_pfn+0x2>
